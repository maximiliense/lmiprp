
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>L’Analyse en Composantes Principales ☕️☕️☕️ &#8212; Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Modèle de Mélange Gaussien et algorithme Expectation-Maximization" href="2_em_gaussin_mixture_model.html" />
    <link rel="prev" title="L’apprentissage non-supervisé" href="0_propos_liminaire.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-72WWYCKNK6"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-72WWYCKNK6');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../0_requisite/rappels_python.html">
   Rappels de Python et Numpy
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../1_what_is_ml/0_propos_liminaire.html">
   <em>
    Machine Learning
   </em>
   , initiation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/1_introduction_ml.html">
     <em>
      Machine learning
     </em>
     et malédiction de la dimension
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/2_regression_and_classification_trees.html">
     Les arbres de régression et de classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../2_regression/0_propos_liminaire.html">
   La
   <em>
    régression
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/1_linear_regression.html">
     La régression linéaire ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/2_optimization.html">
     L’optimisation ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/3_interpolation.html">
     Interpolation ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/4_algo_proximal_lasso.html">
     Sous-différentiel et le cas du Lasso ☕️☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/5_least_square_qr.html">
     Les moindres carrés via une décomposition QR (et plus)☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/6_ridge.html">
     Une analyse de la régularisation Ridge ☕️☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../3_classification/0_propos_liminaire.html">
   La
   <em>
    classification
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/1_logistic_regression.html">
     La régression logistique ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/2_fonctions_proxy.html">
     Les fonctions de perte (loss function) ☕️☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/3_bayes_classifier.html">
     Le classifieur de Bayes ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/4_VC_theory.html">
     Un modèle formel de l’apprentissage ☕️☕️☕️☕️ (💆‍♂️)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../4_kernel_methods/0_propos_liminaire.html">
   Les méthodes à noyaux
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../4_kernel_methods/1_svm.html">
     Le SVM ou l’hypothèse max-margin ☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5_ensembles/0_propos_liminaire.html">
   Les méthodes
   <em>
    ensemblistes
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/1_ensembles.html">
     Méthodes ensemblistes ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/2_bayesian_linear_regression.html">
     Bayesian linear regression ☕️☕️☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../6_deeplearning/0_propos_liminaire.html">
   <em>
    deep learning
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/1_autodiff.html">
     La différentiation automatique et un début de
     <em>
      deep learning
     </em>
     ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/2_filters_representation.html">
     Filtres et espace de représentation des réseaux de neurones ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/3_probabilities_calibration.html">
     Calibration des probabilités et quelques notions ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/4_regularization_deep.html">
     Régularisation en
     <em>
      deep learning
     </em>
     ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/5_transfer_multitask.html">
     <em>
      Transfer learning
     </em>
     et apprentissage multi-tâches ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/6_adversarial.html">
     Les attaques adversaires ☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="0_propos_liminaire.html">
   L’apprentissage non-supervisé
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     L’Analyse en Composantes Principales ☕️☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="2_em_gaussin_mixture_model.html">
     Modèle de Mélange Gaussien et algorithme
     <em>
      Expectation-Maximization
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../8_set_prediction/0_propos_liminaire.html">
   Prédiction d’ensembles
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/1_well_defined.html">
     Jeu d’apprentissage
     <em>
      set-valued
     </em>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/2_ill_defined.html">
     Jeu d’apprentissage uniquement multi-classes ☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../biblio.html">
   Bibliographie
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/7_unsupervised/1_principal_component_analysis.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/maximiliense/lmiprp"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/maximiliense/lmiprp/issues/new?title=Issue%20on%20page%20%2F7_unsupervised/1_principal_component_analysis.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/maximiliense/lmiprp/blob/master/Travaux Pratiques/Machine Learning/Book/7_unsupervised/1_principal_component_analysis.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#i-introduction">
   I. Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ii-quelques-rappels-d-algebre-lineaire">
   II. Quelques rappels d’algèbre linéaire
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-decomposition-en-valeurs-et-vecteurs-propres">
     A. Décomposition en valeurs et vecteurs propres
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-composition-d-endormorphismes-autoadjoints-symetriques">
     B. Composition d’endormorphismes autoadjoints (symétriques)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iii-lien-entre-directions-de-variance-maximale-et-vecteurs-propres">
   III. Lien entre directions de variance maximale et vecteurs propres
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iv-calculer-les-vecteurs-et-valeurs-propres">
   IV. Calculer les vecteurs et valeurs propres
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-l-algorithme-des-puissances-iterees">
     A. L’algorithme des puissances itérées
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-en-pratique">
     B. En pratique
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#v-calcul-des-vecteurs-de-representation-codage-et-reconstructrion-decodage">
   V. Calcul des vecteurs de représentation (codage) et reconstructrion (décodage)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-compression">
     A. Compression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-interpretation-de-l-erreur-de-reconstruction-moyenne">
     B. Interprétation de l’erreur de reconstruction moyenne
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#v-compression-d-une-base-d-images">
   V. Compression d’une base d’images
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-les-donnees-et-notre-acp">
     A. Les données et notre ACP
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-affichage-des-variances-cumullees-et-calcul-du-seuil-de-reconstruction">
     B. Affichage des variances cumullées et calcul du seuil de reconstruction
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#c-affichage-des-vecteur-propres-les-eigen-faces">
     C. Affichage des vecteur propres (les eigen faces)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#d-visualisation-perceptuelle-de-la-qualite-de-reconstruction">
     D. Visualisation perceptuelle de la qualité de reconstruction
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#e-essayons-les-memes-etapes-sur-mnist">
     E. Essayons les mêmes étapes sur mnist
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="l-analyse-en-composantes-principales">
<h1>L’Analyse en Composantes Principales ☕️☕️☕️<a class="headerlink" href="#l-analyse-en-composantes-principales" title="Permalink to this headline">¶</a></h1>
<div class="admonition-objectifs-de-la-sequence admonition">
<p class="admonition-title">Objectifs de la séquence</p>
<ul class="simple">
<li><p>Être sensibilisé :</p>
<ul>
<li><p>aux enjeux mathématiques de l’ACP.</p></li>
</ul>
</li>
<li><p>Être capable :</p>
<ul>
<li><p>d’implémenter une ACP avec <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span>.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="i-introduction">
<h2>I. Introduction<a class="headerlink" href="#i-introduction" title="Permalink to this headline">¶</a></h2>
<p>L’Analyse en Composante Principale (ACP) consiste à extraire des directions dans lesquelles les données s’étalent particulièrement (i.e. la variance y est maximale). Prenons un exemple. Nous disposons d’un jeu de données dont on aimerait extraire l’information la plus “representative” (dans un sens que nous allons expliciter plus loin). Nous choisissons ici un jeu de données tiré selon une loi gaussienne multivariée <span class="math notranslate nohighlight">\(\boldsymbol{x} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{\Sigma})\)</span> avec <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma} \in \mathcal{S}^+(\mathbb{R}^d)\)</span> (i.e. matrice semi-définie positive de dimension <span class="math notranslate nohighlight">\(d\times d\)</span>) la matrice de variance/co-variance des données. Affichons le jeu de données.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">12.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X_rec</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vec</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">circles</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;Dimension must be equal to 2 to plot stuffs.&quot;</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">edgecolors</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">facecolors</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
    <span class="n">x1min_</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="n">eps</span>
    <span class="n">x1max_</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">eps</span>
    <span class="n">x0min_</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="n">eps</span>
    <span class="n">x0max_</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="n">eps</span>
    <span class="k">if</span> <span class="n">vec</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span>
            <span class="n">mean</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mean</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">vec</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">vec</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> 
            <span class="n">head_width</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> 
            <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span>
        <span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span>
            <span class="n">mean</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mean</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">vec</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">vec</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> 
            <span class="n">head_width</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> 
            <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">X_rec</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_rec</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">X_rec</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">facecolors</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
                <span class="p">[</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_rec</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">]],</span> <span class="p">[</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">X_rec</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">1</span><span class="p">]],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> 
                <span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="s1">&#39;0.5&#39;</span>
            <span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="n">x0min_</span><span class="p">,</span> <span class="n">x0max_</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="n">x1min_</span><span class="p">,</span> <span class="n">x1max_</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">random_rotation</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
    <span class="n">Q</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">d</span><span class="p">,</span><span class="n">d</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">Q</span>

<span class="k">def</span> <span class="nf">sample_multivariate_gaussian_data</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">variance_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">variance_decay</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">)])</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">random_rotation</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
    <span class="n">sigma_r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">sigma</span> <span class="p">),</span> <span class="n">Q</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">d</span><span class="p">),</span> <span class="n">sigma_r</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">sample_multivariate_gaussian_data</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(500, 2)
</pre></div>
</div>
<img alt="../_images/1_principal_component_analysis_4_1.png" src="../_images/1_principal_component_analysis_4_1.png" />
</div>
</div>
<div class="admonition-question admonition">
<p class="admonition-title">Question</p>
<p><strong>Dans le code ci-dessus, à quoi sert à la décomposition QR ?</strong></p>
</div>
<p>Dans ce cas précis, chaque point est représenté par deux nombres. Que pourrions-nous faire pour ne représenter chaque point par seulement un nombre en perdant le moins d’information possible sur le signal d’origine ? Nous pourrions ainsi chercher à trouver la direction dans l’espace des données telle que les données soient le plus “étalées”. Plus formellement on cherche un vecteur unitaire <span class="math notranslate nohighlight">\(\boldsymbol{v} \in \mathbb{R}^2\)</span> tel que les projections des données sur <span class="math notranslate nohighlight">\(z_i = \langle \boldsymbol{v}, \boldsymbol{x}_i \rangle\)</span> constituent un échantillon transformé dont la variance empirique soit maximale. Il s’agit de résoudre le problème d’optimisation suivant :</p>
<div class="math notranslate nohighlight">
\[\underset{\boldsymbol{v}}{argmax}\Bigg[\frac{1}{N} \sum_{i=1}^N  (z_i - \bar{z})^2 \Bigg] = \underset{\boldsymbol{v}}{argmax} \Big[ \boldsymbol{v}^t\bar{\boldsymbol{X}}^T\bar{\boldsymbol{X}}\boldsymbol{v} \Big]\]</div>
<p>sous contrainte que <span class="math notranslate nohighlight">\(\lVert\boldsymbol{v}\rVert_2 = 1\)</span> et avec <span class="math notranslate nohighlight">\(\bar{z} = \frac{1}{N}\sum_i z_i\)</span> et <span class="math notranslate nohighlight">\(\bar{\boldsymbol{X}} \in \mathbb{R}^{n\times d}\)</span>, la matrice de <em>design</em> (i.e. qui contient nos données) centrée.</p>
<p>Sans la contrainte <span class="math notranslate nohighlight">\(\lVert\boldsymbol{v}\rVert_2 = 1\)</span>, le problème d’optimisation serait mal posé car la solution ne serait pas unique (il suffirait simplement d’augmenter arbitrairement la norme de n’importe quel vecteur pour avoir une valeur de la fonction objectif arbitrairement grande).</p>
<p>Nous allons montrer que ce problème d’optimisation est strictement équivalent a celui de trouver le vecteur propre de la matrice de covariance <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma} = \bar{\boldsymbol{X}}^T\bar{\boldsymbol{X}}\)</span> correspondant à la valeur propre maximale <span class="math notranslate nohighlight">\(\lambda_{\max}\)</span>.</p>
<div class="admonition-question admonition">
<p class="admonition-title">Question</p>
<p><strong>Montrer l’égalité suivante :</strong></p>
<div class="math notranslate nohighlight">
\[\frac{1}{N} \sum_{i=1}^N  (z_i - \bar{z})^2=\boldsymbol{v}^t\bar{\boldsymbol{X}}^T\bar{\boldsymbol{X}}\boldsymbol{v}\]</div>
</div>
</div>
<div class="section" id="ii-quelques-rappels-d-algebre-lineaire">
<h2>II. Quelques rappels d’algèbre linéaire<a class="headerlink" href="#ii-quelques-rappels-d-algebre-lineaire" title="Permalink to this headline">¶</a></h2>
<div class="section" id="a-decomposition-en-valeurs-et-vecteurs-propres">
<h3>A. Décomposition en valeurs et vecteurs propres<a class="headerlink" href="#a-decomposition-en-valeurs-et-vecteurs-propres" title="Permalink to this headline">¶</a></h3>
<p><strong>Qu’est ce qu’un vecteur propre ?</strong>
Algébriquement, un vecteur propre d’une matrice <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> est un vecteur tel que :</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
    \boldsymbol{A}\boldsymbol{v} = \lambda \boldsymbol{v}
\end{equation*}\]</div>
<p>L’interprétation géométrique est donc qu’il s’agit d’un vecteur dont la direction de son image par l’application linéaire associée à la matrice <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> n’est qu’une <a class="reference external" href="https://fr.wikipedia.org/wiki/Homoth%C3%A9tie">homothétie</a> (i.e. la direction est inchangée et le vecteur n’est qu’étiré). Il est simplement étiré d’un facteur <span class="math notranslate nohighlight">\(\lambda\)</span> qu’on appel sa valeur propre associée.</p>
<p><img alt="Vecteur propre" src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/58/Eigenvalue_equation.svg/2880px-Eigenvalue_equation.svg.png" /></p>
<p><strong>Diagonalisation d’une matrice carré</strong>
Une matrice carrée peut être vue comme un endomorphisme allant d’un espace vectoriel <span class="math notranslate nohighlight">\(E\)</span> vers lui même. On dira que cet endomorphisme est diagonalisable s’il existe une matrice <span class="math notranslate nohighlight">\(\boldsymbol{V}\)</span> inversible et une matrice diagonale <span class="math notranslate nohighlight">\(\boldsymbol{\Lambda}\)</span> telles que:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{A} = \boldsymbol{V} \boldsymbol{\Lambda} \boldsymbol{V}^{-1}\]</div>
<p>où <span class="math notranslate nohighlight">\(\boldsymbol{V}\)</span> est une matrice dont les colonnes forment une base de <span class="math notranslate nohighlight">\(E\)</span> et dont les éléments sont les vecteurs propres et <span class="math notranslate nohighlight">\(\boldsymbol{\Lambda}\)</span> est une matrice dont les éléments diagonaux correspondent aux valeurs propres associées.</p>
<div class="admonition-matrices-semblables admonition">
<p class="admonition-title">Matrices semblables</p>
<p>Deux matrices <span class="math notranslate nohighlight">\(A\)</span> et <span class="math notranslate nohighlight">\(B\)</span> sont dites semblables s’il existe une matrice inversible <span class="math notranslate nohighlight">\(V\)</span> telle que :</p>
<div class="math notranslate nohighlight">
\[A=VBV^{-1}.\]</div>
</div>
<p>De plus, on peut montrer que si la matrice <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> est symmétrique (<span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> = <span class="math notranslate nohighlight">\(\boldsymbol{A}^t\)</span>) à coefficients dans <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> (ces deux propriétés sont vérifiées par notre matrice de variance-covariance <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>), la matrice de passage <span class="math notranslate nohighlight">\(\boldsymbol{V}\)</span> est nécéssairement une matrice orthogonale (<span class="math notranslate nohighlight">\(\boldsymbol{V}^{-1} = \boldsymbol{V}^t\)</span>) et <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> prend alors la forme particulière suivante:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\boldsymbol{A} = \boldsymbol{V} \boldsymbol{\Lambda} \boldsymbol{V}^t = \begin{bmatrix}
    \vert &amp;   &amp; \vert &amp;   &amp;  \vert \\
    \vert &amp;   &amp; \vert &amp;   &amp;  \vert \\
    \vert &amp;   &amp; \vert &amp;  &amp;  \vert \\
    \boldsymbol{v_1}   &amp; \dots &amp; \boldsymbol{v_i} &amp; \dots &amp; \boldsymbol{v_d}   \\
    \vert &amp;   &amp; \vert &amp;   &amp;  \vert \\
    \vert &amp;  &amp; \vert &amp;  &amp; \vert 
\end{bmatrix}
\begin{bmatrix}
    \lambda_1 &amp; 0 &amp; \dots &amp; \dots  &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots &amp; \vdots &amp; \vdots\\
    0 &amp; \dots &amp; \lambda_i &amp; \dots &amp; 0\\
     \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
    0 &amp; \dots &amp; \dots &amp; 0 &amp;\lambda_d\\
\end{bmatrix}
\begin{bmatrix}
    \text{---} &amp; \boldsymbol{v_1} &amp; \text{---} \\
      &amp; \vdots &amp;  \\
    \text{---} &amp; \boldsymbol{v_i} &amp; \text{---} \\
     &amp; \vdots &amp;  \\
    \text{---} &amp; \boldsymbol{v_d} &amp; \text{---}
\end{bmatrix}
\end{aligned}\end{split}\]</div>
<p>où <span class="math notranslate nohighlight">\(\boldsymbol{V}\)</span> correspond cette fois à une matrice orthogonale i.e. :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\boldsymbol{V}^t\boldsymbol{V} &amp;= \begin{bmatrix}
    \vert &amp;    &amp;  \vert \\
    \boldsymbol{v_1}   &amp;  \dots &amp; \boldsymbol{v_d}   \\
    \vert &amp;   &amp; \vert 
\end{bmatrix}
\begin{bmatrix}
    \text{---} &amp; \boldsymbol{v_1} &amp; \text{---} \\
     &amp; \vdots &amp;  \\
    \text{---} &amp; \boldsymbol{v_d} &amp; \text{---}
\end{bmatrix}\\
&amp;= \begin{bmatrix}
    \langle \boldsymbol{v_1}, \boldsymbol{v_1} \rangle &amp; \dots &amp; \langle \boldsymbol{v_1}, \boldsymbol{v_d} \rangle \\
    \vdots &amp; \ddots &amp; \vdots \\
      \langle \boldsymbol{v_d}, \boldsymbol{v_1} \rangle &amp; \dots &amp; \langle \boldsymbol{v_d}, \boldsymbol{v_d} \rangle
\end{bmatrix}\\
&amp;= \begin{bmatrix}
    ||\boldsymbol{v_1}||_2^2=1 &amp; \dots &amp; 0 \\
    0 &amp; \ddots &amp; 0 \\
      0 &amp; \dots &amp; ||\boldsymbol{v_d}||_2^2=1
\end{bmatrix}
\end{aligned}\end{split}\]</div>
<p>les vecteurs propres sont donc tous orthogonaux 2 à 2 et de norme <span class="math notranslate nohighlight">\(1\)</span> et forment donc une base orthonormale.</p>
<div class="margin sidebar">
<p class="sidebar-title">Matrice orthogonale</p>
<p>Les matrices orthogonales (i.e. <span class="math notranslate nohighlight">\(\boldsymbol{Q}^{-1}=\boldsymbol{Q}^T\)</span> ou <span class="math notranslate nohighlight">\(\boldsymbol{Q}^T\boldsymbol{Q}=\boldsymbol{Q}\boldsymbol{Q}^T=I\)</span>) sont typiquement les rotations, réflections ou roto-réflection.</p>
</div>
<p>C’est ce que nous souhaitons pour notre problème. Nous voulons trouver une rotation de sorte à ce que dans la nouvelle base, qu’on appellera composantes, la variance de nos données soit le plus parfaitement décrites par les dites composantes. Nous pouvons donc reformuler de manière plus générale notre problème pour n’importe quel dimension d’entrée <span class="math notranslate nohighlight">\(d\)</span> comme le problème d’optimisation sous contrainte suivant :</p>
<div class="math notranslate nohighlight">
\[\underset{\boldsymbol{V}}{argmax}\Bigg[\frac{1}{N} \sum_{k=1}^d\sum_{i=1}^N  (z_i^k - \bar{z}^k)^2 \Bigg] = \underset{\boldsymbol{V}}{argmax} \Big[ \boldsymbol{V}^t\bar{\boldsymbol{X}}^T\bar{\boldsymbol{X}}\boldsymbol{V} \Big],\text{ s.t. }\boldsymbol{V}^t\boldsymbol{V} = \boldsymbol{I_d}\]</div>
<p>Et nous montrerons que la solution de ce problème consiste à trouver la base des vecteurs propres de <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma} = \bar{\boldsymbol{X}}^T\bar{\boldsymbol{X}}\)</span>.</p>
</div>
<hr class="docutils" />
<div class="section" id="b-composition-d-endormorphismes-autoadjoints-symetriques">
<h3>B. Composition d’endormorphismes autoadjoints (symétriques)<a class="headerlink" href="#b-composition-d-endormorphismes-autoadjoints-symetriques" title="Permalink to this headline">¶</a></h3>
<p>La diagonalisation d’un endomorphisme permet de simplifier certains calculs. Nous restons ici dans le cas symétrique mais une partie du propos se généralise bien sûr au cas diagonalisable quelconque. Nous avons ainsi l’égalité suivante :</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{A}^2 = \boldsymbol{V}\boldsymbol{\Lambda}\underbrace{\boldsymbol{V}^t\boldsymbol{V}}_{\boldsymbol{I}}\boldsymbol{\Lambda}\boldsymbol{V}^t = \boldsymbol{V}\boldsymbol{\Lambda}^2\boldsymbol{V}^t\]</div>
<p>et de manière générale :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\boldsymbol{A}^n = \boldsymbol{V} \boldsymbol{\Lambda}^n \boldsymbol{V}^t = \begin{bmatrix}
    \vert &amp;   &amp; \vert &amp;   &amp;  \vert \\
    \vert &amp;   &amp; \vert &amp;   &amp;  \vert \\
    \vert &amp;   &amp; \vert &amp;  &amp;  \vert \\
    \boldsymbol{v_1}   &amp; \dots &amp; \boldsymbol{v_i} &amp; \dots &amp; \boldsymbol{v_d}   \\
    \vert &amp;   &amp; \vert &amp;   &amp;  \vert \\
    \vert &amp;  &amp; \vert &amp;  &amp; \vert 
\end{bmatrix}
\begin{bmatrix}
    (\lambda_1)^n &amp; 0 &amp; \dots &amp; \dots  &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots &amp; \vdots &amp; \vdots\\
    0 &amp; \dots &amp; (\lambda_i)^n &amp; \dots &amp; 0\\
     \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
    0 &amp; \dots &amp; \dots &amp; 0 &amp;(\lambda_d)^n\\
\end{bmatrix}
\begin{bmatrix}
    \text{---} &amp; \boldsymbol{v_1} &amp; \text{---} \\
      &amp; \vdots &amp;  \\
    \text{---} &amp; \boldsymbol{v_i} &amp; \text{---} \\
     &amp; \vdots &amp;  \\
    \text{---} &amp; \boldsymbol{v_d} &amp; \text{---}
\end{bmatrix}
\end{aligned}\end{split}\]</div>
<p>On remarque ainsi que si <span class="math notranslate nohighlight">\(A\)</span> est symétrique, alors <span class="math notranslate nohighlight">\(A^n\)</span> possède la même matrice de changement de base et ses valeurs propres sont les mêmes à la puissancec <span class="math notranslate nohighlight">\(n\)</span>.</p>
<p>Notez que n’importe quel vecteur <span class="math notranslate nohighlight">\(\boldsymbol{x} \in \mathbb{R}^d\)</span> peut se décomposer dans la base des vecteurs propres :</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{x} = \sum_{k=1}^d \underbrace{\langle \boldsymbol{v_k}, \boldsymbol{x} \rangle}_{z_k} \boldsymbol{v_k}\]</div>
<hr class="docutils" />
<div class="admonition-question admonition">
<p class="admonition-title">Question</p>
<p><strong>Montrez que <span class="math notranslate nohighlight">\(\boldsymbol{A}\boldsymbol{x} =  \sum_{k=1}^d \lambda_k  z_k \boldsymbol{v_k}\)</span>.</strong></p>
</div>
<div class="admonition-question admonition">
<p class="admonition-title">Question</p>
<p><strong>En utilisant la propriété de la mise à la puissance, donnez l’expression de <span class="math notranslate nohighlight">\(\boldsymbol{A}^n\boldsymbol{x}\)</span>.</strong></p>
</div>
</div>
</div>
<div class="section" id="iii-lien-entre-directions-de-variance-maximale-et-vecteurs-propres">
<h2>III. Lien entre directions de variance maximale et vecteurs propres<a class="headerlink" href="#iii-lien-entre-directions-de-variance-maximale-et-vecteurs-propres" title="Permalink to this headline">¶</a></h2>
<div class="admonition-proposition admonition">
<p class="admonition-title">Proposition</p>
<p>La direction de variance maximale dans <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> correspond au vecteur propre associé à la plus grande valeur propre de <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}=\boldsymbol{X}^T\boldsymbol{X}\)</span>.</p>
</div>
<div class="caution dropdown admonition">
<p class="admonition-title">Preuve</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}\boldsymbol{v}^t\bar{\boldsymbol{X}}^T\bar{\boldsymbol{X}}\boldsymbol{v} &amp;= \boldsymbol{v}^t\boldsymbol{\Sigma}\boldsymbol{v} = \Big\langle\boldsymbol{v}, \boldsymbol{\Sigma}\boldsymbol{v} \Big\rangle\\ &amp; = \Bigg\langle\boldsymbol{v}, \Bigg[\sum_{k=1}^d \lambda_k    \langle \boldsymbol{v}, \boldsymbol{v_k}\rangle   \boldsymbol{v_k}\Bigg] \Bigg\rangle \\ &amp;= \sum_{k=1}^d \lambda_k    \langle \boldsymbol{v}, \boldsymbol{v_k}\rangle \langle \boldsymbol{v}, \boldsymbol{v_k}\rangle  \\ &amp;\leq \lambda_{\max} \underbrace{\sum_{k=1}^d     \langle \boldsymbol{v}, \boldsymbol{v_k}\rangle^2}_{=\lVert\boldsymbol{v}\rVert_2^2 = 1}\\ &amp;   = \lambda_{\max}\end{aligned}\end{split}\]</div>
<p>L’égalité est ainsi obtenue pour le vecteur propre associé à <span class="math notranslate nohighlight">\(\lambda_{max}\)</span>.</p>
</div>
<p>Nous avons donc :</p>
<div class="math notranslate nohighlight">
\[\lambda_{\max}=\boldsymbol{v}_{\max}
^t\bar{\boldsymbol{X}}^T\bar{\boldsymbol{X}}\boldsymbol{v}_{\max}\]</div>
<p>Nous devons partir à la recherche de ce vecteur propre particulier pour résoudre notre problème. Pour cela nous allons voir une méthode itérative pratique permettant de retrouver le vecteur associé à la plus grande valeur propre d’une matrice&amp;nbsp: <strong>l’algorithme des puissances itérées</strong>.</p>
</div>
<div class="section" id="iv-calculer-les-vecteurs-et-valeurs-propres">
<h2>IV. Calculer les vecteurs et valeurs propres<a class="headerlink" href="#iv-calculer-les-vecteurs-et-valeurs-propres" title="Permalink to this headline">¶</a></h2>
<div class="section" id="a-l-algorithme-des-puissances-iterees">
<h3>A. L’algorithme des puissances itérées<a class="headerlink" href="#a-l-algorithme-des-puissances-iterees" title="Permalink to this headline">¶</a></h3>
<p>En réutilisant les propriétés vues à propos des matrices diagonalisables, nous pouvons essayer de construire un algorithme qui nous permettra de calculer le vecteur propre associé à la plus grande valeur propre :</p>
<div class="math notranslate nohighlight">
\[\underbrace{\boldsymbol{A}\boldsymbol{A}\dots\boldsymbol{A}}_{\times n}\boldsymbol{v} = \boldsymbol{A}^n\boldsymbol{v} =  \sum_{k=1}^d (\lambda_k)^n  \langle \boldsymbol{v_k}, \boldsymbol{v}\rangle   \boldsymbol{v_k}\]</div>
<p>Et on voit que c’est la contribution du vecteur propre associée à la plus forte valeur propre qui va dominer assymptotiquement. Plus formellement, si on écrit les valeurs propres sous la formes <span class="math notranslate nohighlight">\(\lambda_k = \lambda_{\max}\frac{\lambda_{k}}{\lambda_{\max}}\)</span> :</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{A}^n\boldsymbol{v} = (\lambda_{\max})^n \sum_{k=1}^d \Big(\frac{\lambda_{k}}{\lambda_{\max}}\Big)^n  \langle \boldsymbol{v_k}, \boldsymbol{v}\rangle   \boldsymbol{v_k},\]</div>
<p>on obtient <span class="math notranslate nohighlight">\(\lim_{n \rightarrow \infty} (\frac{\lambda_{k}}{\lambda_{\max}})^n = 1\)</span> si <span class="math notranslate nohighlight">\(\lambda_{k} = \lambda_{\max}\)</span> et <span class="math notranslate nohighlight">\(0\)</span> sinon. Ainsi :</p>
<div class="math notranslate nohighlight">
\[\lim_{n \rightarrow \infty}\frac{\boldsymbol{A}^n\boldsymbol{v}}{\lVert\boldsymbol{A}^n\boldsymbol{v}\rVert_2} = \boldsymbol{v_{\max}}\]</div>
<p>En multipliant itérativement (presque) n’importe quel vecteur d’entrée par notre application linéaire, nous construisons une suite dont l’expression normalisée converge asymptotiquement vers le vecteur propre associé à la valeur propre la plus forte. C’est exactement ce qu’on fait en pratique en alternant multiplication et normalisation afin d’éviter un <span class="math notranslate nohighlight">\(\texttt{overflow}\)</span> de nos variable. L’initialisation est faite aléatoirement :</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\boldsymbol{v}(0) \leftarrow \mathcal{N}\Big(\boldsymbol{0}, \boldsymbol{I}\Big).
\end{equation*}\]</div>
<p>Et chaque itération a la forme suivante :</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\boldsymbol{v}(n+1) =  \frac{\boldsymbol{A}\boldsymbol{v}(n)}{\lVert\boldsymbol{A}\boldsymbol{v}(n)\rVert_2} 
\end{equation*}\]</div>
<div class="admonition-question admonition">
<p class="admonition-title">Question</p>
<p><strong>Maintenant que nous avons un algorithme pour trouver la composante principale, comment faire pour trouver les autres ?</strong></p>
</div>
</div>
<div class="section" id="b-en-pratique">
<h3>B. En pratique<a class="headerlink" href="#b-en-pratique" title="Permalink to this headline">¶</a></h3>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Completer le code des méthodes <span class="math notranslate nohighlight">\(\text{power_iteration}\)</span> et <span class="math notranslate nohighlight">\(\text{fit}\)</span> de la classe <em>ACP</em> ci-dessous qui implemente le calcul d’une ACP. La méthode <span class="math notranslate nohighlight">\(\text{power_iteration}\)</span> doit retourner un tuple contenant le vecteur et sa valeur propre associée. La méthode <span class="math notranslate nohighlight">\(\text{fit}\)</span> doit exécuter les puissances itérées autant de fois que demandé. C’est-à-dire <span class="math notranslate nohighlight">\(\texttt{n}\_\texttt{components}\)</span> fois.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ACP</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span> <span class="o">=</span> <span class="n">n_components</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter</span>

    <span class="k">def</span> <span class="nf">order_vectors</span><span class="p">(</span><span class="bp">self</span><span class="p">,):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Must be fitted before&quot;</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">eigen_values</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eigen_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_values</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">]</span>
    
    <span class="c1">#### Complete the code here #### or die #######################################</span>
    <span class="k">def</span> <span class="nf">power_iteration</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">A</span><span class="p">):</span>
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="k">return</span> <span class="n">vector</span><span class="p">,</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">X_n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_n</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X_n</span><span class="p">)</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">))</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">eigen_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">)</span> 
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">order_vectors</span><span class="p">()</span>
    <span class="c1">###############################################################################</span>
    
            
    <span class="k">def</span> <span class="nf">zeros_pad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">Z</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">Z</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">-</span> <span class="n">k</span><span class="p">))]</span>
        
    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="c1">#### Complete the code here #### or die #######################################</span>
        <span class="c1"># pour un exercice plus bas</span>
        <span class="c1">#</span>
        <span class="c1">#</span>
        <span class="c1">###############################################################################</span>
        <span class="k">pass</span>
        
    
    <span class="k">def</span> <span class="nf">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="c1">#### Complete the code here #### or die #######################################</span>
        <span class="c1"># pour un exercice plus bas</span>
        <span class="c1">#</span>
        <span class="c1">#</span>
        <span class="c1">###############################################################################</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">compress</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">k</span><span class="p">)</span>
</pre></div>
</div>
<p>Appliquons notre méthode à nos données. On affichera le repère associé aux composantes principales trouvés par notre méthode :</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">acp</span> <span class="o">=</span> <span class="n">ACP</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">acp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">vec</span> <span class="o">=</span> <span class="n">acp</span><span class="o">.</span><span class="n">eigen_vectors</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>En pratique, les calculs ne sont pas aussi directs et certaines décompositions sont utilisées afin d’accélérer et de stabiliser les calculs.</p>
</div>
</div>
<div class="section" id="v-calcul-des-vecteurs-de-representation-codage-et-reconstructrion-decodage">
<h2>V. Calcul des vecteurs de représentation (codage) et reconstructrion (décodage)<a class="headerlink" href="#v-calcul-des-vecteurs-de-representation-codage-et-reconstructrion-decodage" title="Permalink to this headline">¶</a></h2>
<div class="section" id="a-compression">
<h3>A. Compression<a class="headerlink" href="#a-compression" title="Permalink to this headline">¶</a></h3>
<p>Nous venons donc de trouver une base telle que les projections des données dans ce nouveau système de coordonnées ont une variance maximale. Rappelons nous que l’objectif de l’apprentissage non-supervisé de représentation à pour objectif de trouver un mapping, une fonction de codage <span class="math notranslate nohighlight">\(\Phi : \boldsymbol{x} \rightarrow \boldsymbol{z} = \Phi(\boldsymbol{x}) :  \mathbb{R}^d \rightarrow \mathbb{R}^K\)</span>, et dans notre cas nous pouvons définir <span class="math notranslate nohighlight">\(\Phi\)</span> comme l’application linéaire suivante :</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{z} = \hat{\boldsymbol{V}}_K^T \boldsymbol{x} \in \mathbb{R}^K\]</div>
<p>où <span class="math notranslate nohighlight">\(\hat{\boldsymbol{V}}_K^T \in \mathbb{R}^{d \times K}\)</span> correspond à la matrice  <span class="math notranslate nohighlight">\(\boldsymbol{V}\)</span> de laquelle on a retirée les <span class="math notranslate nohighlight">\(d-K\)</span> derniers vecteurs colonnes (on part du principe que les vecteur colonne sont triés par ordre décroissant de leur valeur propre associée). Nous avons donc :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\hat{\boldsymbol{V}}_K = 
\begin{bmatrix}
    \vert &amp;   &amp; \vert    \\
    \vert &amp;   &amp; \vert    \\
    \vert &amp;   &amp; \vert \\
    \boldsymbol{v_1}   &amp; \dots &amp; \boldsymbol{v_K}   \\
    \vert &amp;   &amp; \vert    \\
    \vert &amp;  &amp; \vert 
\end{bmatrix}\end{split}\]</div>
<div class="admonition-question admonition">
<p class="admonition-title">Question</p>
<p><strong>Exprimez une application “inverse” (fonction de décodage), qu’on notera <span class="math notranslate nohighlight">\(\Psi\)</span>, qui permettrait de reprojeter la representation <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span> dans l’espace d’origine. On notera <span class="math notranslate nohighlight">\(\hat{\boldsymbol{x}} = \Psi(\boldsymbol{z})\)</span> la reconstruction du vecteur d’origine.</strong></p>
</div>
<p>Il est ainsi évident que si toutes les composantes sont conservées, on retrouve <span class="math notranslate nohighlight">\(\hat{\boldsymbol{x}} = (\Psi \circ \Phi) ( \boldsymbol{x}) =  \underbrace{\boldsymbol{V}\boldsymbol{V}^t}_{\boldsymbol{I_d}} \boldsymbol{x} = \boldsymbol{x}\)</span>.</p>
<p>On notera que comme <span class="math notranslate nohighlight">\(\hat{\boldsymbol{V}}_K\)</span> avec <span class="math notranslate nohighlight">\(K \leq d\)</span> n’est pas carrée de rang plein, elle n’est pas inversible, et l’application <span class="math notranslate nohighlight">\(\Phi\)</span> n’est pas bijective. Ainsi, <span class="math notranslate nohighlight">\(\Psi\)</span> ne peut pas être l’inverse de <span class="math notranslate nohighlight">\(\Phi\)</span>. Nous avions donc plusieurs choix pour <span class="math notranslate nohighlight">\(\Psi\)</span>. Mais nous pouvons montrer que celui que nous avons fait minimise l’erreur de décodage : c’est une propriété souhaitable pour ce genre de problème en basse dimension.</p>
<hr class="docutils" />
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Dans le code de l’ACP ci-dessus, complétez les methodes <span class="math notranslate nohighlight">\(\text{transform}\)</span> et <span class="math notranslate nohighlight">\(\text{inverse_transform}\)</span>.</strong></p>
</div>
<hr class="docutils" />
<p>Appliquons ensuite le code ci dessous pour visualiser les entrées reconstruites (en rouge) par dessus les données d’entrée (en bleu).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X_rec</span> <span class="o">=</span> <span class="n">acp</span><span class="o">.</span><span class="n">compress</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># on ne projette que sur une composante</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">vec</span> <span class="o">=</span> <span class="n">acp</span><span class="o">.</span><span class="n">eigen_vectors</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X_rec</span> <span class="o">=</span> <span class="n">X_rec</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="b-interpretation-de-l-erreur-de-reconstruction-moyenne">
<h3>B. Interprétation de l’erreur de reconstruction moyenne<a class="headerlink" href="#b-interpretation-de-l-erreur-de-reconstruction-moyenne" title="Permalink to this headline">¶</a></h3>
<p>Nous allons voir ici que la fonction de codage/décodage correspondant à l’ACP correspond à un endomorphisme qui est un projecteur orthogonal. De plus la différence moyenne entre le vecteur d’origine et son projeté orthogonal possède la propriété intéressante suivante :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
        \text{err} &amp;= \frac{1}{N}\sum_i^N||\boldsymbol{x}_i - \hat{\boldsymbol{V}}_K\hat{\boldsymbol{V}}_K^t \boldsymbol{x}_i||_2^2\\&amp;=
        \frac{1}{N}\sum_i^N||\sum_{k=1}^d\langle\boldsymbol{x}_i, \boldsymbol{v}_k \rangle \boldsymbol{v}_k - \sum_{k=1}^K\langle\boldsymbol{x}_i, \boldsymbol{v}_k \rangle \boldsymbol{v}_k||_2^2\\ &amp;=
         \frac{1}{N}\sum_i^N||\sum_{k=K+1}^d\langle\boldsymbol{x}_i, \boldsymbol{v}_k \rangle \boldsymbol{v}_k||_2^2 =
         \frac{1}{N}\sum_i^N \sum_{k=K+1}^d\langle\boldsymbol{x}_i, \boldsymbol{v}_k \rangle^2 \\ &amp;=
         \frac{1}{N}\sum_{k=K+1}^d \boldsymbol{v}_k^t \boldsymbol{X}^T\boldsymbol{X}\boldsymbol{v}_k = \sum_{k=K+1}^d \boldsymbol{v}_k^t \boldsymbol{\Sigma}\boldsymbol{v}_k\\ =&amp;
         \sum_{k=K+1}^d \lambda_k \xrightarrow{N\rightarrow\infty}\sum_{k=K+1}^d \text{Var}_{\boldsymbol{z}}[z_k]
\end{aligned}\end{split}\]</div>
<p>Nous pouvons observer qu’à l’étape 3, le vecteur différence est forcément un vecteur de <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> et est orthogonale à tous les vecteurs propres sélectionnés dans <span class="math notranslate nohighlight">\(\hat{\boldsymbol{V}}_K\)</span>. <span class="math notranslate nohighlight">\(\hat{\boldsymbol{V}}_K\hat{\boldsymbol{V}}_K^T\)</span> est donc un projecteur orthogonal sur le sous espace propre correspondant aux composantes sélectionnées. Il est celui qui minimise l’erreur de reconstruction. De plus nous constatons que cette erreur a une norme au carré moyenne qui est une estimation de la somme des variances dans les directions non pertinentes.</p>
<p>En pratique on pourra donc choisir le nombre de composante à conserver <span class="math notranslate nohighlight">\(K\)</span> de sorte à ce que la qualité de la reconstruction souhaitée soit supérieure à un certain seuil <span class="math notranslate nohighlight">\(\tau\)</span> :</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
   \frac{\sum_{k=1}^K \lambda_k}{\sum_{k=1}^d \lambda_k} \geq \tau \in [0,1]
\end{equation*}\]</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Completez les fonction <span class="math notranslate nohighlight">\(\text{compute_cumulative_var}\)</span> et <span class="math notranslate nohighlight">\(\text{find_thresholded_cumulative}\)</span> qui calcul respectivement l quantité exprimez précédement, et détermine le nombre de composante minimale pour avoir un <em>RMSE</em> d’au moins <span class="math notranslate nohighlight">\(\tau\)</span> (<em>tresh</em>).</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_error</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X_rec</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">X_rec</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">compute_cumulative_var</span><span class="p">(</span><span class="n">eigen_values</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="c1">#### Complete the code here #### or die #######################################</span>
    <span class="o">...</span>
    <span class="o">...</span>
    <span class="c1">###############################################################################</span>

<span class="k">def</span> <span class="nf">find_thresholded_cumulative</span><span class="p">(</span><span class="n">eigen_values</span><span class="p">,</span> <span class="n">thresh</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">):</span>
    <span class="c1">#### Complete the code here #### or die #######################################</span>
    <span class="o">...</span>
    <span class="o">...</span>
    <span class="c1">###############################################################################</span>
</pre></div>
</div>
<p>On peut maintenant construire un jeu de donnée de plus haute dimension et tester notre code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">sample_multivariate_gaussian_data</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">acp</span> <span class="o">=</span> <span class="n">ACP</span><span class="p">()</span>
<span class="n">acp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">cumul</span> <span class="o">=</span> <span class="p">[</span><span class="n">compute_cumulative_var</span><span class="p">(</span><span class="n">acp</span><span class="o">.</span><span class="n">eigen_values</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>

<span class="n">K</span> <span class="o">=</span> <span class="n">find_thresholded_cumulative</span><span class="p">(</span><span class="n">acp</span><span class="o">.</span><span class="n">eigen_values</span><span class="p">,</span> <span class="n">thresh</span> <span class="o">=</span> <span class="mf">0.95</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">(</span><span class="n">K</span><span class="p">)</span><span class="o">/</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cumul</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">cumul</span><span class="p">,</span> 
         <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Erreur de reconstruction&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>On voit ici que sulement 0.7% des composantes contiennent 95% de l’information.</p>
</div>
</div>
<div class="section" id="v-compression-d-une-base-d-images">
<h2>V. Compression d’une base d’images<a class="headerlink" href="#v-compression-d-une-base-d-images" title="Permalink to this headline">¶</a></h2>
<div class="section" id="a-les-donnees-et-notre-acp">
<h3>A. Les données et notre ACP<a class="headerlink" href="#a-les-donnees-et-notre-acp" title="Permalink to this headline">¶</a></h3>
<p>Dans cette section nous allons tester notre algorithme d’ACP sur des données d’images brutes en considérant chaque image de dimension <span class="math notranslate nohighlight">\((w \times h)\)</span> comme un vecteur <span class="math notranslate nohighlight">\(\boldsymbol{x} \in \mathbb{R}^{wh}\)</span>. Nous procéderons sur une base d’images de visages centrés en niveau de gris : la base <span class="math notranslate nohighlight">\(\text{Oliveti Faces}\)</span>. Commençons dans un premier temps par charger les données, les afficher et appliquons notre algorthme d’ACP (on prendra plutôt la version <span class="math notranslate nohighlight">\(\text{np.linalg.eig}\)</span> pour calculer les vecteur propres qui est beaucoup plus stables et efficace computationellement que la notre).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ACP</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">order_vectors</span><span class="p">(</span><span class="bp">self</span><span class="p">,):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Must be fitted before&quot;</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">eigen_values</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eigen_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_values</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">X_n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_n</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X_n</span><span class="p">)</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">eigen_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eigen_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_values</span><span class="o">.</span><span class="n">real</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors</span><span class="o">.</span><span class="n">real</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">order_vectors</span><span class="p">()</span>
        
    <span class="k">def</span> <span class="nf">zeros_pad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">Z</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">Z</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">-</span> <span class="n">k</span><span class="p">))]</span>
        
    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;You need to fit the model first !&quot;</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">k</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors</span><span class="p">[:,</span> <span class="p">:</span><span class="n">k</span><span class="p">])</span>
    
    <span class="k">def</span> <span class="nf">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;You need to fit the model first !&quot;</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">k</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">zeros_pad</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span>

    <span class="k">def</span> <span class="nf">compress</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">k</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
 
<span class="c1"># Load data</span>
<span class="n">lfw_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">fetch_olivetti_faces</span><span class="p">()</span> <span class="c1">#datasets.fetch_lfw_people(min_faces_per_person=500)</span>
 
<span class="n">_</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">lfw_dataset</span><span class="o">.</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">lfw_dataset</span><span class="o">.</span><span class="n">data</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(400, 4096)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualization</span>
<span class="k">def</span> <span class="nf">plot_images</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">rows</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">cols</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span><span class="mi">64</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">rows</span> <span class="o">*</span> <span class="n">cols</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">title</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span> <span class="o">+</span> <span class="s2">&quot; : &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(())</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(())</span>

<span class="k">def</span> <span class="nf">plot_images_one_by_one</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">original</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(())</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(())</span>
        <span class="k">if</span> <span class="n">title</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span> <span class="o">+</span> <span class="s2">&quot; : &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">*</span><span class="n">step</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">original</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(())</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(())</span>
        <span class="k">if</span> <span class="n">title</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Original image&#39;</span><span class="p">)</span>
        

            
<span class="n">plot_images</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_principal_component_analysis_40_0.png" src="../_images/1_principal_component_analysis_40_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">acp</span> <span class="o">=</span> <span class="n">ACP</span><span class="p">()</span>
<span class="n">acp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="b-affichage-des-variances-cumullees-et-calcul-du-seuil-de-reconstruction">
<h3>B. Affichage des variances cumullées et calcul du seuil de reconstruction<a class="headerlink" href="#b-affichage-des-variances-cumullees-et-calcul-du-seuil-de-reconstruction" title="Permalink to this headline">¶</a></h3>
<p>Nous calculons ensuite le nombre de composantes à conserver pour obtenir une qualité de reconstruiuction moyenne de <span class="math notranslate nohighlight">\(95\%\)</span> de <span class="math notranslate nohighlight">\(\text{RMSE}\)</span>. On voit ici que l’on peut ne conserver que <span class="math notranslate nohighlight">\(2 \%\)</span> des variables d’entrées pour avoir une qualité moyenne de reconstruction de <span class="math notranslate nohighlight">\(95 \%\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cumul</span> <span class="o">=</span> <span class="p">[</span><span class="n">compute_cumulative_var</span><span class="p">(</span><span class="n">acp</span><span class="o">.</span><span class="n">eigen_values</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>

<span class="n">K</span> <span class="o">=</span> <span class="n">find_thresholded_cumulative</span><span class="p">(</span><span class="n">acp</span><span class="o">.</span><span class="n">eigen_values</span><span class="p">,</span> <span class="n">thresh</span> <span class="o">=</span> <span class="mf">0.95</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">K</span><span class="p">)</span><span class="o">/</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cumul</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">cumul</span><span class="p">,</span> 
         <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Erreur de reconstruction&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="c-affichage-des-vecteur-propres-les-eigen-faces">
<h3>C. Affichage des vecteur propres (les eigen faces)<a class="headerlink" href="#c-affichage-des-vecteur-propres-les-eigen-faces" title="Permalink to this headline">¶</a></h3>
<p>Affichons les vecteurs propres appris sur ce jeu de données. On remarquera qu’on peut interpreter ces vecteurs propres coommes des images et donc qu’on peut les afficher comme tel. On affichera un sous ensemble dans un soucis de lisibilité. Une manière d’interpreter ces images est donc que tout visage de la base peut être reconstruit sans erreur comme une somme pondérée des ces visages “élémentaires”.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plot_images</span><span class="p">(</span><span class="n">acp</span><span class="o">.</span><span class="n">eigen_vectors</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="d-visualisation-perceptuelle-de-la-qualite-de-reconstruction">
<h3>D. Visualisation perceptuelle de la qualité de reconstruction<a class="headerlink" href="#d-visualisation-perceptuelle-de-la-qualite-de-reconstruction" title="Permalink to this headline">¶</a></h3>
<p>Nous pouvons aussi nous amuser visualiser différentes versions reconstruites d’une même image en ne conservant qu’une certaine proportion des composantes <span class="math notranslate nohighlight">\(k \in [1, K]\)</span>. On voit ici que, perceptuellement, même sans aller jusqu’au nombre de composantes seuil définit précédement, on peut très rapidmeent converger vers l’image originale avec peu de composantes.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">idx</span> <span class="o">=</span> <span class="mi">14</span><span class="c1">#np.random.randint(X.shape[0])</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">compressed_images</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">acp</span><span class="o">.</span><span class="n">compress</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="mi">20</span><span class="p">)])</span>

<span class="n">plot_images_one_by_one</span><span class="p">(</span><span class="n">compressed_images</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> 
                       <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Progressive reconstruction </span><span class="se">\n</span><span class="s2"> n_components&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="e-essayons-les-memes-etapes-sur-mnist">
<h3>E. Essayons les mêmes étapes sur mnist<a class="headerlink" href="#e-essayons-les-memes-etapes-sur-mnist" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_openml</span>
<span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">fetch_openml</span><span class="p">(</span><span class="s1">&#39;mnist_784&#39;</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">as_frame</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">plot_images</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_principal_component_analysis_52_0.png" src="../_images/1_principal_component_analysis_52_0.png" />
</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">acp</span> <span class="o">=</span> <span class="n">ACP</span><span class="p">()</span>
<span class="n">acp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1">#errors = [compute_error(X, x_rec) for x_rec in [acp.compress(X, k=k) for k in range(X.shape[1])]]</span>
<span class="n">cumul</span> <span class="o">=</span> <span class="p">[</span><span class="n">compute_cumulative_var</span><span class="p">(</span><span class="n">acp</span><span class="o">.</span><span class="n">eigen_values</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cumul</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">cumul</span><span class="p">,</span> 
    <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Erreur de reconstruction&#39;</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plot_images</span><span class="p">(</span><span class="n">acp</span><span class="o">.</span><span class="n">eigen_vectors</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">K</span> <span class="o">=</span> <span class="n">find_thresholded_cumulative</span><span class="p">(</span><span class="n">acp</span><span class="o">.</span><span class="n">eigen_values</span><span class="p">,</span> <span class="n">thresh</span> <span class="o">=</span> <span class="mf">0.90</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">K</span><span class="p">)</span><span class="o">/</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">idx</span> <span class="o">=</span> <span class="mi">142</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">compressed_images</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">acp</span><span class="o">.</span><span class="n">compress</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="mi">10</span><span class="p">)])</span>

<span class="n">plot_images_one_by_one</span><span class="p">(</span>
    <span class="n">compressed_images</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
    <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Progressive reconstruction </span><span class="se">\n</span><span class="s2"> n_components&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./7_unsupervised"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="0_propos_liminaire.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">L’apprentissage non-supervisé</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="2_em_gaussin_mixture_model.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Modèle de Mélange Gaussien et algorithme <em>Expectation-Maximization</em></p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Servajean, Leveau & Chailan<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>L‚ÄôAnalyse en Composantes Principales ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è &#8212; Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Mod√®le de M√©lange Gaussien et algorithme Expectation-Maximization" href="2_em_gaussin_mixture_model.html" />
    <link rel="prev" title="L‚Äôapprentissage non-supervis√©" href="0_propos_liminaire.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-72WWYCKNK6"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-72WWYCKNK6');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../0_requisite/rappels_python.html">
   Rappels de Python et Numpy
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../1_what_is_ml/0_propos_liminaire.html">
   <em>
    Machine Learning
   </em>
   , initiation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/1_introduction_ml.html">
     <em>
      Machine learning
     </em>
     et mal√©diction de la dimension
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/2_regression_and_classification_trees.html">
     Les arbres de r√©gression et de classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../2_regression/0_propos_liminaire.html">
   La
   <em>
    r√©gression
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/1_linear_regression.html">
     La r√©gression lin√©aire ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/2_optimization.html">
     L‚Äôoptimisation ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/3_interpolation.html">
     Interpolation ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/4_algo_proximal_lasso.html">
     Sous-diff√©rentiel et le cas du Lasso ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/5_least_square_qr.html">
     Les moindres carr√©s via une d√©composition QR (et plus)‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/6_ridge.html">
     Une analyse de la r√©gularisation Ridge ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../3_classification/0_propos_liminaire.html">
   La
   <em>
    classification
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/1_logistic_regression.html">
     La r√©gression logistique ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/2_fonctions_proxy.html">
     Les fonctions de perte (loss function) ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/3_bayes_classifier.html">
     Le classifieur de Bayes ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/4_VC_theory.html">
     Un mod√®le formel de l‚Äôapprentissage ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è (üíÜ‚Äç‚ôÇÔ∏è)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../4_kernel_methods/0_propos_liminaire.html">
   Les m√©thodes √† noyaux
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../4_kernel_methods/1_svm.html">
     Le SVM ou l‚Äôhypoth√®se max-margin ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5_ensembles/0_propos_liminaire.html">
   Les m√©thodes
   <em>
    ensemblistes
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/1_ensembles.html">
     M√©thodes ensemblistes ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/2_bayesian_linear_regression.html">
     Bayesian linear regression ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../6_deeplearning/0_propos_liminaire.html">
   <em>
    deep learning
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/1_autodiff.html">
     La diff√©rentiation automatique et un d√©but de
     <em>
      deep learning
     </em>
     ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/2_filters_representation.html">
     Filtres et espace de repr√©sentation des r√©seaux de neurones ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/3_probabilities_calibration.html">
     Calibration des probabilit√©s et quelques notions ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/4_regularization_deep.html">
     R√©gularisation en
     <em>
      deep learning
     </em>
     ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/5_transfer_multitask.html">
     <em>
      Transfer learning
     </em>
     et apprentissage multi-t√¢ches ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/6_adversarial.html">
     Les attaques adversaires ‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="0_propos_liminaire.html">
   L‚Äôapprentissage non-supervis√©
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     L‚ÄôAnalyse en Composantes Principales ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="2_em_gaussin_mixture_model.html">
     Mod√®le de M√©lange Gaussien et algorithme
     <em>
      Expectation-Maximization
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../8_set_prediction/0_propos_liminaire.html">
   Pr√©diction d‚Äôensembles
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/1_well_defined.html">
     Jeu d‚Äôapprentissage
     <em>
      set-valued
     </em>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/2_ill_defined.html">
     Jeu d‚Äôapprentissage uniquement multi-classes ‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../biblio.html">
   Bibliographie
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/7_unsupervised/1_principal_component_analysis.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/maximiliense/lmiprp"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/maximiliense/lmiprp/issues/new?title=Issue%20on%20page%20%2F7_unsupervised/1_principal_component_analysis.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/maximiliense/lmiprp/blob/master/Travaux Pratiques/Machine Learning/Book/7_unsupervised/1_principal_component_analysis.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#i-introduction">
   I. Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ii-quelques-rappels-d-algebre-lineaire">
   II. Quelques rappels d‚Äôalg√®bre lin√©aire
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-decomposition-en-valeurs-et-vecteurs-propres">
     A. D√©composition en valeurs et vecteurs propres
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-composition-d-endormorphismes-autoadjoints-symetriques">
     B. Composition d‚Äôendormorphismes autoadjoints (sym√©triques)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iii-lien-entre-directions-de-variance-maximale-et-vecteurs-propres">
   III. Lien entre directions de variance maximale et vecteurs propres
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iv-calculer-les-vecteurs-et-valeurs-propres">
   IV. Calculer les vecteurs et valeurs propres
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-l-algorithme-des-puissances-iterees">
     A. L‚Äôalgorithme des puissances it√©r√©es
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-en-pratique">
     B. En pratique
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#v-calcul-des-vecteurs-de-representation-codage-et-reconstructrion-decodage">
   V. Calcul des vecteurs de repr√©sentation (codage) et reconstructrion (d√©codage)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-compression">
     A. Compression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-interpretation-de-l-erreur-de-reconstruction-moyenne">
     B. Interpr√©tation de l‚Äôerreur de reconstruction moyenne
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#v-compression-d-une-base-d-images">
   V. Compression d‚Äôune base d‚Äôimages
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-les-donnees-et-notre-acp">
     A. Les donn√©es et notre ACP
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-affichage-des-variances-cumullees-et-calcul-du-seuil-de-reconstruction">
     B. Affichage des variances cumull√©es et calcul du seuil de reconstruction
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#c-affichage-des-vecteur-propres-les-eigen-faces">
     C. Affichage des vecteur propres (les eigen faces)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#d-visualisation-perceptuelle-de-la-qualite-de-reconstruction">
     D. Visualisation perceptuelle de la qualit√© de reconstruction
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#e-essayons-les-memes-etapes-sur-mnist">
     E. Essayons les m√™mes √©tapes sur mnist
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="l-analyse-en-composantes-principales">
<h1>L‚ÄôAnalyse en Composantes Principales ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è<a class="headerlink" href="#l-analyse-en-composantes-principales" title="Permalink to this headline">¬∂</a></h1>
<div class="admonition-objectifs-de-la-sequence admonition">
<p class="admonition-title">Objectifs de la s√©quence</p>
<ul class="simple">
<li><p>√ätre sensibilis√©¬†:</p>
<ul>
<li><p>aux enjeux math√©matiques de l‚ÄôACP.</p></li>
</ul>
</li>
<li><p>√ätre capable¬†:</p>
<ul>
<li><p>d‚Äôimpl√©menter une ACP avec <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span>.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="i-introduction">
<h2>I. Introduction<a class="headerlink" href="#i-introduction" title="Permalink to this headline">¬∂</a></h2>
<p>L‚ÄôAnalyse en Composante Principale (ACP) consiste √† extraire des directions dans lesquelles les donn√©es s‚Äô√©talent particuli√®rement (i.e. la variance y est maximale). Prenons un exemple. Nous disposons d‚Äôun jeu de donn√©es dont on aimerait extraire l‚Äôinformation la plus ‚Äúrepresentative‚Äù (dans un sens que nous allons expliciter plus loin). Nous choisissons ici un jeu de donn√©es tir√© selon une loi gaussienne multivari√©e <span class="math notranslate nohighlight">\(\boldsymbol{x} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{\Sigma})\)</span> avec <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma} \in \mathcal{S}^+(\mathbb{R}^d)\)</span> (i.e. matrice semi-d√©finie positive de dimension <span class="math notranslate nohighlight">\(d\times d\)</span>) la matrice de variance/co-variance des donn√©es. Affichons le jeu de donn√©es.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">12.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X_rec</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vec</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">circles</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;Dimension must be equal to 2 to plot stuffs.&quot;</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">edgecolors</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">facecolors</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
    <span class="n">x1min_</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="n">eps</span>
    <span class="n">x1max_</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">eps</span>
    <span class="n">x0min_</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="n">eps</span>
    <span class="n">x0max_</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="n">eps</span>
    <span class="k">if</span> <span class="n">vec</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span>
            <span class="n">mean</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mean</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">vec</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">vec</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> 
            <span class="n">head_width</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> 
            <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span>
        <span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span>
            <span class="n">mean</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mean</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">vec</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">vec</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> 
            <span class="n">head_width</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> 
            <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">X_rec</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_rec</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">X_rec</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">facecolors</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
                <span class="p">[</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_rec</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">]],</span> <span class="p">[</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">X_rec</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">1</span><span class="p">]],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> 
                <span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="s1">&#39;0.5&#39;</span>
            <span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="n">x0min_</span><span class="p">,</span> <span class="n">x0max_</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="n">x1min_</span><span class="p">,</span> <span class="n">x1max_</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">random_rotation</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
    <span class="n">Q</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">d</span><span class="p">,</span><span class="n">d</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">Q</span>

<span class="k">def</span> <span class="nf">sample_multivariate_gaussian_data</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">variance_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">variance_decay</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">)])</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">random_rotation</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
    <span class="n">sigma_r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">sigma</span> <span class="p">),</span> <span class="n">Q</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">d</span><span class="p">),</span> <span class="n">sigma_r</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">sample_multivariate_gaussian_data</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(500, 2)
</pre></div>
</div>
<img alt="../_images/1_principal_component_analysis_4_1.png" src="../_images/1_principal_component_analysis_4_1.png" />
</div>
</div>
<div class="admonition-question admonition">
<p class="admonition-title">Question</p>
<p><strong>Dans le code ci-dessus, √† quoi sert √† la d√©composition QR ?</strong></p>
</div>
<p>Dans ce cas pr√©cis, chaque point est repr√©sent√© par deux nombres. Que pourrions-nous faire pour ne repr√©senter chaque point par seulement un nombre en perdant le moins d‚Äôinformation possible sur le signal d‚Äôorigine ? Nous pourrions ainsi chercher √† trouver la direction dans l‚Äôespace des donn√©es telle que les donn√©es soient le plus ‚Äú√©tal√©es‚Äù. Plus formellement on cherche un vecteur unitaire <span class="math notranslate nohighlight">\(\boldsymbol{v} \in \mathbb{R}^2\)</span> tel que les projections des donn√©es sur <span class="math notranslate nohighlight">\(z_i = \langle \boldsymbol{v}, \boldsymbol{x}_i \rangle\)</span> constituent un √©chantillon transform√© dont la variance empirique soit maximale. Il s‚Äôagit de r√©soudre le probl√®me d‚Äôoptimisation suivant¬†:</p>
<div class="math notranslate nohighlight">
\[\underset{\boldsymbol{v}}{argmax}\Bigg[\frac{1}{N} \sum_{i=1}^N  (z_i - \bar{z})^2 \Bigg] = \underset{\boldsymbol{v}}{argmax} \Big[ \boldsymbol{v}^t\bar{\boldsymbol{X}}^T\bar{\boldsymbol{X}}\boldsymbol{v} \Big]\]</div>
<p>sous contrainte que <span class="math notranslate nohighlight">\(\lVert\boldsymbol{v}\rVert_2 = 1\)</span> et avec <span class="math notranslate nohighlight">\(\bar{z} = \frac{1}{N}\sum_i z_i\)</span> et <span class="math notranslate nohighlight">\(\bar{\boldsymbol{X}} \in \mathbb{R}^{n\times d}\)</span>, la matrice de <em>design</em> (i.e. qui contient nos donn√©es) centr√©e.</p>
<p>Sans la contrainte <span class="math notranslate nohighlight">\(\lVert\boldsymbol{v}\rVert_2 = 1\)</span>, le probl√®me d‚Äôoptimisation serait mal pos√© car la solution ne serait pas unique (il suffirait simplement d‚Äôaugmenter arbitrairement la norme de n‚Äôimporte quel vecteur pour avoir une valeur de la fonction objectif arbitrairement grande).</p>
<p>Nous allons montrer que ce probl√®me d‚Äôoptimisation est strictement √©quivalent a celui de trouver le vecteur propre de la matrice de covariance <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma} = \bar{\boldsymbol{X}}^T\bar{\boldsymbol{X}}\)</span> correspondant √† la valeur propre maximale <span class="math notranslate nohighlight">\(\lambda_{\max}\)</span>.</p>
<div class="admonition-question admonition">
<p class="admonition-title">Question</p>
<p><strong>Montrer l‚Äô√©galit√© suivante¬†:</strong></p>
<div class="math notranslate nohighlight">
\[\frac{1}{N} \sum_{i=1}^N  (z_i - \bar{z})^2=\boldsymbol{v}^t\bar{\boldsymbol{X}}^T\bar{\boldsymbol{X}}\boldsymbol{v}\]</div>
</div>
</div>
<div class="section" id="ii-quelques-rappels-d-algebre-lineaire">
<h2>II. Quelques rappels d‚Äôalg√®bre lin√©aire<a class="headerlink" href="#ii-quelques-rappels-d-algebre-lineaire" title="Permalink to this headline">¬∂</a></h2>
<div class="section" id="a-decomposition-en-valeurs-et-vecteurs-propres">
<h3>A. D√©composition en valeurs et vecteurs propres<a class="headerlink" href="#a-decomposition-en-valeurs-et-vecteurs-propres" title="Permalink to this headline">¬∂</a></h3>
<p><strong>Qu‚Äôest ce qu‚Äôun vecteur propre ?</strong>
Alg√©briquement, un vecteur propre d‚Äôune matrice <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> est un vecteur tel que¬†:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
    \boldsymbol{A}\boldsymbol{v} = \lambda \boldsymbol{v}
\end{equation*}\]</div>
<p>L‚Äôinterpr√©tation g√©om√©trique est donc qu‚Äôil s‚Äôagit d‚Äôun vecteur dont la direction de son image par l‚Äôapplication lin√©aire associ√©e √† la matrice <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> n‚Äôest qu‚Äôune <a class="reference external" href="https://fr.wikipedia.org/wiki/Homoth%C3%A9tie">homoth√©tie</a> (i.e. la direction est inchang√©e et le vecteur n‚Äôest qu‚Äô√©tir√©). Il est simplement √©tir√© d‚Äôun facteur <span class="math notranslate nohighlight">\(\lambda\)</span> qu‚Äôon appel sa valeur propre associ√©e.</p>
<p><img alt="Vecteur propre" src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/58/Eigenvalue_equation.svg/2880px-Eigenvalue_equation.svg.png" /></p>
<p><strong>Diagonalisation d‚Äôune matrice carr√©</strong>
Une matrice carr√©e peut √™tre vue comme un endomorphisme allant d‚Äôun espace vectoriel <span class="math notranslate nohighlight">\(E\)</span> vers lui m√™me. On dira que cet endomorphisme est diagonalisable s‚Äôil existe une matrice <span class="math notranslate nohighlight">\(\boldsymbol{V}\)</span> inversible et une matrice diagonale <span class="math notranslate nohighlight">\(\boldsymbol{\Lambda}\)</span> telles que:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{A} = \boldsymbol{V} \boldsymbol{\Lambda} \boldsymbol{V}^{-1}\]</div>
<p>o√π <span class="math notranslate nohighlight">\(\boldsymbol{V}\)</span> est une matrice dont les colonnes forment une base de <span class="math notranslate nohighlight">\(E\)</span> et dont les √©l√©ments sont les vecteurs propres et <span class="math notranslate nohighlight">\(\boldsymbol{\Lambda}\)</span> est une matrice dont les √©l√©ments diagonaux correspondent aux valeurs propres associ√©es.</p>
<div class="admonition-matrices-semblables admonition">
<p class="admonition-title">Matrices semblables</p>
<p>Deux matrices <span class="math notranslate nohighlight">\(A\)</span> et <span class="math notranslate nohighlight">\(B\)</span> sont dites semblables s‚Äôil existe une matrice inversible <span class="math notranslate nohighlight">\(V\)</span> telle que¬†:</p>
<div class="math notranslate nohighlight">
\[A=VBV^{-1}.\]</div>
</div>
<p>De plus, on peut montrer que si la matrice <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> est symm√©trique (<span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> = <span class="math notranslate nohighlight">\(\boldsymbol{A}^t\)</span>) √† coefficients dans <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> (ces deux propri√©t√©s sont v√©rifi√©es par notre matrice de variance-covariance <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>), la matrice de passage <span class="math notranslate nohighlight">\(\boldsymbol{V}\)</span> est n√©c√©ssairement une matrice orthogonale (<span class="math notranslate nohighlight">\(\boldsymbol{V}^{-1} = \boldsymbol{V}^t\)</span>) et <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> prend alors la forme particuli√®re suivante:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\boldsymbol{A} = \boldsymbol{V} \boldsymbol{\Lambda} \boldsymbol{V}^t = \begin{bmatrix}
    \vert &amp;   &amp; \vert &amp;   &amp;  \vert \\
    \vert &amp;   &amp; \vert &amp;   &amp;  \vert \\
    \vert &amp;   &amp; \vert &amp;  &amp;  \vert \\
    \boldsymbol{v_1}   &amp; \dots &amp; \boldsymbol{v_i} &amp; \dots &amp; \boldsymbol{v_d}   \\
    \vert &amp;   &amp; \vert &amp;   &amp;  \vert \\
    \vert &amp;  &amp; \vert &amp;  &amp; \vert 
\end{bmatrix}
\begin{bmatrix}
    \lambda_1 &amp; 0 &amp; \dots &amp; \dots  &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots &amp; \vdots &amp; \vdots\\
    0 &amp; \dots &amp; \lambda_i &amp; \dots &amp; 0\\
     \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
    0 &amp; \dots &amp; \dots &amp; 0 &amp;\lambda_d\\
\end{bmatrix}
\begin{bmatrix}
    \text{---} &amp; \boldsymbol{v_1} &amp; \text{---} \\
      &amp; \vdots &amp;  \\
    \text{---} &amp; \boldsymbol{v_i} &amp; \text{---} \\
     &amp; \vdots &amp;  \\
    \text{---} &amp; \boldsymbol{v_d} &amp; \text{---}
\end{bmatrix}
\end{aligned}\end{split}\]</div>
<p>o√π <span class="math notranslate nohighlight">\(\boldsymbol{V}\)</span> correspond cette fois √† une matrice orthogonale i.e.¬†:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\boldsymbol{V}^t\boldsymbol{V} &amp;= \begin{bmatrix}
    \vert &amp;    &amp;  \vert \\
    \boldsymbol{v_1}   &amp;  \dots &amp; \boldsymbol{v_d}   \\
    \vert &amp;   &amp; \vert 
\end{bmatrix}
\begin{bmatrix}
    \text{---} &amp; \boldsymbol{v_1} &amp; \text{---} \\
     &amp; \vdots &amp;  \\
    \text{---} &amp; \boldsymbol{v_d} &amp; \text{---}
\end{bmatrix}\\
&amp;= \begin{bmatrix}
    \langle \boldsymbol{v_1}, \boldsymbol{v_1} \rangle &amp; \dots &amp; \langle \boldsymbol{v_1}, \boldsymbol{v_d} \rangle \\
    \vdots &amp; \ddots &amp; \vdots \\
      \langle \boldsymbol{v_d}, \boldsymbol{v_1} \rangle &amp; \dots &amp; \langle \boldsymbol{v_d}, \boldsymbol{v_d} \rangle
\end{bmatrix}\\
&amp;= \begin{bmatrix}
    ||\boldsymbol{v_1}||_2^2=1 &amp; \dots &amp; 0 \\
    0 &amp; \ddots &amp; 0 \\
      0 &amp; \dots &amp; ||\boldsymbol{v_d}||_2^2=1
\end{bmatrix}
\end{aligned}\end{split}\]</div>
<p>les vecteurs propres sont donc tous orthogonaux 2 √† 2 et de norme <span class="math notranslate nohighlight">\(1\)</span> et forment donc une base orthonormale.</p>
<div class="margin sidebar">
<p class="sidebar-title">Matrice orthogonale</p>
<p>Les matrices orthogonales (i.e. <span class="math notranslate nohighlight">\(\boldsymbol{Q}^{-1}=\boldsymbol{Q}^T\)</span> ou <span class="math notranslate nohighlight">\(\boldsymbol{Q}^T\boldsymbol{Q}=\boldsymbol{Q}\boldsymbol{Q}^T=I\)</span>) sont typiquement les rotations, r√©flections ou roto-r√©flection.</p>
</div>
<p>C‚Äôest ce que nous souhaitons pour notre probl√®me. Nous voulons trouver une rotation de sorte √† ce que dans la nouvelle base, qu‚Äôon appellera composantes, la variance de nos donn√©es soit le plus parfaitement d√©crites par les dites composantes. Nous pouvons donc reformuler de mani√®re plus g√©n√©rale notre probl√®me pour n‚Äôimporte quel dimension d‚Äôentr√©e <span class="math notranslate nohighlight">\(d\)</span> comme le probl√®me d‚Äôoptimisation sous contrainte suivant¬†:</p>
<div class="math notranslate nohighlight">
\[\underset{\boldsymbol{V}}{argmax}\Bigg[\frac{1}{N} \sum_{k=1}^d\sum_{i=1}^N  (z_i^k - \bar{z}^k)^2 \Bigg] = \underset{\boldsymbol{V}}{argmax} \Big[ \boldsymbol{V}^t\bar{\boldsymbol{X}}^T\bar{\boldsymbol{X}}\boldsymbol{V} \Big],\text{ s.t. }\boldsymbol{V}^t\boldsymbol{V} = \boldsymbol{I_d}\]</div>
<p>Et nous montrerons que la solution de ce probl√®me consiste √† trouver la base des vecteurs propres de <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma} = \bar{\boldsymbol{X}}^T\bar{\boldsymbol{X}}\)</span>.</p>
</div>
<hr class="docutils" />
<div class="section" id="b-composition-d-endormorphismes-autoadjoints-symetriques">
<h3>B. Composition d‚Äôendormorphismes autoadjoints (sym√©triques)<a class="headerlink" href="#b-composition-d-endormorphismes-autoadjoints-symetriques" title="Permalink to this headline">¬∂</a></h3>
<p>La diagonalisation d‚Äôun endomorphisme permet de simplifier certains calculs. Nous restons ici dans le cas sym√©trique mais une partie du propos se g√©n√©ralise bien s√ªr au cas diagonalisable quelconque. Nous avons ainsi l‚Äô√©galit√© suivante¬†:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{A}^2 = \boldsymbol{V}\boldsymbol{\Lambda}\underbrace{\boldsymbol{V}^t\boldsymbol{V}}_{\boldsymbol{I}}\boldsymbol{\Lambda}\boldsymbol{V}^t = \boldsymbol{V}\boldsymbol{\Lambda}^2\boldsymbol{V}^t\]</div>
<p>et de mani√®re g√©n√©rale¬†:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\boldsymbol{A}^n = \boldsymbol{V} \boldsymbol{\Lambda}^n \boldsymbol{V}^t = \begin{bmatrix}
    \vert &amp;   &amp; \vert &amp;   &amp;  \vert \\
    \vert &amp;   &amp; \vert &amp;   &amp;  \vert \\
    \vert &amp;   &amp; \vert &amp;  &amp;  \vert \\
    \boldsymbol{v_1}   &amp; \dots &amp; \boldsymbol{v_i} &amp; \dots &amp; \boldsymbol{v_d}   \\
    \vert &amp;   &amp; \vert &amp;   &amp;  \vert \\
    \vert &amp;  &amp; \vert &amp;  &amp; \vert 
\end{bmatrix}
\begin{bmatrix}
    (\lambda_1)^n &amp; 0 &amp; \dots &amp; \dots  &amp; 0\\
     \vdots &amp; \ddots &amp; \vdots &amp; \vdots &amp; \vdots\\
    0 &amp; \dots &amp; (\lambda_i)^n &amp; \dots &amp; 0\\
     \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
    0 &amp; \dots &amp; \dots &amp; 0 &amp;(\lambda_d)^n\\
\end{bmatrix}
\begin{bmatrix}
    \text{---} &amp; \boldsymbol{v_1} &amp; \text{---} \\
      &amp; \vdots &amp;  \\
    \text{---} &amp; \boldsymbol{v_i} &amp; \text{---} \\
     &amp; \vdots &amp;  \\
    \text{---} &amp; \boldsymbol{v_d} &amp; \text{---}
\end{bmatrix}
\end{aligned}\end{split}\]</div>
<p>On remarque ainsi que si <span class="math notranslate nohighlight">\(A\)</span> est sym√©trique, alors <span class="math notranslate nohighlight">\(A^n\)</span> poss√®de la m√™me matrice de changement de base et ses valeurs propres sont les m√™mes √† la puissancec <span class="math notranslate nohighlight">\(n\)</span>.</p>
<p>Notez que n‚Äôimporte quel vecteur <span class="math notranslate nohighlight">\(\boldsymbol{x} \in \mathbb{R}^d\)</span> peut se d√©composer dans la base des vecteurs propres¬†:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{x} = \sum_{k=1}^d \underbrace{\langle \boldsymbol{v_k}, \boldsymbol{x} \rangle}_{z_k} \boldsymbol{v_k}\]</div>
<hr class="docutils" />
<div class="admonition-question admonition">
<p class="admonition-title">Question</p>
<p><strong>Montrez que <span class="math notranslate nohighlight">\(\boldsymbol{A}\boldsymbol{x} =  \sum_{k=1}^d \lambda_k  z_k \boldsymbol{v_k}\)</span>.</strong></p>
</div>
<div class="admonition-question admonition">
<p class="admonition-title">Question</p>
<p><strong>En utilisant la propri√©t√© de la mise √† la puissance, donnez l‚Äôexpression de <span class="math notranslate nohighlight">\(\boldsymbol{A}^n\boldsymbol{x}\)</span>.</strong></p>
</div>
</div>
</div>
<div class="section" id="iii-lien-entre-directions-de-variance-maximale-et-vecteurs-propres">
<h2>III. Lien entre directions de variance maximale et vecteurs propres<a class="headerlink" href="#iii-lien-entre-directions-de-variance-maximale-et-vecteurs-propres" title="Permalink to this headline">¬∂</a></h2>
<div class="admonition-proposition admonition">
<p class="admonition-title">Proposition</p>
<p>La direction de variance maximale dans <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> correspond au vecteur propre associ√© √† la plus grande valeur propre de <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}=\boldsymbol{X}^T\boldsymbol{X}\)</span>.</p>
</div>
<div class="caution dropdown admonition">
<p class="admonition-title">Preuve</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}\boldsymbol{v}^t\bar{\boldsymbol{X}}^T\bar{\boldsymbol{X}}\boldsymbol{v} &amp;= \boldsymbol{v}^t\boldsymbol{\Sigma}\boldsymbol{v} = \Big\langle\boldsymbol{v}, \boldsymbol{\Sigma}\boldsymbol{v} \Big\rangle\\ &amp; = \Bigg\langle\boldsymbol{v}, \Bigg[\sum_{k=1}^d \lambda_k    \langle \boldsymbol{v}, \boldsymbol{v_k}\rangle   \boldsymbol{v_k}\Bigg] \Bigg\rangle \\ &amp;= \sum_{k=1}^d \lambda_k    \langle \boldsymbol{v}, \boldsymbol{v_k}\rangle \langle \boldsymbol{v}, \boldsymbol{v_k}\rangle  \\ &amp;\leq \lambda_{\max} \underbrace{\sum_{k=1}^d     \langle \boldsymbol{v}, \boldsymbol{v_k}\rangle^2}_{=\lVert\boldsymbol{v}\rVert_2^2 = 1}\\ &amp;   = \lambda_{\max}\end{aligned}\end{split}\]</div>
<p>L‚Äô√©galit√© est ainsi obtenue pour le vecteur propre associ√© √† <span class="math notranslate nohighlight">\(\lambda_{max}\)</span>.</p>
</div>
<p>Nous avons donc¬†:</p>
<div class="math notranslate nohighlight">
\[\lambda_{\max}=\boldsymbol{v}_{\max}
^t\bar{\boldsymbol{X}}^T\bar{\boldsymbol{X}}\boldsymbol{v}_{\max}\]</div>
<p>Nous devons partir √† la recherche de ce vecteur propre particulier pour r√©soudre notre probl√®me. Pour cela nous allons voir une m√©thode it√©rative pratique permettant de retrouver le vecteur associ√© √† la plus grande valeur propre d‚Äôune matrice&amp;nbsp: <strong>l‚Äôalgorithme des puissances it√©r√©es</strong>.</p>
</div>
<div class="section" id="iv-calculer-les-vecteurs-et-valeurs-propres">
<h2>IV. Calculer les vecteurs et valeurs propres<a class="headerlink" href="#iv-calculer-les-vecteurs-et-valeurs-propres" title="Permalink to this headline">¬∂</a></h2>
<div class="section" id="a-l-algorithme-des-puissances-iterees">
<h3>A. L‚Äôalgorithme des puissances it√©r√©es<a class="headerlink" href="#a-l-algorithme-des-puissances-iterees" title="Permalink to this headline">¬∂</a></h3>
<p>En r√©utilisant les propri√©t√©s vues √† propos des matrices diagonalisables, nous pouvons essayer de construire un algorithme qui nous permettra de calculer le vecteur propre associ√© √† la plus grande valeur propre¬†:</p>
<div class="math notranslate nohighlight">
\[\underbrace{\boldsymbol{A}\boldsymbol{A}\dots\boldsymbol{A}}_{\times n}\boldsymbol{v} = \boldsymbol{A}^n\boldsymbol{v} =  \sum_{k=1}^d (\lambda_k)^n  \langle \boldsymbol{v_k}, \boldsymbol{v}\rangle   \boldsymbol{v_k}\]</div>
<p>Et on voit que c‚Äôest la contribution du vecteur propre associ√©e √† la plus forte valeur propre qui va dominer assymptotiquement. Plus formellement, si on √©crit les valeurs propres sous la formes <span class="math notranslate nohighlight">\(\lambda_k = \lambda_{\max}\frac{\lambda_{k}}{\lambda_{\max}}\)</span>¬†:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{A}^n\boldsymbol{v} = (\lambda_{\max})^n \sum_{k=1}^d \Big(\frac{\lambda_{k}}{\lambda_{\max}}\Big)^n  \langle \boldsymbol{v_k}, \boldsymbol{v}\rangle   \boldsymbol{v_k},\]</div>
<p>on obtient <span class="math notranslate nohighlight">\(\lim_{n \rightarrow \infty} (\frac{\lambda_{k}}{\lambda_{\max}})^n = 1\)</span> si <span class="math notranslate nohighlight">\(\lambda_{k} = \lambda_{\max}\)</span> et <span class="math notranslate nohighlight">\(0\)</span> sinon. Ainsi¬†:</p>
<div class="math notranslate nohighlight">
\[\lim_{n \rightarrow \infty}\frac{\boldsymbol{A}^n\boldsymbol{v}}{\lVert\boldsymbol{A}^n\boldsymbol{v}\rVert_2} = \boldsymbol{v_{\max}}\]</div>
<p>En multipliant it√©rativement (presque) n‚Äôimporte quel vecteur d‚Äôentr√©e par notre application lin√©aire, nous construisons une suite dont l‚Äôexpression normalis√©e converge asymptotiquement vers le vecteur propre associ√© √† la valeur propre la plus forte. C‚Äôest exactement ce qu‚Äôon fait en pratique en alternant multiplication et normalisation afin d‚Äô√©viter un <span class="math notranslate nohighlight">\(\texttt{overflow}\)</span> de nos variable. L‚Äôinitialisation est faite al√©atoirement¬†:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\boldsymbol{v}(0) \leftarrow \mathcal{N}\Big(\boldsymbol{0}, \boldsymbol{I}\Big).
\end{equation*}\]</div>
<p>Et chaque it√©ration a la forme suivante¬†:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\boldsymbol{v}(n+1) =  \frac{\boldsymbol{A}\boldsymbol{v}(n)}{\lVert\boldsymbol{A}\boldsymbol{v}(n)\rVert_2} 
\end{equation*}\]</div>
<div class="admonition-question admonition">
<p class="admonition-title">Question</p>
<p><strong>Maintenant que nous avons un algorithme pour trouver la composante principale, comment faire pour trouver les autres ?</strong></p>
</div>
</div>
<div class="section" id="b-en-pratique">
<h3>B. En pratique<a class="headerlink" href="#b-en-pratique" title="Permalink to this headline">¬∂</a></h3>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Completer le code des m√©thodes <span class="math notranslate nohighlight">\(\text{power_iteration}\)</span> et <span class="math notranslate nohighlight">\(\text{fit}\)</span> de la classe <em>ACP</em> ci-dessous qui implemente le calcul d‚Äôune ACP. La m√©thode <span class="math notranslate nohighlight">\(\text{power_iteration}\)</span> doit retourner un tuple contenant le vecteur et sa valeur propre associ√©e. La m√©thode <span class="math notranslate nohighlight">\(\text{fit}\)</span> doit ex√©cuter les puissances it√©r√©es autant de fois que demand√©. C‚Äôest-√†-dire <span class="math notranslate nohighlight">\(\texttt{n}\_\texttt{components}\)</span> fois.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ACP</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span> <span class="o">=</span> <span class="n">n_components</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter</span>

    <span class="k">def</span> <span class="nf">order_vectors</span><span class="p">(</span><span class="bp">self</span><span class="p">,):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Must be fitted before&quot;</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">eigen_values</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eigen_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_values</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">]</span>
    
    <span class="c1">#### Complete the code here #### or die #######################################</span>
    <span class="k">def</span> <span class="nf">power_iteration</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">A</span><span class="p">):</span>
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="k">return</span> <span class="n">vector</span><span class="p">,</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">X_n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_n</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X_n</span><span class="p">)</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">))</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">eigen_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">)</span> 
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">order_vectors</span><span class="p">()</span>
    <span class="c1">###############################################################################</span>
    
            
    <span class="k">def</span> <span class="nf">zeros_pad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">Z</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">Z</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">-</span> <span class="n">k</span><span class="p">))]</span>
        
    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="c1">#### Complete the code here #### or die #######################################</span>
        <span class="c1"># pour un exercice plus bas</span>
        <span class="c1">#</span>
        <span class="c1">#</span>
        <span class="c1">###############################################################################</span>
        <span class="k">pass</span>
        
    
    <span class="k">def</span> <span class="nf">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="c1">#### Complete the code here #### or die #######################################</span>
        <span class="c1"># pour un exercice plus bas</span>
        <span class="c1">#</span>
        <span class="c1">#</span>
        <span class="c1">###############################################################################</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">compress</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">k</span><span class="p">)</span>
</pre></div>
</div>
<p>Appliquons notre m√©thode √† nos donn√©es. On affichera le rep√®re associ√© aux composantes principales trouv√©s par notre m√©thode¬†:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">acp</span> <span class="o">=</span> <span class="n">ACP</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">acp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">vec</span> <span class="o">=</span> <span class="n">acp</span><span class="o">.</span><span class="n">eigen_vectors</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>En pratique, les calculs ne sont pas aussi directs et certaines d√©compositions sont utilis√©es afin d‚Äôacc√©l√©rer et de stabiliser les calculs.</p>
</div>
</div>
<div class="section" id="v-calcul-des-vecteurs-de-representation-codage-et-reconstructrion-decodage">
<h2>V. Calcul des vecteurs de repr√©sentation (codage) et reconstructrion (d√©codage)<a class="headerlink" href="#v-calcul-des-vecteurs-de-representation-codage-et-reconstructrion-decodage" title="Permalink to this headline">¬∂</a></h2>
<div class="section" id="a-compression">
<h3>A. Compression<a class="headerlink" href="#a-compression" title="Permalink to this headline">¬∂</a></h3>
<p>Nous venons donc de trouver une base telle que les projections des donn√©es dans ce nouveau syst√®me de coordonn√©es ont une variance maximale. Rappelons nous que l‚Äôobjectif de l‚Äôapprentissage non-supervis√© de repr√©sentation √† pour objectif de trouver un mapping, une fonction de codage <span class="math notranslate nohighlight">\(\Phi : \boldsymbol{x} \rightarrow \boldsymbol{z} = \Phi(\boldsymbol{x}) :  \mathbb{R}^d \rightarrow \mathbb{R}^K\)</span>, et dans notre cas nous pouvons d√©finir <span class="math notranslate nohighlight">\(\Phi\)</span> comme l‚Äôapplication lin√©aire suivante¬†:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{z} = \hat{\boldsymbol{V}}_K^T \boldsymbol{x} \in \mathbb{R}^K\]</div>
<p>o√π <span class="math notranslate nohighlight">\(\hat{\boldsymbol{V}}_K^T \in \mathbb{R}^{d \times K}\)</span> correspond √† la matrice  <span class="math notranslate nohighlight">\(\boldsymbol{V}\)</span> de laquelle on a retir√©e les <span class="math notranslate nohighlight">\(d-K\)</span> derniers vecteurs colonnes (on part du principe que les vecteur colonne sont tri√©s par ordre d√©croissant de leur valeur propre associ√©e). Nous avons donc¬†:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\hat{\boldsymbol{V}}_K = 
\begin{bmatrix}
    \vert &amp;   &amp; \vert    \\
    \vert &amp;   &amp; \vert    \\
    \vert &amp;   &amp; \vert \\
    \boldsymbol{v_1}   &amp; \dots &amp; \boldsymbol{v_K}   \\
    \vert &amp;   &amp; \vert    \\
    \vert &amp;  &amp; \vert 
\end{bmatrix}\end{split}\]</div>
<div class="admonition-question admonition">
<p class="admonition-title">Question</p>
<p><strong>Exprimez une application ‚Äúinverse‚Äù (fonction de d√©codage), qu‚Äôon notera <span class="math notranslate nohighlight">\(\Psi\)</span>, qui permettrait de reprojeter la representation <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span> dans l‚Äôespace d‚Äôorigine. On notera <span class="math notranslate nohighlight">\(\hat{\boldsymbol{x}} = \Psi(\boldsymbol{z})\)</span> la reconstruction du vecteur d‚Äôorigine.</strong></p>
</div>
<p>Il est ainsi √©vident que si toutes les composantes sont conserv√©es, on retrouve <span class="math notranslate nohighlight">\(\hat{\boldsymbol{x}} = (\Psi \circ \Phi) ( \boldsymbol{x}) =  \underbrace{\boldsymbol{V}\boldsymbol{V}^t}_{\boldsymbol{I_d}} \boldsymbol{x} = \boldsymbol{x}\)</span>.</p>
<p>On notera que comme <span class="math notranslate nohighlight">\(\hat{\boldsymbol{V}}_K\)</span> avec <span class="math notranslate nohighlight">\(K \leq d\)</span> n‚Äôest pas carr√©e de rang plein, elle n‚Äôest pas inversible, et l‚Äôapplication <span class="math notranslate nohighlight">\(\Phi\)</span> n‚Äôest pas bijective. Ainsi, <span class="math notranslate nohighlight">\(\Psi\)</span> ne peut pas √™tre l‚Äôinverse de <span class="math notranslate nohighlight">\(\Phi\)</span>. Nous avions donc plusieurs choix pour <span class="math notranslate nohighlight">\(\Psi\)</span>. Mais nous pouvons montrer que celui que nous avons fait minimise l‚Äôerreur de d√©codage¬†: c‚Äôest une propri√©t√© souhaitable pour ce genre de probl√®me en basse dimension.</p>
<hr class="docutils" />
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Dans le code de l‚ÄôACP ci-dessus, compl√©tez les methodes <span class="math notranslate nohighlight">\(\text{transform}\)</span> et <span class="math notranslate nohighlight">\(\text{inverse_transform}\)</span>.</strong></p>
</div>
<hr class="docutils" />
<p>Appliquons ensuite le code ci dessous pour visualiser les entr√©es reconstruites (en rouge) par dessus les donn√©es d‚Äôentr√©e (en bleu).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X_rec</span> <span class="o">=</span> <span class="n">acp</span><span class="o">.</span><span class="n">compress</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># on ne projette que sur une composante</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">vec</span> <span class="o">=</span> <span class="n">acp</span><span class="o">.</span><span class="n">eigen_vectors</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X_rec</span> <span class="o">=</span> <span class="n">X_rec</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="b-interpretation-de-l-erreur-de-reconstruction-moyenne">
<h3>B. Interpr√©tation de l‚Äôerreur de reconstruction moyenne<a class="headerlink" href="#b-interpretation-de-l-erreur-de-reconstruction-moyenne" title="Permalink to this headline">¬∂</a></h3>
<p>Nous allons voir ici que la fonction de codage/d√©codage correspondant √† l‚ÄôACP correspond √† un endomorphisme qui est un projecteur orthogonal. De plus la diff√©rence moyenne entre le vecteur d‚Äôorigine et son projet√© orthogonal poss√®de la propri√©t√© int√©ressante suivante¬†:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
        \text{err} &amp;= \frac{1}{N}\sum_i^N||\boldsymbol{x}_i - \hat{\boldsymbol{V}}_K\hat{\boldsymbol{V}}_K^t \boldsymbol{x}_i||_2^2\\&amp;=
        \frac{1}{N}\sum_i^N||\sum_{k=1}^d\langle\boldsymbol{x}_i, \boldsymbol{v}_k \rangle \boldsymbol{v}_k - \sum_{k=1}^K\langle\boldsymbol{x}_i, \boldsymbol{v}_k \rangle \boldsymbol{v}_k||_2^2\\ &amp;=
         \frac{1}{N}\sum_i^N||\sum_{k=K+1}^d\langle\boldsymbol{x}_i, \boldsymbol{v}_k \rangle \boldsymbol{v}_k||_2^2 =
         \frac{1}{N}\sum_i^N \sum_{k=K+1}^d\langle\boldsymbol{x}_i, \boldsymbol{v}_k \rangle^2 \\ &amp;=
         \frac{1}{N}\sum_{k=K+1}^d \boldsymbol{v}_k^t \boldsymbol{X}^T\boldsymbol{X}\boldsymbol{v}_k = \sum_{k=K+1}^d \boldsymbol{v}_k^t \boldsymbol{\Sigma}\boldsymbol{v}_k\\ =&amp;
         \sum_{k=K+1}^d \lambda_k \xrightarrow{N\rightarrow\infty}\sum_{k=K+1}^d \text{Var}_{\boldsymbol{z}}[z_k]
\end{aligned}\end{split}\]</div>
<p>Nous pouvons observer qu‚Äô√† l‚Äô√©tape 3, le vecteur diff√©rence est forc√©ment un vecteur de <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> et est orthogonale √† tous les vecteurs propres s√©lectionn√©s dans <span class="math notranslate nohighlight">\(\hat{\boldsymbol{V}}_K\)</span>. <span class="math notranslate nohighlight">\(\hat{\boldsymbol{V}}_K\hat{\boldsymbol{V}}_K^T\)</span> est donc un projecteur orthogonal sur le sous espace propre correspondant aux composantes s√©lectionn√©es. Il est celui qui minimise l‚Äôerreur de reconstruction. De plus nous constatons que cette erreur a une norme au carr√© moyenne qui est une estimation de la somme des variances dans les directions non pertinentes.</p>
<p>En pratique on pourra donc choisir le nombre de composante √† conserver <span class="math notranslate nohighlight">\(K\)</span> de sorte √† ce que la qualit√© de la reconstruction souhait√©e soit sup√©rieure √† un certain seuil <span class="math notranslate nohighlight">\(\tau\)</span>¬†:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
   \frac{\sum_{k=1}^K \lambda_k}{\sum_{k=1}^d \lambda_k} \geq \tau \in [0,1]
\end{equation*}\]</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Completez les fonction <span class="math notranslate nohighlight">\(\text{compute_cumulative_var}\)</span> et <span class="math notranslate nohighlight">\(\text{find_thresholded_cumulative}\)</span> qui calcul respectivement l quantit√© exprimez pr√©c√©dement, et d√©termine le nombre de composante minimale pour avoir un <em>RMSE</em> d‚Äôau moins <span class="math notranslate nohighlight">\(\tau\)</span> (<em>tresh</em>).</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_error</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X_rec</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">X_rec</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">compute_cumulative_var</span><span class="p">(</span><span class="n">eigen_values</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="c1">#### Complete the code here #### or die #######################################</span>
    <span class="o">...</span>
    <span class="o">...</span>
    <span class="c1">###############################################################################</span>

<span class="k">def</span> <span class="nf">find_thresholded_cumulative</span><span class="p">(</span><span class="n">eigen_values</span><span class="p">,</span> <span class="n">thresh</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">):</span>
    <span class="c1">#### Complete the code here #### or die #######################################</span>
    <span class="o">...</span>
    <span class="o">...</span>
    <span class="c1">###############################################################################</span>
</pre></div>
</div>
<p>On peut maintenant construire un jeu de donn√©e de plus haute dimension et tester notre code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">sample_multivariate_gaussian_data</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">acp</span> <span class="o">=</span> <span class="n">ACP</span><span class="p">()</span>
<span class="n">acp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">cumul</span> <span class="o">=</span> <span class="p">[</span><span class="n">compute_cumulative_var</span><span class="p">(</span><span class="n">acp</span><span class="o">.</span><span class="n">eigen_values</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>

<span class="n">K</span> <span class="o">=</span> <span class="n">find_thresholded_cumulative</span><span class="p">(</span><span class="n">acp</span><span class="o">.</span><span class="n">eigen_values</span><span class="p">,</span> <span class="n">thresh</span> <span class="o">=</span> <span class="mf">0.95</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">(</span><span class="n">K</span><span class="p">)</span><span class="o">/</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cumul</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">cumul</span><span class="p">,</span> 
         <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Erreur de reconstruction&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>On voit ici que sulement 0.7% des composantes contiennent 95% de l‚Äôinformation.</p>
</div>
</div>
<div class="section" id="v-compression-d-une-base-d-images">
<h2>V. Compression d‚Äôune base d‚Äôimages<a class="headerlink" href="#v-compression-d-une-base-d-images" title="Permalink to this headline">¬∂</a></h2>
<div class="section" id="a-les-donnees-et-notre-acp">
<h3>A. Les donn√©es et notre ACP<a class="headerlink" href="#a-les-donnees-et-notre-acp" title="Permalink to this headline">¬∂</a></h3>
<p>Dans cette section nous allons tester notre algorithme d‚ÄôACP sur des donn√©es d‚Äôimages brutes en consid√©rant chaque image de dimension <span class="math notranslate nohighlight">\((w \times h)\)</span> comme un vecteur <span class="math notranslate nohighlight">\(\boldsymbol{x} \in \mathbb{R}^{wh}\)</span>. Nous proc√©derons sur une base d‚Äôimages de visages centr√©s en niveau de gris¬†: la base <span class="math notranslate nohighlight">\(\text{Oliveti Faces}\)</span>. Commen√ßons dans un premier temps par charger les donn√©es, les afficher et appliquons notre algorthme d‚ÄôACP (on prendra plut√¥t la version <span class="math notranslate nohighlight">\(\text{np.linalg.eig}\)</span> pour calculer les vecteur propres qui est beaucoup plus stables et efficace computationellement que la notre).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ACP</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">order_vectors</span><span class="p">(</span><span class="bp">self</span><span class="p">,):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Must be fitted before&quot;</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">eigen_values</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eigen_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_values</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">X_n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_n</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X_n</span><span class="p">)</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">eigen_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eigen_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_values</span><span class="o">.</span><span class="n">real</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors</span><span class="o">.</span><span class="n">real</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">order_vectors</span><span class="p">()</span>
        
    <span class="k">def</span> <span class="nf">zeros_pad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">Z</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">Z</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">-</span> <span class="n">k</span><span class="p">))]</span>
        
    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;You need to fit the model first !&quot;</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">k</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors</span><span class="p">[:,</span> <span class="p">:</span><span class="n">k</span><span class="p">])</span>
    
    <span class="k">def</span> <span class="nf">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;You need to fit the model first !&quot;</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">k</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">zeros_pad</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span>

    <span class="k">def</span> <span class="nf">compress</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">k</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
 
<span class="c1"># Load data</span>
<span class="n">lfw_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">fetch_olivetti_faces</span><span class="p">()</span> <span class="c1">#datasets.fetch_lfw_people(min_faces_per_person=500)</span>
 
<span class="n">_</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">lfw_dataset</span><span class="o">.</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">lfw_dataset</span><span class="o">.</span><span class="n">data</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(400, 4096)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualization</span>
<span class="k">def</span> <span class="nf">plot_images</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">rows</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">cols</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span><span class="mi">64</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">rows</span> <span class="o">*</span> <span class="n">cols</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">title</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span> <span class="o">+</span> <span class="s2">&quot; : &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(())</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(())</span>

<span class="k">def</span> <span class="nf">plot_images_one_by_one</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">original</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(())</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(())</span>
        <span class="k">if</span> <span class="n">title</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span> <span class="o">+</span> <span class="s2">&quot; : &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">*</span><span class="n">step</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">original</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(())</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(())</span>
        <span class="k">if</span> <span class="n">title</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Original image&#39;</span><span class="p">)</span>
        

            
<span class="n">plot_images</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_principal_component_analysis_40_0.png" src="../_images/1_principal_component_analysis_40_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">acp</span> <span class="o">=</span> <span class="n">ACP</span><span class="p">()</span>
<span class="n">acp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="b-affichage-des-variances-cumullees-et-calcul-du-seuil-de-reconstruction">
<h3>B. Affichage des variances cumull√©es et calcul du seuil de reconstruction<a class="headerlink" href="#b-affichage-des-variances-cumullees-et-calcul-du-seuil-de-reconstruction" title="Permalink to this headline">¬∂</a></h3>
<p>Nous calculons ensuite le nombre de composantes √† conserver pour obtenir une qualit√© de reconstruiuction moyenne de <span class="math notranslate nohighlight">\(95\%\)</span> de <span class="math notranslate nohighlight">\(\text{RMSE}\)</span>. On voit ici que l‚Äôon peut ne conserver que <span class="math notranslate nohighlight">\(2 \%\)</span> des variables d‚Äôentr√©es pour avoir une qualit√© moyenne de reconstruction de <span class="math notranslate nohighlight">\(95 \%\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cumul</span> <span class="o">=</span> <span class="p">[</span><span class="n">compute_cumulative_var</span><span class="p">(</span><span class="n">acp</span><span class="o">.</span><span class="n">eigen_values</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>

<span class="n">K</span> <span class="o">=</span> <span class="n">find_thresholded_cumulative</span><span class="p">(</span><span class="n">acp</span><span class="o">.</span><span class="n">eigen_values</span><span class="p">,</span> <span class="n">thresh</span> <span class="o">=</span> <span class="mf">0.95</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">K</span><span class="p">)</span><span class="o">/</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cumul</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">cumul</span><span class="p">,</span> 
         <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Erreur de reconstruction&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="c-affichage-des-vecteur-propres-les-eigen-faces">
<h3>C. Affichage des vecteur propres (les eigen faces)<a class="headerlink" href="#c-affichage-des-vecteur-propres-les-eigen-faces" title="Permalink to this headline">¬∂</a></h3>
<p>Affichons les vecteurs propres appris sur ce jeu de donn√©es. On remarquera qu‚Äôon peut interpreter ces vecteurs propres coommes des images et donc qu‚Äôon peut les afficher comme tel. On affichera un sous ensemble dans un soucis de lisibilit√©. Une mani√®re d‚Äôinterpreter ces images est donc que tout visage de la base peut √™tre reconstruit sans erreur comme une somme pond√©r√©e des ces visages ‚Äú√©l√©mentaires‚Äù.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plot_images</span><span class="p">(</span><span class="n">acp</span><span class="o">.</span><span class="n">eigen_vectors</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="d-visualisation-perceptuelle-de-la-qualite-de-reconstruction">
<h3>D. Visualisation perceptuelle de la qualit√© de reconstruction<a class="headerlink" href="#d-visualisation-perceptuelle-de-la-qualite-de-reconstruction" title="Permalink to this headline">¬∂</a></h3>
<p>Nous pouvons aussi nous amuser visualiser diff√©rentes versions reconstruites d‚Äôune m√™me image en ne conservant qu‚Äôune certaine proportion des composantes <span class="math notranslate nohighlight">\(k \in [1, K]\)</span>. On voit ici que, perceptuellement, m√™me sans aller jusqu‚Äôau nombre de composantes seuil d√©finit pr√©c√©dement, on peut tr√®s rapidmeent converger vers l‚Äôimage originale avec peu de composantes.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">idx</span> <span class="o">=</span> <span class="mi">14</span><span class="c1">#np.random.randint(X.shape[0])</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">compressed_images</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">acp</span><span class="o">.</span><span class="n">compress</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="mi">20</span><span class="p">)])</span>

<span class="n">plot_images_one_by_one</span><span class="p">(</span><span class="n">compressed_images</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> 
                       <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Progressive reconstruction </span><span class="se">\n</span><span class="s2"> n_components&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="e-essayons-les-memes-etapes-sur-mnist">
<h3>E. Essayons les m√™mes √©tapes sur mnist<a class="headerlink" href="#e-essayons-les-memes-etapes-sur-mnist" title="Permalink to this headline">¬∂</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_openml</span>
<span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">fetch_openml</span><span class="p">(</span><span class="s1">&#39;mnist_784&#39;</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">as_frame</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">plot_images</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_principal_component_analysis_52_0.png" src="../_images/1_principal_component_analysis_52_0.png" />
</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">acp</span> <span class="o">=</span> <span class="n">ACP</span><span class="p">()</span>
<span class="n">acp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1">#errors = [compute_error(X, x_rec) for x_rec in [acp.compress(X, k=k) for k in range(X.shape[1])]]</span>
<span class="n">cumul</span> <span class="o">=</span> <span class="p">[</span><span class="n">compute_cumulative_var</span><span class="p">(</span><span class="n">acp</span><span class="o">.</span><span class="n">eigen_values</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cumul</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">cumul</span><span class="p">,</span> 
    <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Erreur de reconstruction&#39;</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plot_images</span><span class="p">(</span><span class="n">acp</span><span class="o">.</span><span class="n">eigen_vectors</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">K</span> <span class="o">=</span> <span class="n">find_thresholded_cumulative</span><span class="p">(</span><span class="n">acp</span><span class="o">.</span><span class="n">eigen_values</span><span class="p">,</span> <span class="n">thresh</span> <span class="o">=</span> <span class="mf">0.90</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">K</span><span class="p">)</span><span class="o">/</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">idx</span> <span class="o">=</span> <span class="mi">142</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">compressed_images</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">acp</span><span class="o">.</span><span class="n">compress</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="mi">10</span><span class="p">)])</span>

<span class="n">plot_images_one_by_one</span><span class="p">(</span>
    <span class="n">compressed_images</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
    <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Progressive reconstruction </span><span class="se">\n</span><span class="s2"> n_components&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./7_unsupervised"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="0_propos_liminaire.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">L‚Äôapprentissage non-supervis√©</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="2_em_gaussin_mixture_model.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Mod√®le de M√©lange Gaussien et algorithme <em>Expectation-Maximization</em></p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Servajean, Leveau & Chailan<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>
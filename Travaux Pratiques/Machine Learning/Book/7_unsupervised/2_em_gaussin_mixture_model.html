
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Mod√®le de M√©lange Gaussien et algorithme Expectation-Maximization &#8212; Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Pr√©diction d‚Äôensembles" href="../8_set_prediction/0_propos_liminaire.html" />
    <link rel="prev" title="L‚ÄôAnalyse en Composantes Principales ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è" href="1_principal_component_analysis.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-72WWYCKNK6"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-72WWYCKNK6');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../0_requisite/rappels_python.html">
   Rappels de Python et Numpy
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../1_what_is_ml/0_propos_liminaire.html">
   <em>
    Machine Learning
   </em>
   , initiation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/1_introduction_ml.html">
     <em>
      Machine learning
     </em>
     et mal√©diction de la dimension
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/2_regression_and_classification_trees.html">
     Les arbres de r√©gression et de classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../2_regression/0_propos_liminaire.html">
   La
   <em>
    r√©gression
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/1_linear_regression.html">
     La r√©gression lin√©aire ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/2_optimization.html">
     L‚Äôoptimisation ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/3_interpolation.html">
     Interpolation ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/4_algo_proximal_lasso.html">
     Sous-diff√©rentiel et le cas du Lasso ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/5_least_square_qr.html">
     Les moindres carr√©s via une d√©composition QR (et plus)‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/6_ridge.html">
     Une analyse de la r√©gularisation Ridge ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../3_classification/0_propos_liminaire.html">
   La
   <em>
    classification
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/1_logistic_regression.html">
     La r√©gression logistique ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/2_fonctions_proxy.html">
     Les fonctions de perte (loss function) ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/3_bayes_classifier.html">
     Le classifieur de Bayes ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/4_VC_theory.html">
     Un mod√®le formel de l‚Äôapprentissage ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è (üíÜ‚Äç‚ôÇÔ∏è)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../4_kernel_methods/0_propos_liminaire.html">
   Les m√©thodes √† noyaux
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../4_kernel_methods/1_svm.html">
     Le SVM ou l‚Äôhypoth√®se max-margin ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5_ensembles/0_propos_liminaire.html">
   Les m√©thodes
   <em>
    ensemblistes
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/1_ensembles.html">
     M√©thodes ensemblistes ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/2_bayesian_linear_regression.html">
     Bayesian linear regression ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../6_deeplearning/0_propos_liminaire.html">
   <em>
    deep learning
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/1_autodiff.html">
     La diff√©rentiation automatique et un d√©but de
     <em>
      deep learning
     </em>
     ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/2_filters_representation.html">
     Filtres et espace de repr√©sentation des r√©seaux de neurones ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/3_probabilities_calibration.html">
     Calibration des probabilit√©s et quelques notions ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/4_regularization_deep.html">
     R√©gularisation en
     <em>
      deep learning
     </em>
     ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/5_transfer_multitask.html">
     <em>
      Transfer learning
     </em>
     et apprentissage multi-t√¢ches ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/6_adversarial.html">
     Les attaques adversaires ‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="0_propos_liminaire.html">
   L‚Äôapprentissage non-supervis√©
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="1_principal_component_analysis.html">
     L‚ÄôAnalyse en Composantes Principales ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Mod√®le de M√©lange Gaussien et algorithme
     <em>
      Expectation-Maximization
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../8_set_prediction/0_propos_liminaire.html">
   Pr√©diction d‚Äôensembles
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/1_well_defined.html">
     Jeu d‚Äôapprentissage
     <em>
      set-valued
     </em>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/2_ill_defined.html">
     Jeu d‚Äôapprentissage uniquement multi-classes ‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../biblio.html">
   Bibliographie
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/7_unsupervised/2_em_gaussin_mixture_model.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/maximiliense/lmiprp"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/maximiliense/lmiprp/issues/new?title=Issue%20on%20page%20%2F7_unsupervised/2_em_gaussin_mixture_model.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/maximiliense/lmiprp/blob/master/Travaux Pratiques/Machine Learning/Book/7_unsupervised/2_em_gaussin_mixture_model.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Mod√®le de M√©lange Gaussien et algorithme
   <em>
    Expectation-Maximization
   </em>
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#i-introduction">
   I. Introduction
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-generalites-sur-les-modeles-generatifs">
     A. G√©n√©ralit√©s sur les mod√®les g√©n√©ratifs
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-les-variables-cachees">
     B. Les variables cach√©es
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ii-le-modele-de-melange-gaussien-vu-sous-l-angle-des-modeles-a-variables-cachees">
   II. Le mod√®le de m√©lange Gaussien vu sous l‚Äôangle des mod√®les √† variables cach√©es
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-echantillonage-et-visualisation-d-un-melange-gaussien">
     A. Echantillonage et visualisation d‚Äôun m√©lange Gaussien
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-l-algortihme-em-pour-estimer-un-modele-de-melange-gaussien">
     B. L‚Äôalgortihme EM pour estimer un mod√®le de m√©lange gaussien
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#i-premiere-difference-avec-l-estimation-d-une-gaussienne-multivariee-simple">
       i. Premi√®re diff√©rence avec l‚Äôestimation d‚Äôune gaussienne multivari√©e simple
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ii-optimisation-des-boldsymbol-mu-k">
       ii. Optimisation des
       <span class="math notranslate nohighlight">
        \(\boldsymbol{\mu}_k\)
       </span>
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#iii-optimisation-des-boldsymbol-sigma-k">
       iii. Optimisation des
       <span class="math notranslate nohighlight">
        \(\boldsymbol{\Sigma}_k\)
       </span>
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#iv-optimisation-des-pi-k">
       iv. Optimisation des
       <span class="math notranslate nohighlight">
        \(\pi_k\)
       </span>
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#v-l-algortihme-em">
       V. L‚Äôalgortihme EM
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#c-justification-de-l-algorithme-em-approfondissement">
     C. Justification de l‚Äôalgorithme EM (approfondissement)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#i-retour-sur-le-cas-du-melange-gaussien">
       i. Retour sur le cas du m√©lange Gaussien
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ii-preuve-que-chaque-iteration-de-em-augmente-la-log-vraissemblance">
       ii. Preuve que chaque it√©ration de EM augmente la log-vraissemblance
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="modele-de-melange-gaussien-et-algorithme-expectation-maximization">
<h1>Mod√®le de M√©lange Gaussien et algorithme <em>Expectation-Maximization</em><a class="headerlink" href="#modele-de-melange-gaussien-et-algorithme-expectation-maximization" title="Permalink to this headline">¬∂</a></h1>
<div class="admonition-objectifs-de-la-sequence admonition">
<p class="admonition-title">Objectifs de la s√©quence</p>
<ul class="simple">
<li><p>√ätre sensibilis√©¬†:</p>
<ul>
<li><p>aux mod√®les √† variables cach√©es.</p></li>
</ul>
</li>
<li><p>√ätre capable¬†:</p>
<ul>
<li><p>d‚Äôimpl√©menter une GMM avec <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span>.</p></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="i-introduction">
<h1>I. Introduction<a class="headerlink" href="#i-introduction" title="Permalink to this headline">¬∂</a></h1>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/2_em_gaussin_mixture_model_2_0.png" src="../_images/2_em_gaussin_mixture_model_2_0.png" />
</div>
</div>
<p>Dans ce chapitre nous allons consid√©rer des m√©thodes d‚Äôapprentissage non-supervis√©es probabilistes et plus particuli√®rement des mod√®les g√©n√©ratifs dont l‚Äôid√©e g√©n√©rale est de consid√©rer l‚Äô√©chantillon de donn√©es observ√©es <span class="math notranslate nohighlight">\(\mathcal{S}_n = \{x_1,  \dots, x_n\}\)</span> comme des variable al√©atoires i.i.d de processus g√©n√©rateur <span class="math notranslate nohighlight">\(p_{\mathcal{D}} : x \sim p_{\mathcal{D}}\)</span>. Le cas du m√©lange gaussien est int√©ressant car il part du principe que nos √©chantillons sont simul√©es selon une loi dont la densit√© a la forme suivante¬†:</p>
<div class="math notranslate nohighlight">
\[q(x;\theta)=\sum_{j=1}^K\pi_jf(x;\mu_j,\sigma_j),\]</div>
<p>o√π <span class="math notranslate nohighlight">\(\theta=\{\pi, \mu, \sigma\}\)</span>. Le processus g√©n√©rateur d‚Äôun tel mod√®le, comme nous allons le voir, fonctionne de la mani√®re suivante¬†:</p>
<ol class="simple">
<li><p>On simule, selon une cat√©gorielle de param√®tre <span class="math notranslate nohighlight">\(\pi\)</span>,</p></li>
<li><p>On simule selon la loi <span class="math notranslate nohighlight">\(f\)</span> associ√©e au ‚Äúgroupe‚Äù choisi pr√©c√©demment.</p></li>
</ol>
<p>Notre objectif est de calculer <span class="math notranslate nohighlight">\(\theta\)</span> tel que la vraisemblance de nos donn√©es <span class="math notranslate nohighlight">\(\mathcal{S}_n\)</span> est maximis√©e. Cela se passe par des algorithmes comme le <em>Expectation-Maximization</em> qui introduiront une variable latente <span class="math notranslate nohighlight">\(z\)</span> indiquant le ‚Äúgroupe‚Äù utilis√© afin de g√©n√©rer nos √©chantillon. Dans ces approches, les repr√©sentations des donn√©es seront consid√©r√©es comme ces nouvelles variables al√©atoires <span class="math notranslate nohighlight">\(z\)</span> qu‚Äôon appellera des variables latentes (ou cach√©es) qui prendront leur valeur conjointement avec les donn√©es observ√©es <span class="math notranslate nohighlight">\(x\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Loi et processus g√©n√©rateur</p>
<p>On utilisera par abus de langage l‚Äôexpression <span class="math notranslate nohighlight">\(p\)</span> ou <span class="math notranslate nohighlight">\(q_{\theta}\)</span> pour d√©noter √† la fois le processus g√©n√©rateur des donn√©es, la loi de probabilit√© lui correspondant et la densit√© de probabilit√© associ√©e.</p>
</div>
<div class="section" id="a-generalites-sur-les-modeles-generatifs">
<h2>A. G√©n√©ralit√©s sur les mod√®les g√©n√©ratifs<a class="headerlink" href="#a-generalites-sur-les-modeles-generatifs" title="Permalink to this headline">¬∂</a></h2>
<p>Nous pouvons dans un premier temps introduire un objectif g√©n√©ral des mod√®les non-supervis√©s g√©n√©ratifs. Il s‚Äôagit de trouver dans une famille param√©trique de distributions de probabilit√© <span class="math notranslate nohighlight">\(\mathcal{Q}=\{q_\theta, \theta\in\mathbb{R}^p\}\)</span> une param√©trisation <span class="math notranslate nohighlight">\(q_{\theta}(x) = q(x|{\theta})\)</span> qui ‚Äúexplique le mieux‚Äù les donn√©es observ√©es. Comme pour le cas de l‚Äôapprentissage supervis√©, nous pouvons consid√©rer le crit√®re de maximum de vraisemblance. Nous cherchons donc¬†:</p>
<div class="math notranslate nohighlight">
\[\theta_{\text{ML}} = \arg \max_{\theta} \Big[q(\mathcal{S}_n|\theta)\Big] = \arg \max_{\theta } \Big[\prod_i q(x_i|\theta)\Big].\]</div>
<p>Ou bien, de mani√®re √©quivalente, le maximum de la log-vraissemblance¬†:</p>
<div class="math notranslate nohighlight">
\[\theta_{\text{ML}} = \arg \max_{\theta} \Big[\sum_i \ln\Big(q(x_i|\theta)\Big)\Big].\]</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p>Soit <span class="math notranslate nohighlight">\(\mathcal{S}_n\sim \mathcal{N}(\mu,1)^n\)</span> un √©chantillon simul√© selon une loi normale de variance <span class="math notranslate nohighlight">\(1\)</span> et de moyenne <span class="math notranslate nohighlight">\(\mu\)</span>. Quel est le <span class="math notranslate nohighlight">\(\mu\)</span> qui maximise la vraisemblance ?</p>
</div>
</div>
<div class="section" id="b-les-variables-cachees">
<h2>B. Les variables cach√©es<a class="headerlink" href="#b-les-variables-cachees" title="Permalink to this headline">¬∂</a></h2>
<p>Le calcul de ce maximum de vraisemblance dans le cas d‚Äôun mod√®le de m√©lange n√©cessite d‚Äôintroduire de nouvelles variables¬†: les variables cach√©es. Ces derni√®res seront les repr√©sentations de nos observations. Cela se fait en d√©marginalisant¬†</p>
<div class="math notranslate nohighlight">
\[q_{\theta}(x) = \int q_{\theta}(x,z)dz = \int q_{\theta}(z) q_{\theta}(x|z)dz\]</div>
<div class="tip admonition">
<p class="admonition-title">Remarque</p>
<p>On souhaite √©viter de tomber sur une distribution o√π les variables <span class="math notranslate nohighlight">\(z\)</span> et <span class="math notranslate nohighlight">\(x\)</span> sont ind√©pendantes¬†:</p>
<div class="math notranslate nohighlight">
\[q_{\theta}(x,z) = q_{\theta}(x)q_{\theta}(z) \Leftrightarrow q_{\theta}(z|x) = q_{\theta}(z)\]</div>
<p>car on souhaite d√©composer notre loi initiale en atomes qui seraient plus faciles √† estimer. Pour le m√©lange, si on ‚Äúconnait‚Äù le groupe d‚Äôun de nos points (ce serait notre <span class="math notranslate nohighlight">\(z\)</span>), alors on peut plus facilement estimer la loi du groupe (i.e. c‚Äôest l‚Äôestimation simple de la loi et non du m√©lange complet). De plus, dans le cas o√π les <span class="math notranslate nohighlight">\(x\)</span> seraient ind√©pendants des <span class="math notranslate nohighlight">\(z\)</span>, ce dernier ne pourrait en aucun cas √™tre une bonne repr√©sentation.</p>
</div>
<p>Nous allons donc chercher √† ajouter une variable latente <span class="math notranslate nohighlight">\(z\)</span> pertinente ! Dans le cas de variables cach√©es discr√®tes (comme c‚Äôest le cas dans ce TP)¬†:</p>
<div class="math notranslate nohighlight">
\[q_{\theta}(x) = \sum_{k=1}^K q_{\theta}(z=k)q_{\theta}(x|z=k)\]</div>
<p>Nous allons donc chercher √† maximiser la log-vraissemblance de la forme¬†:</p>
<div class="math notranslate nohighlight">
\[\theta_{\text{ML}} = \arg\max_{\theta } \Big[\sum_i \ln\Big(\sum_{k=1}^c q_{\theta}(z=k)q_{\theta}(x_i|z=k)\Big)\Big]\]</div>
<div class="tip admonition">
<p class="admonition-title">Remarque</p>
<p>Une fa√ßon de voir les variables cach√©es qui va nous int√©resser dans le cadre de l‚Äôapprentissage de repr√©sentation est comme des facteurs explicatifs des donn√©es. C‚Äôest-√†-dire un nombre restreint de variables telles qu‚Äôon peut expliquer/pr√©dire une observation sachant ces variables. On peut voir √ßa comme des facteurs causaux tels que si on connait la cause premi√®re d‚Äôune chose on peut en d√©duire une observation complexes qui en d√©coule ou du moins une part significative. En ce sens, les facteurs explicatifs peuvent √™tre consid√©r√©s comme une abstraction, une repr√©sentation plus structur√©e et compacte des observations. Ainsi une mani√®re de voir le processus g√©n√©rateur des donn√©es est d‚Äô√©chantillonner la cause, la valeur du facteur explicatif <span class="math notranslate nohighlight">\(z_i \sim p(z)\)</span>, puis sachant la cause, √©chantillonner l‚Äôobservation qui en d√©coule par la conditionnelle <span class="math notranslate nohighlight">\(x_i \sim p(x |z=z_i)\)</span>.</p>
</div>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="ii-le-modele-de-melange-gaussien-vu-sous-l-angle-des-modeles-a-variables-cachees">
<h1>II. Le mod√®le de m√©lange Gaussien vu sous l‚Äôangle des mod√®les √† variables cach√©es<a class="headerlink" href="#ii-le-modele-de-melange-gaussien-vu-sous-l-angle-des-modeles-a-variables-cachees" title="Permalink to this headline">¬∂</a></h1>
<p>La famille des mixtures de distributions mettent en jeu des densit√©s de probabilit√© sous la forme d‚Äôune combinaison pond√©r√©e de densit√©s d‚Äôune m√™me famille. Dans ce TP nous allons nous concentrer sur le cas des mixtures de gaussiennes multi-vari√©es. Les mod√®les de mixtures de gausiennes consistent √† mod√©liser une densit√© de probabilit√© o√π les donn√©es sont distribu√©es localement selon des distributions normales multi-vari√©es. Dans ce cas, l‚Äôindice de la gaussienne √† laquelle apaprtient une donn√©e particuli√®re est la variable cach√©e qui lui est associ√©e. La totalit√© de l‚Äôinformation r√©siduelle est ensuite port√©e par la densit√© de probabilit√© de la composante gaussienne. En notant <span class="math notranslate nohighlight">\(z=k\)</span> l‚Äôidentifiant d‚Äôune gausienne qui poss√®de ces param√®tres propres <span class="math notranslate nohighlight">\((\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)\)</span>, la densit√© de probabilit√© du processus g√©n√©rateur des donn√©es prend la forme suivante¬†:</p>
<div class="math notranslate nohighlight">
\[p(\boldsymbol{x}; \boldsymbol{\mu},\boldsymbol{\Sigma}) = \sum_{Z=k}^K p(z=k) f(\boldsymbol{x};\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)\]</div>
<p>o√π on notera par la suite <span class="math notranslate nohighlight">\(p(z=k) = \pi_k\)</span> qui correspond √† la probabilit√© a priori de la <span class="math notranslate nohighlight">\(k\)</span>-i√®me composante (on a donc <span class="math notranslate nohighlight">\(\sum_k \pi_k = 1\)</span>) et <span class="math notranslate nohighlight">\(f(\boldsymbol{x};\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)\)</span> correspond √† la densit√© de probabilit√© de la loi normale multi-vari√©e <span class="math notranslate nohighlight">\(\mathcal{N}(\boldsymbol{x}|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)\)</span> de moyenne et matrice de variance-covariance <span class="math notranslate nohighlight">\((\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)\)</span>¬†:</p>
<div class="math notranslate nohighlight">
\[f(\boldsymbol{x};\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) = \frac{1}{(2\pi)^{\frac{d}{2}}|\boldsymbol{\Sigma}_k|^{\frac{1}{2}}} \exp^{-\frac{1}{2}(\boldsymbol{x} - \boldsymbol{\mu}_k)^T)\boldsymbol{\Sigma}_k^{-1}(\boldsymbol{x} - \boldsymbol{\mu}_k)}\]</div>
<p>Il s‚Äôagit de la densit√© de probabilit√© d‚Äôobserver une variable <span class="math notranslate nohighlight">\(x\)</span> sachant la valeur de la variable cach√©e que l‚Äôon pourra noter <span class="math notranslate nohighlight">\(f_k(\boldsymbol{x})\)</span> par la suite par soucis de lisibilit√©. On peut aussi calculer quelques quantit√©s d‚Äôint√©r√™t qui vont nous servir par la suite comme la densit√© de probabilit√© de la variable cach√©e a posteriori de l‚Äôobservation <span class="math notranslate nohighlight">\(x\)</span> grace √† la formule de Bayes¬†:</p>
<div class="math notranslate nohighlight">
\[p(z=k|x) = \frac{p(z=k)p(x|z=k)}{\sum_j p(z=j)p(x|z=j)} = \frac{\pi_k f(\boldsymbol{x};\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_j \pi_j f(\boldsymbol{x};\boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}\]</div>
<div class="section" id="a-echantillonage-et-visualisation-d-un-melange-gaussien">
<h2>A. Echantillonage et visualisation d‚Äôun m√©lange Gaussien<a class="headerlink" href="#a-echantillonage-et-visualisation-d-un-melange-gaussien" title="Permalink to this headline">¬∂</a></h2>
<p>Comme expliqu√© pr√©c√©demment, on peut donc voir le processus g√©n√©rateur correspondant √† une mixture gaussienne comme se structurant en deux √©tapes. La premi√®re √©tape consiste en une variable cat√©gorielle √† <span class="math notranslate nohighlight">\(K\)</span> valeurs dont les probabilit√©s respectives sont les <span class="math notranslate nohighlight">\(\pi_k\)</span>. Puis connaissant la valeur tir√©e <span class="math notranslate nohighlight">\(k\)</span>; il s‚Äôagit ensuite d‚Äô√©chantillonner selon une loi normale multivari√©e de param√®tres <span class="math notranslate nohighlight">\((\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)\)</span>. Le code ce dessous nous permet selon ce proc√©d√© d‚Äô√©chantillonnage de g√©n√©rer et visualiser un jeu de donn√©es distribu√© selon une mixture de gaussienne ou les composantes ont √©t√© choisies avec des param√®tres al√©atoires.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">multivariate_normal</span> 
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">12.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">MultivariateGaussianMixture</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>        
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">probas</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span> <span class="o">=</span> <span class="n">n_components</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">if</span> <span class="n">probas</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">probas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_components</span><span class="p">)</span><span class="o">/</span><span class="n">n_components</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">probas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">probas</span><span class="p">)</span>
            <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">probas</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">s</span> <span class="o">&lt;</span> <span class="mf">1.0</span> <span class="o">+</span> <span class="mf">1e-10</span> <span class="ow">and</span> <span class="n">s</span> <span class="o">&gt;</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="s2">&quot;Probabilites should add up to 1, but s = </span><span class="si">%f</span><span class="s2">&quot;</span><span class="o">%</span><span class="k">s</span>

        <span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">scater</span> <span class="o">=</span> <span class="mi">10</span>
        <span class="n">sigma</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_components</span><span class="p">):</span>
            <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_rotation</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">sigma</span> <span class="p">),</span> <span class="n">Q</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">scater</span><span class="p">,</span><span class="n">scater</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="n">d</span><span class="p">))</span>
            
    <span class="k">def</span> <span class="nf">random_rotation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
        <span class="n">Q</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">d</span><span class="p">,</span><span class="n">d</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">Q</span>
    
    <span class="k">def</span> <span class="nf">sample_categorical</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
        <span class="n">idx_sort</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">probas</span><span class="p">)</span>
        <span class="n">probas_sort</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">probas</span><span class="p">[</span><span class="n">idx_sort</span><span class="p">]</span>

        <span class="n">c_probas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">probas_sort</span><span class="p">[:</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">)])</span>   
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="n">c</span> <span class="o">=</span> <span class="n">c_probas</span><span class="o">-</span><span class="n">x</span>
            <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx_sort</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="n">c</span><span class="o">&gt;=</span><span class="mi">0</span><span class="p">])]</span>
        <span class="k">return</span> <span class="n">X</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">generate_samples</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_categorical</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="p">[</span><span class="n">z</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="p">[</span><span class="n">z</span><span class="p">])</span> <span class="k">for</span> <span class="n">z</span> <span class="ow">in</span> <span class="n">Z</span><span class="p">]),</span> <span class="n">Z</span>
    
    
<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">means</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">gmd</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_std</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">cumul</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="c1"># Plot the dataset</span>
    <span class="k">if</span> <span class="n">c</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
        
    <span class="c1"># Plot the means</span>
    <span class="k">if</span> <span class="n">means</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">means</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="p">[</span><span class="n">m</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="n">s</span><span class="o">=</span><span class="mi">55</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>

    <span class="c1"># Plot the concentric ellipses to vizualize covariance matrixes of the components</span>
    <span class="k">if</span> <span class="n">gmd</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stds</span> <span class="o">=</span> <span class="p">[</span><span class="n">n_std</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">cumul</span><span class="p">:</span>
            <span class="n">stds</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_std</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>

        <span class="k">for</span> <span class="n">std</span> <span class="ow">in</span> <span class="n">stds</span><span class="p">:</span>
            <span class="c1"># For each component, we look for the maximum variance direction carried by the eigen vectors</span>
            <span class="c1"># The parameters of the elipse are defined by the std deviations ssociated to the sqrt of the eigen values.</span>
            <span class="k">for</span> <span class="n">mu_k</span><span class="p">,</span> <span class="n">sigma_k</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">gmd</span><span class="o">.</span><span class="n">mu</span><span class="p">,</span> <span class="n">gmd</span><span class="o">.</span><span class="n">sigma</span><span class="p">):</span>
                <span class="n">e</span><span class="p">,</span><span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">sigma_k</span><span class="p">)</span>
                <span class="n">top_e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="o">-</span><span class="n">e</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

                <span class="n">a</span><span class="o">=</span><span class="n">std</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">e</span><span class="p">[</span><span class="n">top_e</span><span class="p">])</span>
                <span class="n">b</span><span class="o">=</span><span class="n">std</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">e</span><span class="p">[</span><span class="n">top_e</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

                <span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
                <span class="n">Ell</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">a</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="p">,</span> <span class="n">b</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">t</span><span class="p">)])</span>  

                <span class="n">cos_rot</span><span class="o">=</span><span class="n">w</span><span class="p">[</span><span class="n">top_e</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="n">top_e</span><span class="p">])</span>
                <span class="n">sin_rot</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">cos_rot</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
                <span class="n">R_rot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">cos_rot</span> <span class="p">,</span> <span class="o">-</span><span class="n">sin_rot</span><span class="p">],[</span><span class="n">sin_rot</span> <span class="p">,</span><span class="n">cos_rot</span><span class="p">]])</span>

                <span class="n">Ell_rot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="n">Ell</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Ell</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                    <span class="n">Ell_rot</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">R_rot</span><span class="p">,</span><span class="n">Ell</span><span class="p">[:,</span><span class="n">i</span><span class="p">])</span>

                <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mu_k</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">Ell_rot</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span> <span class="p">,</span> <span class="n">mu_k</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="n">Ell_rot</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">MGM</span> <span class="o">=</span> <span class="n">MultivariateGaussianMixture</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">MGM</span><span class="o">.</span><span class="n">generate_samples</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_em_gaussin_mixture_model_6_0.png" src="../_images/2_em_gaussin_mixture_model_6_0.png" />
</div>
</div>
<p>Ici nous visualisons une mixture de <span class="math notranslate nohighlight">\(K=6\)</span> composantes gaussiennes tir√©es al√©atoirement (avec les <span class="math notranslate nohighlight">\(\pi_k\)</span> choisis de mani√®re uniforme: <span class="math notranslate nohighlight">\(\pi_k=\frac{1}{K} \forall k\)</span>).  Dans la suite de ce TP, nous allons voir comment nous pouvons estimer les param√®tres d‚Äôun mod√®le g√©n√©ratif de ce type par un algorithme it√©ratif qui, √† chaque it√©ration, va proposer un jeu de param√®tres qui maximise de plus en plus la log-vraissemblance de ce mod√®le¬†: l‚Äôalgorithme Expectation Maximization (EM).</p>
<p>Une application ce genre de mod√®les g√©n√©ratifs qui vient directement √† l‚Äôesprit est le clustering. C‚Äôest-√†-dire  qu‚Äôon veut trouver des groupes de donn√©es similaires. La particularit√© ici est que nous choisissons dans une famille de distributions o√π les clusters sont d√©crits par des normales multi-vari√©es. On peut donc voir ce mod√®le de mixtures gaussien comme une sorte de g√©n√©ralisation de l‚Äôalgorithme <em>K-means</em> (nous reviendrons sur le lien entre ces deux concepts un peu plus loin).</p>
</div>
<div class="section" id="b-l-algortihme-em-pour-estimer-un-modele-de-melange-gaussien">
<h2>B. L‚Äôalgortihme EM pour estimer un mod√®le de m√©lange gaussien<a class="headerlink" href="#b-l-algortihme-em-pour-estimer-un-modele-de-melange-gaussien" title="Permalink to this headline">¬∂</a></h2>
<p>Dans cette partie nous allons tenter uniquement √† partir du jeu de donn√©es g√©n√©r√© pr√©c√©dement de retrouver les param√®tres <span class="math notranslate nohighlight">\(\theta^{\star} = \Big\{ \pi_k^{\star}, \boldsymbol{\mu}_k^{\star}, \boldsymbol{\Sigma}_k^{\star} \Big\}_{k \leq K}\)</span> du vrai processus qui nous a permis de g√©n√©rer ce jeu de donn√©es. On va donc se placer dans une famille param√©trique de distributions <span class="math notranslate nohighlight">\(\mathcal{Q}\)</span> telle que chaque distribution <span class="math notranslate nohighlight">\(q_{\theta} \in \mathcal{Q}\)</span> s‚Äô√©crit sous la forme¬†:</p>
<div class="math notranslate nohighlight">
\[q_{\theta}(\boldsymbol{x}) =  \sum_{k=1}^K \pi_k f(\boldsymbol{x};\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)\]</div>
<p>Et on cherchera donc √† trouver le jeu de param√®tres qui va maximiser la log-vraissemblance suivante¬†:</p>
<div class="math notranslate nohighlight">
\[\ln\Big(q_{\theta}(\mathcal{S}_n)\Big) = \sum_i^n \ln \Bigg(\sum_k^K \pi_k f(\boldsymbol{x}_i;\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \Bigg)\]</div>
<div class="section" id="i-premiere-difference-avec-l-estimation-d-une-gaussienne-multivariee-simple">
<h3>i. Premi√®re diff√©rence avec l‚Äôestimation d‚Äôune gaussienne multivari√©e simple<a class="headerlink" href="#i-premiere-difference-avec-l-estimation-d-une-gaussienne-multivariee-simple" title="Permalink to this headline">¬∂</a></h3>
<p>On rappelle que dans le cas d‚Äôune loi gausienne multivari√©e simple, la log-vraissemblance est¬†:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\ln\Big(q_{\theta}(\mathcal{S}_n)\Big) &amp;= \sum_i^n \ln \Big(f(\boldsymbol{x}_i;\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)\Big)= \sum_i^n \ln \Bigg( \frac{1}{(2\pi)^{\frac{d}{2}}|\boldsymbol{\Sigma}|^{\frac{1}{2}}}  \exp^{-\frac{1}{2}(\boldsymbol{x}_i - \boldsymbol{\mu})^T)\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}_i - \boldsymbol{\mu})} \Bigg)\\
&amp; = -\frac{1}{2} \Bigg[\sum_i^n d\ln (2\pi) + ln\Big(|\boldsymbol{\Sigma}|\Big) + (\boldsymbol{x}_i - \boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}_i - \boldsymbol{\mu})\Bigg]
\end{aligned}\end{split}\]</div>
<p>Il existe donc une forme close o√π annuler le gradient de cette expression par rapport √† <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> et <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> conduit √† une unique solution.</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p>Calculer le gradient de la log-vraissemblance de la loi normale multivari√©e par rapport √† ses param√®tres <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> et <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> puis trouver l‚Äôexpression qui annule le gradient.</p>
</div>
<div class="hint dropdown admonition">
<p class="admonition-title">Indices</p>
<p><span class="math notranslate nohighlight">\(\nabla_{\Sigma}log(|\Sigma|) = \Sigma^{-1}\)</span> et <span class="math notranslate nohighlight">\(\nabla_{\Sigma}\boldsymbol{a}^T\Sigma^{-1}\boldsymbol{b} = -\Sigma^{-1}\boldsymbol{b}\boldsymbol{a}^T\Sigma^{-1}\)</span>.</p>
</div>
<p>Dans la suite nous allons voir que ce qui nous arrange dans le cas la loi normale multivari√©e simple est que le logarithme de l‚Äôexpression de la log-vraissemblance ‚Äúannule‚Äù l‚Äôexponentielle ce qui rend l‚Äôexpression de la log-vraisemblance et son gradient tractable. Malheureusement, dans le cas du m√©lange gaussien, une somme d‚Äôexponentielles se trouve dans le logarithme et cela rend les expressions beaucoup plus complexes. C‚Äôest l√† que l‚Äôalgorithme EM prend toute sa pertinence car il va nous permettre malgr√© tout de trouver un processus it√©ratif qui augmente la log-vraisemblance a chaque it√©ration.</p>
</div>
<div class="section" id="ii-optimisation-des-boldsymbol-mu-k">
<h3>ii. Optimisation des <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span><a class="headerlink" href="#ii-optimisation-des-boldsymbol-mu-k" title="Permalink to this headline">¬∂</a></h3>
<p>Calculons l‚Äôexpression du gradient de la log vraissemblance par rapports aux param√®tres <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span>¬†:</p>
<div class="math notranslate nohighlight">
\[\nabla_{\boldsymbol{\mu}_k} \ln\Big(q_{\theta}\mathcal{S}_n\Big) = \Bigg[\sum_i^n  \frac{\partial ln\Big( \sum_j \pi_j f_j(\boldsymbol{x}_i) \Big)}{\partial f_k(\boldsymbol{x}_i)}  \nabla_{\boldsymbol{\mu}_k} f_k(\boldsymbol{x}_i) \Bigg] = \boldsymbol{0}\]</div>
<p>o√π l‚Äôon a utilis√© la r√®gle de composition des d√©riv√©es. Le premier facteur nous donne¬†:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial ln\Big( \sum_j \pi_j f_j(\boldsymbol{x}_i) \Big)}{\partial f_k(\boldsymbol{x}_i)} = \frac{\pi_k }{\sum_{j}^K \pi_j f(\boldsymbol{x}_i;\boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}\]</div>
<hr class="docutils" />
<p>Concernant <span class="math notranslate nohighlight">\(\nabla_{\boldsymbol{\mu}_k} f_k(\boldsymbol{x}_i)\)</span>, introduisons pour des raisons de lisibilit√© les notations suivnates¬†:</p>
<div class="math notranslate nohighlight">
\[f(\boldsymbol{x}_i;\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)  = f_k(\boldsymbol{x}_i) = \frac{e^{g(\boldsymbol{x}_i, \boldsymbol{\mu}_k,\boldsymbol{\Sigma}_k)}}{C(\boldsymbol{\Sigma_k})}\]</div>
<p>o√π¬†:</p>
<div class="math notranslate nohighlight">
\[C(\boldsymbol{\Sigma_k}) = (2\pi)^{\frac{d}{2}}|\boldsymbol{\Sigma}_k|^{\frac{1}{2}}\]</div>
<p>et¬†:</p>
<div class="math notranslate nohighlight">
\[g(\boldsymbol{x}_i, \boldsymbol{\mu}_k,\boldsymbol{\Sigma}_k) = g_k(\boldsymbol{x}_i) = -\frac{1}{2}(\boldsymbol{x}_i - \boldsymbol{\mu}_k)^T)\boldsymbol{\Sigma}_k^{-1}(\boldsymbol{x}_i - \boldsymbol{\mu}_k).\]</div>
<hr class="docutils" />
<p>En utilisant √† nouveau la r√®gle de d√©rivation d‚Äôune composition, nous obtenons¬†:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\nabla_{\boldsymbol{\mu}_k} f_k(\boldsymbol{x}_i) &amp;= \underbrace{\partial_{g_k(\boldsymbol{x}_i)}\Bigg(\frac{e^{g_k(\boldsymbol{x}_i)}}{C(\boldsymbol{\Sigma_k})}\Bigg)}_{ = \frac{e^{g_k(\boldsymbol{x}_i)}}{C(\boldsymbol{\Sigma_k})} = f(\boldsymbol{x}_i; \boldsymbol{\mu}_k,\boldsymbol{\Sigma}_k)} \nabla_{{\boldsymbol{\mu}_k}} g(\boldsymbol{x}_i, \boldsymbol{\mu}_k,\boldsymbol{\Sigma}_k)\\
&amp; = f(\boldsymbol{x}_i; \boldsymbol{\mu}_k,\boldsymbol{\Sigma}_k)
\underbrace{
    \nabla_{\boldsymbol{\mu}_k}\Bigg[ -\frac{1}{2}(\boldsymbol{x}_i - \boldsymbol{\mu}_k)^T)\boldsymbol{\Sigma}_k^{-1}(\boldsymbol{x}_i - \boldsymbol{\mu}_k)\Bigg]
}_{
    =(-\frac{1}{2})(-2)\boldsymbol{\Sigma}_k^{-1}(\boldsymbol{x}_i - \boldsymbol{\mu}_k)
}
\end{aligned}\end{split}\]</div>
<p>En injectant nos r√©sultats interm√©diaires, nous obtenons finalement¬†:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\nabla_{\boldsymbol{\mu}_k} \ln\Big(q_{\theta}\mathcal{S}^n\Big) &amp;= \Bigg[\sum_i^n  \underbrace{\frac{\pi_k }{\sum_{j}^K \pi_j f(\boldsymbol{x}_i;\boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}f(\boldsymbol{x}_i;\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) }_{q_{\theta}(z=k|\boldsymbol{x}_i, \boldsymbol{\mu}_k,\boldsymbol{\Sigma}_k)} (-\frac{1}{2}) (-2) \boldsymbol{\Sigma}^{-1}(\boldsymbol{x}_i - \boldsymbol{\mu}_k)\Bigg]\\
&amp; = \Bigg[\sum_i^n  q_{\theta}(z=k|\boldsymbol{x}_i, \boldsymbol{\mu}_k,\boldsymbol{\Sigma}_k) \boldsymbol{\Sigma}^{-1}(\boldsymbol{x}_i - \boldsymbol{\mu}_k)\Bigg] = \boldsymbol{0}
\end{aligned}\end{split}\]</div>
<p>Remarquons qu‚Äôapparait la probabilit√© a posteriori ‚Äúque le facteur explicatif <span class="math notranslate nohighlight">\(k\)</span> ait caus√©‚Äù l‚Äôobservation <span class="math notranslate nohighlight">\(x_i\)</span>.</p>
<p>Nous noterons ce terme <span class="math notranslate nohighlight">\( \gamma_k^i = q_{\theta}(z=k|\boldsymbol{x}_i, \boldsymbol{\mu}_k,\boldsymbol{\Sigma}_k)\)</span>. En multipliant par <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_k\)</span> des deux cot√© nous pouvons r√©soudre et nous trouvons¬†:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\mu}_k = \frac{1}{\sum_i^n \gamma_k^i}\sum_i^n  \gamma_k^i\boldsymbol{x}_i\]</div>
<p>Remarquons qu‚Äôil s‚Äôagit quasiment du param√®tre de maximum de vraisemblance pour une unique loi normale auquel on rajoute la pond√©ration d‚Äôappartenance √† la <span class="math notranslate nohighlight">\(k\)</span>-i√®me composante.</p>
<div class="tip admonition">
<p class="admonition-title">Lien avec KMeans</p>
<p>Constatons un premier lien avec <em>K-Means</em> en analysant ce √† quoi correspond la probabilit√© <em>a posteriori</em> <span class="math notranslate nohighlight">\(\gamma_k^i = q_{\theta}(z=k|\boldsymbol{x}_i, \boldsymbol{\mu}_k,\boldsymbol{\Sigma}_k)\)</span>¬†:</p>
<div class="math notranslate nohighlight">
\[\gamma_k^i = \frac{\pi_k f(\boldsymbol{x}_i;\boldsymbol{\mu}_k, \lambda\boldsymbol{I}) }{\sum_j \pi_j f(\boldsymbol{x}_i;\boldsymbol{\mu}_j, \lambda\boldsymbol{I}) } \propto \frac{\pi_k \exp(-\frac{1}{2\lambda}\lVert\boldsymbol{x}_i - \boldsymbol{\mu}_k\rVert_2^2 )}{\sum_j \pi_j \exp(-\frac{1}{2\lambda}\lVert\boldsymbol{x}_i - \boldsymbol{\mu}_j\rVert_2^2)}\]</div>
<p>o√π la matrice de covariance est fix√©e √† la matrice identit√©. Nous pouvons remarquer qu‚Äôil s‚Äôagit d‚Äôun score de la distance au centre de la composante normalis√©e. Plus une observation se rapproche du centre <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span> d‚Äôune gaussienne, plus la contribution qui domine au d√©nominateur correspond √† la contribution de la <span class="math notranslate nohighlight">\(k\)</span>-i√®me composante et donc plus probabilit√© √† posteriori <span class="math notranslate nohighlight">\(q_{\theta}(z=k|\boldsymbol{x}_i, \boldsymbol{\mu}_k,\boldsymbol{\Sigma}_k)\)</span> tend vers <span class="math notranslate nohighlight">\(1\)</span> et toutes les autres <span class="math notranslate nohighlight">\(q_{\theta}(z=j|\boldsymbol{x}_i, \boldsymbol{\mu}_j,\boldsymbol{\Sigma}_j) \forall j \neq k\)</span> tendent vers <span class="math notranslate nohighlight">\(0\)</span>. Il est possible de voir <span class="math notranslate nohighlight">\(\gamma_k^i\)</span> comme un score d‚Äôassignation d‚Äôune observation <span class="math notranslate nohighlight">\(x_i\)</span> √† la <span class="math notranslate nohighlight">\(k\)</span>-i√®me gaussienne.</p>
<p>De plus, remarquons que si la variance des composantes tend vers <span class="math notranslate nohighlight">\(0\)</span>, alors la probabilit√© <em>a posteriori</em> tend vers <span class="math notranslate nohighlight">\(1\)</span> pour une observation <span class="math notranslate nohighlight">\(\boldsymbol{x}_i\)</span>¬†:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\lim\limits_{\lambda \rightarrow 0} q_{\theta}(z=k|\boldsymbol{x}_i, \boldsymbol{\mu}_k, \lambda \boldsymbol{I}) = \lim\limits_{\lambda \rightarrow 0} \frac{\pi_k \exp(-\frac{1}{2\lambda}\lVert\boldsymbol{x}_i - \boldsymbol{\mu}_k\rVert_2^2 )}{\sum_j \pi_j \exp(-\frac{1}{2\lambda}\lVert\boldsymbol{x}_i - \boldsymbol{\mu}_j\rVert_2^2)} = r_k^i =
\begin{cases}
1 \text{ si } k = \arg \min_j\lVert\boldsymbol{x}_i - \boldsymbol{\mu}_j\rVert_2^2 \\
0 \text{ sinon }
\end{cases}\end{split}\]</div>
<p>et nous retrouvons exactement la fonction d‚Äôassignation de KMeans et le calcul de <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span> devient:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\mu}_k = \frac{1}{\sum_i^n r_k^i}\sum_i^n  r_k^i\boldsymbol{x}_i\]</div>
<p>qui correspond exactement √† la proc√©dure de mise √† jour des centro√Øde de l‚Äôalgorithme de K-Means en calculant le barycentre des points assign√©s du <span class="math notranslate nohighlight">\(k\)</span>_i√®me cluster.</p>
<p>Une autre quantit√© int√©r√©ssante √† consid√©rer est la suivante:</p>
<div class="math notranslate nohighlight">
\[N_k  = \sum_i^n q_{\theta}(z=k|\boldsymbol{x}_i, \boldsymbol{\mu}_k,\boldsymbol{\Sigma}_k) = \sum_i^n \gamma_k^i\]</div>
<p>qui n‚Äôest d‚Äôautre que l‚Äôestimateur de l‚Äôesp√©rance du nombre d‚Äôobservations assign√©es √† la <span class="math notranslate nohighlight">\(k\)</span>-i√®me composante gaussienne. Observons l‚Äôanalogie avec KMeans o√π <span class="math notranslate nohighlight">\(N_k = \sum_i^n r_k^i\)</span> est exactement le nombre de points associ√©s au <span class="math notranslate nohighlight">\(k\)</span>_i√®me cluster.</p>
</div>
</div>
<div class="section" id="iii-optimisation-des-boldsymbol-sigma-k">
<h3>iii. Optimisation des <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_k\)</span><a class="headerlink" href="#iii-optimisation-des-boldsymbol-sigma-k" title="Permalink to this headline">¬∂</a></h3>
<p>Proc√©dons de mani√®re tr√®s similaire pour le calcul des matrices de variance-covariances. La seule diff√©rence sera que cette fois-ci, il faudra calculer le gradient de <span class="math notranslate nohighlight">\(f_k(\boldsymbol{x}_i)\)</span> par rapport √† <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_k\)</span>¬†:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\nabla_{\boldsymbol{\Sigma}_k} \ln\Big(q_{\theta}\mathcal{S}_n\Big) &amp;= \Bigg[\sum_i^n  \frac{\partial ln\Big( \sum_j \pi_j f_j(\boldsymbol{x}_i) \Big)}{\partial f_k(\boldsymbol{x}_i)}  \nabla_{\boldsymbol{\Sigma}_k} f_k(\boldsymbol{x}_i) \Bigg] \\
&amp;=  \Bigg[\sum_i^n  \frac{\pi_k  }{\sum_{j}^K \pi_j f(\boldsymbol{x}_i;\boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)} \nabla_{{\boldsymbol{\Sigma}_k}} \Bigg(\frac{e^{g_k(\boldsymbol{x}_i)}}{C(\boldsymbol{\Sigma_k})}\Bigg) \Bigg]
\end{aligned}\end{split}\]</div>
<p>En rappellant l‚Äôexpression de la d√©riv√©e d‚Äôun quotient <span class="math notranslate nohighlight">\(\Big(\frac{u}{v}\Big)' = \frac{u'v-uv'}{v^2}\)</span>¬†:</p>
<div class="math notranslate nohighlight">
\[\nabla_{{\boldsymbol{\Sigma}_k}} \Bigg(\frac{e^{g_k(\boldsymbol{x}_i)}}{C(\boldsymbol{\Sigma_k})}\Bigg) = \frac{\nabla_{\boldsymbol{\Sigma}_k} \Big(e^{g_k(\boldsymbol{x}_i)}\Big) C(\boldsymbol{\Sigma_k}) -  e^{g_k(\boldsymbol{x}_i)} \nabla_{\boldsymbol{\Sigma}_k} \Big(C(\boldsymbol{\Sigma_k})\Big)}{C(\boldsymbol{\Sigma_k})^2}\]</div>
<p>avec¬†:</p>
<div class="math notranslate nohighlight">
\[\nabla_{\boldsymbol{\Sigma}_k} \Big(e^{g_k(\boldsymbol{x}_i)}\Big) = \underbrace{\partial_{g_k(\boldsymbol{x}_i)}\Big(e^{g_k(\boldsymbol{x}_i)}\Big)}_{e^{g_k(\boldsymbol{x}_i)}} \nabla_{\boldsymbol{\Sigma}_k} \Big(g_k(\boldsymbol{x}_i)\Big)\]</div>
<p>En factorisant par <span class="math notranslate nohighlight">\(\frac{e^{g_k(\boldsymbol{x}_i)}}{C(\boldsymbol{\Sigma_k})} = f_k(\boldsymbol{x}_i)\)</span>, et en se rappellant que <span class="math notranslate nohighlight">\(\frac{u'}{u} = ln(u)'\)</span>, nous obtenons¬†:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\nabla_{{\boldsymbol{\Sigma}_k}} \Bigg(\frac{e^{g_k(\boldsymbol{x}_i)}}{C(\boldsymbol{\Sigma_k})}\Bigg)&amp;= f_k(\boldsymbol{x}_i)
\Bigg[
\nabla_{\boldsymbol{\Sigma}_k} \Big(g_k(\boldsymbol{x}_i)\Big) - \underbrace{\frac{\nabla_{\boldsymbol{\Sigma}_k} \Big(C(\boldsymbol{\Sigma_k})\Big)}{C(\boldsymbol{\Sigma_k})}}_{ = \nabla_{\boldsymbol{\Sigma}_k} ln \Big(C(\boldsymbol{\Sigma_k})\Big)}
\Bigg]\\
&amp;= f_k(\boldsymbol{x}_i)
\Bigg[
\nabla_{\boldsymbol{\Sigma}_k} \Big[-\frac{1}{2}(\boldsymbol{x}_i - \boldsymbol{\mu}_k)^T)\boldsymbol{\Sigma}_k^{-1}(\boldsymbol{x}_i - \boldsymbol{\mu}_k)\Big] - \nabla_{\boldsymbol{\Sigma}_k} \Big[ \frac{d}{2} ln(2\pi) + \frac{1}{2}ln\Big(|\boldsymbol{\Sigma}_k| \Big) \Big]
\Bigg]\\
&amp;= f_k(\boldsymbol{x}_i) (-\frac{1}{2})\Bigg[-\boldsymbol{\Sigma}^{-1} (\boldsymbol{x}_i - \boldsymbol{\mu})(\boldsymbol{x}_i - \boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1} + \boldsymbol{\Sigma}_k^{-1}\Bigg]
\end{aligned}\end{split}\]</div>
<p>Ainsi, afin d‚Äôannuler le gradient, nous obtenons¬†:</p>
<div class="math notranslate nohighlight">
\[\nabla_{\boldsymbol{\Sigma}_k} \ln\Big(q_{\theta}\mathcal{S}^n\Big)  = -\frac{1}{2} \sum_i^n  \Bigg[ \underbrace{\Bigg[ \frac{\pi_k  f_k(\boldsymbol{x}_i)}{\sum_{j}^K \pi_j f(\boldsymbol{x}_i;\boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}\Bigg]}_{\gamma_k^i} \Bigg[-\boldsymbol{\Sigma}^{-1} (\boldsymbol{x}_i - \boldsymbol{\mu})(\boldsymbol{x}_i - \boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1} + \boldsymbol{\Sigma}_k^{-1}\Bigg]\Bigg] = \boldsymbol{0}\]</div>
<p>Multiplions √† droite et √† gauche par <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_k\)</span> et r√©solvons¬†:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\Sigma}_k = \frac{1}{N_k}\sum_i^n  \gamma_k^i (\boldsymbol{x}_i - \boldsymbol{\mu}_k)(\boldsymbol{x}_i - \boldsymbol{\mu}_k)^T\]</div>
<p>O√π nous retrouvons, de mani√®re analogue au calculs des <span class="math notranslate nohighlight">\(\mu_k\)</span>, les expression des matrices de covariances empiriques et o√π la contribution de chaque √©chantillon est pond√©r√©es par sa probabilit√© a posteriori d‚Äôavoir √©t√© g√©n√©r√© par la <span class="math notranslate nohighlight">\(k\)</span>-i√®me composante gaussienne.</p>
</div>
<div class="section" id="iv-optimisation-des-pi-k">
<h3>iv. Optimisation des <span class="math notranslate nohighlight">\(\pi_k\)</span><a class="headerlink" href="#iv-optimisation-des-pi-k" title="Permalink to this headline">¬∂</a></h3>
<p>Concernant l‚Äôopimisation des <span class="math notranslate nohighlight">\(\pi_k\)</span>, il ne suffit pas simplement d‚Äôannuler le gradient de log vraissemblance car nous avons la contrainte que leur somme sur <span class="math notranslate nohighlight">\(k\)</span> doit valoir <span class="math notranslate nohighlight">\(1\)</span> (il s‚Äôagit d‚Äôune distribution de probabilit√©). Pour cela, nous allons donc considerer un probl√®me d‚Äôoptimisation sous contrainte. D√©finissons le Lagrangien <span class="math notranslate nohighlight">\(\mathcal{L}\Big( \ln(q_{\theta}\mathcal{S}_n) , \lambda\Big)\)</span> (voir la s√©quence ‚ÄúL‚Äôoptimisation‚Äù) qui int√®gre cette contrainte¬†:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}\Big( \ln(q_{\theta}\mathcal{S}_n) , \lambda\Big) = \sum_i^n ln \Big(\sum_k \pi_k f_k(\boldsymbol{x}_i) \Big) + \lambda \Big(\sum_k \pi_k - 1\Big)\]</div>
<p>Annuler la d√©riv√©e du Lagrangien par rapport √† notre multiplicateur de Lagrange <span class="math notranslate nohighlight">\(\lambda\)</span> nous permet bien de retrouver notre contrainte¬†:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial \mathcal{L}\Big( \ln(q_{\theta}\mathcal{S}_n) , \lambda\Big)}{\partial \lambda} =  \Big(\sum_k \pi_k - 1\Big) = 0 \Leftrightarrow \sum_k \pi_k  =  1\]</div>
<p>Annulons maintenant la d√©riv√©e par rapport √† un <span class="math notranslate nohighlight">\(\pi_k\)</span> donn√©e¬†:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial \mathcal{L}\Big( \ln(q_{\theta}\mathcal{S}_n) , \lambda\Big)}{\partial \pi_k} = \sum_i^n \frac{\partial ln\Big( \sum_j \pi_j f_j(\boldsymbol{x}_i) \Big)}{\partial \pi_k}  + \lambda = \sum_i^n \frac{f_k(\boldsymbol{x}_i)}{\sum_j \pi_j f_j(\boldsymbol{x}_i)} + \lambda = 0\]</div>
<p>En mutipliant par <span class="math notranslate nohighlight">\(\pi_k\)</span> des deux cot√©s et en sommant sur <span class="math notranslate nohighlight">\(k\)</span>, nous obtenons¬†:</p>
<div class="math notranslate nohighlight">
\[\sum_k \pi_k\sum_i^n \frac{f_k(\boldsymbol{x}_i)}{\sum_j \pi_j f_j(\boldsymbol{x}_i)} + \lambda  \sum_k \pi_k= 0 = \sum_i^n \underbrace{\frac{\sum_k \pi_kf_k(\boldsymbol{x}_i)}{\sum_j \pi_j f_j(\boldsymbol{x}_i)}}_{=1} + \lambda  \underbrace{\sum_k \pi_k}_{=1}\]</div>
<p>et le multiplicateur de Lagrange vaut ainsi <span class="math notranslate nohighlight">\(\lambda = -n\)</span>. Nous avons √©galement¬†:</p>
<div class="math notranslate nohighlight">
\[\underbrace{\sum_i^n \frac{\pi_k f_k(\boldsymbol{x}_i)}{\sum_j \pi_j f_j(\boldsymbol{x}_i)}}_{=\sum_i^n \gamma_k^i=N_k} + \lambda \pi_k = 0 \Leftrightarrow \lambda = -\frac{N_k}{\pi_k} = -n\]</div>
<p>ce qui nous donne¬†:</p>
<div class="math notranslate nohighlight">
\[\pi_k  = \frac{N_k}{n}\]</div>
<p>C‚Äôest ainsi l‚Äôestimateur de l‚Äôesp√©rance de la fraction de points assign√©s √† la composante <span class="math notranslate nohighlight">\(k\)</span>.</p>
</div>
<div class="section" id="v-l-algortihme-em">
<h3>V. L‚Äôalgortihme EM<a class="headerlink" href="#v-l-algortihme-em" title="Permalink to this headline">¬∂</a></h3>
<p>Les formules pr√©c√©dentes de nous donnent pas une solution en forme close. En effet, ces derni√®res sont reli√©es mutuellement¬†:</p>
<div class="math notranslate nohighlight">
\[\gamma_k^i = \frac{\pi_k f(\boldsymbol{x}_i;\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) }{\sum_j \pi_j f(\boldsymbol{x}_i;\boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}.\]</div>
<p>√Ä chaque fois que nous changerons la valeurs de ces param√®tres, les probabilit√©s <em>a posteriori</em> changerons aussi ce qui donnera lieu √† de nouvelles valeurs des param√®tres et ainsi de suite.</p>
<p>Nous pouvons n√©anmoins d√©finir une proc√©dure it√©rative en alternant ces deux phases de calculs des <span class="math notranslate nohighlight">\(\gamma_k^i\)</span> puis des valeurs optimales des param√™tres en gardant fixe les probabilit√©s <em>a posteriori</em>. Cela donne lieu √† l‚Äôalgorithme Expectation Maximization (EM) qui dans le cas de l‚Äôestimateur de mod√®les de m√©lange Gaussien peut se formuler ainsi¬†:</p>
<ol>
<li><p><strong>Etape E (Expectation)</strong>¬†: Calculer les valeurs en utilisant la valeur courante des param√®tres <span class="math notranslate nohighlight">\(\theta(t) = \Big\{ \pi_k(t), \boldsymbol{\mu}_k(t), \boldsymbol{\Sigma}_k(t) \Big\}_{k \leq K}\)</span> (qu‚Äôon initialisera al√©atoirement pour la premi√®re it√©ration)¬†:</p>
<div class="math notranslate nohighlight">
\[\gamma_k^i (t+1) = \frac{\pi_k(t) f(\boldsymbol{x}_i;\boldsymbol{\mu}_k(t), \boldsymbol{\Sigma}_k(t) }{\sum_j \pi_j f(\boldsymbol{x}_i;\boldsymbol{\mu}_j(t), \boldsymbol{\Sigma}_j(t)}\]</div>
<p>et¬†:</p>
<div class="math notranslate nohighlight">
\[N_k(t+1) = \sum_i \gamma_k^i(t+1)\]</div>
</li>
<li><p><strong>Etape M (Maximization)</strong>¬†: Calculer les nouvelles valeurs des param√®tres en utilisant les valeurs des probabilit√©s <em>a posteriori</em> calcul√©es √† l‚Äô√©tape pr√©c√©dente:</p>
<div class="math notranslate nohighlight">
\[\pi_k(t+1) =   \frac{N_k(t+1)}{n}\]</div>
<div class="math notranslate nohighlight">
\[\boldsymbol{\mu}_k(t+1) = \frac{1}{N_k(t+1)}\sum_i^n  \gamma_k^i(t+1)\boldsymbol{x}_i \]</div>
<div class="math notranslate nohighlight">
\[\boldsymbol{\Sigma}_k(t+1) = \frac{1}{N_k(t+1)}\sum_i^n  \gamma_k^i(t+1) (\boldsymbol{x}_i - \boldsymbol{\mu}_k(t))(\boldsymbol{x}_i - \boldsymbol{\mu}_k(t))^T\]</div>
</li>
</ol>
<p>Etant donn√© ce que nous avons vu pr√©c√©dement sur le lien entre l‚Äôalgorithme EM et KMeans, il est possible d‚Äôinitialiser les param√®tres <span class="math notranslate nohighlight">\(\big\{\boldsymbol{\mu}_k(0)\big\}_{k \leq K}\)</span> non pas al√©atoirement, mais avec les centro√Ødes calcul√©s par un algorithme KMeans. C‚Äôest souvent ce qui est fait en pratique √† la fois pour la qualit√© et le temps de convergence notamment dans les cas o√π <span class="math notranslate nohighlight">\(d\)</span> est grand et que le nombre de param√®tres l‚Äôest aussi. Par la suite, nous allons impl√©menter les √©l√©ments de base de l‚Äôalgorithme EM dans le cas du m√©lange Gaussien et nous utiliserons les deux modes d‚Äôinitialisation.</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p>Dans le code qui suit, remplissez les m√©thodes <em>compute_posteriors</em>, <em>compute_priors</em>, <em>compute_mu</em>, <em>compute_sigma</em> correspondant respectivement aux calculs des <span class="math notranslate nohighlight">\(\gamma_k^i(t)\)</span>, <span class="math notranslate nohighlight">\(\pi_k(t)\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k(t)\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_k(t)\)</span>. Compl√©tez aussi le code <em>evaluate_log_likelyhood</em> qui vous permettra d‚Äô√©valuer la log-vraissemblance pour les valeurs courantes des param√®tres du mod√®le&amp;nbsp:</p>
<div class="math notranslate nohighlight">
\[\ln\Big(q_{\theta(t)}(\mathcal{S}^n)\Big) = \sum_i^n \ln \Bigg(\sum_k^K \pi_k(t) f(\boldsymbol{x}_i;\boldsymbol{\mu}_k(t), \boldsymbol{\Sigma}_k(t)) \Bigg)\]</div>
<p>On utilisera la m√©thode <em>multivariate_normal(mu, sigma).pdf(X)</em> de <em>scipy.stats</em> pour calculer les <span class="math notranslate nohighlight">\(f(\boldsymbol{x}_i|\boldsymbol{\mu}_k(t), \boldsymbol{\Sigma}_k(t))\)</span>.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="k">class</span> <span class="nc">GMM</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;kmeans&quot;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span> <span class="o">=</span> <span class="n">n_components</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="o">=</span> <span class="n">init</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">pi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_components</span><span class="p">)</span><span class="o">/</span><span class="n">n_components</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span>  <span class="o">=</span> <span class="p">[</span><span class="mi">20</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_components</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mu</span>     <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">d</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_components</span><span class="p">)]</span>
        
    <span class="k">def</span> <span class="nf">evaluate_log_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1">#### Complete the code here #### or die #######################################</span>
        <span class="k">return</span> <span class="o">...</span>
        <span class="c1">################################################################################</span>
    
    <span class="k">def</span> <span class="nf">compute_posteriors</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1">#Compute numerator of the posterior from Baye&#39;s Rule</span>
        <span class="c1">#### Complete the code here #### or die #######################################</span>
        <span class="o">...</span>
            
        <span class="k">return</span> <span class="o">...</span>
        <span class="c1">################################################################################</span>
    
    <span class="k">def</span> <span class="nf">compute_priors</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="c1">#### Complete the code here #### or die #######################################</span>
        <span class="k">return</span> <span class="o">...</span>
        <span class="c1">################################################################################</span>
               
    <span class="k">def</span> <span class="nf">compute_mu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">pi</span><span class="p">):</span>
        <span class="c1">#### Complete the code here #### or die #######################################</span>
        <span class="k">return</span> <span class="o">...</span>
        <span class="c1">################################################################################</span>
    
    <span class="k">def</span> <span class="nf">compute_sigma</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">pi</span><span class="p">,</span> <span class="n">mu</span><span class="p">):</span>
        <span class="c1">#### Complete the code here #### or die #######################################</span>
        <span class="k">return</span> <span class="o">...</span>
        <span class="c1">################################################################################</span>
        
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">compute_posteriors</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">]</span>
    
    <span class="k">def</span> <span class="nf">fit_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">LLH</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">LLH</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">LLH_history</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Initialize mu with KMeans</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="o">==</span> <span class="s2">&quot;kmeans&quot;</span><span class="p">:</span>      
            <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">cluster_centers_</span>
        
        <span class="c1"># Peform EM iterations</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span><span class="p">):</span>
            <span class="n">LLH_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluate_log_likelihood</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
            <span class="c1"># TODO uncomment print(&quot;\rIteration: %d - LogLikelyHood = %f&quot;%(i,LLH_history[-1]), end=&quot;&quot;)</span>
                
            <span class="c1">#E step:</span>
            <span class="n">gamma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_posteriors</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

            <span class="c1">#M step:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pi</span>  <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_priors</span><span class="p">(</span><span class="n">gamma</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mu</span>      <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_mu</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span>   <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_sigma</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">LLH_history</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">n_iter</span><span class="o">=</span><span class="mi">50</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GMM</span><span class="p">(</span><span class="n">n_iter</span><span class="o">=</span><span class="n">n_iter</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;random&quot;</span><span class="p">)</span>
<span class="n">Z</span><span class="p">,</span> <span class="n">LLH_random</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">GMM</span><span class="p">(</span><span class="n">n_iter</span><span class="o">=</span><span class="n">n_iter</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;kmeans&quot;</span><span class="p">)</span>
<span class="n">Z</span><span class="p">,</span> <span class="n">LLH_kmeans</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>


<span class="c1"># Plot LogLikelyHood</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">LLH_random</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">LLH_random</span><span class="p">,</span> 
         <span class="n">label</span><span class="o">=</span><span class="s1">&#39;LogLikeliHood Random init&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">LLH_kmeans</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">LLH_kmeans</span><span class="p">,</span> 
         <span class="n">label</span><span class="o">=</span><span class="s1">&#39;LogLikeliHood KMeans init&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Plot gaussian components</span>
<span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">means</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">mu</span><span class="p">,</span> <span class="n">gmd</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="c-justification-de-l-algorithme-em-approfondissement">
<h2>C. Justification de l‚Äôalgorithme EM (approfondissement)<a class="headerlink" href="#c-justification-de-l-algorithme-em-approfondissement" title="Permalink to this headline">¬∂</a></h2>
<p>Nous allons maintenant montrer que cette proc√©dure qui peut sembler arbitraire poss√®de en fait une justification th√©orique qui garantit que la vraissemblance de notre mod√®le cro√Æt bien √† chaque it√©ration (sauf si on se trouve dej√† dans un maximum local). Au-del√† de la croissance, il est √©galement possible de d√©montrer la convergence de cette m√©thode. Une premi√®re chose √† constater est que si nous poss√©dions pour chaque observation <span class="math notranslate nohighlight">\(\boldsymbol{x}_i\)</span> la valeur de sa repr√©sentation associ√©es <span class="math notranslate nohighlight">\(\boldsymbol{z}_i\)</span>, alors notre objectif serait de maximiser la log-vraissemblance suivante¬†:</p>
<div class="math notranslate nohighlight">
\[\hat{\theta} = \arg \max_{\theta} \sum_i^n \ln \Big(q_{\theta}(\boldsymbol{x}_i,\boldsymbol{z}_i)\Big)\]</div>
<p>L‚Äôobjectif est de trouver la param√©trisation qui maximise la probabilit√© d‚Äôobserver conjointement une observation et sa ‚Äúvraie‚Äù variable cach√©e. il est possible de d√©montrer que c‚Äôest ce qu‚Äôon fait en pratique avec l‚Äôalgorithme EM. Cependant, n‚Äôayant acc√®s √† ces variables cach√©es, nous utilisons la meilleure information disponible, √† savoir sa distribution <em>a posteriori</em> √† l‚Äôinstant <span class="math notranslate nohighlight">\(t\)</span> : <span class="math notranslate nohighlight">\(q_{\theta(t)}(\boldsymbol{z} | \boldsymbol{x})\)</span>. Nous effectuons en pratique une optimisation de l‚Äôesp√©rance de cette log-vraissemblance sur la distribution a posteriori¬†:</p>
<div class="math notranslate nohighlight">
\[\theta(t+1) = \arg \max_{\theta} \Bigg[\mathbb{E}_{z_i\sim q_{\theta(t)}(z_i|x_i)} \Bigg[\sum_i^n ln \Big(q_{\theta}(\boldsymbol{x}_i,\boldsymbol{z}_i)\Big)\Bigg]\Bigg].\]</div>
<p>Notons¬†</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\theta, \theta(t)) = \mathbb{E}_{z_i\sim q_{\theta(t)}(z_i|x_i)} \Bigg[\sum_i^n ln \Big(q_{\theta}(\boldsymbol{x}_i,\boldsymbol{z}_i)\Big)\Bigg]\]</div>
<p>Comme tous les √©chantillons sont ind√©pendants et identiquement distribu√©s et que <span class="math notranslate nohighlight">\(z\)</span> est une variable discr√®te, nous pouvons r√©√©crire cette quantit√© de la mani√®re suivante¬†:</p>
<div class="math notranslate nohighlight">
\[\theta(t+1) = \arg \max_{\theta} \Bigg[\sum_i^n \sum_k^K q_{\theta(t)}(z_i=k|\boldsymbol{x}_i) \Big[ \ln (q_{\theta}(z_i=k)) + \ln (q_{\theta}(\boldsymbol{x}_i|z_i=k))\Big]\Bigg]\]</div>
<p>Remarquons que cela revient √† pond√©rer chaque terme de la log-vraissemblance associ√©e √† chaque valeur de la variable cach√©e par la probabilit√© <em>a posteriori</em> de cette variable cach√©e sachant l‚Äôobservation associ√©e.</p>
<p>Ainsi, dans le cadre g√©n√©ral de traitement des mod√®les √† variables cach√©es par l‚Äôalgorithme EM, ce dernier peut se d√©crire ainsi¬†:</p>
<ol>
<li><p><strong>Expectation</strong>¬†: Utiliser la valeur du param√™tre <span class="math notranslate nohighlight">\(\theta(t)\)</span> pour en d√©duire la distribution <em>a post√©riori</em>¬†:</p>
<div class="math notranslate nohighlight">
\[q_{\theta(t)}(z|\boldsymbol{x})\]</div>
</li>
<li><p><strong>Maximization</strong>¬†: Utiliser cette derni√®re pour d√©finir l‚Äôexpression¬†:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\theta, \theta(t)) =  \sum_i^n \sum_k^K q_{\theta(t)}(z_i=k|\boldsymbol{x}_i) \Big[ \ln (q_{\theta}(z_i=k)) + \ln (q_{\theta}(\boldsymbol{x}_i|z_i=k))\Big]\]</div>
<p>qui d√©pend du param√™tre g√©n√©ral <span class="math notranslate nohighlight">\(\theta\)</span> et optimiser <span class="math notranslate nohighlight">\(\mathcal{L}(\theta, \theta(t))\)</span> par rapport √† ce dernier¬†:</p>
<div class="math notranslate nohighlight">
\[\theta(t) = \arg \max_{\theta} \Bigg[\mathcal{L}(\theta, \theta(t))\Bigg]\]</div>
</li>
</ol>
<div class="section" id="i-retour-sur-le-cas-du-melange-gaussien">
<h3>i. Retour sur le cas du m√©lange Gaussien<a class="headerlink" href="#i-retour-sur-le-cas-du-melange-gaussien" title="Permalink to this headline">¬∂</a></h3>
<p>Si l‚Äôon se replace dans le cas du m√©lange Gaussien, en rappellant qu‚Äôon appelle <span class="math notranslate nohighlight">\(\gamma_k^i = q_{\theta(t)}(z_i=k|\boldsymbol{x}_i)\)</span> la probabilit√© <em>a posteriori</em> de la <span class="math notranslate nohighlight">\(k\)</span>-i√®me composante en gardant les param√®tres de l‚Äôinstant <span class="math notranslate nohighlight">\(t\)</span> fixes, l‚Äôexpression √† maximiser devient¬†:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathcal{L}(\theta, \theta(t)) &amp;= \sum_i^n \sum_k^K \gamma_k^i \Big[ ln (\pi_k) + ln \big(f(\boldsymbol{x}_i ; \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)\big)\Big]\\
&amp; = \sum_i^n \sum_k^K \gamma_k^i \Big[ ln (\pi_k) - \frac{d}{2}ln(2\pi) - \frac{1}{2} ln(|\boldsymbol{\Sigma}_k|)- \frac{1}{2} (\boldsymbol{x}_i - \boldsymbol{\mu}_k)^T \boldsymbol{\Sigma}_k^{-1}  (\boldsymbol{x}_i - \boldsymbol{\mu}_k) \Big]
\end{aligned}\end{split}\]</div>
<p>Il est possible de v√©rifier que nous retombons bien sur les param√®tres obtenus dans la partie pr√©c√©dente.</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p>Retrouvez le terme <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k(t+1)\)</span> √† partir de cette expression¬†:</p>
<div class="math notranslate nohighlight">
\[\nabla_{\mu_k} \Big(\mathcal{L}(\theta, \theta(t))\Big) = \sum_i^n\gamma_k^i(t) \Big[- \frac{1}{2} (-2) \boldsymbol{\Sigma}_k^{-1}  (\boldsymbol{x}_i - \boldsymbol{\mu}_k) \Big] = \boldsymbol{0}\]</div>
</div>
<p>Un avantage avec cette formulation est que le terme intervenant dans le logarithme n‚Äô√©tant plus une somme de densit√© gaussienne mais directement une densit√© gaussienne, les calculs de gradient sont beaucoup plus faciles √† manipuler.</p>
</div>
<div class="section" id="ii-preuve-que-chaque-iteration-de-em-augmente-la-log-vraissemblance">
<h3>ii. Preuve que chaque it√©ration de EM augmente la log-vraissemblance<a class="headerlink" href="#ii-preuve-que-chaque-iteration-de-em-augmente-la-log-vraissemblance" title="Permalink to this headline">¬∂</a></h3>
<div class="tip admonition">
<p class="admonition-title">Croissance et convergence</p>
<p>Montrer que l‚Äôalgorithme am√©liore  la fonction objectif (ici la vraisemblance) n‚Äôest pas suffisant pour garantir que notre algorithme est bon. Il faudrait d√©montrer qu‚Äôil converge effectivement vers la quantit√© souhait√©e, mais cela est plus compliqu√©.</p>
</div>
<p>M√™me si nous avons donn√© une vision un peu plus g√©n√©rale de l‚Äôalgorithme EM et que nous avons donn√© une intuition de sa justification, nous n‚Äôavons pas encore montr√© rigoureusement pourquoi nous somme garantis que proc√©der ainsi permet effectivement d‚Äôaccro√Ætre la log-vraissemblance <span class="math notranslate nohighlight">\(ln\Big(q_{\theta}\Big(\mathcal{S}^n\Big)\Big)\)</span> de notre mod√®le g√©n√©ratif √† chaque it√©ration (si nous ne sommes pas d√©j√† dans un maximum local).</p>
<p>Pour cela il convient d‚Äôexprimer cette log-vraissemblance de la mani√®re suivante¬†:</p>
<div class="math notranslate nohighlight">
\[\ln\Big(q_{\theta}\Big(\mathcal{S}_n\Big)\Big) = \ln\Big(q_{\theta}\Big(\mathcal{S}_n, \mathcal{Z}_n\Big)\Big) -  \ln\Big(q_{\theta}\Big(\mathcal{Z}^n | \mathcal{S}_n\Big)\Big)\]</div>
<p>O√π nous notons <span class="math notranslate nohighlight">\(\mathcal{Z}_n\)</span> la variable al√©atoire ‚Äúcach√©e‚Äù correspondant √† un √©chantillon de taille <span class="math notranslate nohighlight">\(n\)</span>. Une astuce nous permet d‚Äô√©crire cette expression de la mani√®re suivante¬†:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\ln\Big(q_{\theta}\Big(\mathcal{S}_n\Big)\Big) 
&amp;= \Bigg[\ln\Big(q_{\theta}\Big(\mathcal{S}_n, \mathcal{Z}_n\Big)\Big) - \ln\Big(q'\Big(\mathcal{Z}_n\Big)\Big)\Bigg] -\Bigg[- \ln\Big(q'\Big(\mathcal{Z}_n\Big)\Big) +  \ln\Big(q_{\theta}\Big(\mathcal{Z}_n | \mathcal{S}_n\Big)\Big)\Bigg]\\
&amp;=\ln\Bigg( \frac{q_{\theta}\Big(\mathcal{S}_n, \mathcal{Z}_n\Big)}{q'\Big(\mathcal{Z}_n\Big)}\Bigg) - \ln\Bigg(\frac{q_{\theta}\Big(\mathcal{Z}_n | \mathcal{S}_n\Big)}{q'\Big(\mathcal{Z}_n\Big)}\Bigg)
\end{aligned}\end{split}\]</div>
<p>O√π l‚Äôon note <span class="math notranslate nohighlight">\(q'\Big(\mathcal{Z}_n\Big)\)</span> comme n‚Äôimporte quelle distribution sur les variables cach√©es dans <span class="math notranslate nohighlight">\(\mathcal{Q}\)</span>. En notant que la marginale de cette derni√®re sur les √©chantillon s‚Äôint√®gre √† <span class="math notranslate nohighlight">\(1\)</span>, on en d√©duit en calculant l‚Äôesp√©rance sur <span class="math notranslate nohighlight">\(q'\Big(\mathcal{Z}^n\Big)\)</span> des deux cot√©s¬†:</p>
<div class="math notranslate nohighlight">
\[\underbrace{\mathbb{E}_{\mathcal{Z}_n \sim q'(z)^n}\Bigg[\ln\Big(q_{\theta}\Big(\mathcal{S}_n\Big)\Big)\Bigg]}_{=  \ln\Big(q_{\theta}\Big(\mathcal{S}_n\Big)\Big)} = \underbrace{\mathbb{E}_{\mathcal{Z}_n \sim q'(z)^n} \Bigg[\ln\Bigg( \frac{q_{\theta}\Big(\mathcal{S}_n, \mathcal{Z}_n\Big)}{q'\Big(\mathcal{Z}_n\Big)}\Bigg)\Bigg]}_{= L(q', \theta)}  \underbrace{-\mathbb{E}_{\mathcal{Z}_n \sim q'(z)^n}\Bigg[ln\Bigg(\frac{q_{\theta}\Big(\mathcal{Z}_n | \mathcal{S}_n\Big)}{q'\Big(\mathcal{Z}_n\Big)}\Bigg)\Bigg]}_{= \mathbb{KL}\big(q'||q\big)}\]</div>
<p>soit¬†:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\\ln\Big(q_{\theta}\Big(\mathcal{S}^n\Big)\Big)= L(q', \theta) + \mathbb{KL}\big(q'||q_{\theta}\big)\end{split}\]</div>
<div class="tip admonition">
<p class="admonition-title">Divergence de Kullback Liebler</p>
<p>Soit <span class="math notranslate nohighlight">\(P\)</span> et <span class="math notranslate nohighlight">\(Q\)</span> deux distributions de probabilit√©. La divergence de Kullback Liebler est donn√©e par¬†</p>
<div class="math notranslate nohighlight">
\[\mathbb{KL}\big(P\lVert Q)=\int p(x)\ln\frac{p(x)}{q(x)}dx.\]</div>
<p>Celle-ci est minimale lorsque <span class="math notranslate nohighlight">\(P=Q\)</span> est vaut alors <span class="math notranslate nohighlight">\(0\)</span>. Notez cependant son manque de sym√©trie¬†:</p>
<div class="math notranslate nohighlight">
\[\mathbb{KL}\big(P\lVert Q)\neq \mathbb{KL}\big(Q\lVert P).\]</div>
</div>
<p>o√π <span class="math notranslate nohighlight">\(\mathbb{KL}\big(q'||q_{\theta}\big)\)</span> correspond √† la divergence de Kullback Liebler. Pour tout choix de <span class="math notranslate nohighlight">\(q'\)</span> et <span class="math notranslate nohighlight">\(\theta\)</span> nous avons n√©cessairement¬†:</p>
<div class="math notranslate nohighlight">
\[L(q', \theta)  = \ln\Big(q_{\theta}\Big(\mathcal{S}_n\Big)\Big) - \underbrace{ \mathbb{KL}\big(q'||q_{\theta}\big)}_{\geq 0} \leq \ln\Big(q_{\theta}\Big(\mathcal{S}_n\Big)\Big)\]</div>
<p>Et <span class="math notranslate nohighlight">\(L(q', \theta)\)</span> est donc forc√©ment un minorant de la log-vraissemblance pour tout choix de <span class="math notranslate nohighlight">\(q'\)</span> et <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>Ce qui est int√©ressant, c‚Äôest que si dans une premi√®re √©tape (que l‚Äôon constatera √™tre exactement l‚Äô√©tape E de l‚Äôalgorithme EM) on essaie de trouver la distribution <span class="math notranslate nohighlight">\(q'\)</span> qui maximise cette borne inf√©rieure en gardant notre valeur courante de <span class="math notranslate nohighlight">\(\theta(t)\)</span> fixe, alors nous obtenons¬†:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
&amp;\arg \max_{q' \in \mathcal{Q}} \Big[ ln\Big(q_{\theta}\Big(\mathcal{S}^n\Big)\Big) - \mathbb{KL}\big(q'||q_{\theta(t)}\big) \Big] \\&amp;=
\arg \min_{q' \in \mathcal{Q}} \Big[ \mathbb{KL}\big(q'\Big(\mathcal{Z}^n\Big)||q_{\theta(t)}\Big(\mathcal{Z}^n | \mathcal{S}^n\Big)\big) \Big] \\&amp;= q_{\theta(t)}\Big(\mathcal{Z}^n | \mathcal{S}^n\Big)
\end{aligned}\end{split}\]</div>
<p>En fixant <span class="math notranslate nohighlight">\(q'\Big(\mathcal{Z}^n\Big) = q_{\theta(t)}\Big(\mathcal{Z}^n | \mathcal{S}^n\Big)\)</span>, c‚Äôest-√†-dire comme la distribution a posteriori (en se fixant un vecteur de param√®tres en cours <span class="math notranslate nohighlight">\(\theta(t)\)</span>), alors ce terme de divergence KL dvient nul et on obtient pour <span class="math notranslate nohighlight">\(\theta = \theta(t)\)</span>¬†:</p>
<div class="math notranslate nohighlight">
\[L\Big(q_{\theta(t)}\Big(\mathcal{Z}_n | \mathcal{S}^n\Big), \theta(t)\Big) = \ln\Big(q_{\theta(t)}\Big(\mathcal{S}_n\Big)\Big)\]</div>
<p>Rappellons que pour n‚Äôimporte quelle autre valeur de <span class="math notranslate nohighlight">\(\theta \neq \theta(t)\)</span> nous aurons toujours¬†:</p>
<div class="math notranslate nohighlight">
\[L\Big(q_{\theta(t)}\Big(\mathcal{Z}^n | \mathcal{S}^n\Big), \theta\Big)  \leq ln\Big(q_{\theta}\Big(\mathcal{S}^n\Big)\Big)\]</div>
<p>Autrement dit la log-vraissemblance  <span class="math notranslate nohighlight">\(\ln\Big(q_{\theta}\Big(\mathcal{S}^n\Big)\Big)\)</span> et son minorant <span class="math notranslate nohighlight">\(L\Big(q_{\theta(t)}\Big(\mathcal{Z}_n | \mathcal{S}_n\Big), \theta\Big)\)</span> sont tangents en la valeur courante du param√®tre <span class="math notranslate nohighlight">\(\theta(t)\)</span> et par d√©finition la premi√®re est toujours strictement au dessus de la deuxi√®me. Par cons√©quent, une valeur <span class="math notranslate nohighlight">\(\theta= \theta(t+1)\)</span> qui accro√Æt <span class="math notranslate nohighlight">\(L\Big(q_{\theta(t)}\Big(\mathcal{Z}^n | \mathcal{S}^n\Big), \theta\Big)\)</span> correspondra aussi √† une valeur de param√®tre permettant d‚Äôacco√Ætre <span class="math notranslate nohighlight">\(\ln\Big(q_{\theta}\Big(\mathcal{S}^n\Big)\Big)\)</span>.</p>
<p>Nous allons voir que fixer <span class="math notranslate nohighlight">\(q'\Big(\mathcal{Z}_n\Big) = q_{\theta(t)}\Big(\mathcal{Z}_n | \mathcal{S}_n\Big)\)</span> correspond √† l‚Äô√©tape E de l‚Äôalgorithme EM, maximiser <span class="math notranslate nohighlight">\(L\Big(q_{\theta(t)}\Big(\mathcal{Z}^n | \mathcal{S}^n\Big), \theta\Big)\)</span>  par rapport √† <span class="math notranslate nohighlight">\(\theta\)</span>(en gardant fixe <span class="math notranslate nohighlight">\(q'\)</span>) revient exactement √† l‚Äô√©tape M de l‚Äôalgorithme EM. Pour cela, il convient de v√©rifier que l‚Äôexpression de <span class="math notranslate nohighlight">\(L\Big(q_{\theta(t)}\Big(\mathcal{Z}^n | \mathcal{S}^n\Big), \theta\Big)\)</span> correspond bien √† la quantit√© √† maximiser vue pr√©c√©demment¬†:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\theta, \theta(t)) = \mathbb{E}_{z_i\sim q_{\theta(t)}(z_i|x_i)} \Bigg[\sum_i^n ln \Big(q_{\theta}(\boldsymbol{x}_i,\boldsymbol{z}_i)\Big)\Bigg]\]</div>
<p>On d√©veloppe donc l‚Äôexpression de <span class="math notranslate nohighlight">\(L\Big(q_{\theta(t)}\Big(\mathcal{Z}_n | \mathcal{S}_n\Big), \theta\Big)\)</span>¬†:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
L\Big(q_{\theta(t)}\Big(\mathcal{Z}_n | \mathcal{S}_n\Big), \theta\Big)&amp;= \mathbb{E}_{\mathcal{Z}_n \sim q_{\theta(t)}(z|x)^n}\Bigg[ ln\Bigg( \frac{q_{\theta}\Big(\mathcal{S}_n, \mathcal{Z}_n\Big)}{q_{\theta(t)}\Big(\mathcal{Z}_n | \mathcal{S}_n\Big)}\Bigg)\Bigg]\\
&amp;= \mathbb{E}_{\mathcal{Z}_n \sim q_{\theta(t)}(z|x)_n}\Bigg[ ln\Bigg( q_{\theta}\Big(\mathcal{S}_n, \mathcal{Z}_n\Big)\Bigg)\Bigg]- \mathbb{E}_{z_i \sim q_{\theta(t)}(z_i|x_i)}\Bigg[ \ln\Big( q_{\theta(t)}(z_i | x_i)\Big)\Bigg]\\
&amp;= \mathbb{E}_{z_i \sim q_{\theta(t)}(z_i|x_i)}\Bigg[\sum_i^n ln\Big( q_{\theta}(x_i, z_i)\Big)\Bigg] - \text{const}\\
&amp;= \mathcal{L}(\theta, \theta(t)) - \text{const}
\end{aligned}\end{split}\]</div>
<p>O√π nous retrouvons le premier terme comme √©tant exactement √©gal √† la quantit√© √† maximizer dans le cadre de l‚Äôalgorithme EM vu pr√©c√©dement, et le deuxi√®me terme ne d√©pend pas de <span class="math notranslate nohighlight">\(\theta\)</span> mais seulement de <span class="math notranslate nohighlight">\(\theta(t)\)</span> qu‚Äôon a fix√© √† la premi√®re √©tape o√π l‚Äôon √† choisi <span class="math notranslate nohighlight">\(q'\)</span>. On vient donc de montrer que¬†:</p>
<ol>
<li><p><strong>√©tape E</strong>: en fixant :</p>
<div class="math notranslate nohighlight">
\[q'\Big(\mathcal{Z}_n\Big) = q_{\theta(t)}\Big(\mathcal{Z}_n | \mathcal{S}_n\Big)\]</div>
</li>
<li><p><strong>√©tape M</strong>: en maximisant la log vraissemblance par rapport √† theta en gardant fixe ce <span class="math notranslate nohighlight">\(q'\)</span>:</p>
<div class="math notranslate nohighlight">
\[\theta(t+1) = \arg \max_{\theta} \Bigg[\mathbb{E}_{z_i \sim q_{\theta(t)}(z_i|x_i)}\Bigg[\sum_i^n \ln\Big( q_{\theta}(x_i, z_i)\Big)\Bigg]\Bigg]\]</div>
</li>
</ol>
<p>alors la quantit√© qu‚Äôon maximise est un minorant qui vaut exactement la log-vraissemblance <span class="math notranslate nohighlight">\(\ln\Big(q_{\theta}\Big(\mathcal{S}^n\Big)\Big)\)</span>. En it√©rant sur ces deux √©tapes on garantit donc de maximiser √† chaque fois la log-vraissemblance sauf si √† un instant <span class="math notranslate nohighlight">\(t\)</span> on se trouve avec une valeur de <span class="math notranslate nohighlight">\(\theta(t)\)</span> qui maximise localement cette derni√®re (le gradient sera nul).</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./7_unsupervised"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="1_principal_component_analysis.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">L‚ÄôAnalyse en Composantes Principales ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../8_set_prediction/0_propos_liminaire.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Pr√©diction d‚Äôensembles</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Servajean, Leveau & Chailan<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>
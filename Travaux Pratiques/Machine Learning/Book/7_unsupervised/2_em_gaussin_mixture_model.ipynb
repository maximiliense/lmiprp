{"cells": [{"cell_type": "markdown", "metadata": {"id": "WGLcChXQ6RU-"}, "source": ["# Mod\u00e8le de M\u00e9lange Gaussien et algorithme *Expectation-Maximization*\n", "\n", "**<span style='color:blue'> Objectifs de la s\u00e9quence</span>** ", "\n", "* \u00catre sensibilis\u00e9&nbsp;:\n", "    * aux mod\u00e8les \u00e0 variables cach\u00e9es.\n", "* \u00catre capable&nbsp;:\n", "    * d'impl\u00e9menter une GMM avec $\\texttt{sklearn}$.\n", "\n", "\n\n ----"]}, {"cell_type": "markdown", "metadata": {"id": "pbSeDE0L6RVF"}, "source": ["# I. Introduction"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["remove-input"]}, "outputs": [], "source": ["import numpy as np\n", "from scipy.stats import multivariate_normal \n", "%matplotlib inline\n", "\n", "import matplotlib.pyplot as plt\n", "plt.rcParams['figure.figsize'] = (12.0, 8.0)\n", "plt.style.use('ggplot')\n", "\n", "class MultivariateGaussianMixture(object):        \n", "    def __init__(self, n_components=2, d=2, probas=None):\n", "        self.n_components = n_components\n", "        self.sigma = []\n", "        self.mu = []\n", "        \n", "        if probas is None:\n", "            self.probas = np.ones(n_components)/n_components\n", "        else:\n", "            self.probas = np.array(probas)\n", "            s = np.sum(self.probas)\n", "            assert s < 1.0 + 1e-10 and s > 1.0 - 1e-10, \"Probabilites should add up to 1, but s = %f\"%s\n", "\n", "        scale = 1\n", "        scater = 10\n", "        sigma = scale * np.diag([1.0, 0.1])\n", "        for k in range(n_components):\n", "            Q = self.random_rotation(d)\n", "            self.sigma.append(np.dot(np.dot(Q, sigma ), Q.T))\n", "            self.mu.append(np.random.uniform(-scater,scater,size=d))\n", "            \n", "    def random_rotation(self, d):\n", "        Q, _ = np.linalg.qr(np.random.random((d,d)))\n", "        return Q\n", "    \n", "    def sample_categorical(self, N):\n", "        idx_sort = np.argsort(-self.probas)\n", "        probas_sort = self.probas[idx_sort]\n", "\n", "        c_probas = np.array([np.sum(probas_sort[:k+1]) for k in range(self.n_components)])   \n", "        X = np.random.uniform(0,1, size=N)\n", "        for i,x in enumerate(X):\n", "            c = c_probas-x\n", "            X[i] = idx_sort[self.n_components-len(c[c>=0])]\n", "        return X.astype(np.uint)\n", "\n", "    def generate_samples(self, N):\n", "        Z = self.sample_categorical(N)\n", "        return np.array([np.random.multivariate_normal(self.mu[z], self.sigma[z]) for z in Z]), Z\n", "    \n", "    \n", "def plot(X, c=None, means=None, gmd=None, n_std=3, cumul=True):\n", "    # Plot the dataset\n", "    if c is not None:\n", "        plt.scatter(X[:, 0], X[:, 1], c=c)\n", "    else:\n", "        plt.scatter(X[:, 0], X[:, 1])\n", "        \n", "    # Plot the means\n", "    if means is not None:\n", "        for m in means:\n", "            plt.scatter([m[0]], [m[1]], s=55, color='red')\n", "\n", "    # Plot the concentric ellipses to vizualize covariance matrixes of the components\n", "    if gmd is not None:\n", "        stds = [n_std]\n", "    if cumul:\n", "        stds = [i for i in range(1, n_std + 1)]\n", "    \n", "    for std in stds:\n", "        # For each component, we look for the maximum variance direction carried by the eigen vectors\n", "        # The parameters of the elipse are defined by the std deviations ssociated to the sqrt of the eigen values.\n", "        for mu_k, sigma_k in zip(gmd.mu, gmd.sigma):\n", "            e,w = np.linalg.eig(sigma_k)\n", "            top_e = np.argsort(-e)[0]\n", "\n", "            a=std*np.sqrt(e[top_e])\n", "            b=std*np.sqrt(e[top_e-1])\n", "\n", "            t = np.linspace(-20, 20,100)\n", "            Ell = np.array([a*np.cos(t) , b*np.sin(t)])  \n", "\n", "            cos_rot=w[top_e][0]/np.linalg.norm(w[top_e])\n", "            sin_rot=np.sqrt(1-cos_rot**2)\n", "            R_rot = np.array([[cos_rot , -sin_rot],[sin_rot ,cos_rot]])\n", "\n", "            Ell_rot = np.zeros((2,Ell.shape[1]))\n", "            for i in range(Ell.shape[1]):\n", "                Ell_rot[:,i] = np.dot(R_rot,Ell[:,i])\n", "\n", "            plt.plot(mu_k[0]+Ell_rot[0,:] , mu_k[1]+Ell_rot[1,:], color=\"black\", lw=0.1)\n", "\n", "MGM = MultivariateGaussianMixture(n_components = 6)\n", "X, Z = MGM.generate_samples(1000)\n", "\n", "plot(X, Z, gmd=MGM)\n", "plt.axis('off')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"id": "Dqn_bG0b6RVH"}, "source": ["Dans ce chapitre nous allons consid\u00e9rer des m\u00e9thodes d'apprentissage non-supervis\u00e9es probabilistes et plus particuli\u00e8rement des mod\u00e8les g\u00e9n\u00e9ratifs dont l'id\u00e9e g\u00e9n\u00e9rale est de consid\u00e9rer l'\u00e9chantillon de donn\u00e9es observ\u00e9es $\\mathcal{S}_n = \\{x_1,  \\dots, x_n\\}$ comme des variable al\u00e9atoires i.i.d de processus g\u00e9n\u00e9rateur $p_{\\mathcal{D}} : x \\sim p_{\\mathcal{D}}$. Le cas du m\u00e9lange gaussien est int\u00e9ressant car il part du principe que nos \u00e9chantillons sont simul\u00e9es selon une loi dont la densit\u00e9 a la forme suivante&nbsp;:\n", "\n", "$$q(x;\\theta)=\\sum_{j=1}^K\\pi_jf(x;\\mu_j,\\sigma_j),$$\n", "\n", "o\u00f9 $\\theta=\\{\\pi, \\mu, \\sigma\\}$. Le processus g\u00e9n\u00e9rateur d'un tel mod\u00e8le, comme nous allons le voir, fonctionne de la mani\u00e8re suivante&nbsp;:\n", "\n", "1.  On simule, selon une cat\u00e9gorielle de param\u00e8tre $\\pi$,\n", "2.  On simule selon la loi $f$ associ\u00e9e au \"groupe\" choisi pr\u00e9c\u00e9demment.\n", "\n", "\n", "Notre objectif est de calculer $\\theta$ tel que la vraisemblance de nos donn\u00e9es $\\mathcal{S}_n$ est maximis\u00e9e. Cela se passe par des algorithmes comme le *Expectation-Maximization* qui introduiront une variable latente $z$ indiquant le \"groupe\" utilis\u00e9 afin de g\u00e9n\u00e9rer nos \u00e9chantillon. Dans ces approches, les repr\u00e9sentations des donn\u00e9es seront consid\u00e9r\u00e9es comme ces nouvelles variables al\u00e9atoires $z$ qu'on appellera des variables latentes (ou cach\u00e9es) qui prendront leur valeur conjointement avec les donn\u00e9es observ\u00e9es $x$.\n", "\n", "**<span style='color:green'> Loi et processus g\u00e9n\u00e9rateur</span>** ", "\n", "On utilisera par abus de langage l'expression $p$ ou $q_{\\theta}$ pour d\u00e9noter \u00e0 la fois le processus g\u00e9n\u00e9rateur des donn\u00e9es, la loi de probabilit\u00e9 lui correspondant et la densit\u00e9 de probabilit\u00e9 associ\u00e9e.\n", "\n\n ----", "\n", "## A. G\u00e9n\u00e9ralit\u00e9s sur les mod\u00e8les g\u00e9n\u00e9ratifs\n", "Nous pouvons dans un premier temps introduire un objectif g\u00e9n\u00e9ral des mod\u00e8les non-supervis\u00e9s g\u00e9n\u00e9ratifs. Il s'agit de trouver dans une famille param\u00e9trique de distributions de probabilit\u00e9 $\\mathcal{Q}=\\{q_\\theta, \\theta\\in\\mathbb{R}^p\\}$ une param\u00e9trisation $q_{\\theta}(x) = q(x|{\\theta})$ qui \"explique le mieux\" les donn\u00e9es observ\u00e9es. Comme pour le cas de l'apprentissage supervis\u00e9, nous pouvons consid\u00e9rer le crit\u00e8re de maximum de vraisemblance. Nous cherchons donc&nbsp;:\n", "\n", "\n", "$$\\theta_{\\text{ML}} = \\arg \\max_{\\theta} \\Big[q(\\mathcal{S}_n|\\theta)\\Big] = \\arg \\max_{\\theta } \\Big[\\prod_i q(x_i|\\theta)\\Big].$$\n", "\n", "\n", "Ou bien, de mani\u00e8re \u00e9quivalente, le maximum de la log-vraissemblance&nbsp;:\n", "\n", "$$\\theta_{\\text{ML}} = \\arg \\max_{\\theta} \\Big[\\sum_i \\ln\\Big(q(x_i|\\theta)\\Big)\\Big].$$\n", "\n", "**<span style='color:blue'> Exercice</span>** ", "Soit $\\mathcal{S}_n\\sim \\mathcal{N}(\\mu,1)^n$ un \u00e9chantillon simul\u00e9 selon une loi normale de variance $1$ et de moyenne $\\mu$. Quel est le $\\mu$ qui maximise la vraisemblance ?\n", "\n\n ----", "\n", "\n", "\n", "## B. Les variables cach\u00e9es\n", "Le calcul de ce maximum de vraisemblance dans le cas d'un mod\u00e8le de m\u00e9lange n\u00e9cessite d'introduire de nouvelles variables&nbsp;: les variables cach\u00e9es. Ces derni\u00e8res seront les repr\u00e9sentations de nos observations. Cela se fait en d\u00e9marginalisant&nbsp;\n", "\n", "$$q_{\\theta}(x) = \\int q_{\\theta}(x,z)dz = \\int q_{\\theta}(z) q_{\\theta}(x|z)dz$$\n", "\n", "\n", "**<span style='color:green'> Remarque</span>** ", "\n", "On souhaite \u00e9viter de tomber sur une distribution o\u00f9 les variables $z$ et $x$ sont ind\u00e9pendantes&nbsp;:\n", "\n", "$$q_{\\theta}(x,z) = q_{\\theta}(x)q_{\\theta}(z) \\Leftrightarrow q_{\\theta}(z|x) = q_{\\theta}(z)$$\n", "\n", "car on souhaite d\u00e9composer notre loi initiale en atomes qui seraient plus faciles \u00e0 estimer. Pour le m\u00e9lange, si on \"connait\" le groupe d'un de nos points (ce serait notre $z$), alors on peut plus facilement estimer la loi du groupe (i.e. c'est l'estimation simple de la loi et non du m\u00e9lange complet). De plus, dans le cas o\u00f9 les $x$ seraient ind\u00e9pendants des $z$, ce dernier ne pourrait en aucun cas \u00eatre une bonne repr\u00e9sentation.\n", "\n\n ----", "\n", "\n", "Nous allons donc chercher \u00e0 ajouter une variable latente $z$ pertinente ! Dans le cas de variables cach\u00e9es discr\u00e8tes (comme c'est le cas dans ce TP)&nbsp;:\n", "\n", "\n", "$$q_{\\theta}(x) = \\sum_{k=1}^K q_{\\theta}(z=k)q_{\\theta}(x|z=k)$$\n", "\n", "\n", "Nous allons donc chercher \u00e0 maximiser la log-vraissemblance de la forme&nbsp;:\n", "\n", "$$\\theta_{\\text{ML}} = \\arg\\max_{\\theta } \\Big[\\sum_i \\ln\\Big(\\sum_{k=1}^c q_{\\theta}(z=k)q_{\\theta}(x_i|z=k)\\Big)\\Big]$$\n", "\n", "\n", "**<span style='color:green'> Remarque</span>** ", "\n", "Une fa\u00e7on de voir les variables cach\u00e9es qui va nous int\u00e9resser dans le cadre de l'apprentissage de repr\u00e9sentation est comme des facteurs explicatifs des donn\u00e9es. C\u2019est-\u00e0-dire un nombre restreint de variables telles qu'on peut expliquer/pr\u00e9dire une observation sachant ces variables. On peut voir \u00e7a comme des facteurs causaux tels que si on connait la cause premi\u00e8re d'une chose on peut en d\u00e9duire une observation complexes qui en d\u00e9coule ou du moins une part significative. En ce sens, les facteurs explicatifs peuvent \u00eatre consid\u00e9r\u00e9s comme une abstraction, une repr\u00e9sentation plus structur\u00e9e et compacte des observations. Ainsi une mani\u00e8re de voir le processus g\u00e9n\u00e9rateur des donn\u00e9es est d\u2019\u00e9chantillonner la cause, la valeur du facteur explicatif $z_i \\sim p(z)$, puis sachant la cause, \u00e9chantillonner l'observation qui en d\u00e9coule par la conditionnelle $x_i \\sim p(x |z=z_i)$.\n", "\n\n ----"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# II. Le mod\u00e8le de m\u00e9lange Gaussien vu sous l'angle des mod\u00e8les \u00e0 variables cach\u00e9es\n", "\n", "La famille des mixtures de distributions mettent en jeu des densit\u00e9s de probabilit\u00e9 sous la forme d'une combinaison pond\u00e9r\u00e9e de densit\u00e9s d'une m\u00eame famille. Dans ce TP nous allons nous concentrer sur le cas des mixtures de gaussiennes multi-vari\u00e9es. Les mod\u00e8les de mixtures de gausiennes consistent \u00e0 mod\u00e9liser une densit\u00e9 de probabilit\u00e9 o\u00f9 les donn\u00e9es sont distribu\u00e9es localement selon des distributions normales multi-vari\u00e9es. Dans ce cas, l'indice de la gaussienne \u00e0 laquelle apaprtient une donn\u00e9e particuli\u00e8re est la variable cach\u00e9e qui lui est associ\u00e9e. La totalit\u00e9 de l'information r\u00e9siduelle est ensuite port\u00e9e par la densit\u00e9 de probabilit\u00e9 de la composante gaussienne. En notant $z=k$ l'identifiant d'une gausienne qui poss\u00e8de ces param\u00e8tres propres $(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$, la densit\u00e9 de probabilit\u00e9 du processus g\u00e9n\u00e9rateur des donn\u00e9es prend la forme suivante&nbsp;:\n", "\n", "\n", "$$p(\\boldsymbol{x}; \\boldsymbol{\\mu},\\boldsymbol{\\Sigma}) = \\sum_{Z=k}^K p(z=k) f(\\boldsymbol{x};\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$$\n", "\n", "\n", "o\u00f9 on notera par la suite $p(z=k) = \\pi_k$ qui correspond \u00e0 la probabilit\u00e9 a priori de la $k$-i\u00e8me composante (on a donc $\\sum_k \\pi_k = 1$) et $f(\\boldsymbol{x};\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$ correspond \u00e0 la densit\u00e9 de probabilit\u00e9 de la loi normale multi-vari\u00e9e $\\mathcal{N}(\\boldsymbol{x}|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$ de moyenne et matrice de variance-covariance $(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$&nbsp;:\n", "\n", "$$f(\\boldsymbol{x};\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) = \\frac{1}{(2\\pi)^{\\frac{d}{2}}|\\boldsymbol{\\Sigma}_k|^{\\frac{1}{2}}} \\exp^{-\\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{\\mu}_k)^T)\\boldsymbol{\\Sigma}_k^{-1}(\\boldsymbol{x} - \\boldsymbol{\\mu}_k)}$$\n", "\n", "\n", "Il s'agit de la densit\u00e9 de probabilit\u00e9 d'observer une variable $x$ sachant la valeur de la variable cach\u00e9e que l'on pourra noter $f_k(\\boldsymbol{x})$ par la suite par soucis de lisibilit\u00e9. On peut aussi calculer quelques quantit\u00e9s d'int\u00e9r\u00eat qui vont nous servir par la suite comme la densit\u00e9 de probabilit\u00e9 de la variable cach\u00e9e a posteriori de l'observation $x$ grace \u00e0 la formule de Bayes&nbsp;:\n", "\n", "$$p(z=k|x) = \\frac{p(z=k)p(x|z=k)}{\\sum_j p(z=j)p(x|z=j)} = \\frac{\\pi_k f(\\boldsymbol{x};\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}{\\sum_j \\pi_j f(\\boldsymbol{x};\\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)}$$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## A. Echantillonage et visualisation d'un m\u00e9lange Gaussien\n", "\n", "Comme expliqu\u00e9 pr\u00e9c\u00e9demment, on peut donc voir le processus g\u00e9n\u00e9rateur correspondant \u00e0 une mixture gaussienne comme se structurant en deux \u00e9tapes. La premi\u00e8re \u00e9tape consiste en une variable cat\u00e9gorielle \u00e0 $K$ valeurs dont les probabilit\u00e9s respectives sont les $\\pi_k$. Puis connaissant la valeur tir\u00e9e $k$; il s'agit ensuite d\u2019\u00e9chantillonner selon une loi normale multivari\u00e9e de param\u00e8tres $(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$. Le code ce dessous nous permet selon ce proc\u00e9d\u00e9 d\u2019\u00e9chantillonnage de g\u00e9n\u00e9rer et visualiser un jeu de donn\u00e9es distribu\u00e9 selon une mixture de gaussienne ou les composantes ont \u00e9t\u00e9 choisies avec des param\u00e8tres al\u00e9atoires."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 500}, "id": "pUut7dS26RVJ", "outputId": "b0eda091-909d-45fd-aa88-9908a8d299a5"}, "outputs": [], "source": ["import numpy as np\n", "from scipy.stats import multivariate_normal \n", "%matplotlib inline\n", "\n", "import matplotlib.pyplot as plt\n", "plt.rcParams['figure.figsize'] = (12.0, 8.0)\n", "plt.style.use('ggplot')\n", "\n", "class MultivariateGaussianMixture(object):        \n", "    def __init__(self, n_components=2, d=2, probas=None):\n", "        self.n_components = n_components\n", "        self.sigma = []\n", "        self.mu = []\n", "        \n", "        if probas is None:\n", "            self.probas = np.ones(n_components)/n_components\n", "        else:\n", "            self.probas = np.array(probas)\n", "            s = np.sum(self.probas)\n", "            assert s < 1.0 + 1e-10 and s > 1.0 - 1e-10, \"Probabilites should add up to 1, but s = %f\"%s\n", "\n", "        scale = 1\n", "        scater = 10\n", "        sigma = scale * np.diag([1.0, 0.1])\n", "        for k in range(n_components):\n", "            Q = self.random_rotation(d)\n", "            self.sigma.append(np.dot(np.dot(Q, sigma ), Q.T))\n", "            self.mu.append(np.random.uniform(-scater,scater,size=d))\n", "            \n", "    def random_rotation(self, d):\n", "        Q, _ = np.linalg.qr(np.random.random((d,d)))\n", "        return Q\n", "    \n", "    def sample_categorical(self, N):\n", "        idx_sort = np.argsort(-self.probas)\n", "        probas_sort = self.probas[idx_sort]\n", "\n", "        c_probas = np.array([np.sum(probas_sort[:k+1]) for k in range(self.n_components)])   \n", "        X = np.random.uniform(0,1, size=N)\n", "        for i,x in enumerate(X):\n", "            c = c_probas-x\n", "            X[i] = idx_sort[self.n_components-len(c[c>=0])]\n", "        return X.astype(np.uint)\n", "\n", "    def generate_samples(self, N):\n", "        Z = self.sample_categorical(N)\n", "        return np.array([np.random.multivariate_normal(self.mu[z], self.sigma[z]) for z in Z]), Z\n", "    \n", "    \n", "def plot(X, c=None, means=None, gmd=None, n_std=3, cumul=True):\n", "    # Plot the dataset\n", "    if c is not None:\n", "        plt.scatter(X[:, 0], X[:, 1], c=c)\n", "    else:\n", "        plt.scatter(X[:, 0], X[:, 1])\n", "        \n", "    # Plot the means\n", "    if means is not None:\n", "        for m in means:\n", "            plt.scatter([m[0]], [m[1]], s=55, color='red')\n", "\n", "    # Plot the concentric ellipses to vizualize covariance matrixes of the components\n", "    if gmd is not None:\n", "        stds = [n_std]\n", "        if cumul:\n", "            stds = [i for i in range(1, n_std + 1)]\n", "\n", "        for std in stds:\n", "            # For each component, we look for the maximum variance direction carried by the eigen vectors\n", "            # The parameters of the elipse are defined by the std deviations ssociated to the sqrt of the eigen values.\n", "            for mu_k, sigma_k in zip(gmd.mu, gmd.sigma):\n", "                e,w = np.linalg.eig(sigma_k)\n", "                top_e = np.argsort(-e)[0]\n", "\n", "                a=std*np.sqrt(e[top_e])\n", "                b=std*np.sqrt(e[top_e-1])\n", "\n", "                t = np.linspace(-20, 20,100)\n", "                Ell = np.array([a*np.cos(t) , b*np.sin(t)])  \n", "\n", "                cos_rot=w[top_e][0]/np.linalg.norm(w[top_e])\n", "                sin_rot=np.sqrt(1-cos_rot**2)\n", "                R_rot = np.array([[cos_rot , -sin_rot],[sin_rot ,cos_rot]])\n", "\n", "                Ell_rot = np.zeros((2,Ell.shape[1]))\n", "                for i in range(Ell.shape[1]):\n", "                    Ell_rot[:,i] = np.dot(R_rot,Ell[:,i])\n", "\n", "                plt.plot(mu_k[0]+Ell_rot[0,:] , mu_k[1]+Ell_rot[1,:], color=\"black\", lw=0.1)\n", "\n", "MGM = MultivariateGaussianMixture(n_components = 6)\n", "X, _ = MGM.generate_samples(1000)\n", "\n", "plot(X)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ici nous visualisons une mixture de $K=6$ composantes gaussiennes tir\u00e9es al\u00e9atoirement (avec les $\\pi_k$ choisis de mani\u00e8re uniforme: $\\pi_k=\\frac{1}{K} \\forall k$).  Dans la suite de ce TP, nous allons voir comment nous pouvons estimer les param\u00e8tres d'un mod\u00e8le g\u00e9n\u00e9ratif de ce type par un algorithme it\u00e9ratif qui, \u00e0 chaque it\u00e9ration, va proposer un jeu de param\u00e8tres qui maximise de plus en plus la log-vraissemblance de ce mod\u00e8le&nbsp;: l'algorithme Expectation Maximization (EM).\n", "\n", "Une application ce genre de mod\u00e8les g\u00e9n\u00e9ratifs qui vient directement \u00e0 l'esprit est le clustering. C'est-\u00e0-dire  qu'on veut trouver des groupes de donn\u00e9es similaires. La particularit\u00e9 ici est que nous choisissons dans une famille de distributions o\u00f9 les clusters sont d\u00e9crits par des normales multi-vari\u00e9es. On peut donc voir ce mod\u00e8le de mixtures gaussien comme une sorte de g\u00e9n\u00e9ralisation de l'algorithme *K-means* (nous reviendrons sur le lien entre ces deux concepts un peu plus loin)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## B. L'algortihme EM pour estimer un mod\u00e8le de m\u00e9lange gaussien\n", "\n", "Dans cette partie nous allons tenter uniquement \u00e0 partir du jeu de donn\u00e9es g\u00e9n\u00e9r\u00e9 pr\u00e9c\u00e9dement de retrouver les param\u00e8tres $\\theta^{\\star} = \\Big\\{ \\pi_k^{\\star}, \\boldsymbol{\\mu}_k^{\\star}, \\boldsymbol{\\Sigma}_k^{\\star} \\Big\\}_{k \\leq K}$ du vrai processus qui nous a permis de g\u00e9n\u00e9rer ce jeu de donn\u00e9es. On va donc se placer dans une famille param\u00e9trique de distributions $\\mathcal{Q}$ telle que chaque distribution $q_{\\theta} \\in \\mathcal{Q}$ s'\u00e9crit sous la forme&nbsp;:\n", "\n", "\n", "$$q_{\\theta}(\\boldsymbol{x}) =  \\sum_{k=1}^K \\pi_k f(\\boldsymbol{x};\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$$\n", "\n", "\n", "Et on cherchera donc \u00e0 trouver le jeu de param\u00e8tres qui va maximiser la log-vraissemblance suivante&nbsp;:\n", "\n", "$$\\ln\\Big(q_{\\theta}(\\mathcal{S}_n)\\Big) = \\sum_i^n \\ln \\Bigg(\\sum_k^K \\pi_k f(\\boldsymbol{x}_i;\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) \\Bigg)$$\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### i. Premi\u00e8re diff\u00e9rence avec l'estimation d'une gaussienne multivari\u00e9e simple\n", "\n", "On rappelle que dans le cas d'une loi gausienne multivari\u00e9e simple, la log-vraissemblance est&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "\\ln\\Big(q_{\\theta}(\\mathcal{S}_n)\\Big) &= \\sum_i^n \\ln \\Big(f(\\boldsymbol{x}_i;\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\\Big)= \\sum_i^n \\ln \\Bigg( \\frac{1}{(2\\pi)^{\\frac{d}{2}}|\\boldsymbol{\\Sigma}|^{\\frac{1}{2}}}  \\exp^{-\\frac{1}{2}(\\boldsymbol{x}_i - \\boldsymbol{\\mu})^T)\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x}_i - \\boldsymbol{\\mu})} \\Bigg)\\\\\n", "& = -\\frac{1}{2} \\Bigg[\\sum_i^n d\\ln (2\\pi) + ln\\Big(|\\boldsymbol{\\Sigma}|\\Big) + (\\boldsymbol{x}_i - \\boldsymbol{\\mu})^T\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x}_i - \\boldsymbol{\\mu})\\Bigg]\n", "\\end{aligned}$$\n", "\n", "Il existe donc une forme close o\u00f9 annuler le gradient de cette expression par rapport \u00e0 $\\boldsymbol{\\mu}$ et $\\boldsymbol{\\Sigma}$ conduit \u00e0 une unique solution.\n", "\n", "**<span style='color:blue'> Exercice</span>** ", "Calculer le gradient de la log-vraissemblance de la loi normale multivari\u00e9e par rapport \u00e0 ses param\u00e8tres $\\boldsymbol{\\mu}$ et $\\boldsymbol{\\Sigma}$ puis trouver l'expression qui annule le gradient. \n", "\n", "\n\n ----", "\n", "**<span style='color:green'> Indices</span>** ", "\n", "$\\nabla_{\\Sigma}log(|\\Sigma|) = \\Sigma^{-1}$ et $\\nabla_{\\Sigma}\\boldsymbol{a}^T\\Sigma^{-1}\\boldsymbol{b} = -\\Sigma^{-1}\\boldsymbol{b}\\boldsymbol{a}^T\\Sigma^{-1}$.\n", "\n\n ----", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Dans la suite nous allons voir que ce qui nous arrange dans le cas la loi normale multivari\u00e9e simple est que le logarithme de l'expression de la log-vraissemblance \"annule\" l'exponentielle ce qui rend l'expression de la log-vraisemblance et son gradient tractable. Malheureusement, dans le cas du m\u00e9lange gaussien, une somme d'exponentielles se trouve dans le logarithme et cela rend les expressions beaucoup plus complexes. C'est l\u00e0 que l'algorithme EM prend toute sa pertinence car il va nous permettre malgr\u00e9 tout de trouver un processus it\u00e9ratif qui augmente la log-vraisemblance a chaque it\u00e9ration."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### ii. Optimisation des $\\boldsymbol{\\mu}_k$\n", "\n", "Calculons l'expression du gradient de la log vraissemblance par rapports aux param\u00e8tres $\\boldsymbol{\\mu}_k$&nbsp;:\n", "\n", "\n", "$$\\nabla_{\\boldsymbol{\\mu}_k} \\ln\\Big(q_{\\theta}\\mathcal{S}_n\\Big) = \\Bigg[\\sum_i^n  \\frac{\\partial ln\\Big( \\sum_j \\pi_j f_j(\\boldsymbol{x}_i) \\Big)}{\\partial f_k(\\boldsymbol{x}_i)}  \\nabla_{\\boldsymbol{\\mu}_k} f_k(\\boldsymbol{x}_i) \\Bigg] = \\boldsymbol{0}$$\n", "\n", "o\u00f9 l'on a utilis\u00e9 la r\u00e8gle de composition des d\u00e9riv\u00e9es. Le premier facteur nous donne&nbsp;:\n", "\n", "$$\\frac{\\partial ln\\Big( \\sum_j \\pi_j f_j(\\boldsymbol{x}_i) \\Big)}{\\partial f_k(\\boldsymbol{x}_i)} = \\frac{\\pi_k }{\\sum_{j}^K \\pi_j f(\\boldsymbol{x}_i;\\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)}$$\n", "\n", "\n", "---\n", "\n", "Concernant $\\nabla_{\\boldsymbol{\\mu}_k} f_k(\\boldsymbol{x}_i)$, introduisons pour des raisons de lisibilit\u00e9 les notations suivnates&nbsp;:\n", "\n", "$$f(\\boldsymbol{x}_i;\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)  = f_k(\\boldsymbol{x}_i) = \\frac{e^{g(\\boldsymbol{x}_i, \\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k)}}{C(\\boldsymbol{\\Sigma_k})}$$\n", "\n", "\n", "o\u00f9&nbsp;:\n", "\n", "$$C(\\boldsymbol{\\Sigma_k}) = (2\\pi)^{\\frac{d}{2}}|\\boldsymbol{\\Sigma}_k|^{\\frac{1}{2}}$$\n", "\n", "et&nbsp;:\n", "\n", "$$g(\\boldsymbol{x}_i, \\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k) = g_k(\\boldsymbol{x}_i) = -\\frac{1}{2}(\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k)^T)\\boldsymbol{\\Sigma}_k^{-1}(\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k).$$\n", "\n", "\n", "---\n", "\n", "En utilisant \u00e0 nouveau la r\u00e8gle de d\u00e9rivation d'une composition, nous obtenons&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "\\nabla_{\\boldsymbol{\\mu}_k} f_k(\\boldsymbol{x}_i) &= \\underbrace{\\partial_{g_k(\\boldsymbol{x}_i)}\\Bigg(\\frac{e^{g_k(\\boldsymbol{x}_i)}}{C(\\boldsymbol{\\Sigma_k})}\\Bigg)}_{ = \\frac{e^{g_k(\\boldsymbol{x}_i)}}{C(\\boldsymbol{\\Sigma_k})} = f(\\boldsymbol{x}_i; \\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k)} \\nabla_{{\\boldsymbol{\\mu}_k}} g(\\boldsymbol{x}_i, \\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k)\\\\\n", "& = f(\\boldsymbol{x}_i; \\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k)\n", "\\underbrace{\n", "    \\nabla_{\\boldsymbol{\\mu}_k}\\Bigg[ -\\frac{1}{2}(\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k)^T)\\boldsymbol{\\Sigma}_k^{-1}(\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k)\\Bigg]\n", "}_{\n", "    =(-\\frac{1}{2})(-2)\\boldsymbol{\\Sigma}_k^{-1}(\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k)\n", "}\n", "\\end{aligned}$$\n", "\n", "En injectant nos r\u00e9sultats interm\u00e9diaires, nous obtenons finalement&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "\\nabla_{\\boldsymbol{\\mu}_k} \\ln\\Big(q_{\\theta}\\mathcal{S}^n\\Big) &= \\Bigg[\\sum_i^n  \\underbrace{\\frac{\\pi_k }{\\sum_{j}^K \\pi_j f(\\boldsymbol{x}_i;\\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)}f(\\boldsymbol{x}_i;\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) }_{q_{\\theta}(z=k|\\boldsymbol{x}_i, \\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k)} (-\\frac{1}{2}) (-2) \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k)\\Bigg]\\\\\n", "& = \\Bigg[\\sum_i^n  q_{\\theta}(z=k|\\boldsymbol{x}_i, \\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k) \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k)\\Bigg] = \\boldsymbol{0}\n", "\\end{aligned}$$\n", "\n", "\n", "Remarquons qu'apparait la probabilit\u00e9 a posteriori \"que le facteur explicatif $k$ ait caus\u00e9\" l'observation $x_i$.\n", "\n", "Nous noterons ce terme $ \\gamma_k^i = q_{\\theta}(z=k|\\boldsymbol{x}_i, \\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k)$. En multipliant par $\\boldsymbol{\\Sigma}_k$ des deux cot\u00e9 nous pouvons r\u00e9soudre et nous trouvons&nbsp;:\n", "\n", "\n", "$$\\boldsymbol{\\mu}_k = \\frac{1}{\\sum_i^n \\gamma_k^i}\\sum_i^n  \\gamma_k^i\\boldsymbol{x}_i$$\n", "\n", "Remarquons qu'il s'agit quasiment du param\u00e8tre de maximum de vraisemblance pour une unique loi normale auquel on rajoute la pond\u00e9ration d'appartenance \u00e0 la $k$-i\u00e8me composante.\n", "\n", "**<span style='color:green'> Lien avec KMeans</span>** ", "\n", "Constatons un premier lien avec *K-Means* en analysant ce \u00e0 quoi correspond la probabilit\u00e9 *a posteriori* $\\gamma_k^i = q_{\\theta}(z=k|\\boldsymbol{x}_i, \\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k)$&nbsp;:\n", "\n", "$$\\gamma_k^i = \\frac{\\pi_k f(\\boldsymbol{x}_i;\\boldsymbol{\\mu}_k, \\lambda\\boldsymbol{I}) }{\\sum_j \\pi_j f(\\boldsymbol{x}_i;\\boldsymbol{\\mu}_j, \\lambda\\boldsymbol{I}) } \\propto \\frac{\\pi_k \\exp(-\\frac{1}{2\\lambda}\\lVert\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k\\rVert_2^2 )}{\\sum_j \\pi_j \\exp(-\\frac{1}{2\\lambda}\\lVert\\boldsymbol{x}_i - \\boldsymbol{\\mu}_j\\rVert_2^2)}$$\n", "\n", "o\u00f9 la matrice de covariance est fix\u00e9e \u00e0 la matrice identit\u00e9. Nous pouvons remarquer qu'il s'agit d'un score de la distance au centre de la composante normalis\u00e9e. Plus une observation se rapproche du centre $\\boldsymbol{\\mu}_k$ d'une gaussienne, plus la contribution qui domine au d\u00e9nominateur correspond \u00e0 la contribution de la $k$-i\u00e8me composante et donc plus probabilit\u00e9 \u00e0 posteriori $q_{\\theta}(z=k|\\boldsymbol{x}_i, \\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k)$ tend vers $1$ et toutes les autres $q_{\\theta}(z=j|\\boldsymbol{x}_i, \\boldsymbol{\\mu}_j,\\boldsymbol{\\Sigma}_j) \\forall j \\neq k$ tendent vers $0$. Il est possible de voir $\\gamma_k^i$ comme un score d'assignation d'une observation $x_i$ \u00e0 la $k$-i\u00e8me gaussienne. \n", "\n", "De plus, remarquons que si la variance des composantes tend vers $0$, alors la probabilit\u00e9 *a posteriori* tend vers $1$ pour une observation $\\boldsymbol{x}_i$&nbsp;:\n", "\n", "$$\\lim\\limits_{\\lambda \\rightarrow 0} q_{\\theta}(z=k|\\boldsymbol{x}_i, \\boldsymbol{\\mu}_k, \\lambda \\boldsymbol{I}) = \\lim\\limits_{\\lambda \\rightarrow 0} \\frac{\\pi_k \\exp(-\\frac{1}{2\\lambda}\\lVert\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k\\rVert_2^2 )}{\\sum_j \\pi_j \\exp(-\\frac{1}{2\\lambda}\\lVert\\boldsymbol{x}_i - \\boldsymbol{\\mu}_j\\rVert_2^2)} = r_k^i =\n", "\\begin{cases}\n", "1 \\text{ si } k = \\arg \\min_j\\lVert\\boldsymbol{x}_i - \\boldsymbol{\\mu}_j\\rVert_2^2 \\\\\n", "0 \\text{ sinon }\n", "\\end{cases}$$\n", "\n", "et nous retrouvons exactement la fonction d'assignation de KMeans et le calcul de $\\boldsymbol{\\mu}_k$ devient:\n", "\n", "$$\\boldsymbol{\\mu}_k = \\frac{1}{\\sum_i^n r_k^i}\\sum_i^n  r_k^i\\boldsymbol{x}_i$$\n", "\n", "qui correspond exactement \u00e0 la proc\u00e9dure de mise \u00e0 jour des centro\u00efde de l'algorithme de K-Means en calculant le barycentre des points assign\u00e9s du $k$_i\u00e8me cluster.\n", "\n", "Une autre quantit\u00e9 int\u00e9r\u00e9ssante \u00e0 consid\u00e9rer est la suivante:\n", "\n", "$$N_k  = \\sum_i^n q_{\\theta}(z=k|\\boldsymbol{x}_i, \\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k) = \\sum_i^n \\gamma_k^i$$\n", "\n", "qui n'est d'autre que l'estimateur de l'esp\u00e9rance du nombre d'observations assign\u00e9es \u00e0 la $k$-i\u00e8me composante gaussienne. Observons l'analogie avec KMeans o\u00f9 $N_k = \\sum_i^n r_k^i$ est exactement le nombre de points associ\u00e9s au $k$_i\u00e8me cluster.\n", "\n\n ----"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### iii. Optimisation des $\\boldsymbol{\\Sigma}_k$\n", "Proc\u00e9dons de mani\u00e8re tr\u00e8s similaire pour le calcul des matrices de variance-covariances. La seule diff\u00e9rence sera que cette fois-ci, il faudra calculer le gradient de $f_k(\\boldsymbol{x}_i)$ par rapport \u00e0 $\\boldsymbol{\\Sigma}_k$&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "\\nabla_{\\boldsymbol{\\Sigma}_k} \\ln\\Big(q_{\\theta}\\mathcal{S}_n\\Big) &= \\Bigg[\\sum_i^n  \\frac{\\partial ln\\Big( \\sum_j \\pi_j f_j(\\boldsymbol{x}_i) \\Big)}{\\partial f_k(\\boldsymbol{x}_i)}  \\nabla_{\\boldsymbol{\\Sigma}_k} f_k(\\boldsymbol{x}_i) \\Bigg] \\\\\n", "&=  \\Bigg[\\sum_i^n  \\frac{\\pi_k  }{\\sum_{j}^K \\pi_j f(\\boldsymbol{x}_i;\\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)} \\nabla_{{\\boldsymbol{\\Sigma}_k}} \\Bigg(\\frac{e^{g_k(\\boldsymbol{x}_i)}}{C(\\boldsymbol{\\Sigma_k})}\\Bigg) \\Bigg]\n", "\\end{aligned}$$\n", "\n", "En rappellant l'expression de la d\u00e9riv\u00e9e d'un quotient $\\Big(\\frac{u}{v}\\Big)' = \\frac{u'v-uv'}{v^2}$&nbsp;:\n", "\n", "\n", "$$\\nabla_{{\\boldsymbol{\\Sigma}_k}} \\Bigg(\\frac{e^{g_k(\\boldsymbol{x}_i)}}{C(\\boldsymbol{\\Sigma_k})}\\Bigg) = \\frac{\\nabla_{\\boldsymbol{\\Sigma}_k} \\Big(e^{g_k(\\boldsymbol{x}_i)}\\Big) C(\\boldsymbol{\\Sigma_k}) -  e^{g_k(\\boldsymbol{x}_i)} \\nabla_{\\boldsymbol{\\Sigma}_k} \\Big(C(\\boldsymbol{\\Sigma_k})\\Big)}{C(\\boldsymbol{\\Sigma_k})^2}$$\n", "\n", "avec&nbsp;:\n", "\n", "$$\\nabla_{\\boldsymbol{\\Sigma}_k} \\Big(e^{g_k(\\boldsymbol{x}_i)}\\Big) = \\underbrace{\\partial_{g_k(\\boldsymbol{x}_i)}\\Big(e^{g_k(\\boldsymbol{x}_i)}\\Big)}_{e^{g_k(\\boldsymbol{x}_i)}} \\nabla_{\\boldsymbol{\\Sigma}_k} \\Big(g_k(\\boldsymbol{x}_i)\\Big)$$\n", "\n", "En factorisant par $\\frac{e^{g_k(\\boldsymbol{x}_i)}}{C(\\boldsymbol{\\Sigma_k})} = f_k(\\boldsymbol{x}_i)$, et en se rappellant que $\\frac{u'}{u} = ln(u)'$, nous obtenons&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "\\nabla_{{\\boldsymbol{\\Sigma}_k}} \\Bigg(\\frac{e^{g_k(\\boldsymbol{x}_i)}}{C(\\boldsymbol{\\Sigma_k})}\\Bigg)&= f_k(\\boldsymbol{x}_i)\n", "\\Bigg[\n", "\\nabla_{\\boldsymbol{\\Sigma}_k} \\Big(g_k(\\boldsymbol{x}_i)\\Big) - \\underbrace{\\frac{\\nabla_{\\boldsymbol{\\Sigma}_k} \\Big(C(\\boldsymbol{\\Sigma_k})\\Big)}{C(\\boldsymbol{\\Sigma_k})}}_{ = \\nabla_{\\boldsymbol{\\Sigma}_k} ln \\Big(C(\\boldsymbol{\\Sigma_k})\\Big)}\n", "\\Bigg]\\\\\n", "&= f_k(\\boldsymbol{x}_i)\n", "\\Bigg[\n", "\\nabla_{\\boldsymbol{\\Sigma}_k} \\Big[-\\frac{1}{2}(\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k)^T)\\boldsymbol{\\Sigma}_k^{-1}(\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k)\\Big] - \\nabla_{\\boldsymbol{\\Sigma}_k} \\Big[ \\frac{d}{2} ln(2\\pi) + \\frac{1}{2}ln\\Big(|\\boldsymbol{\\Sigma}_k| \\Big) \\Big]\n", "\\Bigg]\\\\\n", "&= f_k(\\boldsymbol{x}_i) (-\\frac{1}{2})\\Bigg[-\\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{x}_i - \\boldsymbol{\\mu})(\\boldsymbol{x}_i - \\boldsymbol{\\mu})^T\\boldsymbol{\\Sigma}^{-1} + \\boldsymbol{\\Sigma}_k^{-1}\\Bigg]\n", "\\end{aligned}$$\n", "\n", "Ainsi, afin d'annuler le gradient, nous obtenons&nbsp;:\n", "\n", "$$\\nabla_{\\boldsymbol{\\Sigma}_k} \\ln\\Big(q_{\\theta}\\mathcal{S}^n\\Big)  = -\\frac{1}{2} \\sum_i^n  \\Bigg[ \\underbrace{\\Bigg[ \\frac{\\pi_k  f_k(\\boldsymbol{x}_i)}{\\sum_{j}^K \\pi_j f(\\boldsymbol{x}_i;\\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)}\\Bigg]}_{\\gamma_k^i} \\Bigg[-\\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{x}_i - \\boldsymbol{\\mu})(\\boldsymbol{x}_i - \\boldsymbol{\\mu})^T\\boldsymbol{\\Sigma}^{-1} + \\boldsymbol{\\Sigma}_k^{-1}\\Bigg]\\Bigg] = \\boldsymbol{0}$$\n", "\n", "\n", "Multiplions \u00e0 droite et \u00e0 gauche par $\\boldsymbol{\\Sigma}_k$ et r\u00e9solvons&nbsp;:\n", "\n", "\n", "$$\\boldsymbol{\\Sigma}_k = \\frac{1}{N_k}\\sum_i^n  \\gamma_k^i (\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k)(\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k)^T$$\n", "\n", "\n", "O\u00f9 nous retrouvons, de mani\u00e8re analogue au calculs des $\\mu_k$, les expression des matrices de covariances empiriques et o\u00f9 la contribution de chaque \u00e9chantillon est pond\u00e9r\u00e9es par sa probabilit\u00e9 a posteriori d'avoir \u00e9t\u00e9 g\u00e9n\u00e9r\u00e9 par la $k$-i\u00e8me composante gaussienne."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### iv. Optimisation des $\\pi_k$\n", "Concernant l'opimisation des $\\pi_k$, il ne suffit pas simplement d'annuler le gradient de log vraissemblance car nous avons la contrainte que leur somme sur $k$ doit valoir $1$ (il s'agit d'une distribution de probabilit\u00e9). Pour cela, nous allons donc considerer un probl\u00e8me d'optimisation sous contrainte. D\u00e9finissons le Lagrangien $\\mathcal{L}\\Big( \\ln(q_{\\theta}\\mathcal{S}_n) , \\lambda\\Big)$ (voir la s\u00e9quence \"L'optimisation\") qui int\u00e8gre cette contrainte&nbsp;:\n", "\n", "$$\\mathcal{L}\\Big( \\ln(q_{\\theta}\\mathcal{S}_n) , \\lambda\\Big) = \\sum_i^n ln \\Big(\\sum_k \\pi_k f_k(\\boldsymbol{x}_i) \\Big) + \\lambda \\Big(\\sum_k \\pi_k - 1\\Big)$$\n", "\n", "\n", "Annuler la d\u00e9riv\u00e9e du Lagrangien par rapport \u00e0 notre multiplicateur de Lagrange $\\lambda$ nous permet bien de retrouver notre contrainte&nbsp;:\n", "\n", "\n", "$$\\frac{\\partial \\mathcal{L}\\Big( \\ln(q_{\\theta}\\mathcal{S}_n) , \\lambda\\Big)}{\\partial \\lambda} =  \\Big(\\sum_k \\pi_k - 1\\Big) = 0 \\Leftrightarrow \\sum_k \\pi_k  =  1$$\n", "\n", "\n", "Annulons maintenant la d\u00e9riv\u00e9e par rapport \u00e0 un $\\pi_k$ donn\u00e9e&nbsp;:\n", "\n", "$$\\frac{\\partial \\mathcal{L}\\Big( \\ln(q_{\\theta}\\mathcal{S}_n) , \\lambda\\Big)}{\\partial \\pi_k} = \\sum_i^n \\frac{\\partial ln\\Big( \\sum_j \\pi_j f_j(\\boldsymbol{x}_i) \\Big)}{\\partial \\pi_k}  + \\lambda = \\sum_i^n \\frac{f_k(\\boldsymbol{x}_i)}{\\sum_j \\pi_j f_j(\\boldsymbol{x}_i)} + \\lambda = 0$$\n", "\n", "\n", "En mutipliant par $\\pi_k$ des deux cot\u00e9s et en sommant sur $k$, nous obtenons&nbsp;:\n", "\n", "$$\\sum_k \\pi_k\\sum_i^n \\frac{f_k(\\boldsymbol{x}_i)}{\\sum_j \\pi_j f_j(\\boldsymbol{x}_i)} + \\lambda  \\sum_k \\pi_k= 0 = \\sum_i^n \\underbrace{\\frac{\\sum_k \\pi_kf_k(\\boldsymbol{x}_i)}{\\sum_j \\pi_j f_j(\\boldsymbol{x}_i)}}_{=1} + \\lambda  \\underbrace{\\sum_k \\pi_k}_{=1}$$\n", "\n", "\n", "et le multiplicateur de Lagrange vaut ainsi $\\lambda = -n$. Nous avons \u00e9galement&nbsp;:\n", "\n", "\n", "$$\\underbrace{\\sum_i^n \\frac{\\pi_k f_k(\\boldsymbol{x}_i)}{\\sum_j \\pi_j f_j(\\boldsymbol{x}_i)}}_{=\\sum_i^n \\gamma_k^i=N_k} + \\lambda \\pi_k = 0 \\Leftrightarrow \\lambda = -\\frac{N_k}{\\pi_k} = -n$$\n", "\n", "ce qui nous donne&nbsp;:\n", "\n", "$$\\pi_k  = \\frac{N_k}{n}$$\n", "\n", "C'est ainsi l'estimateur de l'esp\u00e9rance de la fraction de points assign\u00e9s \u00e0 la composante $k$."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### V. L'algortihme EM\n", "Les formules pr\u00e9c\u00e9dentes de nous donnent pas une solution en forme close. En effet, ces derni\u00e8res sont reli\u00e9es mutuellement&nbsp;:\n", "\n", "\n", "$$\\gamma_k^i = \\frac{\\pi_k f(\\boldsymbol{x}_i;\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) }{\\sum_j \\pi_j f(\\boldsymbol{x}_i;\\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)}.$$\n", "\n", "\n", "\u00c0 chaque fois que nous changerons la valeurs de ces param\u00e8tres, les probabilit\u00e9s *a posteriori* changerons aussi ce qui donnera lieu \u00e0 de nouvelles valeurs des param\u00e8tres et ainsi de suite.\n", "\n", "Nous pouvons n\u00e9anmoins d\u00e9finir une proc\u00e9dure it\u00e9rative en alternant ces deux phases de calculs des $\\gamma_k^i$ puis des valeurs optimales des param\u00eatres en gardant fixe les probabilit\u00e9s *a posteriori*. Cela donne lieu \u00e0 l'algorithme Expectation Maximization (EM) qui dans le cas de l'estimateur de mod\u00e8les de m\u00e9lange Gaussien peut se formuler ainsi&nbsp;:\n", "\n", "1) **Etape E (Expectation)**&nbsp;: Calculer les valeurs en utilisant la valeur courante des param\u00e8tres $\\theta(t) = \\Big\\{ \\pi_k(t), \\boldsymbol{\\mu}_k(t), \\boldsymbol{\\Sigma}_k(t) \\Big\\}_{k \\leq K}$ (qu'on initialisera al\u00e9atoirement pour la premi\u00e8re it\u00e9ration)&nbsp;:\n", "\n", "    $$\\gamma_k^i (t+1) = \\frac{\\pi_k(t) f(\\boldsymbol{x}_i;\\boldsymbol{\\mu}_k(t), \\boldsymbol{\\Sigma}_k(t) }{\\sum_j \\pi_j f(\\boldsymbol{x}_i;\\boldsymbol{\\mu}_j(t), \\boldsymbol{\\Sigma}_j(t)}$$\n", "\n", "\n", "    et&nbsp;:\n", "\n", "    $$N_k(t+1) = \\sum_i \\gamma_k^i(t+1)$$\n", "\n", "\n", "2) **Etape M (Maximization)**&nbsp;: Calculer les nouvelles valeurs des param\u00e8tres en utilisant les valeurs des probabilit\u00e9s *a posteriori* calcul\u00e9es \u00e0 l'\u00e9tape pr\u00e9c\u00e9dente:\n", "\n", "    $$\\pi_k(t+1) =   \\frac{N_k(t+1)}{n}$$\n", "\n", "\n", "    $$\\boldsymbol{\\mu}_k(t+1) = \\frac{1}{N_k(t+1)}\\sum_i^n  \\gamma_k^i(t+1)\\boldsymbol{x}_i $$\n", "\n", "\n", "    $$\\boldsymbol{\\Sigma}_k(t+1) = \\frac{1}{N_k(t+1)}\\sum_i^n  \\gamma_k^i(t+1) (\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k(t))(\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k(t))^T$$\n", "\n", "\n", "Etant donn\u00e9 ce que nous avons vu pr\u00e9c\u00e9dement sur le lien entre l'algorithme EM et KMeans, il est possible d'initialiser les param\u00e8tres $\\big\\{\\boldsymbol{\\mu}_k(0)\\big\\}_{k \\leq K}$ non pas al\u00e9atoirement, mais avec les centro\u00efdes calcul\u00e9s par un algorithme KMeans. C'est souvent ce qui est fait en pratique \u00e0 la fois pour la qualit\u00e9 et le temps de convergence notamment dans les cas o\u00f9 $d$ est grand et que le nombre de param\u00e8tres l'est aussi. Par la suite, nous allons impl\u00e9menter les \u00e9l\u00e9ments de base de l'algorithme EM dans le cas du m\u00e9lange Gaussien et nous utiliserons les deux modes d'initialisation. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "Dans le code qui suit, remplissez les m\u00e9thodes *compute_posteriors*, *compute_priors*, *compute_mu*, *compute_sigma* correspondant respectivement aux calculs des $\\gamma_k^i(t)$, $\\pi_k(t)$, $\\boldsymbol{\\mu}_k(t)$, $\\boldsymbol{\\Sigma}_k(t)$. Compl\u00e9tez aussi le code *evaluate_log_likelyhood* qui vous permettra d'\u00e9valuer la log-vraissemblance pour les valeurs courantes des param\u00e8tres du mod\u00e8le&nbsp:\n", "\n", "$$\\ln\\Big(q_{\\theta(t)}(\\mathcal{S}^n)\\Big) = \\sum_i^n \\ln \\Bigg(\\sum_k^K \\pi_k(t) f(\\boldsymbol{x}_i;\\boldsymbol{\\mu}_k(t), \\boldsymbol{\\Sigma}_k(t)) \\Bigg)$$\n", "\n", "On utilisera la m\u00e9thode *multivariate_normal(mu, sigma).pdf(X)* de *scipy.stats* pour calculer les $f(\\boldsymbol{x}_i|\\boldsymbol{\\mu}_k(t), \\boldsymbol{\\Sigma}_k(t))$.\n", "\n\n ----"]}, {"cell_type": "code", "metadata": {}, "source": ["from sklearn.cluster import KMeans\n", "class GMM(object):\n", "    def __init__(self, n_components=2, d=2, n_iter=5, init=\"kmeans\"):\n", "        self.n_components = n_components\n", "        self.n_iter = n_iter\n", "        self.init = init\n", "        \n", "        self.pi = np.ones(n_components)/n_components\n", "        self.sigma  = [20*np.eye(d) for _ in range(n_components)]\n", "        self.mu     = [np.random.uniform(-1,1,size=(d)) for _ in range(n_components)]\n", "        \n", "    def evaluate_log_likelihood(self, X):\n", "        #### Complete the code here #### or die #######################################\n", "        return ...\n", "        ################################################################################\n", "    \n", "    def compute_posteriors(self, X):\n", "        #Compute numerator of the posterior from Baye's Rule\n", "        #### Complete the code here #### or die #######################################\n", "        ...\n", "            \n", "        return ...\n", "        ################################################################################\n", "    \n", "    def compute_priors(self, p):\n", "        #### Complete the code here #### or die #######################################\n", "        return ...\n", "        ################################################################################\n", "               \n", "    def compute_mu(self, X, p, pi):\n", "        #### Complete the code here #### or die #######################################\n", "        return ...\n", "        ################################################################################\n", "    \n", "    def compute_sigma(self, X, p, pi, mu):\n", "        #### Complete the code here #### or die #######################################\n", "        return ...\n", "        ################################################################################\n", "        \n", "    def predict(self, X):\n", "        return np.argsort(-self.compute_posteriors(X), axis = 1)[:, 0]\n", "    \n", "    def fit_predict(self, X):\n", "        LLH = self.fit(X)\n", "        return self.predict(X), LLH\n", "    \n", "    def fit(self, X):\n", "        LLH_history = []\n", "        # Initialize mu with KMeans\n", "        if self.init == \"kmeans\":      \n", "            self.mu = KMeans(n_clusters=self.n_components, max_iter=1).fit(X).cluster_centers_\n", "        \n", "        # Peform EM iterations\n", "        for i in range(self.n_iter):\n", "            LLH_history.append(self.evaluate_log_likelihood(X))\n", "            # TODO uncomment print(\"\\rIteration: %d - LogLikelyHood = %f\"%(i,LLH_history[-1]), end=\"\")\n", "                \n", "            #E step:\n", "            gamma = self.compute_posteriors(X)\n", "\n", "            #M step:\n", "            self.pi  = self.compute_priors(gamma)\n", "            self.mu      = self.compute_mu(X, gamma, self.pi)\n", "            self.sigma   = self.compute_sigma(X, gamma, self.pi, self.mu)\n", "        return LLH_history\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "SAkbBjZK6RVR"}, "source": ["n_iter=50\n", "model = GMM(n_iter=n_iter, n_components=6, init=\"random\")\n", "Z, LLH_random = model.fit_predict(X)\n", "\n", "model = GMM(n_iter=n_iter, n_components=6, init=\"kmeans\")\n", "Z, LLH_kmeans = model.fit_predict(X)\n", "\n", "\n", "# Plot LogLikelyHood\n", "plt.figure()\n", "plt.plot([i for i in range(1, len(LLH_random)+1)], LLH_random, \n", "         label='LogLikeliHood Random init')\n", "plt.plot([i for i in range(1, len(LLH_kmeans)+1)], LLH_kmeans, \n", "         label='LogLikeliHood KMeans init')\n", "plt.legend()\n", "plt.show()\n", "\n", "# Plot gaussian components\n", "plot(X, Z, means=model.mu, gmd=model)\n", "plt.show()\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## C. Justification de l'algorithme EM (approfondissement)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Nous allons maintenant montrer que cette proc\u00e9dure qui peut sembler arbitraire poss\u00e8de en fait une justification th\u00e9orique qui garantit que la vraissemblance de notre mod\u00e8le cro\u00eet bien \u00e0 chaque it\u00e9ration (sauf si on se trouve dej\u00e0 dans un maximum local). Au-del\u00e0 de la croissance, il est \u00e9galement possible de d\u00e9montrer la convergence de cette m\u00e9thode. Une premi\u00e8re chose \u00e0 constater est que si nous poss\u00e9dions pour chaque observation $\\boldsymbol{x}_i$ la valeur de sa repr\u00e9sentation associ\u00e9es $\\boldsymbol{z}_i$, alors notre objectif serait de maximiser la log-vraissemblance suivante&nbsp;:\n", "\n", "\n", "$$\\hat{\\theta} = \\arg \\max_{\\theta} \\sum_i^n \\ln \\Big(q_{\\theta}(\\boldsymbol{x}_i,\\boldsymbol{z}_i)\\Big)$$\n", "\n", "\n", "L'objectif est de trouver la param\u00e9trisation qui maximise la probabilit\u00e9 d'observer conjointement une observation et sa \"vraie\" variable cach\u00e9e. il est possible de d\u00e9montrer que c'est ce qu'on fait en pratique avec l'algorithme EM. Cependant, n'ayant acc\u00e8s \u00e0 ces variables cach\u00e9es, nous utilisons la meilleure information disponible, \u00e0 savoir sa distribution *a posteriori* \u00e0 l'instant $t$ : $q_{\\theta(t)}(\\boldsymbol{z} | \\boldsymbol{x})$. Nous effectuons en pratique une optimisation de l'esp\u00e9rance de cette log-vraissemblance sur la distribution a posteriori&nbsp;:\n", "\n", "\n", "$$\\theta(t+1) = \\arg \\max_{\\theta} \\Bigg[\\mathbb{E}_{z_i\\sim q_{\\theta(t)}(z_i|x_i)} \\Bigg[\\sum_i^n ln \\Big(q_{\\theta}(\\boldsymbol{x}_i,\\boldsymbol{z}_i)\\Big)\\Bigg]\\Bigg].$$\n", "\n", "\n", "Notons&nbsp;\n", "\n", "$$\\mathcal{L}(\\theta, \\theta(t)) = \\mathbb{E}_{z_i\\sim q_{\\theta(t)}(z_i|x_i)} \\Bigg[\\sum_i^n ln \\Big(q_{\\theta}(\\boldsymbol{x}_i,\\boldsymbol{z}_i)\\Big)\\Bigg]$$\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Comme tous les \u00e9chantillons sont ind\u00e9pendants et identiquement distribu\u00e9s et que $z$ est une variable discr\u00e8te, nous pouvons r\u00e9\u00e9crire cette quantit\u00e9 de la mani\u00e8re suivante&nbsp;:\n", "\n", "$$\\theta(t+1) = \\arg \\max_{\\theta} \\Bigg[\\sum_i^n \\sum_k^K q_{\\theta(t)}(z_i=k|\\boldsymbol{x}_i) \\Big[ \\ln (q_{\\theta}(z_i=k)) + \\ln (q_{\\theta}(\\boldsymbol{x}_i|z_i=k))\\Big]\\Bigg]$$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Remarquons que cela revient \u00e0 pond\u00e9rer chaque terme de la log-vraissemblance associ\u00e9e \u00e0 chaque valeur de la variable cach\u00e9e par la probabilit\u00e9 *a posteriori* de cette variable cach\u00e9e sachant l'observation associ\u00e9e. \n", "\n", "Ainsi, dans le cadre g\u00e9n\u00e9ral de traitement des mod\u00e8les \u00e0 variables cach\u00e9es par l'algorithme EM, ce dernier peut se d\u00e9crire ainsi&nbsp;:\n", "1) **Expectation**&nbsp;: Utiliser la valeur du param\u00eatre $\\theta(t)$ pour en d\u00e9duire la distribution *a post\u00e9riori*&nbsp;:\n", "\n", "    $$q_{\\theta(t)}(z|\\boldsymbol{x})$$\n", "\n", "\n", "2) **Maximization**&nbsp;: Utiliser cette derni\u00e8re pour d\u00e9finir l'expression&nbsp;:\n", "\n", "    $$\\mathcal{L}(\\theta, \\theta(t)) =  \\sum_i^n \\sum_k^K q_{\\theta(t)}(z_i=k|\\boldsymbol{x}_i) \\Big[ \\ln (q_{\\theta}(z_i=k)) + \\ln (q_{\\theta}(\\boldsymbol{x}_i|z_i=k))\\Big]$$\n", "\n", "\n", "    qui d\u00e9pend du param\u00eatre g\u00e9n\u00e9ral $\\theta$ et optimiser $\\mathcal{L}(\\theta, \\theta(t))$ par rapport \u00e0 ce dernier&nbsp;:\n", "\n", "\n", "    $$\\theta(t) = \\arg \\max_{\\theta} \\Bigg[\\mathcal{L}(\\theta, \\theta(t))\\Bigg]$$\n", "\n", "\n", "### i. Retour sur le cas du m\u00e9lange Gaussien\n", "Si l'on se replace dans le cas du m\u00e9lange Gaussien, en rappellant qu'on appelle $\\gamma_k^i = q_{\\theta(t)}(z_i=k|\\boldsymbol{x}_i)$ la probabilit\u00e9 *a posteriori* de la $k$-i\u00e8me composante en gardant les param\u00e8tres de l'instant $t$ fixes, l'expression \u00e0 maximiser devient&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "\\mathcal{L}(\\theta, \\theta(t)) &= \\sum_i^n \\sum_k^K \\gamma_k^i \\Big[ ln (\\pi_k) + ln \\big(f(\\boldsymbol{x}_i ; \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\\big)\\Big]\\\\\n", "& = \\sum_i^n \\sum_k^K \\gamma_k^i \\Big[ ln (\\pi_k) - \\frac{d}{2}ln(2\\pi) - \\frac{1}{2} ln(|\\boldsymbol{\\Sigma}_k|)- \\frac{1}{2} (\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}_k^{-1}  (\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k) \\Big]\n", "\\end{aligned}$$\n", "\n", "Il est possible de v\u00e9rifier que nous retombons bien sur les param\u00e8tres obtenus dans la partie pr\u00e9c\u00e9dente.\n", "\n", "**<span style='color:blue'> Exercice</span>** ", "Retrouvez le terme $\\boldsymbol{\\mu}_k(t+1)$ \u00e0 partir de cette expression&nbsp;:\n", "\n", "$$\\nabla_{\\mu_k} \\Big(\\mathcal{L}(\\theta, \\theta(t))\\Big) = \\sum_i^n\\gamma_k^i(t) \\Big[- \\frac{1}{2} (-2) \\boldsymbol{\\Sigma}_k^{-1}  (\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k) \\Big] = \\boldsymbol{0}$$\n", "\n", "\n\n ----", "\n", "\n", "Un avantage avec cette formulation est que le terme intervenant dans le logarithme n'\u00e9tant plus une somme de densit\u00e9 gaussienne mais directement une densit\u00e9 gaussienne, les calculs de gradient sont beaucoup plus faciles \u00e0 manipuler. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["### ii. Preuve que chaque it\u00e9ration de EM augmente la log-vraissemblance"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**<span style='color:green'> Croissance et convergence</span>** ", "\n", "Montrer que l'algorithme am\u00e9liore  la fonction objectif (ici la vraisemblance) n'est pas suffisant pour garantir que notre algorithme est bon. Il faudrait d\u00e9montrer qu'il converge effectivement vers la quantit\u00e9 souhait\u00e9e, mais cela est plus compliqu\u00e9.\n", "\n\n ----", "\n", "M\u00eame si nous avons donn\u00e9 une vision un peu plus g\u00e9n\u00e9rale de l'algorithme EM et que nous avons donn\u00e9 une intuition de sa justification, nous n'avons pas encore montr\u00e9 rigoureusement pourquoi nous somme garantis que proc\u00e9der ainsi permet effectivement d\u2019accro\u00eetre la log-vraissemblance $ln\\Big(q_{\\theta}\\Big(\\mathcal{S}^n\\Big)\\Big)$ de notre mod\u00e8le g\u00e9n\u00e9ratif \u00e0 chaque it\u00e9ration (si nous ne sommes pas d\u00e9j\u00e0 dans un maximum local).\n", "\n", "Pour cela il convient d'exprimer cette log-vraissemblance de la mani\u00e8re suivante&nbsp;:\n", "\n", "\n", "$$\\ln\\Big(q_{\\theta}\\Big(\\mathcal{S}_n\\Big)\\Big) = \\ln\\Big(q_{\\theta}\\Big(\\mathcal{S}_n, \\mathcal{Z}_n\\Big)\\Big) -  \\ln\\Big(q_{\\theta}\\Big(\\mathcal{Z}^n | \\mathcal{S}_n\\Big)\\Big)$$\n", "\n", "\n", "O\u00f9 nous notons $\\mathcal{Z}_n$ la variable al\u00e9atoire \"cach\u00e9e\" correspondant \u00e0 un \u00e9chantillon de taille $n$. Une astuce nous permet d'\u00e9crire cette expression de la mani\u00e8re suivante&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "\\ln\\Big(q_{\\theta}\\Big(\\mathcal{S}_n\\Big)\\Big) \n", "&= \\Bigg[\\ln\\Big(q_{\\theta}\\Big(\\mathcal{S}_n, \\mathcal{Z}_n\\Big)\\Big) - \\ln\\Big(q'\\Big(\\mathcal{Z}_n\\Big)\\Big)\\Bigg] -\\Bigg[- \\ln\\Big(q'\\Big(\\mathcal{Z}_n\\Big)\\Big) +  \\ln\\Big(q_{\\theta}\\Big(\\mathcal{Z}_n | \\mathcal{S}_n\\Big)\\Big)\\Bigg]\\\\\n", "&=\\ln\\Bigg( \\frac{q_{\\theta}\\Big(\\mathcal{S}_n, \\mathcal{Z}_n\\Big)}{q'\\Big(\\mathcal{Z}_n\\Big)}\\Bigg) - \\ln\\Bigg(\\frac{q_{\\theta}\\Big(\\mathcal{Z}_n | \\mathcal{S}_n\\Big)}{q'\\Big(\\mathcal{Z}_n\\Big)}\\Bigg)\n", "\\end{aligned}$$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["O\u00f9 l'on note $q'\\Big(\\mathcal{Z}_n\\Big)$ comme n'importe quelle distribution sur les variables cach\u00e9es dans $\\mathcal{Q}$. En notant que la marginale de cette derni\u00e8re sur les \u00e9chantillon s'int\u00e8gre \u00e0 $1$, on en d\u00e9duit en calculant l'esp\u00e9rance sur $q'\\Big(\\mathcal{Z}^n\\Big)$ des deux cot\u00e9s&nbsp;:\n", "\n", "\n", "$$\\underbrace{\\mathbb{E}_{\\mathcal{Z}_n \\sim q'(z)^n}\\Bigg[\\ln\\Big(q_{\\theta}\\Big(\\mathcal{S}_n\\Big)\\Big)\\Bigg]}_{=  \\ln\\Big(q_{\\theta}\\Big(\\mathcal{S}_n\\Big)\\Big)} = \\underbrace{\\mathbb{E}_{\\mathcal{Z}_n \\sim q'(z)^n} \\Bigg[\\ln\\Bigg( \\frac{q_{\\theta}\\Big(\\mathcal{S}_n, \\mathcal{Z}_n\\Big)}{q'\\Big(\\mathcal{Z}_n\\Big)}\\Bigg)\\Bigg]}_{= L(q', \\theta)}  \\underbrace{-\\mathbb{E}_{\\mathcal{Z}_n \\sim q'(z)^n}\\Bigg[ln\\Bigg(\\frac{q_{\\theta}\\Big(\\mathcal{Z}_n | \\mathcal{S}_n\\Big)}{q'\\Big(\\mathcal{Z}_n\\Big)}\\Bigg)\\Bigg]}_{= \\mathbb{KL}\\big(q'||q\\big)}$$\n", "\n", "\n", "soit&nbsp;:\n", "\n", "$$\\\\ln\\Big(q_{\\theta}\\Big(\\mathcal{S}^n\\Big)\\Big)= L(q', \\theta) + \\mathbb{KL}\\big(q'||q_{\\theta}\\big)$$\n", "\n", "**<span style='color:green'> Divergence de Kullback Liebler</span>** ", "\n", "Soit $P$ et $Q$ deux distributions de probabilit\u00e9. La divergence de Kullback Liebler est donn\u00e9e par&nbsp;\n", "\n", "$$\\mathbb{KL}\\big(P\\lVert Q)=\\int p(x)\\ln\\frac{p(x)}{q(x)}dx.$$\n", "\n", "Celle-ci est minimale lorsque $P=Q$ est vaut alors $0$. Notez cependant son manque de sym\u00e9trie&nbsp;:\n", "\n", "$$\\mathbb{KL}\\big(P\\lVert Q)\\neq \\mathbb{KL}\\big(Q\\lVert P).$$\n", "\n", "\n\n ----", "\n", "o\u00f9 $\\mathbb{KL}\\big(q'||q_{\\theta}\\big)$ correspond \u00e0 la divergence de Kullback Liebler. Pour tout choix de $q'$ et $\\theta$ nous avons n\u00e9cessairement&nbsp;:\n", "\n", "$$L(q', \\theta)  = \\ln\\Big(q_{\\theta}\\Big(\\mathcal{S}_n\\Big)\\Big) - \\underbrace{ \\mathbb{KL}\\big(q'||q_{\\theta}\\big)}_{\\geq 0} \\leq \\ln\\Big(q_{\\theta}\\Big(\\mathcal{S}_n\\Big)\\Big)$$\n", "\n", "\n", "Et $L(q', \\theta)$ est donc forc\u00e9ment un minorant de la log-vraissemblance pour tout choix de $q'$ et $\\theta$. \n", "\n", "Ce qui est int\u00e9ressant, c'est que si dans une premi\u00e8re \u00e9tape (que l'on constatera \u00eatre exactement l'\u00e9tape E de l'algorithme EM) on essaie de trouver la distribution $q'$ qui maximise cette borne inf\u00e9rieure en gardant notre valeur courante de $\\theta(t)$ fixe, alors nous obtenons&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "&\\arg \\max_{q' \\in \\mathcal{Q}} \\Big[ ln\\Big(q_{\\theta}\\Big(\\mathcal{S}^n\\Big)\\Big) - \\mathbb{KL}\\big(q'||q_{\\theta(t)}\\big) \\Big] \\\\&=\n", "\\arg \\min_{q' \\in \\mathcal{Q}} \\Big[ \\mathbb{KL}\\big(q'\\Big(\\mathcal{Z}^n\\Big)||q_{\\theta(t)}\\Big(\\mathcal{Z}^n | \\mathcal{S}^n\\Big)\\big) \\Big] \\\\&= q_{\\theta(t)}\\Big(\\mathcal{Z}^n | \\mathcal{S}^n\\Big)\n", "\\end{aligned}$$\n", "\n", "En fixant $q'\\Big(\\mathcal{Z}^n\\Big) = q_{\\theta(t)}\\Big(\\mathcal{Z}^n | \\mathcal{S}^n\\Big)$, c'est-\u00e0-dire comme la distribution a posteriori (en se fixant un vecteur de param\u00e8tres en cours $\\theta(t)$), alors ce terme de divergence KL dvient nul et on obtient pour $\\theta = \\theta(t)$&nbsp;:\n", "\n", "$$L\\Big(q_{\\theta(t)}\\Big(\\mathcal{Z}_n | \\mathcal{S}^n\\Big), \\theta(t)\\Big) = \\ln\\Big(q_{\\theta(t)}\\Big(\\mathcal{S}_n\\Big)\\Big)$$\n", "\n", "\n", "Rappellons que pour n'importe quelle autre valeur de $\\theta \\neq \\theta(t)$ nous aurons toujours&nbsp;:\n", "\n", "$$L\\Big(q_{\\theta(t)}\\Big(\\mathcal{Z}^n | \\mathcal{S}^n\\Big), \\theta\\Big)  \\leq ln\\Big(q_{\\theta}\\Big(\\mathcal{S}^n\\Big)\\Big)$$\n", "\n", "Autrement dit la log-vraissemblance  $\\ln\\Big(q_{\\theta}\\Big(\\mathcal{S}^n\\Big)\\Big)$ et son minorant $L\\Big(q_{\\theta(t)}\\Big(\\mathcal{Z}_n | \\mathcal{S}_n\\Big), \\theta\\Big)$ sont tangents en la valeur courante du param\u00e8tre $\\theta(t)$ et par d\u00e9finition la premi\u00e8re est toujours strictement au dessus de la deuxi\u00e8me. Par cons\u00e9quent, une valeur $\\theta= \\theta(t+1)$ qui accro\u00eet $L\\Big(q_{\\theta(t)}\\Big(\\mathcal{Z}^n | \\mathcal{S}^n\\Big), \\theta\\Big)$ correspondra aussi \u00e0 une valeur de param\u00e8tre permettant d'acco\u00eetre $\\ln\\Big(q_{\\theta}\\Big(\\mathcal{S}^n\\Big)\\Big)$. \n", "\n", "Nous allons voir que fixer $q'\\Big(\\mathcal{Z}_n\\Big) = q_{\\theta(t)}\\Big(\\mathcal{Z}_n | \\mathcal{S}_n\\Big)$ correspond \u00e0 l'\u00e9tape E de l'algorithme EM, maximiser $L\\Big(q_{\\theta(t)}\\Big(\\mathcal{Z}^n | \\mathcal{S}^n\\Big), \\theta\\Big)$  par rapport \u00e0 $\\theta$(en gardant fixe $q'$) revient exactement \u00e0 l'\u00e9tape M de l'algorithme EM. Pour cela, il convient de v\u00e9rifier que l'expression de $L\\Big(q_{\\theta(t)}\\Big(\\mathcal{Z}^n | \\mathcal{S}^n\\Big), \\theta\\Big)$ correspond bien \u00e0 la quantit\u00e9 \u00e0 maximiser vue pr\u00e9c\u00e9demment&nbsp;:\n", "\n", "\n", "$$\\mathcal{L}(\\theta, \\theta(t)) = \\mathbb{E}_{z_i\\sim q_{\\theta(t)}(z_i|x_i)} \\Bigg[\\sum_i^n ln \\Big(q_{\\theta}(\\boldsymbol{x}_i,\\boldsymbol{z}_i)\\Big)\\Bigg]$$\n", "\n", "\n", "On d\u00e9veloppe donc l'expression de $L\\Big(q_{\\theta(t)}\\Big(\\mathcal{Z}_n | \\mathcal{S}_n\\Big), \\theta\\Big)$&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "L\\Big(q_{\\theta(t)}\\Big(\\mathcal{Z}_n | \\mathcal{S}_n\\Big), \\theta\\Big)&= \\mathbb{E}_{\\mathcal{Z}_n \\sim q_{\\theta(t)}(z|x)^n}\\Bigg[ ln\\Bigg( \\frac{q_{\\theta}\\Big(\\mathcal{S}_n, \\mathcal{Z}_n\\Big)}{q_{\\theta(t)}\\Big(\\mathcal{Z}_n | \\mathcal{S}_n\\Big)}\\Bigg)\\Bigg]\\\\\n", "&= \\mathbb{E}_{\\mathcal{Z}_n \\sim q_{\\theta(t)}(z|x)_n}\\Bigg[ ln\\Bigg( q_{\\theta}\\Big(\\mathcal{S}_n, \\mathcal{Z}_n\\Big)\\Bigg)\\Bigg]- \\mathbb{E}_{z_i \\sim q_{\\theta(t)}(z_i|x_i)}\\Bigg[ \\ln\\Big( q_{\\theta(t)}(z_i | x_i)\\Big)\\Bigg]\\\\\n", "&= \\mathbb{E}_{z_i \\sim q_{\\theta(t)}(z_i|x_i)}\\Bigg[\\sum_i^n ln\\Big( q_{\\theta}(x_i, z_i)\\Big)\\Bigg] - \\text{const}\\\\\n", "&= \\mathcal{L}(\\theta, \\theta(t)) - \\text{const}\n", "\\end{aligned}$$\n", "\n", "O\u00f9 nous retrouvons le premier terme comme \u00e9tant exactement \u00e9gal \u00e0 la quantit\u00e9 \u00e0 maximizer dans le cadre de l'algorithme EM vu pr\u00e9c\u00e9dement, et le deuxi\u00e8me terme ne d\u00e9pend pas de $\\theta$ mais seulement de $\\theta(t)$ qu'on a fix\u00e9 \u00e0 la premi\u00e8re \u00e9tape o\u00f9 l'on \u00e0 choisi $q'$. On vient donc de montrer que&nbsp;:\n", "\n", "1) **\u00e9tape E**: en fixant :\n", "\n", "    $$q'\\Big(\\mathcal{Z}_n\\Big) = q_{\\theta(t)}\\Big(\\mathcal{Z}_n | \\mathcal{S}_n\\Big)$$\n", "\n", "\n", "2) **\u00e9tape M**: en maximisant la log vraissemblance par rapport \u00e0 theta en gardant fixe ce $q'$:\n", "\n", "    $$\\theta(t+1) = \\arg \\max_{\\theta} \\Bigg[\\mathbb{E}_{z_i \\sim q_{\\theta(t)}(z_i|x_i)}\\Bigg[\\sum_i^n \\ln\\Big( q_{\\theta}(x_i, z_i)\\Big)\\Bigg]\\Bigg]$$\n", "\n", "\n", "alors la quantit\u00e9 qu'on maximise est un minorant qui vaut exactement la log-vraissemblance $\\ln\\Big(q_{\\theta}\\Big(\\mathcal{S}^n\\Big)\\Big)$. En it\u00e9rant sur ces deux \u00e9tapes on garantit donc de maximiser \u00e0 chaque fois la log-vraissemblance sauf si \u00e0 un instant $t$ on se trouve avec une valeur de $\\theta(t)$ qui maximise localement cette derni\u00e8re (le gradient sera nul)."]}], "metadata": {"colab": {"name": "M\u00e9lange de mod\u00e8les gaussiens et EM.ipynb", "provenance": []}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.2"}}, "nbformat": 4, "nbformat_minor": 4}
{"cells": [{"cell_type": "markdown", "id": "a1e8c38e", "metadata": {}, "source": ["# *Machine learning* et mal\u00e9diction de la dimension"]}, {"cell_type": "markdown", "id": "839fb312", "metadata": {}, "source": ["**<span style='color:blue'> Objectifs de la s\u00e9quence</span>** ", "\n", "* \u00catre capable de diff\u00e9rencier l'apprentissage supervis\u00e9 et non supervis\u00e9.\n", "* \u00catre initi\u00e9 \u00e0&nbsp;:\n", "    * Construire un mod\u00e8le \u00e0 partir d'un jeu de donn\u00e9es,\n", "    * \u00c9valuer ce mod\u00e8le,\n", "    * Le sur-apprentissage,\n", "    * La mal\u00e9diction de la dimension.\n", "* Manipuler&nbsp;:\n", "    * Quelques mod\u00e8les standards de la librairie $\\texttt{sklearn}$.\n", "\n", "\n\n ----"]}, {"cell_type": "markdown", "id": "3eb39022", "metadata": {}, "source": ["## I. Introduction"]}, {"cell_type": "markdown", "id": "ca678ade", "metadata": {}, "source": ["Imaginons que nous souhaitions construire une application qui prendrait en entr\u00e9e une image de chien ou de chat et qui doive pr\u00e9dire laquelle des deux esp\u00e8ces est repr\u00e9sent\u00e9e. Imaginons encore une application qui prendrait en entr\u00e9e un mail qu'elle classifierait comme SPAM ou NONSPAM. Supposons qu'il existe deux cat\u00e9gories de clients qu'on ne connait pas *a priori* et que l'entreprise souhaite pr\u00e9dire pour chacun des clients sa cat\u00e9gorie. On peut vouloir pr\u00e9dire la temp\u00e9rature qu'il fera demain \u00e0 partir de donn\u00e9es relev\u00e9es aujourd'hui.\n", "\n", "Une constante est partag\u00e9e par l'ensemble de ces sc\u00e9narios. Il y a tout d'abord une donn\u00e9e d'entr\u00e9e plus ou moins complexe et structur\u00e9e. On notera $\\mathcal{X}$ l'espace auquel elle appartient. Ensuite, \u00e0 partir de cette donn\u00e9e, l'objectif est de faire une pr\u00e9diction. Notons $\\mathcal{Y}$ l'espace auquel appartient cette pr\u00e9diction. On appelle \u00e7a aussi nos labels ou nos variables \u00e0 expliquer. Notre objectif, en tant que *machine learner* est de construire une fonction $h:\\mathcal{X}\\mapsto\\mathcal{Y}$ qui aura de *bonnes performances* \"en production\", c'est-\u00e0-dire sur des donn\u00e9es nouvelles que nous n'avons jamais vues (i.e. on ne veut pas pr\u00e9dire la m\u00e9t\u00e9o d'hier \u00e0 partir d'avant hier, mais bien de demain \u00e0 partir d'aujourd'hui). \n", "\n", "Deux types d'apprentissage sont g\u00e9n\u00e9ralement oppos\u00e9s&nbsp;: l'apprentissage supervis\u00e9 (AS) et non-supervis\u00e9 (ANS).\n", "\n", "### A. L'apprentissage supervis\u00e9\n", "\n", "L'apprentissage supervis\u00e9 part du principe que (1) nos labels (i.e. l'espace $\\mathcal{Y}$) est bien d\u00e9fini et (2) que nous avons acc\u00e8s \u00e0 des donn\u00e9es associant des \u00e9l\u00e9ments de $\\mathcal{X}$ \u00e0 leur label $\\mathcal{Y}$.\n", "\n", "On parlera de probl\u00e8me de r\u00e9gression si par exemple $\\mathcal{Y}\\subseteq\\mathbb{R}$ ou de probl\u00e8me de classification si $\\mathcal{Y}=\\{1, \\ldots, C\\}$ o\u00f9 l'ordre n'est pas important. Par exemple, pr\u00e9dire la temp\u00e9rature est un probl\u00e8me de r\u00e9gression alors que pr\u00e9dire si la photo repr\u00e9sente un chien ou un chat est un probl\u00e8me de classification.\n", "\n", "A fortiori, toutes les observations dans $\\mathcal{X}$ ne sont pas n\u00e9cessairement \u00e9quiprobables. Certains clients ont peut-\u00eatre, par exemple, un profil plus commun que d'autres. Afin de pouvoir d\u00e9finir plus rigoureusement ce qu'on entend par *bonnes performances*, notons $X\\in\\mathcal{X}$ une variable al\u00e9atoire qui d\u00e9crit nos donn\u00e9es observ\u00e9es et $Y\\in\\mathcal{Y}$ la variable al\u00e9atoire associ\u00e9e \u00e0 nos labels. Assez na\u00efvement, notons $\\mathbb{P}$ la loi de notre couple $X,Y$&nbsp;:\n", "\n", "$$X, Y\\sim \\mathbb{P}.$$\n", "\n", "Notons $r:\\mathcal{Y}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}^+$ une mesure d'erreur, un risque \u00e9l\u00e9mentaire. On a par exemple, dans le cas d'un probl\u00e8me de r\u00e9gression, l'erreur quadratique&nbsp;:\n", "\n", "$$r(\\hat{y}, y)=(\\hat{y}-y)^2,$$\n", "\n", "o\u00f9 $\\hat{y}$ est la pr\u00e9diction que ferait notre mod\u00e8le. Ou encore, dans le cas de la classification nous pouvons avoir cette fois-ci l'erreur $0.1$&nbsp;:\n", "\n", "$$r(\\hat{y}, y)=\\textbf{1}\\{\\hat{y}\\neq y\\},$$\n", "\n", "qui vaut $1$ si la pr\u00e9diction est mauvaise ou $0$ sinon.\n", "\n", "Notre objectif est tout naturellement de trouver une application $h:\\mathcal{X}\\mapsto\\mathcal{Y}$ telle que $R(h)=\\mathbb{E}\\big[r(h(X), Y)\\big]$ est petit. On veut un bon mod\u00e8le sur de nouvelles donn\u00e9es. L'id\u00e9e va \u00eatre de collecter des donn\u00e9es repr\u00e9sentatives (dans le sens iid) et de construire notre mod\u00e8le avec ces derni\u00e8res. Notons&nbsp;:\n", "\n", "$$S_n=\\{(X_i, Y_i)\\}_{i\\leq n}$$\n", "\n", "un jeu de donn\u00e9es de taille $n$.\n", "\n", "### B. L'apprentissage non-supervis\u00e9\n", "\n", "Ici, c'est l'inverse. Nous avons acc\u00e8s \u00e0 l'espace des observations $\\mathcal{X}$ duquel on peut collecter des donn\u00e9es (toujours selon la loi de la variable al\u00e9atoire $X$). On sait qu'il existe un espace $\\mathcal{Y}$ cible mais (1) il n'est pas n\u00e9cessairement connu et (2) nous ne connaissons pas d'exemple de liens entre exemples d'apprentissage et cibles associ\u00e9es. \n", "\n", "Par exemple, si $\\mathcal{X}$ repr\u00e9sente des donn\u00e9es clients, on peut savoir (se douter) qu'il existe des groupes de clients qui se ressemblent mais ne pas les conna\u00eetre et ne pas savoir combien il y en a. Il s'agit ici d'une t\u00e2che de *clustering* o\u00f9 on cherche \u00e0 regrouper des donn\u00e9es entre-elles toujours de mani\u00e8re \u00e0 ce que le regroupement se g\u00e9n\u00e9ralise \u00e0 de nouvelles donn\u00e9es.\n", "\n", "On peut chercher \u00e0 transformer nos donn\u00e9es dans $\\mathcal{X}$ dans un espace qu'on notera cette fois-ci $\\mathcal{Z}$ o\u00f9 ces derni\u00e8res auront de meilleures propri\u00e9t\u00e9. On note cet \"espace de repr\u00e9sentation\" $\\mathcal{Z}$ et non $\\mathcal{Y}$ car il s'agit souvent d'une \u00e9tape interm\u00e9diaire avant une t\u00e2che supervis\u00e9e o\u00f9 on chercherait \u00e0 pr\u00e9dire un label dans $\\mathcal{Y}$. C'est ce qu'on appelle l'apprentissage de repr\u00e9sentation. Ainsi, si $\\mathcal{X}$ est l'ensemble des photos de chiens et de chats, $\\mathcal{Z}$ est l'ensemble de ces derni\u00e8res o\u00f9 on a mis \"d'un c\u00f4t\u00e9\" les photos de chiens et de \"l'autre\" celles de chats. Il devient simple de construire une t\u00e2che supervis\u00e9e permettant de pr\u00e9dire le bon label \"chien/chat\"."]}, {"cell_type": "markdown", "id": "99b3ada4", "metadata": {}, "source": ["## II. On casse une id\u00e9e pr\u00e9con\u00e7ue (AS)"]}, {"cell_type": "markdown", "id": "a165931e", "metadata": {}, "source": ["Soit $S=\\{(x_i, y_i)\\}_{i\\leq n}$ un jeu de donn\u00e9es repr\u00e9sentatif de taille $n$. Un mod\u00e8le tr\u00e8s performant sur ces donn\u00e9es est-il performant sur des donn\u00e9es nouvelles ? R\u00e9fl\u00e9chissez quelques instants et \u00e9crivez votre r\u00e9ponse dans un coin."]}, {"cell_type": "code", "execution_count": null, "id": "bf4764e9", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "from sklearn import datasets, metrics\n", "\n", "import numpy as np\n", "\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "markdown", "id": "f9e8dcf7", "metadata": {}, "source": ["Chargeons et affichons notre jeu de donn\u00e9es. Ce dernier consiste en des chiffres \u00e9crits \u00e0 la main. L'objectif va \u00eatre de faire un mod\u00e8le qui permet de pr\u00e9dire ces derniers."]}, {"cell_type": "code", "execution_count": null, "id": "a87c1f79", "metadata": {"tags": ["hide-input"]}, "outputs": [], "source": ["digits = datasets.load_digits()\n", "\n", "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n", "for ax, image, label in zip(axes, digits.images, digits.target):\n", "    ax.set_axis_off()\n", "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n", "    ax.set_title('Training: %i' % label)"]}, {"cell_type": "markdown", "id": "49452096", "metadata": {}, "source": ["Construisons notre premier mod\u00e8le. Ce dernier correspond \u00e0 la fonction math\u00e9matique suivante&nbsp;: \n", "\n", "\n", "$$\\begin{aligned}h(x)=\\begin{cases}y&\\text{ si } (x, y)\\in S_n\\\\\\text{al\u00e9atoire}()&\\text{ sinon.}\\end{cases}\\end{aligned}$$\n", "\n", "On retourne le label connu si on a d\u00e9j\u00e0 vu le \"point\" et on retourne un label au hasard sinon."]}, {"cell_type": "code", "execution_count": null, "id": "3bb9e36c", "metadata": {}, "outputs": [], "source": ["class Memorize(object):\n", "    def __init__(self):\n", "        pass\n", "    \n", "    def fit(self, X, y):\n", "        self.X, self.y = X, y\n", "        \n", "    def predict(self, X):\n", "        y = np.zeros(X.shape[0])\n", "        for i in range(X.shape[0]):\n", "            memorized = False\n", "            for j in range(self.X.shape[0]):\n", "                # On compare les pixels un a un\n", "                # et on regarde s'ils sont tous identiques\n", "                # pixel wise comparison\n", "                if (self.X[j] == X[i]).sum() == X.shape[1]:\n", "                    y[i] = self.y[j]\n", "                    memorized = True\n", "                    break\n", "            if not memorized:\n", "                y[i] = np.random.randint(0, 10)\n", "        return y"]}, {"cell_type": "markdown", "id": "edf85c78", "metadata": {}, "source": ["Le code suivant pr\u00e9pare nos donn\u00e9es (des images) afin qu'elles deviennent facilement manipulables par notre mod\u00e8le."]}, {"cell_type": "code", "execution_count": null, "id": "9982ddde", "metadata": {}, "outputs": [], "source": ["# flatten the images\n", "n_samples = len(digits.images)\n", "data = digits.images.reshape((n_samples, -1))"]}, {"cell_type": "code", "execution_count": null, "id": "0e5b774c", "metadata": {}, "outputs": [], "source": ["# Split data into 50% train and 50% test subsets\n", "X_train, X_test, y_train, y_test = train_test_split(\n", "    data, digits.target, test_size=0.5, shuffle=False)"]}, {"cell_type": "markdown", "id": "44e19938", "metadata": {}, "source": ["Les donn\u00e9es de test nous permettront de tester notre mod\u00e8le sur des donn\u00e9es qu'il n'a pas utilis\u00e9 pour se construire. Cela nous permet de tester notre mod\u00e8le sur de \"nouvelles donn\u00e9es\". C'est son pouvoir de g\u00e9n\u00e9ralisation qui nous int\u00e9resse.\n", "\n", "**<span style='color:green'> \u00c9valuation d'un mod\u00e8le</span>** ", "\n", "Il n'existe pas une seule mani\u00e8re d'\u00e9valuer les performances d'un mod\u00e8le. En voici quelques-une.\n", "\n", "*Pr\u00e9cision*\n", "\n", "Consid\u00e9rons une t\u00e2che de classification ($y=\\{1,\\ldots, C\\}$), la pr\u00e9cision d'un mod\u00e8le $h$ pour la classe $c$ est\n", "\n", "$$\\text{prec}_c(h)=\\frac{\\sum_i \\textbf{1}\\{y_i=c, h(x_i)=y_i\\}}{\\sum_i \\textbf{1}\\{h(x_i)=c\\}}$$\n", "\n", "On veut que le score soit proche de $1$.\n", "\n", "---\n", "\n", "*Rappel*\n", "\n", "Consid\u00e9rons une t\u00e2che de classification ($y=\\{1,\\ldots, C\\}$), le rappel d'un mod\u00e8le $h$ pour la classe $c$ est\n", "\n", "$$\\text{rec}_c(h)=\\frac{\\sum_i \\textbf{1}\\{y_i=c, h(x_i)=y_i\\}}{\\sum_i \\textbf{1}\\{y_i=c\\}}$$\n", "\n", "On veut que le score soit proche de $1$.\n", "\n", "---\n", "\n", "*Score f1*\n", "\n", "Ce dernier combine le rappel et la pr\u00e9cision&nbsp;:\n", "\n", "$$\\text{f1}_c(h)=2\\frac{\\text{prec}_c(h)\\cdot\\text{rec}_c(h)}{\\text{prec}_c(h)+\\text{rec}_c(h)}$$\n", "\n", "On veut que le score soit proche de $1$.\n", "\n", "---\n", "\n", "*L'accuracy*\n", "\n", "C'est une sorte de g\u00e9n\u00e9ralisation de la pr\u00e9cision&nbsp;:\n", "\n", "$$\\text{acc}(h)=\\frac{\\sum_i\\textbf{1}\\{h(x_i)=y_i\\}}{n},$$\n", "\n", "c'est le ratio de bonnes pr\u00e9dictions.\n", "\n", "---\n", "\n", "*Le support*\n", "\n", "C'est le nombre de points de notre jeu de donn\u00e9es qui sont concern\u00e9es par la m\u00e9trique.\n", "\n", "\n\n ----"]}, {"cell_type": "code", "execution_count": null, "id": "3163ae79", "metadata": {}, "outputs": [], "source": ["model = Memorize()\n", "model.fit(X_train, y_train)"]}, {"cell_type": "markdown", "id": "6cb32d27", "metadata": {}, "source": ["On commence par tester les performances de notre mod\u00e8le sur notre jeu d'apprentissage."]}, {"cell_type": "code", "execution_count": null, "id": "4c74e0e2", "metadata": {}, "outputs": [], "source": ["predicted = model.predict(X_train)"]}, {"cell_type": "code", "execution_count": null, "id": "8dba3cbd", "metadata": {}, "outputs": [], "source": ["print(f'Classification report for classifier Memorize on train:\\n'\n", "      f'{metrics.classification_report(y_train, predicted)}\\n')"]}, {"cell_type": "markdown", "id": "044533cf", "metadata": {}, "source": ["Notre mod\u00e8le est parfait ! Aucune erreur. On ne peut pas faire mieux ! Et du c\u00f4t\u00e9 du test ?"]}, {"cell_type": "code", "execution_count": null, "id": "af2841cd", "metadata": {}, "outputs": [], "source": ["predicted = model.predict(X_test)"]}, {"cell_type": "code", "execution_count": null, "id": "3e2e93d6", "metadata": {}, "outputs": [], "source": ["print(f'Classification report for classifier Memorize on test:\\n'\n", "      f'{metrics.classification_report(y_test, predicted)}\\n')"]}, {"cell_type": "markdown", "id": "77fb108f", "metadata": {}, "source": ["C'est ridiculement mauvais : on se trompe neuf fois sur dix, soit exactement ce qu'on attendrait d'une r\u00e9ponse al\u00e9atoire."]}, {"cell_type": "markdown", "id": "f34652a9", "metadata": {}, "source": ["Oui mais on a fait expr\u00e8s de construire le mod\u00e8le de cette mani\u00e8re ! En r\u00e9alit\u00e9, il existe une infinit\u00e9 de fonctions, param\u00e9triques ou non, qu'on peut rendre aussi bonne qu'on veut sur nos donn\u00e9es mais qui seraient particuli\u00e8rement mauvaises sur de nouvelles donn\u00e9es (cela inclut les mod\u00e8les usuels et c'est pour cela qu'on a besoin d'experts !)... Toute la difficult\u00e9 du *machine learner* va \u00eatre de contr\u00f4ler cela.\n", "\n", "**<span style='color:blue'> $\\texttt{memorize}$ en pratique</span>** ", "\n", "Cette approche n'est pas totalement absurde. Si $|\\mathcal{X}|<\\infty$, ou si $\\mathcal{X}$ est discret, alors plus on collectera de donn\u00e9es plus les nouvelles donn\u00e9es auront d\u00e9j\u00e0 \u00e9t\u00e9 vues et nos pr\u00e9dictions deviendront int\u00e9ressantes. C'est exactement ce que nous faisons face \u00e0 un nouveau paquet de bonbons dont nous ne connaissons pas le go\u00fbt.\n", "\n", "\n\n ----"]}, {"cell_type": "markdown", "id": "af98575b", "metadata": {}, "source": ["## III. Une autre premi\u00e8re approche logique&nbsp;: le KNN (AS)"]}, {"cell_type": "markdown", "id": "84597902", "metadata": {}, "source": ["Intuitivement, on a envie de dire que nos donn\u00e9es ne sont pas compl\u00e8tement d\u00e9structur\u00e9es. Deux clients tr\u00e8s similaires ach\u00e8teront tr\u00e8s probablement des produits tr\u00e8s similaires. Un _trois_ ressemble plus \u00e0 un _trois_ qu\u2019\u00e0 un _cinq_ et un _cinq_ ressemble plus \u00e0 un _cinq_ qu\u2019\u00e0 un _trois_. Finalement, on g\u00e9n\u00e9ralise un petit peu l'exemple pr\u00e9c\u00e9dent. Au lieu de r\u00e9pondre al\u00e9atoirement si je ne connais pas la donn\u00e9e, je cherche l'exemple le plus proche et je pr\u00e9dis le m\u00eame label ! Plus rigoureusement, notre mod\u00e8le de pr\u00e9diction fonctionne comme suit&nbsp;: \n", "\n", "\n", "$$\\hat{y}_\\text{new}=y\\text{ avec }(x, y)=\\text{argmin}_{(x, y)\\in S}\\lVert x-x_{\\text{new}}\\rVert_2.$$\n", "\n", "On peut imaginer que si plusieurs points sont \u00e9quidistants, la r\u00e9ponse se fait al\u00e9atoirement entre les labels possibles. "]}, {"cell_type": "code", "execution_count": null, "id": "8c257407", "metadata": {}, "outputs": [], "source": ["from sklearn.neighbors import KNeighborsClassifier"]}, {"cell_type": "markdown", "id": "96824700", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Utilisez l'objet $\\texttt{KNeighborsClassifier}$ avec le param\u00e8tre $\\texttt{n}\\_\\texttt{neighbors=1}$ et entra\u00eenez le sur $\\texttt{X}\\_\\texttt{train}$.**\n", "\n\n ----"]}, {"cell_type": "code", "id": "b3fa109b", "metadata": {}, "source": ["####### Complete this part ######## or die ####################\n", "model = \n", "model.fit(X_train, y_train)\n", "###############################################################\n", "\n", "predicted = model.predict(X_train)\n", "\n", "print(f'Classification report for classifier {model} on train:\\n'\n", "      f'{metrics.classification_report(y_train, predicted)}\\n')\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "0837870e", "metadata": {}, "source": ["On a test\u00e9 le mod\u00e8le sur le jeu d'apprentissage et on est toujours aussi bon sur le jeu d'apprentissage ! Cependant, c'est attendu car l'image qui ressemble le plus \u00e0 une autre est l'image elle-m\u00eame. Pr\u00e9disons maintenant sur le test.\n", "\n", "**<span style='color:green'> La matrice de confusion</span>** ", "\n", "Les m\u00e9triques pr\u00e9c\u00e9dentes nous donnent les performances du mod\u00e8le en moyenne ainsi que par classe. Cependant, cela ne nous donne que tr\u00e8s peu d'information quant au type d'erreur qu'il fait. La **matrice de confusion** permet de comprendre le type d'erreurs que fait notre mod\u00e8le. La cellule $i,j$ indique le nombre d'\u00e9l\u00e9ments de la classe $i$ qui ont \u00e9t\u00e9 pr\u00e9dits de la classe $j$.\n", "\n\n ----"]}, {"cell_type": "code", "id": "36b8ec56", "metadata": {}, "source": ["predicted = model.predict(X_test)\n", "\n", "print(f'Classification report for classifier {model} on test:\\n'\n", "      f'{metrics.classification_report(y_test, predicted)}\\n')\n", "\n", "fig, ax = plt.subplots(figsize=(12, 8))\n", "disp = metrics.plot_confusion_matrix(model, X_test, y_test, ax=ax)\n", "disp.figure_.suptitle(\"Confusion Matrix\")\n", "plt.show()\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "fe1cc668", "metadata": {}, "source": ["\n", "\n", "Is Machine learning solved ? Minute papillon ! Ce mod\u00e8le est tr\u00e8s sensible au bruit ! Supposons qu\u2019une de nos donn\u00e9es soit bruit\u00e9e (e.g. un 3 qui ressemble \u00e0 un 8). Si une nouvelle donn\u00e9e repr\u00e9sentant un $8$ se retrouve \u00e0 c\u00f4t\u00e9 de cette anomalie, elle sera mal pr\u00e9dite. Nous pouvons adresser cette limite de la mani\u00e8re suivante&nbsp;: au lieu de regarder le point le plus proche, on regarde les $k$ points les plus proches et on fait un vote \u00e0 la majorit\u00e9. Plus formellement la pr\u00e9diction est faite comme suit&nbsp;:\n", "\n", "$$\\hat{y}_\\text{new}=\\text{majority$\\_$voting}(\\texttt{KNN}.\\texttt{labels})\\text{  o\u00f9  }\\texttt{KNN}=\\text{argmin}_{S^\\prime\\subset S,\\ |S^\\prime|=K}\\sum_i \\lVert x_i- x_{\\text{new}}\\rVert.$$\n", "\n", "Dans le cas o\u00f9 on chercherait \u00e0 faire une r\u00e9gression, on remplace le vote \u00e0 la majorit\u00e9 par une moyenne.\n", "\n", "R\u00e9cup\u00e9rons le jeu de donn\u00e9es $\\texttt{iris}$."]}, {"cell_type": "code", "execution_count": null, "id": "7900ae12", "metadata": {}, "outputs": [], "source": ["iris = datasets.load_iris()\n", "# d\u00e9commentez la ligne suivante pour obtenir des informations\n", "# sur le dataset iris.\n", "# print(iris.DESCR)"]}, {"cell_type": "markdown", "id": "45de3bc8", "metadata": {}, "source": ["De la m\u00eame mani\u00e8re que pr\u00e9c\u00e9demment, on construit notre jeu d'apprentissage pour construire notre mod\u00e8le et notre jeu de test pour en tester les performances."]}, {"cell_type": "code", "execution_count": null, "id": "08513f7e", "metadata": {}, "outputs": [], "source": ["X_train, X_test, y_train, y_test = train_test_split(\n", "    iris.data, iris.target, test_size=0.5, shuffle=True\n", ")"]}, {"cell_type": "markdown", "id": "a6daa568", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Utilisez l'objet $\\texttt{KNeighborsClassifier}$ avec le param\u00e8tre $\\texttt{n}\\_\\texttt{neighbors=1}$ et entra\u00eenez le sur $\\texttt{X}\\_\\texttt{train}$. Faites une pr\u00e9diction sur $\\texttt{predicted=X}\\_\\texttt{test}$.**\n", "\n\n ----"]}, {"cell_type": "code", "id": "24a6b061", "metadata": {}, "source": ["####### Complete this part ######## or die ####################\n", "model = ...\n", "model.fit(...\n", "\n", "predicted = ... \n", "###############################################################\n", "\n", "fig, ax = plt.subplots(figsize=(12, 8))\n", "disp = metrics.plot_confusion_matrix(model, X_test, y_test, ax=ax)\n", "disp.figure_.suptitle(\"Confusion Matrix\")\n", "plt.show()\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "5466c204", "metadata": {}, "source": ["Les performances sont d\u00e9j\u00e0 tr\u00e8s bonnes ! Mais il est possible de gagner un tout petit peu de performances en consid\u00e9rant plus de voisins :"]}, {"cell_type": "markdown", "id": "c4f03f9e", "metadata": {}, "source": ["**<span style='color:blue'> Exercice </span>** ", "**Utilisez l'objet $\\texttt{KNeighborsClassifier}$ avec le param\u00e8tre $\\texttt{n}\\_\\texttt{neighbors=10}$ et entra\u00eenez le sur $\\texttt{X}\\_\\texttt{train}$. Faites une pr\u00e9diction sur $\\texttt{predicted=X}\\_\\texttt{test}$.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "id": "78701edd", "metadata": {}, "source": ["####### Complete this part ######## or die ####################\n", "model = ...\n", "model.fit(...\n", "\n", "predicted = ...\n", "###############################################################\n", "\n", "fig, ax = plt.subplots(figsize=(12, 8))\n", "disp = metrics.plot_confusion_matrix(model, X_test, y_test, ax=ax)\n", "disp.figure_.suptitle(\"Confusion Matrix\")\n", "plt.show()\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "1e55d05a", "metadata": {}, "source": ["## IV. Les arbres de d\u00e9cision ou Classification and Regression Tree (CART) (AS)"]}, {"cell_type": "markdown", "id": "ab327247", "metadata": {}, "source": ["De la m\u00eame mani\u00e8re que pour l'algorithme KNN, on supposera ici que nos donn\u00e9es admettent une certaine structure et que des points proches poss\u00e8dent probablement le m\u00eame label ou une prediction proche dans le cas de la r\u00e9gression. Ici, \u00e0 la diff\u00e9rence du KNN, la notion de voisinage se construit au travers d'hyperrectangles parall\u00e8les aux axes. Afin de bien comprendre le fonctionnement, supposons que notre arbre de d\u00e9cision soit d\u00e9j\u00e0 construit et soit celui repr\u00e9sent\u00e9 par la figure ci-dessous. Prenons une nouvelle donn\u00e9e&nbsp;:\n", "\n", "$$x_{\\text{new}}=[\\text{ecoute:}1, \\text{math:}12,\\text{info:}18]^T,$$ \n", "\n", "\n", "et partons de la racine de notre arbre. Cette racine poss\u00e8de deux branches sortantes. Le choix de la branche se fait \u00e0 partir d'un crit\u00e8re sur une des variables explicatives de $x$. Dans notre exemple la variable explicative est l'\u00e9coute en cours. Si l'\u00e9tudiant \u00e9coute, on prend la branche de droite, sinon celle de gauche. Dans notre, ce sera donc la branche de droite. La nouvelle variable \u00e0 regarder est la note de math et sa valeur doit \u00eatre sup\u00e9rieure \u00e0 10. C'est notre cas et nous prenons la branche de droite. Nous sommes \u00e0 une feuille et la pr\u00e9diction \u00e0 faire est \"oui, l'\u00e9tudiant s'en sortira\".\n", "\n", "![Decision tree](https://raw.githubusercontent.com/maximiliense/lmiprp/main/Travaux%20Pratiques/Machine%20Learning/Introduction/data/Introduction/cart.jpg)\n", "\n", "\n", "\n", "Le choix de la r\u00e8gle de d\u00e9cision \u00e0 chaque noeud peut \u00eatre adapt\u00e9 afin d'obtenir des r\u00e9gions \u00e0 la g\u00e9om\u00e9trie variable. La construction d'un arbre se fait en partant de la racine vers les feuilles et en choisissant int\u00e9rativement les variables explicatives qui ont le plus d'effet sur notre pr\u00e9diction (via diverses crit\u00e8res).\n", "\n", "**<span style='color:blue'> Les feuilles de l'arbre</span>** ", "\n", "En pratique, les feuilles de notre arbre contiennent les exemples d'apprentissage de notre jeu de donn\u00e9es. La pr\u00e9diction d\u00e9pend des exemples contenus dans la feuille qui nous concerne.\n", "\n", "\n\n ----", "\n", "\n", "La s\u00e9quence suivante abordera plus en d\u00e9tails les arbres de classification et de r\u00e9gression."]}, {"cell_type": "code", "execution_count": null, "id": "30a644c4", "metadata": {}, "outputs": [], "source": ["from sklearn.tree import DecisionTreeClassifier"]}, {"cell_type": "markdown", "id": "9dc48796", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "**Utilisez l'objet $\\texttt{DecisionTreeClassifier}$ avec le param\u00e8tre $\\texttt{max}\\_\\texttt{depth=2}$ et entra\u00eenez le sur $\\texttt{X}\\_\\texttt{train}$. Faites une pr\u00e9diction sur $\\texttt{predicted=X}\\_\\texttt{test}$.**\n", "\n\n ----"]}, {"cell_type": "code", "id": "ee8a0093", "metadata": {}, "source": ["####### Complete this part ######## or die ####################\n", "model = ...\n", "model.fit(...\n", "\n", "predicted = ...\n", "###############################################################\n", "\n", "fig, ax = plt.subplots(figsize=(12, 8))\n", "disp = metrics.plot_confusion_matrix(model, X_test, y_test, ax=ax)\n", "disp.figure_.suptitle(\"Confusion Matrix\")\n", "plt.show()\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "ccbd572f", "metadata": {}, "source": ["## V. Les for\u00eats al\u00e9atoires ou Random Forest (RF) (AS)"]}, {"cell_type": "markdown", "id": "c87781c5", "metadata": {}, "source": ["Les arbres de d\u00e9cision peuvent \u00eatre sujets au surapprentissage. Une mani\u00e8re de compenser le probl\u00e8me est d'en construire plusieurs o\u00f9 chaque arbre est construit en ne voyant qu'une partie des donn\u00e9es. Enfin leurs pr\u00e9dictions sont aggr\u00e9g\u00e9es. Ces approches sont g\u00e9n\u00e9ralement beaucoup plus performantes que les arbres simples. Malheureusement, autant avec un arbre simple on pouvait essayer de comprendre la pr\u00e9diction, autant ici, cela devient difficile."]}, {"cell_type": "code", "execution_count": null, "id": "d863da31", "metadata": {}, "outputs": [], "source": ["from sklearn.ensemble import RandomForestClassifier"]}, {"cell_type": "markdown", "id": "329926de", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "**Utilisez l'objet $\\texttt{RandomForestClassifier}$ et entra\u00eenez le sur $\\texttt{X}\\_\\texttt{train}$. Faites une pr\u00e9diction sur $\\texttt{predicted=X}\\_\\texttt{test}$.**\n", "\n\n ----"]}, {"cell_type": "code", "id": "01fc9ad9", "metadata": {}, "source": ["####### Complete this part ######## or die ####################\n", "...\n", "...\n", "...\n", "###############################################################\n", "\n", "fig, ax = plt.subplots(figsize=(12, 8))\n", "disp = metrics.plot_confusion_matrix(model, X_test, y_test, ax=ax)\n", "disp.figure_.suptitle(\"Confusion Matrix\")\n", "plt.show()\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "e7315985", "metadata": {}, "source": ["## VI. Choix des hyperparam\u00e8tres (AS)"]}, {"cell_type": "markdown", "id": "cadbf302", "metadata": {}, "source": ["Pour les mod\u00e8les pr\u00e9c\u00e9dents, nous avons d\u00fb choisir diff\u00e9rents param\u00e8tres qui affectaient les performances de notre mod\u00e8le. Nous les avons choisis en regardant les performances de notre mod\u00e8le sur le jeu de test. Cependant, les bonnes performances \u00e9taient peut-\u00eatre un coup de chance !\n", "\n", "Il existe deux strat\u00e9gies d'\u00e9valuation sans biais de la qualit\u00e9 de notre mod\u00e8le&nbsp;:\n", "* La validation non crois\u00e9e o\u00f9 une partie de notre jeu de donn\u00e9es est cach\u00e9e pendant l'apprentissage puis utilis\u00e9e afin d'\u00e9valuer les performances du mod\u00e8le. Il s'agit du d\u00e9coupage train/test. Cette strat\u00e9gie est un estimateur sans biais de la qualit\u00e9 de notre mod\u00e8le mais poss\u00e8de une variance plus forte que la validation crois\u00e9e. Elle peut-\u00eatre particuli\u00e8rement utile lorsque le coup d'apprentissage d'un mod\u00e8le est tr\u00e8s \u00e9lev\u00e9 (e.g. *deep learning*)\n", "* La validation crois\u00e9e o\u00f9 notre jeu de donn\u00e9es est divis\u00e9 en *k* parties (on parle aussi de *k-fold*). \u00c9videmment, $k\\in\\{2, ..., n\\}$ o\u00f9 $n$ est la taille du jeu de donn\u00e9es. Chacune des parties jouera successivement le r\u00f4le de jeu de test pendant que les $k-1$ autres parties serviront \u00e0 calculer notre mod\u00e8le. Le r\u00e9sultat de cette proc\u00e9dure est un vecteur de $k$ scores dont on peut calculer la moyenne, la variance, etc.\n", "\n", "\n", "On peut illustrer la m\u00e9thode des *k-folds* via l'exemple suivant&nbsp;:\n", "\n", "$$\\begin{align*}\n", "\\text{Appartient au train set: } \\color{red}{\\boxed{}}&\\text{ et appartient au test set: }\\color{green}{\\boxed{}}\n", "\\end{align*}$$\n", "\n", "$$\\begin{align*}\n", "\\text{Step 1: }\\color{green}{\\boxed{}}\\color{red}{\\boxed{}}&\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\n", "\\end{align*}$$\n", "\n", "$$\\begin{align*}\n", "\\text{Step 2: }\\color{red}{\\boxed{}}\\color{green}{\\boxed{}}&\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\n", "\\end{align*}$$\n", "\n", "$$\\begin{align*}\n", "\\text{Step 3: }\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}&\\color{green}{\\boxed{}}\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\n", "\\end{align*}$$\n", "\n", "$$\\begin{align*}\n", "\\text{Step 4: }\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}&\\color{red}{\\boxed{}}\\color{green}{\\boxed{}}\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\n", "\\end{align*}$$\n", "\n", "$$\\begin{align*}\n", "\\text{Step 5: }\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}&\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\\color{green}{\\boxed{}}\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\n", "\\end{align*}$$\n", "\n", "$$\\begin{align*}\n", "\\text{Step 6: }\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}&\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\\color{green}{\\boxed{}}\\color{red}{\\boxed{}}\n", "\\end{align*}$$\n", "\n", "$$\\begin{align*}\n", "\\text{Step 7: }\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}&\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\\color{green}{\\boxed{}}\n", "\\end{align*}$$\n", "\n", "**<span style='color:orange'> Overfitter \u00e0 la main</span>** ", "\n", "Attention, quand on fait quelques tests \u00e0 la main et que l'on \u00e9value nos performances sur le jeu de test, on est d\u00e9j\u00e0 entrain de faire de la s\u00e9lection de mod\u00e8le. On se sert alors du test comme d\u2019un ensemble de validation. C\u2019est d\u2019autant plus vrai lorsqu\u2019on a peu de donn\u00e9es d'apprentissage.\n", "\n\n ----", "\n", "La m\u00e9thode $\\texttt{cross_val_score}$ de $\\texttt{sklearn}$ permet de r\u00e9aliser cette proc\u00e9dure. On pourra renseigner le param\u00e8tre $\\texttt{cv}$ qui indique le nombre $k$ et le param\u00e8tre $\\texttt{scoring}$ qui donne la m\u00e9trique que l'on souhaite calculer.\n", "\n", "Si on cherche \u00e0 trouver une valeur d\u2019un hyper-param\u00e8tre du mod\u00e8le, l\u2019objet $\\texttt{\ud835\ude76\ud835\ude9b\ud835\ude92\ud835\ude8d\ud835\ude82\ud835\ude8e\ud835\ude8a\ud835\ude9b\ud835\ude8c\ud835\ude91\ud835\ude72\ud835\ude85}$ applique une validation crois\u00e9e en cherchant diff\u00e9rentes valeurs de cet hyper-param\u00e8tre.\n"]}, {"cell_type": "code", "execution_count": null, "id": "a4f8c315", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import GridSearchCV"]}, {"cell_type": "code", "execution_count": null, "id": "f839ffda", "metadata": {}, "outputs": [], "source": ["params = {\n", "    'max_depth': [2, 3, 4, 5, 10, None]\n", "}"]}, {"cell_type": "markdown", "id": "846b24f4", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "**Utilisez l'objet $\\texttt{GridSearchCV}$ pour faire une recherche par grille sur le mod\u00e8le $\\texttt{RandomForestClassifier}$ et entra\u00eenez le sur $\\texttt{X}\\_\\texttt{train}$.**\n", "\n\n ----"]}, {"cell_type": "code", "id": "e00898d6", "metadata": {}, "source": ["####### Complete this part ######## or die ####################\n", "...\n", "...\n", "...\n", "###############################################################\n", "\n", "fig, ax = plt.subplots(figsize=(12, 8))\n", "disp = metrics.plot_confusion_matrix(model, X_test, y_test, ax=ax)\n", "disp.figure_.suptitle(\"Confusion Matrix\")\n", "plt.show()\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "b5dad5f1", "metadata": {}, "source": ["Notre RandomForest de base \u00e9tait d\u00e9j\u00e0 tr\u00e8s bon !"]}, {"cell_type": "markdown", "id": "e3eb1d2e", "metadata": {}, "source": ["## VII. L'algorithme des K-Moyennes (ANS)\n", "\n", "Il s'agit ici d'un algorithme non supervis\u00e9. Imaginons que nous ayons collect\u00e9 un jeu de donn\u00e9es $S_n=\\{(X_i)\\}_{i\\leq n}$. On sait qu'il existe des groupes dans nos donn\u00e9es. Supposons m\u00eame qu'on s\u00e2che qu'il existe $K$ groupes. L'id\u00e9e de l'algorithme des K-Moyennes va \u00eatre de d\u00e9tecter ces $K$ groupes en trouvant une solution au probl\u00e8me d'optimisation suivant&nbsp;: \n", "\n", "$$\\text{KMeans}=\\text{argmin}_{m_1, \\ldots, m_K\\in\\mathcal{X}, c_1,\\ldots,c_n\\in\\{1,\\ldots,K\\}}\\sum_{i=1}^K\\sum_{j=1}^n\\textbf{1}\\{c_j=i\\}\\lVert m_i-x_j\\rVert_2=\\text{argmin}_{m_1, \\ldots, m_K\\in\\mathcal{X}, c_1,\\ldots,c_n\\in\\{1,\\ldots,K\\}}\\sum_{j=1}^n\\lVert x_j-m_{c_j}\\rVert_2.$$\n", "\n", "Dit autrement, chaque groupe est repr\u00e9sent\u00e9 par une coordonn\u00e9e $m_i$ (qui s\u2019av\u00e8re \u00eatre la moyenne des \u00e9l\u00e9ments du groupe) et chaque \u00e9l\u00e9ment de notre jeu de donn\u00e9es n\u2019est associ\u00e9 qu\u2019\u00e0 un seul groupe $c_j$."]}, {"cell_type": "markdown", "id": "35245d53", "metadata": {}, "source": ["Consid\u00e9rons le jeu de donn\u00e9es suivant."]}, {"cell_type": "code", "execution_count": null, "id": "58232488", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "\n", "def sample_data(n):\n", "    means = np.array([[-0.5, 0.5], [1, 1], [0.5, -0.5]])\n", "    cov = np.diag([1, 1])/15\n", "    X = np.concatenate([\n", "        np.random.multivariate_normal(m, cov, size=n) for m in means\n", "    ], axis=0)\n", "    y = [i//n for i in range(3*n)]\n", "    return X, y\n", "    \n", "X, y = sample_data(10)"]}, {"cell_type": "code", "execution_count": null, "id": "720813a7", "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "def plot_clusters(X, c, means=None, path=None):\n", "    plt.figure(figsize=(12, 8))\n", "    plt.axis('off')\n", "    plt.scatter(X[:, 0], X[:, 1], c=c)\n", "    plt.title('Nos donn\u00e9es et leur label inconnu')\n", "    if means is not None:\n", "        for m in means:\n", "            plt.scatter([m[0]], [m[1]], s=55, color='red')\n", "    if path is not None:\n", "        for p in path:\n", "            plt.plot(p[:, 0], p[:, 1], color='red')\n", "    plt.show()\n", "plot_clusters(X, y)"]}, {"cell_type": "markdown", "id": "916b32a3", "metadata": {}, "source": ["Le probl\u00e8me de K-Means est NP-Difficile. Pour cela, nous utilisons en pratique un heuristique appel\u00e9 \"algorithme de LLoyd\" qui fonctionnde la mani\u00e8re suivante&nbsp;:\n", "\n", "1.  On initialise les k moyennes\n", "2.  On assigne tous nos points \u00e0 leur moyenne la plus proche\n", "3.  On met \u00e0 jour les moyennes avec les nouveaux points\n", "4.  Si le d\u00e9placement des moyennes est significatif, on reprend \u00e0 l'\u00e9tape 2\n", "\n", "**<span style='color:green'> Fixer le $k$</span>** ", "\n", "Le $k$ peut \u00eatre fix\u00e9 via une selection de param\u00e8tres et une validation crois\u00e9e.\n", "\n", "\n\n ----"]}, {"cell_type": "markdown", "id": "59847cd7", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "**Le probl\u00e8me est NP-Difficile et un algorithme permettant de le r\u00e9soudre est l'algorithme de LLoyd. Impl\u00e9mentez le.**\n", "\n\n ----"]}, {"cell_type": "code", "id": "c0ccc4f6", "metadata": {}, "source": ["class KMeans(object):\n", "    def __init__(self, k=3, epsilon=0.001):\n", "        self.k = k\n", "        self.epsilon = epsilon\n", "    \n", "    def fit(self, X):\n", "        ####### Complete this part ######## or die ####################\n", "        # K means initialization\n", "        ...\n", "        # Step 1\n", "        ...\n", "        # Step 2\n", "        ...\n", "        ###############################################################\n", "        return assignment, means, path\n", "            \n", "\n", "model = KMeans(3)\n", "plot_clusters(X, *model.fit(X))\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "5749c63b", "metadata": {}, "source": ["On se rend compte qu'on arrive \u00e0 retrouver les 3 groupes automatiquement (Les couleurs sont celles calcul\u00e9es par notre mod\u00e8le des K-Moyennes) !"]}, {"cell_type": "markdown", "id": "da6d2453", "metadata": {}, "source": ["## VIII. La mal\u00e9diction de la dimension"]}, {"cell_type": "markdown", "id": "91bf6840", "metadata": {}, "source": ["Nous avons pu observer des sc\u00e9narios o\u00f9 l'erreur sur notre jeu de donn\u00e9es d'apprentissage \u00e9tait $0$ alors que notre mod\u00e8le n'\u00e9tait pas si bon que cela sur notre jeu de test. Cet \u00e9cart peut m\u00eame devenir catastrophique ! De mani\u00e8re plus rigoureuse, le gap de g\u00e9n\u00e9ralisation de notre estimateur $\\hat{h}$ est la quantit\u00e9 suivante&nbsp;:\n", "\n", "\n", "$$\\text{gap}(\\hat{h})=|Re(\\hat{h})-R(\\hat{h})|.$$\n", "\n", "\n", "O\u00f9 $Re$ fait r\u00e9f\u00e9rence \u00e0 notre risque empirique, c'est-\u00e0-dire l'erreur sur le jeu d'apprentissage et $R$ \u00e0 l'erreur en esp\u00e9rance.\n", "\n", "Il est possible d'avoir une id\u00e9e de $R(\\hat{h})$ en passant par un jeu de test ou par une autre strat\u00e9gie d'\u00e9valuation via un jeu de test par exemple, comme nous avons pu le voir. Nous allons ici nous rendre compte que les mod\u00e8les qui regardent le voisinage de nos donn\u00e9es souffrent d'une grosse limite li\u00e9e \u00e0 ce qu'on appelle *la mal\u00e9diction de la dimension* et qui affecte grandement ce *gap*."]}, {"cell_type": "markdown", "id": "f28f1858", "metadata": {}, "source": ["### En d\u00e9tails"]}, {"cell_type": "markdown", "id": "e5a80276", "metadata": {}, "source": ["La mal\u00e9diction de la dimension fait r\u00e9f\u00e9rence aux r\u00e9sultats contre-intuitifs qui apparaissent lorsque la dimension augmente. \n", "\n", "**<span style='color:blue'> Une illustration avec l'orange multi-dimensionnelle</span>** ", "\n", "Cette exemple donne une image plus visuelle de la mal\u00e9diction de la dimension. Mod\u00e9lisons une orange comme une boule parfaite de rayon $8\\text{cm}$ avec une enveloppe (i.e. la peau) d'une \u00e9paisseur de $5\\text{mm}$ partout. Le volume d'une boule $\\mathcal{B}_r$ de rayon $r$ dans un espace de dimension $d$ est donn\u00e9 par&nbsp;:\n", "\n", "$$\\text{vol}(\\mathcal{B}_r)=K r^d,$$\n", "\n", "o\u00f9 $K$ est une constante qui d\u00e9pend de la dimension. La question qu'on peut se poser est celle du volume occup\u00e9 par la peau de l'orange relativement au volume total de l'orange. C'est le volume total de l'orange auquel on soustrait le volume de l'orange sans la peau divis\u00e9 par le volume total de l'orange&nbsp;:\n", "\n", "$$\\text{ratio}=\\frac{\\text{vol}(\\mathcal{B}_8)-\\text{vol}(\\mathcal{B}_{8-0.05})}{\\text{vol}(\\mathcal{B}_8)}.$$\n", "\n", "En dimension $3$, cela nous donne $\\text{ratio}=0.018$. Moins de $2\\%$ du volume de l'orange est occup\u00e9 par la peau. Que se passe-t-il en dimension 500 ? On obtient $\\text{ratio}\\approx 96\\%$. Presque tout le volume de l'orange est occup\u00e9 par la peau ! Et $500$ ne repr\u00e9sente pas la tr\u00e8s grande dimension (une image en couleur de 200px$\\times$200px a une dimension de 120000).\n", "\n", "\n\n ----", "\n", "\n", "Une premi\u00e8re mani\u00e8re de l'observer en *machine learning* est possible gr\u00e2ce au KNN. Ce dernier classe un nouvel \u00e9l\u00e9ment en fonction de ses voisins dans le jeu d'apprentissage. Nous allons en particulier \u00e9tudier l'\u00e9volution du risque de g\u00e9n\u00e9ralisation en fonction de la dimension. Plus pr\u00e9cis\u00e9ment, les donn\u00e9es synth\u00e9tiques sont construites de la mani\u00e8re suivante&nbsp;:"]}, {"cell_type": "code", "execution_count": null, "id": "e13e6568", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "def sample_data(n, k=3, d=3, mu=1):\n", "    y = np.random.randint(0, 2, size=(n, 1))\n", "    \n", "    X = np.random.normal(mu, 1, size=(n, k))\n", "    X = y*X-(1-y)*X # positive have mean mu and negative, -mu\n", "    noise = np.random.normal(0, 1, size=(n, d-k))\n", "    X = np.concatenate([X, noise], axis=1)\n", "    \n", "    return X, y"]}, {"cell_type": "markdown", "id": "b7d30745", "metadata": {}, "source": ["Dit autrement, $k$ dimensions contiennent le signal int\u00e9ressant pour notre t\u00e2che et $d$ dimensions ne servent \u00e0 rien. Nous observons ci-dessous ce qui se passe lorsqu'on rajouter des dimensions de bruits (i.e. qui ne servent \u00e0 rien). C'est typiquement ce pourrait se passer avec des images. Une photo de chien ne contient pas que des pixels descriptifs du concept de chien."]}, {"cell_type": "code", "execution_count": null, "id": "eae56507", "metadata": {}, "outputs": [], "source": ["from sklearn.neighbors import KNeighborsClassifier\n", "\n", "scores = []\n", "redo = 5\n", "max_dim = 5000\n", "first_dim = 10\n", "steps = 100\n", "\n", "for d in range(first_dim, max_dim, steps):\n", "    s = 0\n", "    for _ in range(redo):\n", "        X, y = sample_data(100, d=d)\n", "        X_test, y_test = sample_data(200, d=d)\n", "        c = KNeighborsClassifier()\n", "        c.fit(X, y.reshape((y.shape[0],)))\n", "        s += c.score(X_test, y_test.reshape((y_test.shape[0],)))/redo\n", "    scores.append(s)"]}, {"cell_type": "code", "execution_count": null, "id": "32dd37c6", "metadata": {}, "outputs": [], "source": ["import matplotlib\n", "import matplotlib.pyplot as plt\n", "\n", "# configuration generale de matplotlib\n", "%matplotlib inline\n", "matplotlib.rcParams['figure.figsize'] = (12.0, 8.0)\n", "plt.style.use('ggplot')\n", "\n", "plt.plot(list(range(first_dim, max_dim, steps)), scores)\n", "plt.title('Evolution de l\\'accuracy en fonction de la dimension du probleme')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "c5eb9185", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Quelle est l'accuracy d'un classifieur al\u00e9atoire ?**\n", "\n\n ----", "\n", "**<span style='color:blue'> Exercice</span>** ", "**Expliquez pourquoi l'accuracy diminue lorsqu'on rajoute des dimensions sans signal.**\n", "\n\n ----"]}, {"cell_type": "markdown", "id": "190821f3", "metadata": {}, "source": ["De mani\u00e8re similaire, \u00e9tudions l'\u00e9volution des distances lorsque la dimension \u00e9volue. Nous allons tirer al\u00e9atoirement un ensemble de vecteurs dans $\\mathbb{R}^d$ et nous calculerons la norme $\\lVert x\\rVert^2_2$ moyenne, maximale et minimale de notre tirage. De la m\u00eame mani\u00e8re nous calculerons la distance $\\lVert x-y\\rVert_2$ moyenne, maximale et minimale entre les couples de points de notre jeu de donn\u00e9es. Enfin, l'objectif sera d'\u00e9tudier ces quantit\u00e9s en faisant \u00e9voluer la dimension $d$ du probl\u00e8me."]}, {"cell_type": "code", "execution_count": null, "id": "5e826889", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from scipy.spatial import distance_matrix\n", "\n", "def sample_data(n, d):\n", "    return np.random.uniform(-1, 1, size=(n, d))/np.sqrt(d)\n", "X = sample_data(100, 10)"]}, {"cell_type": "code", "execution_count": null, "id": "2332c495", "metadata": {}, "outputs": [], "source": ["redo = 50\n", "def experiment_(d):\n", "    min_ = 0\n", "    max_ = 0\n", "    mean_ = 0\n", "    for _ in range(redo):\n", "        X = sample_data(100, d)\n", "        vec = np.sqrt((X**2).sum(axis=1))\n", "        min_ += vec.min()/redo\n", "        max_ += vec.max()/redo\n", "        mean_ += vec.mean()/redo\n", "        \n", "        mat = distance_matrix(X[:25], X[:25])\n", "        triu = np.triu(mat)\n", "        triu = triu[triu!=0]\n", "        dist_max = triu.max()\n", "        dist_min = triu.min()\n", "        dist_mean = triu.mean()\n", "    return min_, max_, mean_, dist_min, dist_max, dist_mean\n", "idx = []\n", "val = []\n", "for d in range(10, 1000, 100):\n", "    idx.append([d])\n", "    val.append(experiment_(d))\n", "for d in range(2000, 10000, 1000):\n", "    idx.append([d])\n", "    val.append(experiment_(d))\n", "arr = np.concatenate([np.array(idx), np.array(val)], axis=1)"]}, {"cell_type": "code", "execution_count": null, "id": "2e12e45c", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(15, 7))\n", "plt.subplot(1, 2, 1)\n", "plt.plot(arr[:, 0], arr[:, 1], label='Min')\n", "plt.plot(arr[:, 0], arr[:, 2], label='Max')\n", "plt.plot(arr[:, 0], arr[:, 3], label='Moy')\n", "plt.xlabel('Dimension du probl\u00e8me')\n", "plt.ylabel('Norme $\\ell_2$ des vecteurs')\n", "plt.legend()\n", "plt.title('Evolution des normes')\n", "plt.subplot(1, 2, 2)\n", "plt.plot(arr[:, 0], arr[:, 4], label='Min')\n", "plt.plot(arr[:, 0], arr[:, 5], label='Max')\n", "plt.plot(arr[:, 0], arr[:, 6], label='Moy')\n", "plt.xlabel('Dimension du probl\u00e8me')\n", "plt.ylabel('Distance $\\ell_2$ entre nos vecteurs')\n", "plt.legend()\n", "plt.title('Evolution des distances')"]}, {"cell_type": "markdown", "id": "0b61ec32", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Quel ph\u00e9nom\u00e8ne math\u00e9matique pouvons nous invoquer afin d'expliquer cela ?**\n", "\n", "\n\n ----", "\n", "Soit $x_\\text{new}$ une nouvelle donn\u00e9e. Une petite perturbation du point de notre jeu d'apprentissage le plus diff\u00e9rent de $x_\\text{new}$ peut le transformer en le point le plus proche est inversement... C'est une grosse limite des mod\u00e8les pr\u00e9c\u00e9dents. Il faut soit r\u00e9fl\u00e9chir \u00e0 r\u00e9duire la dimension, soit injecter de la connaissance dans nos mod\u00e8les, etc."]}, {"cell_type": "markdown", "id": "9622d4a2", "metadata": {}, "source": ["## X. Quelques mod\u00e8les propos\u00e9s par $\\texttt{sklearn}$\n", "\n", "Voici une liste (absolument non exhaustive) de quelques mod\u00e8les propos\u00e9s par la librairie $\\texttt{sklearn}$. Certains (pas n\u00e9cessairement pr\u00e9sents dans cette liste) seront approfondis dans les prochaines s\u00e9quences.\n", "\n", "*  Supervis\u00e9&nbsp;:\n", " * R\u00e9gression dans $\\mathbb{R}$&nbsp;:\n", "    * [R\u00e9gression lin\u00e9aire](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) et ses variantes&nbsp;:\n", "      * [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)\n", "      * [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)\n", "      * [Elastic-Net](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)\n", "    * [R\u00e9gression KNN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)\n", "    * [Arbre de r\u00e9gression](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)\n", "    * [For\u00eats al\u00e9atoires](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor)\n", "    * etc.\n", "  * Classification (ou assimil\u00e9s)&nbsp;:\n", "    * [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n", "    * [Classification KNN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n", "    * [R\u00e9gression Logistique](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n", "    * [Arbre de classification](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n", "    * [For\u00eats al\u00e9atoires](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n", "* Non-supervis\u00e9&nbsp;:\n", "  * [K-means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) (clustering)\n", "  * [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) (apprentissage de *manifold*)\n", "  * [OneclassSVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html) (d\u00e9tection de nouveaut\u00e9)\n", "  \n", "  \n", "  \n", "Pour une liste plus exhaustive des diff\u00e9rents outils $\\texttt{sklearn}$ merci de consulter le lien suivant&nbsp; :\n", "* https://scikit-learn.org/stable/supervised_learning.html."]}, {"cell_type": "markdown", "id": "f4833e28", "metadata": {}, "source": ["## XI. Le *no-free-lunch* Theorem"]}, {"cell_type": "markdown", "id": "4be9f209", "metadata": {}, "source": ["Nous avons vu plus haut que le mod\u00e8le $\\texttt{memorize}$ pouvait \u00eatre parfaitement bon sur les donn\u00e9es d'apprentissage mais \u00e9chouer sur de nouvelles donn\u00e9es : il ne g\u00e9n\u00e9ralise pas. Pour cela, nous avons rajout\u00e9 certaines hypoth\u00e8ses et construit de nouveaux mod\u00e8les comme le $\\texttt{KNN}$ qui exploite le voisinage d'un point pour pouvoir faire une pr\u00e9diction. La mal\u00e9diction de la dimension, malheureusement, p\u00e9nalise ce fonctionnement d\u00e8s que la dimension de l'espace d'entr\u00e9e devient trop grande.\n", "\n", "Une question que nous pouvons nous poser est la suivante : pouvons nous construire une r\u00e8gle de classification g\u00e9n\u00e9rique, optimis\u00e9e \u00e0 partir d'un jeu de donn\u00e9es, qui puisse offrir la garantie de toujours fonctionner ?\n", "\n", "La r\u00e9ponse est non, et au-del\u00e0 de cela, pour chaque mod\u00e8le que nous allons utiliser, nous devrons faire des hypoth\u00e8ses sur les donn\u00e9es. Dans les cas o\u00f9 ces hypoth\u00e8ses seraient falsifi\u00e9es alors notre strat\u00e9gie \u00e9chouera.\n", "\n", "**<span style='color:blue'> *No-free-lunch Theorem*</span>** ", "Soit $\\mathcal{D}_n=\\{(X_i, Y_i)\\}_{i\\leq n}$ un jeu de donn\u00e9es et $\\hat{h}_n:\\mathcal{X}\\mapsto\\mathcal{Y}$ un mod\u00e8le de classification (un classifieur) construit \u00e0 partir de $\\mathcal{D}$ selon une r\u00e8gle au choix. L'indice $n$ montre la d\u00e9pendence sur la taille de notre jeu de donn\u00e9es. Soit $1/16\\geq\\{a_n\\}_{n>0}>0$ une suite qui converge vers $0$ mais \u00e0 une vitesse aussi lente qu'on le veuille. Alors, il existe un probl\u00e8me (i.e. une distribution sur $X\\times Y$) tel que le meilleur classifieur $h^\\star$ fasse $0$, mais que notre estimateur ait une erreur satisfaisant l'in\u00e9galit\u00e9&nbsp;:\n", "\n", "$$\\mathbb{E}\\big[R(h_n)\\big]>a_n.$$\n", "\n\n ----", "\n", "Dit autrement, plus le jeu de donn\u00e9es sera grand plus l'erreur sera faible. Mais cette d\u00e9croissance de l'erreur peut \u00eatre arbitrairement lente."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.2"}}, "nbformat": 4, "nbformat_minor": 5}
{"cells": [{"cell_type": "markdown", "id": "572cbe4d", "metadata": {}, "source": ["# L'Analyse en Composantes Principales \u2615\ufe0f\u2615\ufe0f\u2615\ufe0f\n", "\n", "**<span style='color:blue'> Objectifs de la s\u00e9quence</span>** ", "\n", "* \u00catre sensibilis\u00e9&nbsp;:\n", "    * aux enjeux math\u00e9matiques de l'ACP.\n", "* \u00catre capable&nbsp;:\n", "    * d'impl\u00e9menter une ACP avec $\\texttt{sklearn}$.\n", "\n", "\n\n ----"]}, {"cell_type": "markdown", "id": "5d3cd9ab", "metadata": {}, "source": ["## I. Introduction\n", "\n", "L'Analyse en Composante Principale (ACP) consiste \u00e0 extraire des directions dans lesquelles les donn\u00e9es s'\u00e9talent particuli\u00e8rement (i.e. la variance y est maximale). Prenons un exemple. Nous disposons d'un jeu de donn\u00e9es dont on aimerait extraire l'information la plus \"representative\" (dans un sens que nous allons expliciter plus loin). Nous choisissons ici un jeu de donn\u00e9es tir\u00e9 selon une loi gaussienne multivari\u00e9e $\\boldsymbol{x} \\sim \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{\\Sigma})$ avec $\\boldsymbol{\\Sigma} \\in \\mathcal{S}^+(\\mathbb{R}^d)$ (i.e. matrice semi-d\u00e9finie positive de dimension $d\\times d$) la matrice de variance/co-variance des donn\u00e9es. Affichons le jeu de donn\u00e9es."]}, {"cell_type": "code", "execution_count": null, "id": "fd12944a", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "%matplotlib inline\n", "\n", "import matplotlib.pyplot as plt\n", "plt.rcParams['figure.figsize'] = (12.0, 8.0)\n", "plt.style.use('ggplot')"]}, {"cell_type": "code", "execution_count": null, "id": "487ae0d4", "metadata": {}, "outputs": [], "source": ["def plot(X, X_rec=None, vec=None, color=\"blue\", circles = False):\n", "    assert X.shape[1] == 2, \"Dimension must be equal to 2 to plot stuffs.\"\n", "    eps = 0.1\n", "    plt.scatter(X[:,0],X[:,1], edgecolors=color, facecolors=color)\n", "    x1min_ = X[1].min() - eps\n", "    x1max_ = X[1].max() - eps\n", "    x0min_ = X[0].min() - eps\n", "    x0max_ = X[0].max() + eps\n", "    if vec is not None:\n", "        mean = X.mean(axis=0)\n", "        plt.arrow(\n", "            mean[0], mean[1], vec[0][0], vec[0][1], \n", "            head_width=0.1, head_length=0.1, fc='g', \n", "            ec='g', width=0.025, zorder=2\n", "        )\n", "        plt.arrow(\n", "            mean[0], mean[1], vec[1][0], vec[1][1], \n", "            head_width=0.1, head_length=0.1, fc='r', \n", "            ec='r', width=0.025, zorder=2\n", "        )\n", "    if X_rec is not None:\n", "        plt.scatter(X_rec[:,0],X_rec[:,1], edgecolors='r', facecolors='none')\n", "        for i in range(X.shape[0]):\n", "            plt.plot(\n", "                [X[i,0], X_rec[i,0]], [X[i,1], X_rec[i,1]], color='r', \n", "                linestyle = 'dashed', linewidth='0.5'\n", "            )\n", "\n", "    plt.xlim((x0min_, x0max_))\n", "    plt.ylim((x1min_, x1max_))\n", "    plt.axis('equal')"]}, {"cell_type": "code", "execution_count": null, "id": "d79031f8", "metadata": {}, "outputs": [], "source": ["def random_rotation(d):\n", "    Q, _ =  np.linalg.qr(np.random.random((d,d)))\n", "    return Q\n", "\n", "def sample_multivariate_gaussian_data(N, d, variance_decay=0.1):\n", "    sigma = np.diag([np.power(variance_decay, i) for i in range(d)])\n", "    Q = random_rotation(d)\n", "    sigma_r = np.dot(np.dot(Q, sigma ), Q.T)\n", "    return np.random.multivariate_normal(np.ones(d), sigma_r, N)\n", "\n", "X = sample_multivariate_gaussian_data(500, 2)\n", "print(X.shape)\n", "\n", "plot(X, vec = np.eye(X.shape[1]))\n", "plt.show()"]}, {"cell_type": "markdown", "id": "b28a260b", "metadata": {}, "source": ["**<span style='color:blue'> Question </span>** ", "\n", "**Dans le code ci-dessus, \u00e0 quoi sert \u00e0 la d\u00e9composition QR ?** \n", "\n", "\n\n ----", "\n"]}, {"cell_type": "markdown", "id": "5cda40c8", "metadata": {}, "source": ["Dans ce cas pr\u00e9cis, chaque point est repr\u00e9sent\u00e9 par deux nombres. Que pourrions-nous faire pour ne repr\u00e9senter chaque point par seulement un nombre en perdant le moins d'information possible sur le signal d'origine ? Nous pourrions ainsi chercher \u00e0 trouver la direction dans l'espace des donn\u00e9es telle que les donn\u00e9es soient le plus \"\u00e9tal\u00e9es\". Plus formellement on cherche un vecteur unitaire $\\boldsymbol{v} \\in \\mathbb{R}^2$ tel que les projections des donn\u00e9es sur $z_i = \\langle \\boldsymbol{v}, \\boldsymbol{x}_i \\rangle$ constituent un \u00e9chantillon transform\u00e9 dont la variance empirique soit maximale. Il s'agit de r\u00e9soudre le probl\u00e8me d'optimisation suivant&nbsp;:\n", "\n", "$$\\underset{\\boldsymbol{v}}{argmax}\\Bigg[\\frac{1}{N} \\sum_{i=1}^N  (z_i - \\bar{z})^2 \\Bigg] = \\underset{\\boldsymbol{v}}{argmax} \\Big[ \\boldsymbol{v}^t\\bar{\\boldsymbol{X}}^T\\bar{\\boldsymbol{X}}\\boldsymbol{v} \\Big]$$\n", "\n", "sous contrainte que $\\lVert\\boldsymbol{v}\\rVert_2 = 1$ et avec $\\bar{z} = \\frac{1}{N}\\sum_i z_i$ et $\\bar{\\boldsymbol{X}} \\in \\mathbb{R}^{n\\times d}$, la matrice de *design* (i.e. qui contient nos donn\u00e9es) centr\u00e9e.\n", "\n", "Sans la contrainte $\\lVert\\boldsymbol{v}\\rVert_2 = 1$, le probl\u00e8me d'optimisation serait mal pos\u00e9 car la solution ne serait pas unique (il suffirait simplement d'augmenter arbitrairement la norme de n'importe quel vecteur pour avoir une valeur de la fonction objectif arbitrairement grande).\n", "\n", "\n", "Nous allons montrer que ce probl\u00e8me d'optimisation est strictement \u00e9quivalent a celui de trouver le vecteur propre de la matrice de covariance $\\boldsymbol{\\Sigma} = \\bar{\\boldsymbol{X}}^T\\bar{\\boldsymbol{X}}$ correspondant \u00e0 la valeur propre maximale $\\lambda_{\\max}$."]}, {"cell_type": "markdown", "id": "fb31ed21", "metadata": {}, "source": ["**<span style='color:blue'> Question </span>** ", "\n", "**Montrer l'\u00e9galit\u00e9 suivante&nbsp;:**\n", "\n", "$$\\frac{1}{N} \\sum_{i=1}^N  (z_i - \\bar{z})^2=\\boldsymbol{v}^t\\bar{\\boldsymbol{X}}^T\\bar{\\boldsymbol{X}}\\boldsymbol{v}$$\n", "\n", "\n\n ----", "\n"]}, {"cell_type": "markdown", "id": "0b89ad58", "metadata": {}, "source": ["## II. Quelques rappels d'alg\u00e8bre lin\u00e9aire\n", "\n", "\n", "### A. D\u00e9composition en valeurs et vecteurs propres"]}, {"cell_type": "markdown", "id": "11cf6687", "metadata": {}, "source": ["**Qu'est ce qu'un vecteur propre ?** \n", "Alg\u00e9briquement, un vecteur propre d'une matrice $\\boldsymbol{A}$ est un vecteur tel que&nbsp;:\n", "\\begin{equation*}\n", "    \\boldsymbol{A}\\boldsymbol{v} = \\lambda \\boldsymbol{v}\n", "\\end{equation*}\n", "\n", "L'interpr\u00e9tation g\u00e9om\u00e9trique est donc qu'il s'agit d'un vecteur dont la direction de son image par l'application lin\u00e9aire associ\u00e9e \u00e0 la matrice $\\boldsymbol{A}$ n'est qu'une [homoth\u00e9tie](https://fr.wikipedia.org/wiki/Homoth\u00e9tie) (i.e. la direction est inchang\u00e9e et le vecteur n'est qu'\u00e9tir\u00e9). Il est simplement \u00e9tir\u00e9 d'un facteur $\\lambda$ qu'on appel sa valeur propre associ\u00e9e.\n", "\n", "![Vecteur propre](https://upload.wikimedia.org/wikipedia/commons/thumb/5/58/Eigenvalue_equation.svg/2880px-Eigenvalue_equation.svg.png)\n", "\n", "\n", "**Diagonalisation d'une matrice carr\u00e9** \n", "Une matrice carr\u00e9e peut \u00eatre vue comme un endomorphisme allant d'un espace vectoriel $E$ vers lui m\u00eame. On dira que cet endomorphisme est diagonalisable s'il existe une matrice $\\boldsymbol{V}$ inversible et une matrice diagonale $\\boldsymbol{\\Lambda}$ telles que: \n", "\n", "$$\\boldsymbol{A} = \\boldsymbol{V} \\boldsymbol{\\Lambda} \\boldsymbol{V}^{-1}$$\n", "\n", "\n", "\n", "o\u00f9 $\\boldsymbol{V}$ est une matrice dont les colonnes forment une base de $E$ et dont les \u00e9l\u00e9ments sont les vecteurs propres et $\\boldsymbol{\\Lambda}$ est une matrice dont les \u00e9l\u00e9ments diagonaux correspondent aux valeurs propres associ\u00e9es. \n", "\n", "**<span style='color:blue'> Matrices semblables</span>** ", "Deux matrices $A$ et $B$ sont dites semblables s'il existe une matrice inversible $V$ telle que&nbsp;:\n", "\n", "$$A=VBV^{-1}.$$\n", "\n", "\n\n ----", "\n", "\n", "De plus, on peut montrer que si la matrice $\\boldsymbol{A}$ est symm\u00e9trique ($\\boldsymbol{A}$ = $\\boldsymbol{A}^t$) \u00e0 coefficients dans $\\mathbb{R}$ (ces deux propri\u00e9t\u00e9s sont v\u00e9rifi\u00e9es par notre matrice de variance-covariance $\\boldsymbol{\\Sigma}$), la matrice de passage $\\boldsymbol{V}$ est n\u00e9c\u00e9ssairement une matrice orthogonale ($\\boldsymbol{V}^{-1} = \\boldsymbol{V}^t$) et $\\boldsymbol{A}$ prend alors la forme particuli\u00e8re suivante:\n", "\n", "$$\\begin{aligned}\n", "\\boldsymbol{A} = \\boldsymbol{V} \\boldsymbol{\\Lambda} \\boldsymbol{V}^t = \\begin{bmatrix}\n", "    \\vert &   & \\vert &   &  \\vert \\\\\n", "    \\vert &   & \\vert &   &  \\vert \\\\\n", "    \\vert &   & \\vert &  &  \\vert \\\\\n", "    \\boldsymbol{v_1}   & \\dots & \\boldsymbol{v_i} & \\dots & \\boldsymbol{v_d}   \\\\\n", "    \\vert &   & \\vert &   &  \\vert \\\\\n", "    \\vert &  & \\vert &  & \\vert \n", "\\end{bmatrix}\n", "\\begin{bmatrix}\n", "    \\lambda_1 & 0 & \\dots & \\dots  & 0\\\\\n", "     \\vdots & \\ddots & \\vdots & \\vdots & \\vdots\\\\\n", "    0 & \\dots & \\lambda_i & \\dots & 0\\\\\n", "     \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n", "    0 & \\dots & \\dots & 0 &\\lambda_d\\\\\n", "\\end{bmatrix}\n", "\\begin{bmatrix}\n", "    \\text{---} & \\boldsymbol{v_1} & \\text{---} \\\\\n", "      & \\vdots &  \\\\\n", "    \\text{---} & \\boldsymbol{v_i} & \\text{---} \\\\\n", "     & \\vdots &  \\\\\n", "    \\text{---} & \\boldsymbol{v_d} & \\text{---}\n", "\\end{bmatrix}\n", "\\end{aligned}$$\n", "\n", "o\u00f9 $\\boldsymbol{V}$ correspond cette fois \u00e0 une matrice orthogonale i.e.&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "\\boldsymbol{V}^t\\boldsymbol{V} &= \\begin{bmatrix}\n", "    \\vert &    &  \\vert \\\\\n", "    \\boldsymbol{v_1}   &  \\dots & \\boldsymbol{v_d}   \\\\\n", "    \\vert &   & \\vert \n", "\\end{bmatrix}\n", "\\begin{bmatrix}\n", "    \\text{---} & \\boldsymbol{v_1} & \\text{---} \\\\\n", "     & \\vdots &  \\\\\n", "    \\text{---} & \\boldsymbol{v_d} & \\text{---}\n", "\\end{bmatrix}\\\\\n", "&= \\begin{bmatrix}\n", "    \\langle \\boldsymbol{v_1}, \\boldsymbol{v_1} \\rangle & \\dots & \\langle \\boldsymbol{v_1}, \\boldsymbol{v_d} \\rangle \\\\\n", "    \\vdots & \\ddots & \\vdots \\\\\n", "      \\langle \\boldsymbol{v_d}, \\boldsymbol{v_1} \\rangle & \\dots & \\langle \\boldsymbol{v_d}, \\boldsymbol{v_d} \\rangle\n", "\\end{bmatrix}\\\\\n", "&= \\begin{bmatrix}\n", "    ||\\boldsymbol{v_1}||_2^2=1 & \\dots & 0 \\\\\n", "    0 & \\ddots & 0 \\\\\n", "      0 & \\dots & ||\\boldsymbol{v_d}||_2^2=1\n", "\\end{bmatrix}\n", "\\end{aligned}$$\n", "\n", "les vecteurs propres sont donc tous orthogonaux 2 \u00e0 2 et de norme $1$ et forment donc une base orthonormale. \n", "\n", "\n", "C'est ce que nous souhaitons pour notre probl\u00e8me. Nous voulons trouver une rotation de sorte \u00e0 ce que dans la nouvelle base, qu\u2019on appellera composantes, la variance de nos donn\u00e9es soit le plus parfaitement d\u00e9crites par les dites composantes. Nous pouvons donc reformuler de mani\u00e8re plus g\u00e9n\u00e9rale notre probl\u00e8me pour n'importe quel dimension d'entr\u00e9e $d$ comme le probl\u00e8me d\u2019optimisation sous contrainte suivant&nbsp;:\n", "\n", "$$\\underset{\\boldsymbol{V}}{argmax}\\Bigg[\\frac{1}{N} \\sum_{k=1}^d\\sum_{i=1}^N  (z_i^k - \\bar{z}^k)^2 \\Bigg] = \\underset{\\boldsymbol{V}}{argmax} \\Big[ \\boldsymbol{V}^t\\bar{\\boldsymbol{X}}^T\\bar{\\boldsymbol{X}}\\boldsymbol{V} \\Big],\\text{ s.t. }\\boldsymbol{V}^t\\boldsymbol{V} = \\boldsymbol{I_d}$$\n", "\n", "Et nous montrerons que la solution de ce probl\u00e8me consiste \u00e0 trouver la base des vecteurs propres de $\\boldsymbol{\\Sigma} = \\bar{\\boldsymbol{X}}^T\\bar{\\boldsymbol{X}}$.\n", "\n", "---"]}, {"cell_type": "markdown", "id": "71b3a5e5", "metadata": {}, "source": ["### B. Composition d'endormorphismes autoadjoints (sym\u00e9triques)"]}, {"cell_type": "markdown", "id": "0f878593", "metadata": {}, "source": ["La diagonalisation d'un endomorphisme permet de simplifier certains calculs. Nous restons ici dans le cas sym\u00e9trique mais une partie du propos se g\u00e9n\u00e9ralise bien s\u00fbr au cas diagonalisable quelconque. Nous avons ainsi l'\u00e9galit\u00e9 suivante&nbsp;:\n", "\n", "\n", "$$\\boldsymbol{A}^2 = \\boldsymbol{V}\\boldsymbol{\\Lambda}\\underbrace{\\boldsymbol{V}^t\\boldsymbol{V}}_{\\boldsymbol{I}}\\boldsymbol{\\Lambda}\\boldsymbol{V}^t = \\boldsymbol{V}\\boldsymbol{\\Lambda}^2\\boldsymbol{V}^t$$\n", "\n", "\n", "et de mani\u00e8re g\u00e9n\u00e9rale&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "\\boldsymbol{A}^n = \\boldsymbol{V} \\boldsymbol{\\Lambda}^n \\boldsymbol{V}^t = \\begin{bmatrix}\n", "    \\vert &   & \\vert &   &  \\vert \\\\\n", "    \\vert &   & \\vert &   &  \\vert \\\\\n", "    \\vert &   & \\vert &  &  \\vert \\\\\n", "    \\boldsymbol{v_1}   & \\dots & \\boldsymbol{v_i} & \\dots & \\boldsymbol{v_d}   \\\\\n", "    \\vert &   & \\vert &   &  \\vert \\\\\n", "    \\vert &  & \\vert &  & \\vert \n", "\\end{bmatrix}\n", "\\begin{bmatrix}\n", "    (\\lambda_1)^n & 0 & \\dots & \\dots  & 0\\\\\n", "     \\vdots & \\ddots & \\vdots & \\vdots & \\vdots\\\\\n", "    0 & \\dots & (\\lambda_i)^n & \\dots & 0\\\\\n", "     \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n", "    0 & \\dots & \\dots & 0 &(\\lambda_d)^n\\\\\n", "\\end{bmatrix}\n", "\\begin{bmatrix}\n", "    \\text{---} & \\boldsymbol{v_1} & \\text{---} \\\\\n", "      & \\vdots &  \\\\\n", "    \\text{---} & \\boldsymbol{v_i} & \\text{---} \\\\\n", "     & \\vdots &  \\\\\n", "    \\text{---} & \\boldsymbol{v_d} & \\text{---}\n", "\\end{bmatrix}\n", "\\end{aligned}$$\n", "\n", "On remarque ainsi que si $A$ est sym\u00e9trique, alors $A^n$ poss\u00e8de la m\u00eame matrice de changement de base et ses valeurs propres sont les m\u00eames \u00e0 la puissancec $n$.\n", "\n", "Notez que n'importe quel vecteur $\\boldsymbol{x} \\in \\mathbb{R}^d$ peut se d\u00e9composer dans la base des vecteurs propres&nbsp;:\n", "\n", "$$\\boldsymbol{x} = \\sum_{k=1}^d \\underbrace{\\langle \\boldsymbol{v_k}, \\boldsymbol{x} \\rangle}_{z_k} \\boldsymbol{v_k}$$\n", "\n", "---\n", "\n", "**<span style='color:blue'> Question </span>** ", "\n", "**Montrez que $\\boldsymbol{A}\\boldsymbol{x} =  \\sum_{k=1}^d \\lambda_k  z_k \\boldsymbol{v_k}$.**\n", "\n\n ----", "\n", "\n", "\n", "**<span style='color:blue'> Question</span>** ", "\n", "**En utilisant la propri\u00e9t\u00e9 de la mise \u00e0 la puissance, donnez l'expression de $\\boldsymbol{A}^n\\boldsymbol{x}$.**\n", "\n\n ----", "\n"]}, {"cell_type": "markdown", "id": "099d2d1a", "metadata": {}, "source": ["## III. Lien entre directions de variance maximale et vecteurs propres"]}, {"cell_type": "markdown", "id": "9602e617", "metadata": {}, "source": ["**<span style='color:blue'> Proposition</span>** ", "\n", "La direction de variance maximale dans $\\boldsymbol{X}$ correspond au vecteur propre associ\u00e9 \u00e0 la plus grande valeur propre de $\\boldsymbol{\\Sigma}=\\boldsymbol{X}^T\\boldsymbol{X}$.\n", "\n", "\n\n ----", "\n", "\n", "**<span style='color:orange'> Preuve</span>** ", "\n", "$$\\begin{aligned}\\boldsymbol{v}^t\\bar{\\boldsymbol{X}}^T\\bar{\\boldsymbol{X}}\\boldsymbol{v} &= \\boldsymbol{v}^t\\boldsymbol{\\Sigma}\\boldsymbol{v} = \\Big\\langle\\boldsymbol{v}, \\boldsymbol{\\Sigma}\\boldsymbol{v} \\Big\\rangle\\\\ & = \\Bigg\\langle\\boldsymbol{v}, \\Bigg[\\sum_{k=1}^d \\lambda_k    \\langle \\boldsymbol{v}, \\boldsymbol{v_k}\\rangle   \\boldsymbol{v_k}\\Bigg] \\Bigg\\rangle \\\\ &= \\sum_{k=1}^d \\lambda_k    \\langle \\boldsymbol{v}, \\boldsymbol{v_k}\\rangle \\langle \\boldsymbol{v}, \\boldsymbol{v_k}\\rangle  \\\\ &\\leq \\lambda_{\\max} \\underbrace{\\sum_{k=1}^d     \\langle \\boldsymbol{v}, \\boldsymbol{v_k}\\rangle^2}_{=\\lVert\\boldsymbol{v}\\rVert_2^2 = 1}\\\\ &   = \\lambda_{\\max}\\end{aligned}$$\n", "\n", "L'\u00e9galit\u00e9 est ainsi obtenue pour le vecteur propre associ\u00e9 \u00e0 $\\lambda_{max}$. \n", "\n", "\n\n ----", "\n", "Nous avons donc&nbsp;:\n", "\n", "\n", "$$\\lambda_{\\max}=\\boldsymbol{v}_{\\max}\n", "^t\\bar{\\boldsymbol{X}}^T\\bar{\\boldsymbol{X}}\\boldsymbol{v}_{\\max}$$\n", "\n", "Nous devons partir \u00e0 la recherche de ce vecteur propre particulier pour r\u00e9soudre notre probl\u00e8me. Pour cela nous allons voir une m\u00e9thode it\u00e9rative pratique permettant de retrouver le vecteur associ\u00e9 \u00e0 la plus grande valeur propre d'une matrice&nbsp: **l'algorithme des puissances it\u00e9r\u00e9es**."]}, {"cell_type": "markdown", "id": "efc3e1ad", "metadata": {}, "source": ["## IV. Calculer les vecteurs et valeurs propres"]}, {"cell_type": "markdown", "id": "46398721", "metadata": {}, "source": ["### A. L'algorithme des puissances it\u00e9r\u00e9es"]}, {"cell_type": "markdown", "id": "b2e0fa3b", "metadata": {}, "source": ["En r\u00e9utilisant les propri\u00e9t\u00e9s vues \u00e0 propos des matrices diagonalisables, nous pouvons essayer de construire un algorithme qui nous permettra de calculer le vecteur propre associ\u00e9 \u00e0 la plus grande valeur propre&nbsp;:\n", "\n", "$$\\underbrace{\\boldsymbol{A}\\boldsymbol{A}\\dots\\boldsymbol{A}}_{\\times n}\\boldsymbol{v} = \\boldsymbol{A}^n\\boldsymbol{v} =  \\sum_{k=1}^d (\\lambda_k)^n  \\langle \\boldsymbol{v_k}, \\boldsymbol{v}\\rangle   \\boldsymbol{v_k}$$\n", "\n", "Et on voit que c'est la contribution du vecteur propre associ\u00e9e \u00e0 la plus forte valeur propre qui va dominer assymptotiquement. Plus formellement, si on \u00e9crit les valeurs propres sous la formes $\\lambda_k = \\lambda_{\\max}\\frac{\\lambda_{k}}{\\lambda_{\\max}}$&nbsp;:\n", "\n", "$$\\boldsymbol{A}^n\\boldsymbol{v} = (\\lambda_{\\max})^n \\sum_{k=1}^d \\Big(\\frac{\\lambda_{k}}{\\lambda_{\\max}}\\Big)^n  \\langle \\boldsymbol{v_k}, \\boldsymbol{v}\\rangle   \\boldsymbol{v_k},$$\n", "\n", "on obtient $\\lim_{n \\rightarrow \\infty} (\\frac{\\lambda_{k}}{\\lambda_{\\max}})^n = 1$ si $\\lambda_{k} = \\lambda_{\\max}$ et $0$ sinon. Ainsi&nbsp;:\n", "\n", "\n", "$$\\lim_{n \\rightarrow \\infty}\\frac{\\boldsymbol{A}^n\\boldsymbol{v}}{\\lVert\\boldsymbol{A}^n\\boldsymbol{v}\\rVert_2} = \\boldsymbol{v_{\\max}}$$\n"]}, {"cell_type": "markdown", "id": "8814f769", "metadata": {}, "source": ["En multipliant it\u00e9rativement (presque) n'importe quel vecteur d'entr\u00e9e par notre application lin\u00e9aire, nous construisons une suite dont l'expression normalis\u00e9e converge asymptotiquement vers le vecteur propre associ\u00e9 \u00e0 la valeur propre la plus forte. C'est exactement ce qu'on fait en pratique en alternant multiplication et normalisation afin d'\u00e9viter un $\\texttt{overflow}$ de nos variable. L'initialisation est faite al\u00e9atoirement&nbsp;:\n", "\n", "\\begin{equation*}\n", "\\boldsymbol{v}(0) \\leftarrow \\mathcal{N}\\Big(\\boldsymbol{0}, \\boldsymbol{I}\\Big).\n", "\\end{equation*}\n", "\n", "Et chaque it\u00e9ration a la forme suivante&nbsp;:\n", "\n", "\\begin{equation*}\n", "\\boldsymbol{v}(n+1) =  \\frac{\\boldsymbol{A}\\boldsymbol{v}(n)}{\\lVert\\boldsymbol{A}\\boldsymbol{v}(n)\\rVert_2} \n", "\\end{equation*}\n", "\n", "\n", "**<span style='color:blue'> Question </span>** ", "\n", "**Maintenant que nous avons un algorithme pour trouver la composante principale, comment faire pour trouver les autres ?**\n", "\n\n ----", "\n"]}, {"cell_type": "markdown", "id": "d9668b17", "metadata": {}, "source": ["### B. En pratique"]}, {"cell_type": "markdown", "id": "9625c835", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Completer le code des m\u00e9thodes $\\text{power_iteration}$ et $\\text{fit}$ de la classe *ACP* ci-dessous qui implemente le calcul d'une ACP. La m\u00e9thode $\\text{power_iteration}$ doit retourner un tuple contenant le vecteur et sa valeur propre associ\u00e9e. La m\u00e9thode $\\text{fit}$ doit ex\u00e9cuter les puissances it\u00e9r\u00e9es autant de fois que demand\u00e9. C'est-\u00e0-dire $\\texttt{n}\\_\\texttt{components}$ fois.**\n", "\n\n ----"]}, {"cell_type": "code", "id": "be37d5cf", "metadata": {}, "source": ["class ACP(object):\n", "    def __init__(self, n_components=2, n_iter=1):\n", "        self.n_components = n_components\n", "        self.n_iter = n_iter\n", "\n", "    def order_vectors(self,):\n", "        assert self.eigen_vectors is not None or self.eigen_values is not None, \"Must be fitted before\"\n", "        idx = np.argsort(-self.eigen_values)\n", "        self.eigen_values = self.eigen_values[idx]\n", "        self.eigen_vectors = self.eigen_vectors[:, idx]\n", "    \n", "    #### Complete the code here #### or die #######################################\n", "    def power_iteration(self, A):\n", "        ...\n", "        ...\n", "        return vector, value\n", "\n", "    def fit(self, X, ):\n", "        self.X = X\n", "        self.N = X.shape[0]\n", "        self.d = X.shape[1]\n", "\n", "        self.mean = X.mean(axis=0)\n", "        X_n = self.X - self.mean\n", "        self.sigma = np.dot(X_n.T, X_n)/float(self.N)\n", "\n", "        self.eigen_vectors = np.zeros((self.d, self.n_components)) \n", "        self.eigen_values = np.zeros(self.n_components) \n", "        ...\n", "        ...\n", "        ...\n", "        ...\n", "        self.order_vectors()\n", "    ###############################################################################\n", "    \n", "            \n", "    def zeros_pad(self, Z, k):\n", "        return np.c_[Z, np.zeros((Z.shape[0], self.d - k))]\n", "        \n", "    def transform(self, X, k = None):\n", "        #### Complete the code here #### or die #######################################\n", "        # pour un exercice plus bas\n", "        #\n", "        #\n", "        ###############################################################################\n", "        pass\n", "        \n", "    \n", "    def inverse_transform(self, Z, k = None):\n", "        #### Complete the code here #### or die #######################################\n", "        # pour un exercice plus bas\n", "        #\n", "        #\n", "        ###############################################################################\n", "        pass\n", "\n", "    def compress(self, X, k = None):\n", "        return self.inverse_transform(self.transform(X, k), k)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "5bb8aad9", "metadata": {}, "source": ["Appliquons notre m\u00e9thode \u00e0 nos donn\u00e9es. On affichera le rep\u00e8re associ\u00e9 aux composantes principales trouv\u00e9s par notre m\u00e9thode&nbsp;:"]}, {"cell_type": "code", "id": "826a4722", "metadata": {}, "source": ["acp = ACP(n_components=2, n_iter=10)\n", "acp.fit(X)\n", "\n", "plot(X, vec = acp.eigen_vectors.T)\n", "plt.show()\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "e3021c1c", "metadata": {}, "source": ["En pratique, les calculs ne sont pas aussi directs et certaines d\u00e9compositions sont utilis\u00e9es afin d'acc\u00e9l\u00e9rer et de stabiliser les calculs."]}, {"cell_type": "markdown", "id": "0c7ff06a", "metadata": {}, "source": ["## V. Calcul des vecteurs de repr\u00e9sentation (codage) et reconstructrion (d\u00e9codage) \n", "\n", "### A. Compression"]}, {"cell_type": "markdown", "id": "1c616cf8", "metadata": {}, "source": ["Nous venons donc de trouver une base telle que les projections des donn\u00e9es dans ce nouveau syst\u00e8me de coordonn\u00e9es ont une variance maximale. Rappelons nous que l'objectif de l'apprentissage non-supervis\u00e9 de repr\u00e9sentation \u00e0 pour objectif de trouver un mapping, une fonction de codage $\\Phi : \\boldsymbol{x} \\rightarrow \\boldsymbol{z} = \\Phi(\\boldsymbol{x}) :  \\mathbb{R}^d \\rightarrow \\mathbb{R}^K$, et dans notre cas nous pouvons d\u00e9finir $\\Phi$ comme l'application lin\u00e9aire suivante&nbsp;:\n", "\n", "\n", "$$\\boldsymbol{z} = \\hat{\\boldsymbol{V}}_K^T \\boldsymbol{x} \\in \\mathbb{R}^K$$\n", "\n", "o\u00f9 $\\hat{\\boldsymbol{V}}_K^T \\in \\mathbb{R}^{d \\times K}$ correspond \u00e0 la matrice  $\\boldsymbol{V}$ de laquelle on a retir\u00e9e les $d-K$ derniers vecteurs colonnes (on part du principe que les vecteur colonne sont tri\u00e9s par ordre d\u00e9croissant de leur valeur propre associ\u00e9e). Nous avons donc&nbsp;:\n", "\n", "\n", "$$\\hat{\\boldsymbol{V}}_K = \n", "\\begin{bmatrix}\n", "    \\vert &   & \\vert    \\\\\n", "    \\vert &   & \\vert    \\\\\n", "    \\vert &   & \\vert \\\\\n", "    \\boldsymbol{v_1}   & \\dots & \\boldsymbol{v_K}   \\\\\n", "    \\vert &   & \\vert    \\\\\n", "    \\vert &  & \\vert \n", "\\end{bmatrix}$$\n", "\n", "\n", "**<span style='color:blue'> Question</span>** ", "\n", "**Exprimez une application \"inverse\" (fonction de d\u00e9codage), qu'on notera $\\Psi$, qui permettrait de reprojeter la representation $\\boldsymbol{z}$ dans l'espace d'origine. On notera $\\hat{\\boldsymbol{x}} = \\Psi(\\boldsymbol{z})$ la reconstruction du vecteur d'origine.**\n", "\n\n ----", "\n", "\n", "Il est ainsi \u00e9vident que si toutes les composantes sont conserv\u00e9es, on retrouve $\\hat{\\boldsymbol{x}} = (\\Psi \\circ \\Phi) ( \\boldsymbol{x}) =  \\underbrace{\\boldsymbol{V}\\boldsymbol{V}^t}_{\\boldsymbol{I_d}} \\boldsymbol{x} = \\boldsymbol{x}$.\n", "\n", "On notera que comme $\\hat{\\boldsymbol{V}}_K$ avec $K \\leq d$ n'est pas carr\u00e9e de rang plein, elle n'est pas inversible, et l'application $\\Phi$ n'est pas bijective. Ainsi, $\\Psi$ ne peut pas \u00eatre l'inverse de $\\Phi$. Nous avions donc plusieurs choix pour $\\Psi$. Mais nous pouvons montrer que celui que nous avons fait minimise l'erreur de d\u00e9codage&nbsp;: c'est une propri\u00e9t\u00e9 souhaitable pour ce genre de probl\u00e8me en basse dimension."]}, {"cell_type": "markdown", "id": "6d12df3e", "metadata": {}, "source": ["---\n", "**<span style='color:blue'> Exercice</span>** ", "\n", "**Dans le code de l'ACP ci-dessus, compl\u00e9tez les methodes $\\text{transform}$ et $\\text{inverse_transform}$.**\n", "\n\n ----", "---"]}, {"cell_type": "markdown", "id": "abed3294", "metadata": {}, "source": ["Appliquons ensuite le code ci dessous pour visualiser les entr\u00e9es reconstruites (en rouge) par dessus les donn\u00e9es d'entr\u00e9e (en bleu)."]}, {"cell_type": "code", "id": "7210c980", "metadata": {}, "source": ["X_rec = acp.compress(X, k=1)  # on ne projette que sur une composante\n", "\n", "plot(X, vec = acp.eigen_vectors.T, X_rec = X_rec)\n", "plt.show()\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "673cc9a2", "metadata": {}, "source": ["### B. Interpr\u00e9tation de l'erreur de reconstruction moyenne"]}, {"cell_type": "markdown", "id": "d1356f54", "metadata": {}, "source": ["Nous allons voir ici que la fonction de codage/d\u00e9codage correspondant \u00e0 l'ACP correspond \u00e0 un endomorphisme qui est un projecteur orthogonal. De plus la diff\u00e9rence moyenne entre le vecteur d'origine et son projet\u00e9 orthogonal poss\u00e8de la propri\u00e9t\u00e9 int\u00e9ressante suivante&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "        \\text{err} &= \\frac{1}{N}\\sum_i^N||\\boldsymbol{x}_i - \\hat{\\boldsymbol{V}}_K\\hat{\\boldsymbol{V}}_K^t \\boldsymbol{x}_i||_2^2\\\\&=\n", "        \\frac{1}{N}\\sum_i^N||\\sum_{k=1}^d\\langle\\boldsymbol{x}_i, \\boldsymbol{v}_k \\rangle \\boldsymbol{v}_k - \\sum_{k=1}^K\\langle\\boldsymbol{x}_i, \\boldsymbol{v}_k \\rangle \\boldsymbol{v}_k||_2^2\\\\ &=\n", "         \\frac{1}{N}\\sum_i^N||\\sum_{k=K+1}^d\\langle\\boldsymbol{x}_i, \\boldsymbol{v}_k \\rangle \\boldsymbol{v}_k||_2^2 =\n", "         \\frac{1}{N}\\sum_i^N \\sum_{k=K+1}^d\\langle\\boldsymbol{x}_i, \\boldsymbol{v}_k \\rangle^2 \\\\ &=\n", "         \\frac{1}{N}\\sum_{k=K+1}^d \\boldsymbol{v}_k^t \\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{v}_k = \\sum_{k=K+1}^d \\boldsymbol{v}_k^t \\boldsymbol{\\Sigma}\\boldsymbol{v}_k\\\\ =&\n", "         \\sum_{k=K+1}^d \\lambda_k \\xrightarrow{N\\rightarrow\\infty}\\sum_{k=K+1}^d \\text{Var}_{\\boldsymbol{z}}[z_k]\n", "\\end{aligned}$$\n", "\n", "Nous pouvons observer qu'\u00e0 l'\u00e9tape 3, le vecteur diff\u00e9rence est forc\u00e9ment un vecteur de $\\mathbb{R}^d$ et est orthogonale \u00e0 tous les vecteurs propres s\u00e9lectionn\u00e9s dans $\\hat{\\boldsymbol{V}}_K$. $\\hat{\\boldsymbol{V}}_K\\hat{\\boldsymbol{V}}_K^T$ est donc un projecteur orthogonal sur le sous espace propre correspondant aux composantes s\u00e9lectionn\u00e9es. Il est celui qui minimise l'erreur de reconstruction. De plus nous constatons que cette erreur a une norme au carr\u00e9 moyenne qui est une estimation de la somme des variances dans les directions non pertinentes. \n", "\n", "\n", "En pratique on pourra donc choisir le nombre de composante \u00e0 conserver $K$ de sorte \u00e0 ce que la qualit\u00e9 de la reconstruction souhait\u00e9e soit sup\u00e9rieure \u00e0 un certain seuil $\\tau$&nbsp;:\n", "\\begin{equation*}\n", "   \\frac{\\sum_{k=1}^K \\lambda_k}{\\sum_{k=1}^d \\lambda_k} \\geq \\tau \\in [0,1]\n", "\\end{equation*}"]}, {"cell_type": "markdown", "id": "cfdd180c", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Completez les fonction $\\text{compute_cumulative_var}$ et $\\text{find_thresholded_cumulative}$ qui calcul respectivement l quantit\u00e9 exprimez pr\u00e9c\u00e9dement, et d\u00e9termine le nombre de composante minimale pour avoir un *RMSE* d'au moins $\\tau$ (*tresh*).**\n", "\n\n ----"]}, {"cell_type": "code", "id": "efdef53c", "metadata": {}, "source": ["def compute_error(X, X_rec):\n", "    return np.linalg.norm(X - X_rec)/np.linalg.norm(X)\n", "\n", "def compute_cumulative_var(eigen_values, k):\n", "    #### Complete the code here #### or die #######################################\n", "    ...\n", "    ...\n", "    ###############################################################################\n", "\n", "def find_thresholded_cumulative(eigen_values, thresh = 0.9):\n", "    #### Complete the code here #### or die #######################################\n", "    ...\n", "    ...\n", "    ###############################################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "ee2f8b4d", "metadata": {}, "source": ["On peut maintenant construire un jeu de donn\u00e9e de plus haute dimension et tester notre code:"]}, {"cell_type": "code", "id": "207d2056", "metadata": {}, "source": ["X = sample_multivariate_gaussian_data(500, 128)\n", "print(X.shape)\n", "\n", "acp = ACP()\n", "acp.fit(X)\n", "\n", "cumul = [compute_cumulative_var(acp.eigen_values, k=k)for k in range(X.shape[1])]\n", "\n", "K = find_thresholded_cumulative(acp.eigen_values, thresh = 0.95)\n", "print(K, np.float64(K)/X.shape[1])\n", "\n", "plt.figure()\n", "plt.plot([i for i in range(1, len(cumul)+1)], cumul, \n", "         label='Erreur de reconstruction')\n", "plt.legend()\n", "plt.show()\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "c02563f3", "metadata": {}, "source": ["On voit ici que sulement 0.7% des composantes contiennent 95% de l'information."]}, {"cell_type": "markdown", "id": "832ca96e", "metadata": {}, "source": ["## V. Compression d'une base d'images"]}, {"cell_type": "markdown", "id": "3bf124e3", "metadata": {}, "source": ["### A. Les donn\u00e9es et notre ACP\n", "\n", "Dans cette section nous allons tester notre algorithme d'ACP sur des donn\u00e9es d'images brutes en consid\u00e9rant chaque image de dimension $(w \\times h)$ comme un vecteur $\\boldsymbol{x} \\in \\mathbb{R}^{wh}$. Nous proc\u00e9derons sur une base d'images de visages centr\u00e9s en niveau de gris&nbsp;: la base $\\text{Oliveti Faces}$. Commen\u00e7ons dans un premier temps par charger les donn\u00e9es, les afficher et appliquons notre algorthme d'ACP (on prendra plut\u00f4t la version $\\text{np.linalg.eig}$ pour calculer les vecteur propres qui est beaucoup plus stables et efficace computationellement que la notre).  "]}, {"cell_type": "code", "execution_count": null, "id": "3d230e88", "metadata": {}, "outputs": [], "source": ["class ACP(object):\n", "    def __init__(self):\n", "        pass\n", "\n", "    def order_vectors(self,):\n", "        assert self.eigen_vectors is not None or self.eigen_values is not None, \"Must be fitted before\"\n", "        idx = np.argsort(-self.eigen_values)\n", "        self.eigen_values = self.eigen_values[idx]\n", "        self.eigen_vectors = self.eigen_vectors[:, idx]\n", "\n", "    def fit(self, X, ):\n", "        self.X = X\n", "        self.N = X.shape[0]\n", "        self.d = X.shape[1]\n", "\n", "        self.mean = X.mean(axis=0)\n", "        X_n = self.X - self.mean\n", "        self.sigma = np.dot(X_n.T, X_n)/float(self.N)\n", "\n", "        self.eigen_values, self.eigen_vectors = np.linalg.eig(self.sigma)\n", "        self.eigen_values = self.eigen_values.real\n", "        self.eigen_vectors = self.eigen_vectors.real\n", "        self.order_vectors()\n", "        \n", "    def zeros_pad(self, Z, k):\n", "        return np.c_[Z, np.zeros((Z.shape[0], self.d - k))]\n", "        \n", "    def transform(self, X, k = None):\n", "        assert self.eigen_vectors is not None, \"You need to fit the model first !\"\n", "        k = self.n_components if k is None else k\n", "        return np.dot(X - self.mean, self.eigen_vectors[:, :k])\n", "    \n", "    def inverse_transform(self, Z, k = None):\n", "        assert self.eigen_vectors is not None, \"You need to fit the model first !\"\n", "        k = self.n_components if k is None else k\n", "        return np.dot(self.zeros_pad(Z, k), self.eigen_vectors.T) + self.mean\n", "\n", "    def compress(self, X, k = None):\n", "        return self.inverse_transform(self.transform(X, k), k)"]}, {"cell_type": "code", "execution_count": null, "id": "41bbad72", "metadata": {}, "outputs": [], "source": ["from sklearn import datasets\n", " \n", "# Load data\n", "lfw_dataset = datasets.fetch_olivetti_faces() #datasets.fetch_lfw_people(min_faces_per_person=500)\n", " \n", "_, h, w = lfw_dataset.images.shape\n", "X = lfw_dataset.data\n", "print(X.shape)"]}, {"cell_type": "code", "execution_count": null, "id": "20f0ea28", "metadata": {}, "outputs": [], "source": ["# Visualization\n", "def plot_images(images, h, w, rows=8, cols=8, title=None):\n", "    plt.figure(figsize=(64,64))\n", "    for i in range(rows * cols):\n", "        plt.subplot(rows, cols, i + 1)\n", "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n", "        if title is not None:\n", "            plt.title(title + \" : \" + str(i+1))\n", "        plt.xticks(())\n", "        plt.yticks(())\n", "\n", "def plot_images_one_by_one(images, original, h, w, step=1, title=None):\n", "    for i in range(images.shape[0]):\n", "        plt.figure(figsize=(6, 12))\n", "\n", "        plt.subplot(1, 2, 1)\n", "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n", "        plt.xticks(())\n", "        plt.yticks(())\n", "        if title is not None:\n", "            plt.title(title + \" : \" + str(i*step+1))\n", "\n", "        plt.subplot(1, 2, 2)\n", "        plt.imshow(original.reshape((h, w)), cmap=plt.cm.gray)\n", "        plt.xticks(())\n", "        plt.yticks(())\n", "        if title is not None:\n", "            plt.title('Original image')\n", "        \n", "\n", "            \n", "plot_images(X, h, w)"]}, {"cell_type": "code", "execution_count": null, "id": "582399c0", "metadata": {}, "outputs": [], "source": ["acp = ACP()\n", "acp.fit(X)"]}, {"cell_type": "markdown", "id": "12302798", "metadata": {}, "source": ["### B. Affichage des variances cumull\u00e9es et calcul du seuil de reconstruction"]}, {"cell_type": "markdown", "id": "3a51a6c4", "metadata": {}, "source": ["Nous calculons ensuite le nombre de composantes \u00e0 conserver pour obtenir une qualit\u00e9 de reconstruiuction moyenne de $95\\%$ de $\\text{RMSE}$. On voit ici que l'on peut ne conserver que $2 \\%$ des variables d'entr\u00e9es pour avoir une qualit\u00e9 moyenne de reconstruction de $95 \\%$."]}, {"cell_type": "code", "id": "b6b95f79", "metadata": {}, "source": ["cumul = [compute_cumulative_var(acp.eigen_values, k=k)for k in range(X.shape[1])]\n", "\n", "K = find_thresholded_cumulative(acp.eigen_values, thresh = 0.95)\n", "print(K, float(K)/X.shape[1])\n", "\n", "plt.figure()\n", "plt.plot([i for i in range(1, len(cumul)+1)], cumul, \n", "         label='Erreur de reconstruction')\n", "plt.legend()\n", "plt.show()\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "88cd1f32", "metadata": {}, "source": ["### C. Affichage des vecteur propres (les eigen faces)"]}, {"cell_type": "markdown", "id": "4ef3cd47", "metadata": {}, "source": ["Affichons les vecteurs propres appris sur ce jeu de donn\u00e9es. On remarquera qu'on peut interpreter ces vecteurs propres coommes des images et donc qu'on peut les afficher comme tel. On affichera un sous ensemble dans un soucis de lisibilit\u00e9. Une mani\u00e8re d'interpreter ces images est donc que tout visage de la base peut \u00eatre reconstruit sans erreur comme une somme pond\u00e9r\u00e9e des ces visages \"\u00e9l\u00e9mentaires\"."]}, {"cell_type": "code", "id": "7574a00c", "metadata": {}, "source": ["plot_images(acp.eigen_vectors.T, h,w)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "7e7e290d", "metadata": {}, "source": ["### D. Visualisation perceptuelle de la qualit\u00e9 de reconstruction"]}, {"cell_type": "markdown", "id": "a37b2ef8", "metadata": {}, "source": ["Nous pouvons aussi nous amuser visualiser diff\u00e9rentes versions reconstruites d'une m\u00eame image en ne conservant qu'une certaine proportion des composantes $k \\in [1, K]$. On voit ici que, perceptuellement, m\u00eame sans aller jusqu'au nombre de composantes seuil d\u00e9finit pr\u00e9c\u00e9dement, on peut tr\u00e8s rapidmeent converger vers l'image originale avec peu de composantes."]}, {"cell_type": "code", "id": "a99f2611", "metadata": {}, "source": ["idx = 14#np.random.randint(X.shape[0])\n", "image = X[idx].reshape(1,X.shape[1])\n", "compressed_images = np.array([acp.compress(image, k=k) for k in range(0, K, 20)])\n", "\n", "plot_images_one_by_one(compressed_images, image, h, w, step=20, \n", "                       title=\"Progressive reconstruction \\n n_components\")\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "4147e7ce", "metadata": {}, "source": ["### E. Essayons les m\u00eames \u00e9tapes sur mnist"]}, {"cell_type": "code", "execution_count": null, "id": "fb62cc5c", "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import fetch_openml\n", "X, _ = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n", "\n", "plot_images(X, 28, 28)"]}, {"cell_type": "code", "id": "4a1a39cf", "metadata": {}, "source": ["acp = ACP()\n", "acp.fit(X)\n", "\n", "#errors = [compute_error(X, x_rec) for x_rec in [acp.compress(X, k=k) for k in range(X.shape[1])]]\n", "cumul = [compute_cumulative_var(acp.eigen_values, k=k)for k in range(X.shape[1])]\n", "plt.figure()\n", "plt.plot(\n", "    [i for i in range(1, len(cumul)+1)], cumul, \n", "    label='Erreur de reconstruction'\n", ")\n", "plt.legend()\n", "plt.show()\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "id": "42414aef", "metadata": {}, "source": ["plot_images(acp.eigen_vectors.T, 28, 28)\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "id": "532ca64c", "metadata": {}, "source": ["K = find_thresholded_cumulative(acp.eigen_values, thresh = 0.90)\n", "print(K, float(K)/X.shape[1])\n", "\n", "idx = 142\n", "image = X[idx].reshape(1,X.shape[1])\n", "\n", "compressed_images = np.array([acp.compress(image, k=k) for k in range(0, K, 10)])\n", "\n", "plot_images_one_by_one(\n", "    compressed_images, image, 28, 28, step=10, \n", "    title=\"Progressive reconstruction \\n n_components\"\n", ")\n"], "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.2"}}, "nbformat": 4, "nbformat_minor": 5}
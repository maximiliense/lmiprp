{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# La r\u00e9gression lin\u00e9aire \u2615\ufe0f \n", "\n", "**<span style='color:blue'> Objectifs de la s\u00e9quence</span>** ", "\n", "* Concevoir&nbsp;:\n", "    * la r\u00e9gression lin\u00e9aire d'un point de vue pr\u00e9dictif,\n", "    * la r\u00e9gression lin\u00e9aire au travers d'un probl\u00e8me d'optimisation.\n", "* \u00catre capable&nbsp;:\n", "    * d'impl\u00e9menter un algorithme de descente de gradient,\n", "    * de transformer les variables d'entr\u00e9e pour rendre le mod\u00e8le non lin\u00e9aire,\n", "    * d'utiliser la librairie $\\texttt{sklearn}$.\n", "* De s'initier \u00e0 la notion de r\u00e9gularisation et de s\u00e9lection de variables.\n", "\n", "\n\n ----"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## I. Introduction\n", "\n", "La r\u00e9gression lin\u00e9aire est un mod\u00e8le cherchant \u00e0 \u00e9tablir un lien lin\u00e9aire entre des donn\u00e9es d'observation et des donn\u00e9es \u00e0 pr\u00e9dire. Plus concr\u00e8tement, les donn\u00e9es observ\u00e9es sont d\u00e9crites par un vecteur $\\mathbf{x} \\in \\mathbb{R}^d$ et la variable \u00e0 pr\u00e9dire, par une quantit\u00e9 scalaire (un r\u00e9el) $y \\in \\mathbb{R}$. On notera $\\mathbf{x}, y\\sim \\mathbb{P}$ la loi jointe du couple (par un abus de langage important, $\\mathbf{x}$ et $y$ expriment \u00e0 la fois une variable al\u00e9atoire et sa r\u00e9alisation) avec $\\mu$ la marginale de $\\mathbf{x}$ et le lien s'exprime sous le format suivant&nbsp;:\n", "\n", "\n", "$$y = \\beta_0 +  x_1 \\beta_1 + x_2 \\beta_2 + x_3 \\beta_3 + ... + x_d \\beta_d +\\epsilon = \\beta_0 + \\sum_i^d x_i \\beta_i+\\epsilon,\\ \\epsilon\\sim\\mathcal{N}(0, \\sigma)$$\n", "\n", "\n", "que l'on peut aussi \u00e9crire en notation vectorielle&nbsp;:\n", "\n", "\n", "$$y = \\beta_0  + \\langle \\boldsymbol{\\beta}, \\mathbf{x} \\rangle +\\epsilon,\\ \\epsilon\\sim\\mathcal{N}(0, \\sigma)$$\n", "\n", "\n", "o\u00f9 $\\boldsymbol{\\beta} \\in \\mathbb{R}^d$ et $\\beta_0\\in\\mathbb{R}$ correspondent respectivement au vecteur et au scalaire contenant les param\u00e8tres du \"vrai\" mod\u00e8le qui d\u00e9fini le lien entre les donn\u00e9es et que l'on va vouloir apprendre pour pr\u00e9dire la bonne valeur de $y$ en fonction du vecteur $\\mathbf{x}$. Le mod\u00e8le lin\u00e9aire ne peut pr\u00e9dire la variable $y$ qu'\u00e0 un bruit $\\epsilon$ pr\u00e8s. Une fois ces param\u00e8tres appris par notre algorithme d'apprentissage, on pourra utiliser la fonction de pr\u00e9diction $f_{\\boldsymbol{\\beta}}(\\mathbf{x}): \\mathbb{R}^d \\rightarrow \\mathbb{R}$ apprise pour pr\u00e9dire la valeur $y_{new}$ associ\u00e9e \u00e0 un nouveau vecteur $\\mathbf{x_{new}}$ que l'on n'a pas encore observ\u00e9&nbsp;:\n", "\n", "\n", "$$\\hat{y}_{new} = f_{\\boldsymbol{\\beta}}(\\boldsymbol{x_{new}}) = \\beta_0  + \\langle \\boldsymbol{\\beta}, \\boldsymbol{x_{new}} \\rangle$$\n", "\n", "Pour simplifier les calculs et les notations, on pr\u00e9f\u00e8re que la fonction de pr\u00e9diction puisse se calculer \u00e0 partir d'une notation compl\u00e8tement vectorielle. C'est ce que l'on fait en pratique, en ajoutant une composante suppl\u00e9mentaire $x_0$ au vecteur $\\mathbf{x}$ \u00e9gale \u00e0 $1$&nbsp;:\n", "\n", "$$\\begin{align*}\n", "    \\mathbf{x} &= \\begin{bmatrix}\n", "          1 \\\\\n", "           x_{1} \\\\\n", "           \\vdots \\\\\n", "           x_{d}\n", "         \\end{bmatrix},\n", "\\end{align*}$$\n", "\n", "de sorte \u00e0 ce que la fonction de pr\u00e9diction lin\u00e9aire puisse s'exprimer simplement sous la forme du produit scalaire: \n", "\n", "$$\\begin{aligned}\n", "    f_{\\boldsymbol{\\beta}}(\\mathbf{x}) &= \\langle \\boldsymbol{\\beta}, \\mathbf{x} \\rangle &=\n", "          \\begin{bmatrix}\n", "           \\beta_{0} \\\\           \n", "           \\vdots \\\\\n", "           \\beta_{d}\n", "          \\end{bmatrix}^T\n", "          \\begin{bmatrix}\n", "           1 \\\\\n", "           \\vdots \\\\\n", "           x_{d}\n", "         \\end{bmatrix} &= \\sum_{i=0}^d x_i \\beta_i=\\langle \\boldsymbol{x}, \\boldsymbol{\\beta}\\rangle_{\\mathbb{R}^{d+1}}\n", "\\end{aligned}$$\n", "\n", "o\u00f9 cette fois $\\boldsymbol{\\beta} \\in \\mathbb{R}^{d+1}$, $\\boldsymbol{x} \\in \\mathbb{R}^{d+1}$ et $\\langle \\cdot, \\cdot\\rangle_{\\mathbb{R}^{d+1}}$ est le produit scalaire dans $\\mathbb{R}^{d+1}$. Le but d'un algorithme d'apprentissage sera de trouver un estimateur $\\boldsymbol{\\hat{\\beta}}$ de $\\boldsymbol{\\beta}$ \u00e0 partir d'un ensemble fini de $n$ exemples d'apprentissage $(\\boldsymbol{x}, \\langle \\boldsymbol{\\beta}, \\boldsymbol{x} \\rangle + \\epsilon) \\in \\mathbb{R}^2$ pr\u00e9alablement collect\u00e9s. On notera $\\mathcal{S}=\\{(\\boldsymbol{x_i}, y_i)\\}_{i\\leq n}$ le jeu de donn\u00e9es.\n", "\n", "\n\n ----", "Les mod\u00e8les pr\u00e9dictifs cherchent \u00e0 r\u00e9aliser une pr\u00e9diction relative \u00e0 une t\u00e2che quelconque apr\u00e8s avoir pu observer une donn\u00e9e $x$. Le mod\u00e8le \u00e9tant bien entendu \"appris\" \u00e0 partir d'un jeu de donn\u00e9es d'apprentissage.\n", "\n", "Les mod\u00e8les descriptifs cherchent \u00e0 quantifier les liens entre les variables explicatives et la variable \u00e0 expliquer. \n", "\n", "Que ce soit pour une analyse pr\u00e9dictive ou une analyse descriptive, nous pouvons utiliser un mod\u00e8le lin\u00e9aire. Cependant, dans le cadre d'une analyse pr\u00e9dictive, le crit\u00e8re cl\u00e9 sera vraiment la qualit\u00e9 de la pr\u00e9diction sur des ***donn\u00e9es nouvelles***.\n", "\n\n ----", "\n", "Nous commencerons par impl\u00e9menter le cas simple d'une r\u00e9gr\u00e9ssion lin\u00e9aire \u00e0 une seule variable d'entr\u00e9e et une seule variable de sortie qui pourra donc s'\u00e9crire sous la forme&nbsp;:\n", "\n", "\n", "$$\\hat{y} = f_{\\boldsymbol{\\beta}}(\\mathbf{x}) = \\beta_0  + \\beta_1 x$$\n", "\n", "\n", "C'est \u00e0 dire une fonction affine dont on pourra afficher la repr\u00e9sentation graphique (une droite) sur une figure en 2 dimensions. Par la suite vous aurez donc \u00e0 impl\u00e9menter le calcul de la fonction de co\u00fbt du mod\u00e8le sur l'ensemble d'apprentissage, le calcul du gradient de cette fonction de co\u00fbt ainsi que l'algorithme de descente de gradient qui, \u00e0 partir du gradient, permet d'obtenir le vecteur $\\hat{\\beta}$.\n", "\n", "La seconde partie de ce notebook \u00e9tendra ces notions \u00e0 des concepts plus compliqu\u00e9s."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## II. Construction d'un jeu de donn\u00e9es"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Commen\u00e7ons tout d'abord par simuler notre jeu de donn\u00e9es avec le mod\u00e8le g\u00e9n\u00e9ratif suivant&nbsp; :\n", "\n", "\n", "$$\\boldsymbol{x} \\sim \\mathcal{N}(\\mu, \\sigma) \\in \\mathbb{R}$$\n", "\n", "\n", "ou $\\sigma$ correspond \u00e0 la variance de la variable explicative. Nous choissons une r\u00e8gle arbitraire pour g\u00e9n\u00e9rer al\u00e9atoirement les param\u00e8tres du \"vrai\" mod\u00e8le&nbsp;:\n", "\n", "\n", "$$\\boldsymbol{\\beta} \\sim \\mathbb{U}(-1, 1)^2 \\in \\mathbb{R}^2$$\n", "\n", "\n", "o\u00f9 $\\mathbb{U}^2$ est la loi uniforme dans $\\mathbb{R}^2$. Enfin, le bruit est construit de la mani\u00e8re suivante&nbsp;:\n", "\n", "\n", "$$\\epsilon \\sim \\mathcal{N}(0, 1).$$\n", "\n", "Chaque exemple d'apprentissage correspond donc \u00e0 un couple de r\u00e9els $(x_j, y_j = \\beta_0  + \\beta_1 x_j + \\epsilon) \\in \\mathbb{R}^2$. Le code ci dessous construit et affiche le jeux de donn\u00e9es ainsi que la repr\u00e9sentation graphique de $f(x)=\\beta_1x+\\beta_0$."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "\n", "# on simule le vecteur de parametre\n", "beta = np.random.uniform(-1, 1, size=(2,1))\n", "\n", "# on construit un jeu de donnees de 10 points selon la methode \n", "# decrite ci-dessus.\n", "def sample_data(n, sigma=5, add_noise=True):\n", "    X = np.random.normal(0, sigma, size=n)\n", "    noise = np.random.normal(0, 1, size=n)*(add_noise*1)\n", "    y = beta[1] * X + noise + beta[0]\n", "    return X, y\n", "\n", "X, y = sample_data(10, add_noise=True)  # jouer avec le bruit"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib\n", "import matplotlib.pyplot as plt\n", "\n", "# configuration generale de matplotlib\n", "%matplotlib inline\n", "matplotlib.rcParams['figure.figsize'] = (12.0, 8.0)\n", "plt.style.use('ggplot')\n", "\n", "# plot de la fonction\n", "def plot(X, y, beta=None, func=None):\n", "    eps = 0.1\n", "    plt.scatter(X, y)\n", "    ymin_ = y.min() - eps\n", "    ymax_ = y.max() + eps\n", "    min_ = X.min() - eps\n", "    max_ = X.max() + eps\n", "    if beta is not None or func is not None:\n", "        x_ = np.linspace(min_, max_, 500)\n", "        if func is None:\n", "            y_ = beta[1] * x_ + beta[0]\n", "            plt.plot(x_, y_)\n", "        else:\n", "            func = [(func, '')] if type(func) is not list else func\n", "            disp_legend = False\n", "            for t in func:\n", "                y_ = t[0](x_.reshape((x_.shape[0], 1)))\n", "                plt.plot(x_, y_, label=t[1])\n", "                disp_legend = disp_legend or t[1] != ''\n", "            if disp_legend:\n", "                plt.legend()\n", "\n", "    plt.xlim((min_, max_))\n", "    plt.ylim((ymin_, ymax_))\n", "    plt.show()\n", "    \n", "# on plot le dataset precedent\n", "plot(X, y, beta)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Que se passe-t-il si le bruit $\\epsilon$ est nul ? Quelle est alors la m\u00e9thode la plus rapide pour trouver les param\u00e8tres du vrai mod\u00e8le ?**\n", "\n", "\n\n ----"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "**<span style='color:green'> Interpolation</span>** ", "\n", "Lorsque le bruit disparait, la droite passe exactement par tous les points (\u00e0 moins que l'hypoth\u00e8se $y=\\langle\\beta, x\\rangle$ soit fausse). On parle alors d'interpolation.\n", "\n\n ----"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## III. Du mod\u00e8le statistique \u00e0 l'optimisation"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### A. La fonction objectif"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Nous souhaitons en pratique trouver un param\u00eatre $\\boldsymbol{\\hat{\\beta}}$ qui minimise le risque du mod\u00e8le, c'est-\u00e0-dire la quantit\u00e9 d'erreur en esp\u00e9rance de n'importe quel mod\u00e8le $\\boldsymbol{\\beta}$. On notera $\\boldsymbol{\\beta}^\\star$ le \"vrai\" mod\u00e8le, soit celui qui minimise le risque en esp\u00e9rance. Pour la r\u00e9gression lin\u00e9aire, on peut d\u00e9finir ce risque comme&nbsp;:\n", "\n", "\n", "$$R(\\boldsymbol{\\beta}) = \\mathbb{E}_{X\\times Y}\\Big[ (f_{\\boldsymbol{\\beta}}(\\mathbf{X}) - Y)^2 \\Big].$$\n", "\n", "On ne sait pas calculer cette fonction car on ne conna\u00eet pas la distribution jointe et o\u00f9 qu\u2019on ne sait pas calculer l\u2019int\u00e9grale. Cependant, on peut en avoir un estimateur via un jeu de donn\u00e9es $\\mathcal{S}$, o\u00f9 $\\mathcal{S} = \\Big\\{ \\big(\\boldsymbol{x_j}, y_j \\big) \\Big\\}_{j\\leq n}$ est un jeu de donn\u00e9es compos\u00e9 de $n$ points ind\u00e9pendants et identiquement distribu\u00e9s selon le mod\u00e8le g\u00e9n\u00e9ratif d\u00e9crit pr\u00e9c\u00e9dement. \n", "\n", "\n", "A d\u00e9faut d'avoir acc\u00e8s au risque (i.e. \u00e0 l'erreur en esp\u00e9rance), on peut utiliser une autre quantit\u00e9 qui consiste en la somme des carr\u00e9s des erreurs de pr\u00e9dictions pour chaque exemple d'apprentissage, c'est **le risque emprique**&nbsp;:\n", "\n", "\n", "$$J(\\boldsymbol{\\beta}) = R_{emp}(\\boldsymbol{\\beta}) = \\frac{1}{n}\\sum_j^n (f_{\\boldsymbol{\\beta}}(x_j) - y_j)^2$$\n", "\n", "o\u00f9 $f_{\\boldsymbol{\\beta}}(x_j) = \\beta_0  + \\beta_1 x_j$. On montre assez facilement que pour un $\\boldsymbol{\\beta}$ quelconque&nbsp;:\n", "\n", "\n", "$$R(\\boldsymbol{\\beta})=\\mathbb{E}_{\\mathcal{S \\sim \\mathbb{P}_S}}\\big[J(\\boldsymbol{\\beta})\\big],$$\n", "\n", "Notons que minimiser ce risque empirique revient \u00e0 chercher le maximum de vraisemblance du mod\u00e8le statistique. Effectivement, avec l'hypoth\u00e8se gaussienne, la vraissemblance de n'importe quel mod\u00e8le de param\u00e8tres $\\boldsymbol{\\beta}$ pour un jeu de donn\u00e9es $\\mathcal{S}$ peut s'\u00e9crire&nbsp;:\n", "\n", "\n", "$$\\mathcal{L}_{\\mathcal{S}}(\\boldsymbol{\\beta}) \\propto \\prod_{\\boldsymbol{x}\\times y\\in\\mathcal{S}} \\exp\\Bigg(-\\frac{\\big(f_{\\boldsymbol{\\beta}}(\\mathbf{x}) - y\\big)^2}{2}\\Bigg)$$\n", "\n", "Le param\u00e8tre maximisant la vraissamblance est aussi celui minimisant la log-vraissamblance n\u00e9gative&nbsp;:\n", "\n", "$$- \\text{log} \\Big( \\mathcal{L}_{\\mathcal{S}}(\\boldsymbol{\\beta})\\Big) = \\sum_{\\boldsymbol{x}\\times y\\in\\mathcal{S}}\\frac{\\big(f_{\\boldsymbol{\\beta}}(\\mathbf{x}) - y\\big)^2}{2}\\propto\\sum_{\\boldsymbol{x}\\times y\\in\\mathcal{S}}\\big(f_{\\boldsymbol{\\beta}}(\\mathbf{x}) - y\\big)^2$$\n", "\n", "N'ayant acc\u00e8s au v\u00e9ritable risque, on cherche $\\boldsymbol{\\hat{\\beta}}$ tel que&nbsp;:\n", "\n", "\n", "$$\\boldsymbol{\\hat{\\beta}} = \\text{argmin}_{\\boldsymbol{\\beta}} \\Big[ - \\log \\Big( \\mathcal{L}_{\\mathcal{S}}(\\boldsymbol{\\beta})\\Big) \\Big]$$\n", "\n", "\n", "Minimiser le risque emprique se traduit donc naturellement par un probl\u00e8me d'optimisation de la fonction de co\u00fbt $J(\\boldsymbol{\\beta}) : \\mathbb{R}^2 \\rightarrow \\mathbb{R}$ ($\\mathbb{R}^2$ dans notre exemple courant, $\\mathbb{R}^{d+1}$ dans le cas g\u00e9n\u00e9ral) par rapport \u00e0 $\\boldsymbol{\\beta}$.\n", "\n", "En pratique et pour des raisons de simplicit\u00e9, on ne minimise pas $\\sum_{\\boldsymbol{x}\\times y\\in\\mathcal{S}}\\big(f_{\\boldsymbol{\\beta}}(\\mathbf{x}) - y\\big)^2$ mais&nbsp;:\n", "\n", "$$J(\\boldsymbol{\\beta}) = \\frac{1}{2n}\\sum_{\\boldsymbol{x}\\times y\\in\\mathcal{S}} (f_{\\boldsymbol{\\beta}}(x) - y)^2$$\n", "\n", "Le r\u00e9sultat est bien \u00e9videmment le m\u00eame. La division par $2$ est l\u00e0 pour simplifier l'expresion du gradient que l'on calculera et la division par $n$ permet de rendre la norme du gradient ind\u00e9pendente de la taille de notre jeu de donn\u00e9es. C'est une propri\u00e9t\u00e9 importante pour l'algorithme de descente de gradient dont la taille des d\u00e9placements affecte sa stabilit\u00e9.\n", "\n", "**Note - Notation vectorielle de la r\u00e9gression lin\u00e9aire :** On peut aussi exprimer ce calcul avec une \u00e9quation en notation vectorielle. Pour cela, on exprime dans un premier temps le r\u00e9sultat de la fonction de pr\u00e9diction en notation vectorielle (il s'agit de la pr\u00e9diction pour tout notre jeu de donn\u00e9es)&nbsp;:\n", "\n", "$$f_{\\boldsymbol{\\beta}}(\\mathbf{X}) = \\mathbf{X}\\boldsymbol{\\beta}\\in\\mathbb{R}^n$$\n", "\n", "o\u00f9 $\\boldsymbol{\\beta} \\in \\mathbb{R}^{d+1}$ est une matrice de dimensions $(d+1)\\times 1$ (en $\\texttt{numpy}$, la dimension $1$ est importante) et $\\mathbf{X}$ est une matrice de dimensions $n\\times (d+1)$ dont les $n$ vecteurs lignes correspondent aux vecteurs d'apprentissage d'entr\u00e9e. Dans notre cas (celui de la r\u00e9gression lin\u00e9aire \u00e0 $1$ variable) la matrice prend la forme suivante&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "\\mathbf{X} = \n", "\\begin{pmatrix} \n", "1 & x_{1} \\\\\n", "\\vdots & \\vdots\\\\\n", "1 & x_{j} \\\\\n", "\\vdots & \\vdots\\\\\n", "1 & x_{n} \n", "\\end{pmatrix},\\ \\boldsymbol{\\beta}=\n", "\\begin{bmatrix}\n", "\\beta_{0} \\\\           \n", "\\beta_{1}\n", "\\end{bmatrix}\n", "\\end{aligned}$$\n", "\n", "La fonction de co\u00fbt peut ainsi s'exprimer&nbsp;:\n", "\n", "\n", "$$J(\\boldsymbol{\\beta}) = \\frac{1}{2n} (\\mathbf{X}\\boldsymbol{\\beta} - \\mathbf{y})^T(\\mathbf{X}\\boldsymbol{\\beta} - \\mathbf{y})$$\n", "\n", "que l'on peut r\u00e9\u00e9crire&nbsp;:\n", "\n", "$$J(\\boldsymbol{\\beta}) = \\frac{1}{2n} (\\mathbf{\\hat{y}} - \\mathbf{y})^T(\\mathbf{\\hat{y}} - \\mathbf{y}) =  \\frac{1}{2n} ||\\mathbf{\\hat{y}} - \\mathbf{y}||_2^2$$\n", "\n", "o\u00f9 $\\mathbf{y} \\in  \\mathbb{R}^n$ est le vecteur dont chacune des composantes $y_j$ sont les valeurs \u00e0 pr\u00e9dire \u00e0 partir de leur $x_j$ correspondant, et $\\hat{y} \\in  \\mathbb{R}^n$ correspond aux valeurs pr\u00e9dites par le mod\u00e8le. On note ici que la fonction objectif \u00e0 optimiser peut se calculer ais\u00e9ment en utilisant la norme euclidienne au carr\u00e9 du vecteur d'erreur."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### B. Optimisation par Descente de gradient\n", "\n", "**<span style='color:blue'> Le gradient est orthogonal aux lignes de niveau</span>** ", "\n", "Soit $c:\\mathbb{R}^+\\mapsto\\mathbb{R}^d$ un arc param\u00e9tr\u00e9 qui suit une ligne de niveau de $f$ (un arc param\u00e9tr\u00e9 prend en argument le \"temps\" et retourne une coordonn\u00e9e dans l'espace). Si $c$ suit une ligne de niveau, nous avons donc&nbsp;:\n", "\n", "$$f(c(t))=f(c(0))=\\text{const}.$$\n", "\n", "Cela implique que nous ayons aussi&nbsp;:\n", "\n", "$$(f(c(t)))^\\prime=\\langle \\nabla f(c(t)), c^\\prime(t)\\rangle=0,$$\n", "\n", "o\u00f9 $\\nabla f(c(t))$ est le gradient en $c(t)$ et $c^\\prime(t)$ donne la direction de l'arc param\u00e9tr\u00e9 (i.e. de la ligne de niveau) en $c(t)$. Les deux sont bien ainsi orthogonaux.\n", "\n\n ----", "\n", "La descente de gradient est une m\u00e9thode d'optimisation num\u00e9rique permettant de trouver les valeurs des param\u00e8tres qui minimisent une fonction. Dans notre cas, nous voulons minimiser l'erreur de pr\u00e9diction moyenne de notre mod\u00e8le, fonction d\u00e9finie pr\u00e9c\u00e9demment. Cette m\u00e9thode d'optimisation consiste \u00e0 calculer le gradient de notre fonction objectif par rapport aux param\u00e8tres courant du mod\u00e8le et de les d\u00e9placer par un \"petit\" pas dans la direction oppos\u00e9e au gradient (i.e. le gradient donne la plus forte croissance et son oppos\u00e9 la plus forte d\u00e9croissance).\n", "\n", "\n", "\n", "**D\u00e9finition g\u00e9n\u00e9rale du gradient d'une fonction \u00e0 plusieurs variables&nbsp;:** Il s'agit simplement du vecteur contenant les d\u00e9riv\u00e9es partielles de la fonction, c-\u00e0-d les d\u00e9riv\u00e9es de la fonction par rapport \u00e0 chaque variable ind\u00e9pendamment des autres&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "\\nabla_{\\boldsymbol{\\beta}} J(\\boldsymbol{\\beta}) = \\frac{\\partial J(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = \n", "\\begin{bmatrix}\n", "\\frac{\\partial J(\\beta)}{\\partial \\beta_0}\\\\\n", "\\frac{\\partial J(\\beta)}{\\partial \\beta_1}\\\\\n", " \\vdots \\\\\n", "\\frac{\\partial J(\\beta)}{\\partial \\beta_d}\n", "\\end{bmatrix}\n", "\\end{aligned}$$\n", "\n", "**<span style='color:blue'> Le gradient est la plus forte pente</span>** ", "\n", "Soit $c:\\mathbb{R}^+\\mapsto\\mathbb{R}^d$ un arc param\u00e9tr\u00e9 et soit $f:\\mathbb{R}^d\\mapsto\\mathbb{R}$. Nous \u00e9tudions l'\u00e9volution de $f$ le long de $c$&nbsp;:\n", "\n", "$$f(c(t)).$$\n", "\n", "L'accroissement de $f$ le long de $c(t)$ est donn\u00e9 par la d\u00e9riv\u00e9e&nbsp;:\n", "\n", "$$(f(c(t))^\\prime=\\langle \\nabla f(c(t)), c^\\prime(t)\\rangle.$$\n", "\n", "Nous avons par Cauchy-Schwartz&nbsp;:\n", "\n", "$$\\lVert\\langle \\nabla f(c), c^\\prime\\rangle\\rVert \\leq \\lVert\\nabla f(c)\\rVert\\lVert c^\\prime\\rVert,$$\n", "\n", "o\u00f9 le gradient et l'arc sont \u00e9valu\u00e9s en $t$. L'\u00e9galit\u00e9 est atteinte lorsque les vecteurs sont colin\u00e9aires. La plus forte pente est donc la direction du gradient.\n", "\n", "\n\n ----", "\n", "En descente de gradient, la mise \u00e0 jour de chaque param\u00e8tre $\\beta_j$ du mod\u00e8le \u00e0 l'it\u00e9ration $t$ se fait donc avec la r\u00e8gle suivante&nbsp;:\n", "\n", "$$\\beta_j^{(t+1)} = \\beta_j^{(t)} - \\rho  \\frac{\\partial J(\\beta^{(t)})}{\\partial \\beta_j}$$\n", "\n", "ou bien, en notation vectorielle&nbsp;:\n", "\n", "\n", "$$\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - \\rho  \\nabla_{\\boldsymbol{\\beta}} J(\\boldsymbol{\\beta})^{(t)}$$\n", "\n", "\n", "\n", "o\u00f9 $\\rho$ est le learning rate (pas d'apprentissage). Un pas d'apprentissage $\\rho$ trop petit nous fera nous d\u00e9placer trop lentement et trop grand rendra l'optimisation instable."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### C. \u00c0 vous de jouer"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**<span style='color:blue'> Question 1</span>** ", "**Compl\u00e9tez la m\u00e9thode $\\texttt{val}$ de l'objet $\\texttt{LeastSquare}$ ci-dessous.**\n", "\n\n ----", "\n", "\n", "**<span style='color:blue'> Question 2</span>** ", "**Calculez les d\u00e9riv\u00e9es partielles $\\partial J(\\beta)/\\partial \\beta_0$ et $\\partial J(\\beta)/\\partial \\beta_1$ de la fonction de co\u00fbt de notre mod\u00e8le de r\u00e9gr\u00e9ssion lin\u00e9aire. Compl\u00e9tez la m\u00e9thode $\\texttt{grad}$ de l'objet $\\texttt{LeastSquare}$ ci dessous.**\n", "\n", "\n\n ----", "\n", "**<span style='color:green'> Indices</span>** ", "\n", "Rappellez vous que la d\u00e9riv\u00e9e d'une composition de fonctions s'\u00e9crit $(g \\circ f)^\\prime (x) = f^\\prime(x) g^\\prime(f(x))$ et que la fonction de co\u00fbt de notre mod\u00e8le s'\u00e9crit&nbsp;:\n", "\n", "\n", "$$J(\\boldsymbol{\\beta}) = \\frac{1}{2n}\\sum_j^n g(f_{\\boldsymbol{\\beta}}(x_j) - y_j)$$\n", "\n", "\n", "avec $g(z) = z ^ 2$ et $f_{\\boldsymbol{\\beta}}(x_j) = \\beta_0  + \\beta_1 x_j$.\n", "\n", "\n\n ----", "\n", "\n", "\n", "\n", "**<span style='color:blue'> Question 3 (dur)</span>** ", "**Calculez le gradient de la fonction $J(\\boldsymbol{\\beta})$ en utilisant les d\u00e9riv\u00e9es matricielles. Compl\u00e9tez la m\u00e9thode $\\texttt{grad}$ de l'objet $\\texttt{LeastSquare}$ avec le gradient en notation vectorielle.**\n", "\n", "\n\n ----", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["remove-cell"]}, "outputs": [], "source": ["# Le code ci-dessous permettra d'afficher notre fonction de cout (le risque empirique)\n", "from matplotlib import cm\n", "from mpl_toolkits.mplot3d import axes3d, Axes3D\n", "%matplotlib inline\n", "%config InlineBackend.print_figure_kwargs = {'bbox_inches':None}\n", "matplotlib.rcParams['figure.figsize'] = (12.0, 8.0)\n", "plt.style.use('ggplot')\n", "\n", "def plot_loss_contour(l, param_trace=None, figsize=None, three_dim=False, rotate=12):\n", "    \n", "    x, y = np.mgrid[slice(-4, 4 + 0.1, 0.1),\n", "                    slice(-4, 4 + 0.1, 0.1)]\n", "    z = np.zeros(x.shape)\n", "    for i in range(x.shape[0]):\n", "        for j in range(x.shape[1]):\n", "            z[i, j] = l.val([x[i, j], y[i, j]])\n", "    if figsize is not None:\n", "        f = plt.figure(figsize=figsize)\n", "    else:\n", "        f = plt.figure(figsize=(12.0, 8.0))\n", "    if three_dim:\n", "        ax = Axes3D(f, auto_add_to_figure=False)\n", "        f.add_axes(ax)\n", "    else:\n", "        ax = f.gca()\n", "    \n", "    if three_dim:\n", "        m = ax.plot_surface(x, y, np.sqrt(z), cmap=cm.viridis)\n", "    else:\n", "        m = ax.contourf(x, y, np.sqrt(z), levels = 10)\n", "    #\n", "    if param_trace is not None:\n", "        if three_dim:\n", "            eps = 0.5\n", "            #ax.scatter(param_trace[:, 0], param_trace[:, 1], param_trace[:, 2] + eps, \n", "            #           color='blue', alpha=1)\n", "            ax.plot(param_trace[:, 0], param_trace[:, 1], param_trace[:, 2] + eps, \n", "                    color='red')\n", "            ax.view_init(50, rotate)\n", "                \n", "        else:\n", "            param_trace = np.array(param_trace) if type(param_trace) is list else param_trace\n", "            plt.plot(param_trace[:, 0], param_trace[:, 1])\n", "            plt.scatter(param_trace[:, 0], param_trace[:, 1])\n", "            f.colorbar(m)\n", "    #plt.show()"]}, {"cell_type": "code", "metadata": {}, "source": ["class LeastSquare(object):\n", "    def __init__(self, X, y):\n", "        self.X = np.insert(LeastSquare._format_ndarray(X), 0, 1, axis=1)\n", "        self.y = LeastSquare._format_ndarray(y)\n", "        self.idx = np.array([i for i in range(self.X.shape[0])])\n", "        self._pos = 0\n", "        \n", "    def _format_ndarray(arr):\n", "        arr = np.array(arr) if type(arr) is not np.ndarray else arr\n", "        return arr.reshape((arr.shape[0], 1)) if len(arr.shape) == 1 else arr\n", "    \n", "    def val(self, beta):\n", "        beta = LeastSquare._format_ndarray(beta)\n", "        ####### Complete this part ######## or die ####################\n", "        ...\n", "        ...\n", "        ...\n", "        ###############################################################\n", "        return val\n", "    \n", "    def _shuffle(self):\n", "        np.random.shuffle(self.idx)\n", "    \n", "    def grad(self, beta, batch_size=-1):\n", "        batch_size = self.X.shape[0] if batch_size == -1 else batch_size\n", "        idx = self.idx[self._pos:self._pos+batch_size]\n", "\n", "        self._pos = (self._pos+batch_size) % self.X.shape[0]\n", "        if self._pos == 0:\n", "            self._shuffle()\n", "            \n", "        X, y = self.X[idx], self.y[idx]\n", "\n", "        beta = LeastSquare._format_ndarray(beta)\n", "        \n", "        ####### Complete this part ######## or die ####################\n", "        ...\n", "        ###############################################################\n", "        return grad\n", "    \n", "\n", "l = LeastSquare(X, y)\n", "print('La valeur de la loss pour le vrai parametre est', l.val(beta))\n", "print('La valeur du gradient pour le vrai parametre est\\n', l.grad(beta))\n", "plot_loss_contour(l, three_dim=True)\n", "plot_loss_contour(l, three_dim=False)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Attention, pour des raisons de temps de calcul, l'estimation du gradient n'est pas tout le temps faite sur tout le jeu de donn\u00e9es mais sur une partie de celui-ci. Un estimateur calcul\u00e9 de cette mani\u00e8re l\u00e0 aura en esp\u00e9rance la m\u00eame valeur qu'un gradient calcul\u00e9 sur toutes les donn\u00e9es. On appelle g\u00e9n\u00e9ralement Descente de Gradient Stochastique ou SGD une approche qui ne fait qu'estimer le gradient \u00e0 partir d'un batch de donn\u00e9es.\n", "\n", "**<span style='color:blue'> Question</span>** ", "\n", "**Saurez-vous retrouver dans le code ci-dessus ce qui permet de jouer sur la taille du batch lors du calcul du gradient ?**\n", "\n", "\n\n ----"]}, {"cell_type": "markdown", "metadata": {}, "source": ["###  D. L'algorithme de descente de gradient"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Compl\u00e9tez le code de descente de gradient.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "metadata": {}, "source": ["class GradientDescent(object):\n", "    init = np.random.uniform(-4, 4, size=2).reshape((2, 1))\n", "    def __init__(self, X, y, loss=LeastSquare):\n", "        self.loss = loss(X, y)\n", "        \n", "    def optimize(self, learning_rate = 0.005, nb_iterations=100, beta=init, batch_size=-1):\n", "        param_trace = [beta.T[0]]\n", "        loss_trace = [self.loss.val(beta)]\n", "        for i in range(nb_iterations):\n", "            ####### Complete this part ######## or die ####################\n", "            ...\n", "            ###############################################################\n", "            param_trace.append(beta.T[0])\n", "            loss_trace.append(self.loss.val(beta))\n", "            \n", "        return param_trace, loss_trace\n", "        \n", "gd = GradientDescent(X, y)\n", "\n", "param_trace, loss_trace = gd.optimize(nb_iterations=500)\n", "\n", "param_trace = np.array(param_trace)\n", "loss_trace = np.array(loss_trace)\n", "loss_trace = loss_trace.reshape((loss_trace.shape[0], 1))\n", "xyz = np.concatenate([param_trace, np.sqrt(loss_trace)], axis=1)\n", "\n", "plot_loss_contour(l, param_trace=xyz, three_dim=True)\n", "plot_loss_contour(l, param_trace=param_trace, three_dim=False)\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"tags": ["remove-cell"]}, "source": ["# Pensez a installer ipywidgets\n", "import ipywidgets as widgets\n", "from IPython.display import display\n", "from IPython.display import clear_output\n", "%matplotlib inline\n", "\n", "output = widgets.Output()\n", "\n", "@output.capture()\n", "def interactive_gradient_descent(learning_rate, batch_size):\n", "    clear_output()\n", "    param_trace , loss_trace = gd.optimize(learning_rate = learning_rate, batch_size = batch_size)\n", "    plot_loss_contour(gd.loss, param_trace, figsize=(14.0, 6.0))\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"tags": ["remove-cell"]}, "source": ["widgets.interact(interactive_gradient_descent,\n", "                 learning_rate=widgets.FloatSlider(value=1e-5, min=1e-5, max=0.05, step=0.0001, \n", "                                                   continuous_update=False, readout_format='.5f'),\n", "                 batch_size=widgets.IntSlider(value=X.shape[0], min=1, max=X.shape[0], step=1, \n", "                                              continuous_update=False)\n", ")\n", "display(output)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["**<span style='color:blue'> Convergence de la descente de gradient</span>** ", "\n", "Notons $\\beta^\\star$ une solution du probl\u00e8me d'optimisation ci-dessus. Si le pas d'apprentissage est assez petit, alors il est possible de d\u00e9montrer que l'algorithme de descente de gradient converge n\u00e9cessairement vers $J(\\beta^\\star)$ \u00e0 une vitesse proportionnelle \u00e0 $1/k$ o\u00f9 $k$ est le nombre d'it\u00e9rations. La s\u00e9quence ***optimisation*** de ce cours d\u00e9montrera rigoureusement cela.\n", "\n", "\n\n ----"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**<span style='color:blue'> Stochastic Gradient Descent</span>** ", "\n", "La propri\u00e9t\u00e9 d'orthogonalit\u00e9 par rapport aux lignes de niveau de la fonction de co\u00fbt est elle conserv\u00e9e dans ce cas ? Pourquoi ? Ne suit-on pourtant toujours pas le gradient ? Que pouvez vous dire sur la nature et la \"vitesse\" de convergence vers le minimum de la fonction ? R\u00e9fl\u00e9chissez d'un point de vue calculatoire sur ce qui se passe sur des tailles d'\u00e9chantillons tr\u00e8s grandes ? \n", "\n", "\n\n ----"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### E. Les \u00e9quations normales de la r\u00e9gression lin\u00e9aire : la solution par pseudo-inverse"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Comme calcul\u00e9 plus haut, l'expression du gradient est donn\u00e9e par $(X^TX\\boldsymbol{\\beta}-X^T\\boldsymbol{y})/n$.  La fonction $J$ \u00e9tant coercive et convexe, elle admet au moins un minimum local/global. Les points critiques sont donn\u00e9s en annulant le gradient&nbsp;:\n", "\n", "\n", "$$X^TX\\boldsymbol{\\beta}-X^T\\boldsymbol{y} = 0 \\Leftrightarrow X^TX\\boldsymbol{\\beta}=X^t\\boldsymbol{y}.$$\n", "\n", "\n", "Il s'agit des \u00e9quations dites \"normales\". Tout vecteur $\\boldsymbol{\\beta}$ solution de ces \u00e9quations est donc n\u00e9cessairement un minimiseur de $J(\\boldsymbol{\\beta})$."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Dans le cas standard (i.e. sur-d\u00e9termin\u00e9)** o\u00f9 chaque variable explicative est lin\u00e9airement ind\u00e9pendante des autres et o\u00f9 le nombre d'\u00e9chantillons de notre jeu de donn\u00e9es est sup\u00e9rieur ou \u00e9gal \u00e0 la dimension du probl\u00e8me consid\u00e9r\u00e9, la matrice $X^TX$ est inversible (i.e. $\\text{det}(X^TX)\\neq 0$). Dit autrement, il existe une unique solution aux \u00e9quations normales donn\u00e9e par&nbsp;:\n", "\n", "\n", "$$\\hat{\\boldsymbol{\\beta}}=(X^TX)^{-1}X^T\\boldsymbol{y}.$$\n", "\n", "\n", "On appelle $X^\\dagger = (X^TX)^{-1}X^T$  pseudo-inverse de $X$ (ou inverse g\u00e9n\u00e9ralis\u00e9e) et la solution analytique \u00e0 notre probl\u00e8me est donn\u00e9e par $\\hat{\\boldsymbol{\\beta}}=X^\\dagger \\boldsymbol{y}$.\n", "\n", "**<span style='color:green'> L'effet du bruit</span>** ", "\n", "Soit $\\boldsymbol{y}=X\\boldsymbol{\\beta}+\\eta$ o\u00f9 on utilise $\\eta$ plut\u00f4t que $\\epsilon$ pour diff\u00e9rentier la r\u00e9alisation effective du bruit de la variable al\u00e9atoire. Notre estimateur est donc&nbsp;:\n", "\n", "$$\\begin{aligned}\\hat{\\beta}&=X^\\dagger y = X^\\dagger(X\\boldsymbol{\\beta} + \\eta)\\\\ &= (X^\\dagger X)\\boldsymbol{\\beta} + X^\\dagger\\eta.\\end{aligned}$$\n", "\n", "On observe, par propri\u00e9t\u00e9 de la pseudo-inverse, que la premi\u00e8re contribution est la projection orthogonale du vrai mod\u00e8le sur l'espace des vecteurs ligne de $X$. Il est donc une combinaison lin\u00e9aire des vecteurs que l'on voit pendant l'apprentissage ! La deuxi\u00e8me contribution est l'effet du bruit sur la solution optimale. Nous discuterons plus loin de ces contributions et d'effets \u00e9tranges qui peuvent se produire notament quand la matrice $X$ est mal conditionn\u00e9e (le ratio entre la plus grande valeur propre de $X^TX$ et sa plus petite valeur propre est tr\u00e8s grand).\n", "\n", "\n\n ----", "\n", "\n", "**Dans le cas non standard (i.e. sous-d\u00e9termin\u00e9)** o\u00f9 certaines variables peuvent \u00eatre des combinaisons lin\u00e9aires d'autres variables (inutile en pratique) ou si le nombre d'\u00e9chantillons est inf\u00e9rieur \u00e0 la dimension, $X^TX$ n'est plus inversible. Dans ce cas de figure, il existe une infinit\u00e9 de solutions aux \u00e9quations normales (i.e. une infinit\u00e9 de minimiseurs). Dans ce cas, la solution de norme minimale est donn\u00e9e par&nbsp;:\n", "\n", "$$\\hat{\\beta}=X^\\dagger y = X^T(XX^T)^{-1}y.$$\n", "\n", "**Concernant le cas g\u00e9n\u00e9ral (i.e. sous et sur-d\u00e9termin\u00e9),.** E. H. Moore (1920), A. Bjerhammar (1951) et R. Penrose (1955) proposent ind\u00e9pendamment une expression g\u00e9n\u00e9rale de $X^\\dagger$ appel\u00e9e pseudo-inverse de Moore-Penrose et calculable \u00e0 partir d'une d\u00e9composition en valeur singuli\u00e8re, not\u00e9e $X^\\dagger$. Celle-ci co\u00efncide bien s\u00fbr avec l'expression standard lorsqu'elle existe. On obtient donc une expression analytique g\u00e9n\u00e9rale, solution des \u00e9quations normales&nbsp;:\n", "\n", "\n", "$$\\hat{\\boldsymbol{\\beta}}=X^\\dagger\\boldsymbol{y},$$\n", "\n", "\n", "o\u00f9 $X^\\dagger$ est le pseudo-inverse de Moore-Penrose. Celle-ci est obtenue par une d\u00e9composition en valeurs singuli\u00e8res $X=U\\Sigma V^T$ de la mani\u00e8re suivante&nbsp;: $X^\\dagger = V\\Sigma^{-1}U^T$ o\u00f9 $\\Sigma^{-1}$ est la matrice $\\Sigma$ o\u00f9 nous avons invers\u00e9s les valeurs singuli\u00e8res non nulles. La s\u00e9quence de cours \"Les moindres carr\u00e9s via une d\u00e9composition QR (et plus)\" d\u00e9taille cela.\n", "\n", "\n", "**<span style='color:green'> Un syst\u00e8me d'\u00e9quations</span>** ", "\n", "En r\u00e9alit\u00e9, minimiser les moindres carr\u00e9s revient \u00e0 r\u00e9soudre un syst\u00e8me d'\u00e9quation&nbsp;:\n", "\n", "$$Ax=y\\text{ ou }Ax\\approx y.$$\n", "\n", "Si le probl\u00e8me poss\u00e8de autant d'\u00e9quation que d'inconnues, alors le probl\u00e8me est bien pos\u00e9 et admet une solution. Si le nombre d'\u00e9quations (lin\u00e9airement ind\u00e9pendantes) est sup\u00e9rieur au nombre d'inconnues, alors le probl\u00e8me est sur-d\u00e9termin\u00e9 et on ne peut r\u00e9soudre que $Ax\\approx y$. Enfin, si le nombre d'\u00e9quations est inf\u00e9rieur au nombre d'inconnues, alors il existe une infinit\u00e9 de solutions et on choisira celle de norme minimale.\n", "\n", "\n\n ----", "\n", "***Quelques pr\u00e9cisions d'alg\u00e8bre***&nbsp; : finalement, quel est le lien entre une pseudo-inverse et l'inverse classique. Soit une application lin\u00e9aire $A:\\mathbb{R}^n\\mapsto\\mathbb{R}^n$ repr\u00e9sent\u00e9e par une matrice $A\\in\\mathbb{R}^{n\\times n}$. On appelle inverse de $A$ l'unique matrice, not\u00e9e $A^{-1}$, telle que  $A^{-1}A=\\text{Id}$. Dans le cas inversible, l'inverse de $A^{-1}$ est donc de mani\u00e8re \u00e9vidente $A$. Cela revient \u00e0 transformer un vecteur $x\\in\\mathbb{R}^n$ par $A$ puis \u00e0 annuler sa transformation par $A^{-1}$. L'inverse n'existe cependant pas toujours. Ainsi, par exemple, si $\\text{ker}(A)\\neq \\{\\boldsymbol{0}\\}$ (i.e. le noyau ne se r\u00e9sume pas \u00e0 l'\u00e9l\u00e9ment nul, nous avons $\\forall x\\in\\mathbb{R}^n,\\ u\\in\\text{ker}(A)$ que $A(x+u)=Ax$. Finalement l'inverse de $Ax$ est-il $x$ ou $x+u$ ?\n", "\n", "Reprenons le cas de la pseudo-inverse. Quelques propri\u00e9t\u00e9s qui peuvent sembler \u00e9videntes \u00e9mergent&nbsp;:\n", "\n", "\n", "$$\\begin{align*}\n", "AA^\\dagger A&=A\\text{ (appliquer }A\\text{, son inverse }A^\\dagger\\text{ puis }A\\text{ \u00e0 nouveau revient \u00e0 appliquer }A\\text{)}\\\\\n", "A^\\dagger AA^\\dagger&=A^\\dagger\\text{ (c'est la m\u00eame chose du point de vu de l'inverse)}\\\\\n", "(AA^\\dagger)^T&=AA^\\dagger\\text{ (la transposition n'a pas d'effet)}\\\\\n", "(A^\\dagger A)^T&=A^\\dagger A\\text{ (m\u00eame chose que pr\u00e9c\u00e9demment du point de vu de l'inverse)}\n", "\\end{align*}$$\n", "\n", "La pseudo inverse est l'unique matrice $A^\\dagger$ satisfaisant les propri\u00e9t\u00e9s pr\u00e9c\u00e9dentes. Dans le cas o\u00f9 $A$ est inversible, on a alors $A^\\dagger=A^{-1}$. Intuitivement, l'id\u00e9e est de ne consid\u00e9rer \"que\" les \u00e9l\u00e9ments qui ne sont pas dans le noyaux. Ainsi $\\text{Im}(A)=\\text{Ker}(A^\\dagger)^\\perp$ et inversement.\n", "\n", "**<span style='color:blue'> Exercice</span>** ", "\n", "**V\u00e9rifier que les deux inverses propos\u00e9es dans les cas sur-d\u00e9termin\u00e9 et sous-d\u00e9termin\u00e9 v\u00e9rifient bien les \u00e9galit\u00e9s pr\u00e9c\u00e9dentes.**\n", "\n", "\n\n ----"]}, {"cell_type": "markdown", "metadata": {}, "source": ["La question \u00e0 laquelle nous pouvons maintenant essayer de r\u00e9pondre est en quoi une pseudo-inverse permet d'obtenir une solution acceptable au probl\u00e8me des moindres carr\u00e9s. Id\u00e9alement, nous aurions&nbsp;:\n", "\n", "$$Ax = y \\text{ g\u00e9n\u00e9ralement not\u00e9 }X\\beta=y.$$\n", "\n", "En pratique, il n'existe pas n\u00e9cessairement de vecteur $x$ tel que cette \u00e9galit\u00e9 soit satisfaite. Dit autrement, $y$ n'est pas dans l'image de $A$, not\u00e9e $\\text{Im}(A)$. Nous cherchons ainsi \u00e0 trouver $x$ tel que $\\lVert Ax-y\\rVert_2$ est minimis\u00e9. Notons $K(A)$ le noyau de $A$. Consid\u00e9rons maintenant la proposition suivante&nbsp;:\n", "\n", "**<span style='color:blue'> Proposition</span>** ", "\n", "$$\\text{proj}_{\\text{Im}(A)}=AA^\\dagger \\text{ (projection orthogonale sur l'image de }A\\text{)}$$\n", "\n", "$$\\text{proj}_{\\text{K}(A^T)}=I-AA^\\dagger$$\n", "\n", "$$\\text{proj}_{\\text{Im}(A^T)}=A^\\dagger A$$\n", "\n", "$$\\text{proj}_{\\text{K}(A)}=I-A^\\dagger A$$\n", "\n", "\n\n ----", "\n", "**<span style='color:orange'> Preuve</span>** ", "\n", "D\u00e9montrons la premi\u00e8re \u00e9galit\u00e9.\n", "\n", "Nous avons $(AA^\\dagger)^T=AA^\\dagger$ et notre application est donc sym\u00e9trique. De plus, \n", "\n", "$$(AA^\\dagger)(AA^\\dagger)=(AA^\\dagger A)A^\\dagger = AA^\\dagger$$\n", "\n", "et notre application est donc [idempotente](https://fr.wikipedia.org/wiki/Idempotence). L'idempotence implique que $\\text{proj}_{\\text{Im}(X)}$ est un projecteur (non n\u00e9cessairement orthogonal).\n", "\n", "Soit $y\\in\\text{Im}(A)$. Il existe donc $x$ tel que $y=Ax$. Nous avons donc&nbsp;:\n", "\n", "$$\\text{proj}_{\\text{Im}(A)}(y)=AA^\\dagger y=AA^\\dagger A x=Ax=y.$$\n", "\n", "Ainsi, si $y\\in\\text{Im}(A)$, le projecteur le projette sur lui-m\u00eame. Consid\u00e9rons maintenant $z\\in \\text{Im}(A)^\\perp$. Dit autrement, $(Ax)^Tz=0=x^TA^Tz,\\ \\forall x$ impliquant $A^Tz=0$.\n", "Nous avons donc&nbsp;:\n", "\n", "$$\\text{proj}_{\\text{Im}(A)}(z)=AA^\\dagger z = (AA^\\dagger)^T z = (A^\\dagger)^TA^Tz = 0.$$\n", "\n", "Tous les \u00e9l\u00e9ments orthogonaux \u00e0 l'image de $A$ sont projet\u00e9s sur $0$. Nous avons donc bien un projecteur orthogonal.\n", "\n\n ----", "\n", "Cela nous permet de r\u00e9diger la proposition suivante&nbsp;:\n", "\n", "**<span style='color:blue'> Proposition</span>** ", "\n", "La solution de norme minimale de $\\lVert Ax-y\\rVert_2$ est donn\u00e9e par&nbsp;:\n", "    \n", "$$x=A^\\dagger y$$\n", "\n", "\n\n ----", "\n", "**<span style='color:orange'> Preuve</span>** ", "\n", "Comme nous l'avons vu, en pratique l'\u00e9quation&nbsp;:\n", "\n", "$$Ax= y$$\n", "\n", "n'admet pas de solution et nous devons consid\u00e9rer celle qui minimise l'erreur au carr\u00e9 (i.e. la norme euclidienne). Le vecteur de r\u00e9sidus qui minimisera l'erreur est celui qui sera orthogonal \u00e0 l'image de $A$ (la plus courte distance, Pythagore tout \u00e7a). Dit autrement, on cherche \u00e0 r\u00e9soudre le syst\u00e8me d'\u00e9quations suivant&nbsp;:\n", "\n", "$$Ax=\\text{proj}_{\\text{Im}(A)}(y)=AA^\\dagger y.$$\n", "\n", "Si $y$ est d\u00e9j\u00e0 dans l'image et qu'une solution existe, alors c'est cela ne change rien puisque $\\text{proj}_{\\text{Im}(A)}(y)=y$, sinon on cherche la solution la plus proche. Nous avons donc&nbsp;:\n", "\n", "$$Ax=AA^\\dagger y\\Leftrightarrow A(x-A^\\dagger y)=0$$\n", "\n", "On constate ainsi que $(x-A^\\dagger y)\\in K(A)$. N'importe quel \u00e9l\u00e9ment $x$ tel que $x-A^\\dagger y$ est dans le noyau de $A$ satisfait l'\u00e9quation ci-dessus&nbsp;:\n", "\n", "$$x-A^\\dagger y = \\text{proj}_{\\text{K}(A)}(w)=(I-A^\\dagger A)w$$\n", "\n", "o\u00f9 $w$ est choisi arbitrairement. Une solution g\u00e9n\u00e9rale pour $x$ est donc&nbsp;:\n", "\n", "$$x=A^\\dagger y + (I-A^\\dagger A)w,$$\n", "\n", "o\u00f9 on remarque que les deux termes sont des vecteurs orthogonaux. Ainsi, si on privil\u00e9gie le vecteur de norme minimale, nous obtenons $x=A^\\dagger y$ et la pseudo-inverse est une m\u00e9thode acceptable pour d\u00e9terminer la solution des moindres carr\u00e9s.\n", "\n\n ----"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### F. \u00c0 vous de jouer"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Calculez la solution du probl\u00e8me de r\u00e9gression lin\u00e9aire en utilisant la pseudo-inverse de Moore-Penrose propos\u00e9e par $\\texttt{numpy}$ via $\\texttt{np.linalg.pinv}$.**\n", "\n\n ----"]}, {"cell_type": "code", "metadata": {}, "source": ["matplotlib.rcParams['figure.figsize'] = (12.0, 8.0)\n", "# descente de gradient\n", "\n", "# descente de gradient sans stochasticite\n", "param_trace , loss_trace = gd.optimize(learning_rate = 0.04, nb_iterations=50) \n", "\n", "####### Complete this part ######## or die ####################\n", "# solution par pseudo inverse\n", "...\n", "...\n", "###############################################################\n", "\n", "print('La loss pour la solution par pseudo-inverse est', l.val(beta_pinv))\n", "print('La loss pour la solution obtenue par descente de gradient est', loss_trace[-1])\n", "\n", "plot(X, y, beta_pinv)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["**<span style='color:blue'> Remarques et question</span>** ", "**On remarque ici que la valeur de la loss atteinte par GD est plus haute que celle atteinte par la solution de la pseudo-inverse. A votre avis pourquoi ? Augmentez le nombre d'it\u00e9rations de GD. Que constatez vous ? Est-ce \u00e9tonnant par rapport \u00e0 votre brillante d\u00e9monstration sur GD dans la section pr\u00e9c\u00e9dente ? Au passage, on pourrait s'amuser \u00e0 montrer qu'avec l'initialisation $\\beta=\\boldsymbol{0}$, chaque step reste bien dans l'espace engendr\u00e9 par les vecteurs lignes de X. Qui veut passer au tableau ?**\n", "\n", "\n\n ----"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### G. Avec sklearn"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Proposez une r\u00e9gression lin\u00e9aire sur le m\u00eame probl\u00e8me en utilisant $\\texttt{sklearn}$.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "metadata": {}, "source": ["from sklearn.linear_model import LinearRegression\n", "\n", "####### Complete this part ######## or die ####################\n", "...\n", "...\n", "###############################################################\n", "coef = list(model.coef_)\n", "coef.insert(0, model.intercept_)\n", "\n", "print('La loss pour la solution obtenue par Sklearn est', l.val(coef))\n", "\n", "plot(X, y, coef)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## IV. Features - Variables explicatives transform\u00e9es"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Dans beaucoup de probl\u00e8mes r\u00e9els, la variable \u00e0 expliquer n'est pas une simple combinaison lin\u00e9aire des variables explicatives. Cela peut-\u00eatre une d\u00e9pendence non lin\u00e9aire (e.g. quadratique), ou des d\u00e9pendences crois\u00e9es entre nos variables explicatives. La strat\u00e9gie permettant d'aborder cette probl\u00e9matique consiste \u00e0 transformer notre vecteur $\\boldsymbol{x}$ en rajoutant par exemple des transformations quadratiques et \u00e0 optimiser notre mod\u00e8le lin\u00e9aire sur le vecteur transform\u00e9. Afin de simplifier les notations, nous allons volontairement omettre le biais $\\beta_0$ de nos notations.\n", "\n", "Construire nos *features* consiste \u00e0 chercher une fonction $\\phi:\\mathbb{R}^d\\mapsto\\mathbb{R}^p$ qui transforme non-lin\u00e9airement nos variables explicatives initiales.\n", "\n", "\n", "Le probl\u00e8me se reformule ainsi de la mani\u00e8re suivante :\n", "\n", "\n", "$$\\hat{y}=\\langle \\phi(\\boldsymbol{x}), \\boldsymbol{\\beta}\\rangle$$\n", "\n", "\n", "Il suffit donc de transformer nos variables explicatives par $\\phi$ et de consid\u00e9rer le r\u00e9sultat comme nos nouvelles variables explicatives."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### A. Construction du jeu de donn\u00e9es polynomial"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "\n", "# vrais parametres\n", "beta_cube = np.random.uniform(-4, 4, size=(4,1))\n", "\n", "def sample_data_cube(n, sigma=1):\n", "    X = np.random.normal(0, sigma, size=n)\n", "    noise = np.random.normal(1, 1, size=n)/2\n", "    y = beta_cube[3] * X ** 3 + beta_cube[2] * X ** 2 + beta_cube[1] * X + beta_cube[0] + noise\n", "    return X, y\n", "X, y = sample_data_cube(6)\n", "X_test, y_test = sample_data_cube(150)\n", "\n", "#affichage du polynome\n", "plot(X_test, y_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### B. Solution par pseudo-inverse"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "**Compl\u00e9tez le code ci-dessous en utilisant une solution par pseudo-inverse via $\\texttt{numpy}$.**\n", "\n\n ----"]}, {"cell_type": "code", "metadata": {}, "source": ["class Polynomial(object):\n", "    def __init__(self, deg):\n", "        self.deg = deg\n", "\n", "    def _transform(self, X):\n", "        # here we transform the input into a polynomial\n", "        ####### Complete this part ######## or die ####################\n", "        ...\n", "        ...\n", "        ...\n", "        return ...\n", "        ###############################################################\n", "    \n", "    def fit(self, X, y):\n", "        ####### Complete this part ######## or die ####################\n", "        ...\n", "        ...\n", "        ###############################################################\n", "        \n", "    def predict(self, X):\n", "        if self.beta is None:\n", "            print('You must fit the model first')\n", "        else:\n", "            X_transformed = self._transform(X)\n", "            return np.dot(X_transformed, self.beta)\n", "    def score(self, X, y):\n", "        prediction = self.predict(X)\n", "        errors = (prediction - y) **2\n", "        return errors.sum()/errors.shape[0]\n", "    \n", "# vraie solution\n", "real_model = Polynomial(deg=3)\n", "real_model.beta = beta_cube\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Le plot est affich\u00e9 avec un jeu dit de test. Il s'agit d'un ensemble de points qui n'ont pas \u00e9t\u00e9 utilis\u00e9s lors de notre apprentissage (par pseudo-inverse). Le jeu d'apprentissaget et de test n'ont souvent pas la m\u00eame taille.**\n", "\n", "**Jouez avec le degr\u00e9 du polyn\u00f4me que vous manipulez et observez le r\u00e9sultat. Que constatez-vous ?**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "metadata": {}, "source": ["deg = 6\n", "model = Polynomial(deg)\n", "\n", "model.fit(X, y)\n", "\n", "plot(X_test, y_test, func=[(model.predict, 'Our model'), (real_model.predict, 'Real model')])\n", "\n", "print('Empirical risk: ', model.score(X, y))\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["**<span style='color:blue'> Remarque et question</span>** ", "\n", "**Le risque empirique est celui calcul\u00e9 directement sur les donn\u00e9es utilis\u00e9es lors du calcul du pseudo-inverse. Que constatez-vous par rapport \u00e0 ce dernier lorsque vous jouez avec le degr\u00e9 du polyn\u00f4me ?**\n", "\n", "**Est-il un bon indicateur du v\u00e9ritable risque de g\u00e9n\u00e9ralisation ? Autrement dit, est-il un bon indicateur de la qualit\u00e9 du polyn\u00f4me obtenu.**\n", "\n", "\n\n ----"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### C. Solution Sklearn"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "**Proposez la m\u00eame solution polynomiale via $\\texttt{sklearn}$. Choisissez le m\u00eame degr\u00e9 qu'utilis\u00e9 au-dessus et comparez les r\u00e9sultats.**\n", "\n\n ----"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import PolynomialFeatures\n", "from sklearn.linear_model import LinearRegression\n", "from sklearn.pipeline import make_pipeline"]}, {"cell_type": "code", "metadata": {}, "source": ["####### Complete this part ######## or die ####################\n", "...\n", "...\n", "###############################################################\n", "\n", "plot(X_test, y_test, func=[(model.predict, 'sklearn'), (real_model.predict, 'Real model')])\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## V. La validation crois\u00e9e et le dilemme biais-variance"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Vous avez pu constater qu'en fonction du degr\u00e9 du polyn\u00f4me choisi dans l'exercice pr\u00e9c\u00e9dent, le mod\u00e8le obtenu \u00e9tait plus ou moins loin de la solution id\u00e9ale. De plus, le risque empirique s'est montr\u00e9 \u00eatre un pi\u00e8tre estimateur de la qualit\u00e9 de notre solution estim\u00e9e.\n", "\n", "En r\u00e9alit\u00e9, le risque empirique est un estimateur sans biais du risque de g\u00e9n\u00e9ralisation pour un vecteur de param\u00e8tres quelconque. Ce n'est plus vrai si on choisit la solution estim\u00e9e via notre optimisation. Dit autrement&nbsp;:\n", "\n", "$$R(\\boldsymbol{\\beta})=\\mathbb{E}_{\\mathcal{S}}\\Big[J(\\boldsymbol{\\beta})\\Big],\\text{ }\\boldsymbol{\\beta}\\text{ quelconque, et }R(\\text{argmin}_{\\boldsymbol{\\beta}}J(\\boldsymbol{\\beta}))\\neq \\mathbb{E}_{\\mathcal{S}}\\Big[\\text{argmin}_{\\boldsymbol{\\beta}}J(\\boldsymbol{\\beta})\\Big]$$\n", "\n", "---\n", "\n", "Nous pouvons d\u00e9composer l'erreur attendue en briques \u00e9l\u00e9mentaires qui nous permettront d'en saisir l'origine. Reprenons l'erreur suivante&nbsp;:\n", "\n", "$$R(\\boldsymbol{\\hat{\\beta}})=\\mathbb{E}\\big[J(\\boldsymbol{\\hat{\\beta}})\\big]=\\mathbb{E}\\big[(y-\\hat{f}(x))^2\\big],$$\n", "\n", "o\u00f9 $y=f(x)+\\epsilon$, $\\epsilon\\sim \\mathcal{N}(0, \\sigma^2)$ avec $f$ la \"vraie\" fonction et $\\hat{f}$ notre estimateur de celle-ci. Nous avons ainsi&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "\\mathbb{E}\\big[(y-\\hat{f}(x))^2\\big]&=\\mathbb{E}\\big[y^2\\big]-2\\mathbb{E}\\big[y\\hat{f}(x)\\big]+\\mathbb{E}\\big[\\hat{f}(x)^2\\big].\n", "\\end{aligned}$$\n", "\n", "Remarquons tout d'abord&nbsp;:\n", "\n", "$$\\mathbb{E}\\big[\\hat{f}(x)^2\\big]=\\text{Var}\\big(\\hat{f}(x)\\big)+\\mathbb{E}\\big[\\hat{f}(x)\\big]^2,$$\n", "\n", "ainsi que&nbsp;:\n", "\n", "$$\\mathbb{E}\\big[y^2\\big]=\\text{Var}\\big(y\\big)+\\mathbb{E}\\big[y\\big]^2,$$\n", "\n", "o\u00f9&nbsp;:\n", "\n", "$$\\mathbb{E}\\big[y\\big]=\\mathbb{E}\\big[f(x)+\\epsilon\\big]=\\mathbb{E}\\big[f(x)\\big]=f(x),$$\n", "\n", "et enfin&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "\\mathbb{E}\\big[y\\hat{f}(x)\\big]&=\\mathbb{E}\\big[(f(x)+\\epsilon)\\hat{f}(x)\\big]\\\\\n", "&=\\mathbb{E}\\big[f(x)\\hat{f}(x)\\big]+\\mathbb{E}\\big[\\epsilon\\hat{f}(x)\\big]\\\\\n", "&=f(x)\\mathbb{E}\\big[\\hat{f}(x)\\big]+\\mathbb{E}\\big[\\epsilon\\big]\\mathbb{E}\\big[\\hat{f}(x)\\big]\\\\\n", "&=f(x)\\mathbb{E}\\big[\\hat{f}(x)\\big].\n", "\\end{aligned}$$\n", "\n", "En regroupant tout ensemble, nous avons donc&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "\\mathbb{E}\\big[(y-\\hat{f}(x))^2\\big]&=\\text{Var}\\big(y\\big)+f(x)^2-2f(x)\\mathbb{E}\\big[\\hat{f}(x)\\big]+\\text{Var}\\big(\\hat{f}(x)\\big)+\\mathbb{E}\\big[\\hat{f}(x)\\big]^2\\\\\n", "&=\\sigma^2+\\text{Var}\\big(\\hat{f}(x)\\big)+\\mathbb{E}\\big[f(x)-\\hat{f}(x)\\big]^2\\\\\n", "&=\\sigma^2+\\text{Var}\\big(\\hat{f}\\big)+\\text{bias}(\\hat{f})^2.\\end{aligned}$$\n", "\n", "On observe donc que l'erreur de notre mod\u00e8le se d\u00e9compose th\u00e9oriquement en trois \u00e9l\u00e9ments&nbsp;:\n", "\n", "* **Erreur irr\u00e9ductible :** $\\sigma^2$. Quand bien m\u00eame nous conna\u00eetrions le v\u00e9ritable processus g\u00e9n\u00e9rateur de nos donn\u00e9es, notre erreur quadratique serait toujours de $\\sigma^2$,\n", "* **Variance :** $\\text{Var}\\big(\\hat{f}\\big)$. Elle d\u00e9crit la fluctuation de notre apprentissage autour de son esp\u00e9rance,\n", "* **Biais (au carr\u00e9) :** $\\text{bias}(\\hat{f})^2$. Il nous donne l'\u00e9cart en esp\u00e9rance entre notre estimateur et le v\u00e9ritable processus g\u00e9n\u00e9rateur.\n", "\n", "Toute la difficult\u00e9 vient du fait qu'il est tr\u00e8s facile de r\u00e9duire le biais en augmentant la complexit\u00e9 de notre mod\u00e8le entra\u00eenant par la m\u00eame une augmentation de la variance...\n", "\n", "---\n", "\n", "Cela implique de mettre en place une proc\u00e9dure exp\u00e9rimentale permettant d'\u00e9valuer la qualit\u00e9 de notre mod\u00e8le."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### A. Construction du jeu de donn\u00e9es"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "\n", "beta_cube = np.random.uniform(-4, 4, size=(4,1))\n", "\n", "def sample_data_cube(n, sigma=1):\n", "    X = np.random.normal(0, sigma, size=n)\n", "    noise = np.random.normal(1, 1, size=n)/2\n", "    y = beta_cube[3] * X ** 3 + beta_cube[2] * X ** 2 + beta_cube[1] * X + beta_cube[0] + noise\n", "    return X, y\n", "X, y = sample_data_cube(50)\n", "X_test, y_test = sample_data_cube(150)\n", "\n", "X = X.reshape((X.shape[0], 1))\n", "X_test = X_test.reshape((X_test.shape[0], 1))\n", "\n", "plot(X_test, y_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### B. Optimiser une fonction est-il suffisant pour parler d'apprentissage ?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Il existe deux strat\u00e9gies d'\u00e9valuation sans biais de la qualit\u00e9 de notre mod\u00e8le :\n", "* La validation non crois\u00e9e o\u00f9 une partie de notre jeu de donn\u00e9e est cach\u00e9e pendant l'apprentissage puis utilis\u00e9e afin d'\u00e9valuer les performances du mod\u00e8le. Il s'agit du d\u00e9coupage train/test. Cette strat\u00e9gie est un estimateur sans biais de la qualit\u00e9 de notre mod\u00e8le mais poss\u00e8de une variance plus forte que la validation crois\u00e9e. Elle peut-\u00eatre particuli\u00e8rement utile lorsque le co\u00fbt d'apprentissage d'un mod\u00e8le est tr\u00e8s \u00e9lev\u00e9 (e.g. *deep learning*)\n", "* La validation crois\u00e9e o\u00f9 notre jeu de donn\u00e9es est divis\u00e9 en *k* parties (on parle aussi de *k-fold*). \u00c9videmment, $k\\in\\{2, ..., n\\}$ o\u00f9 $n$ est la taille du jeu de donn\u00e9es. Chacune des parties jouera successivement le r\u00f4le de jeu de test pendant que les $k-1$ autres parties serviront \u00e0 calculer notre mod\u00e8le. Le r\u00e9sultat de cette proc\u00e9dure est un vecteur de $k$ scores dont on peut calculer la moyenne, la variance, etc.\n", "\n", "On peut illustrer la m\u00e9thode des *k-folds* via l'exemple suivant :\n", "\n", "$$\\begin{align}\n", "\\text{Appartient au train set: } \\color{red}{\\boxed{}}&\\text{ et appartient au test set: }\\color{green}{\\boxed{}}\n", "\\end{align}$$\n", "\n", "$$\\begin{align}\n", "\\text{Step 1: }\\color{green}{\\boxed{}}\\color{red}{\\boxed{}}&\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\n", "\\end{align}$$\n", "\n", "$$\\begin{align}\n", "\\text{Step 2: }\\color{red}{\\boxed{}}\\color{green}{\\boxed{}}&\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\n", "\\end{align}$$\n", "\n", "$$\\begin{align}\n", "\\text{Step 3: }\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}&\\color{green}{\\boxed{}}\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\n", "\\end{align}$$\n", "\n", "$$\\begin{align}\n", "\\text{Step 4: }\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}&\\color{red}{\\boxed{}}\\color{green}{\\boxed{}}\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\n", "\\end{align}$$\n", "\n", "$$\\begin{align}\n", "\\text{Step 5: }\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}&\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\\color{green}{\\boxed{}}\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\n", "\\end{align}$$\n", "\n", "$$\\begin{align}\n", "\\text{Step 6: }\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}&\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\\color{green}{\\boxed{}}\\color{red}{\\boxed{}}\n", "\\end{align}$$\n", "\n", "$$\\begin{align}\n", "\\text{Step 7: }\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}&\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\\color{red}{\\boxed{}}\\color{green}{\\boxed{}}\n", "\\end{align}$$\n", "\n", "La m\u00e9thode $\\texttt{cross_val_score}$ de $\\texttt{sklearn}$ permet de r\u00e9aliser cette proc\u00e9dure. On pourra renseigner le param\u00e8tre $\\texttt{cv}$ qui indique le nombre $k$ et le param\u00e8tre $\\texttt{scoring}$ qui donne la m\u00e9trique que l'on souhaite calculer."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "**Proposez un *5-fold* avec la m\u00e9trique $R^2$ que vous appliquerez \u00e0 une r\u00e9gression polynomiale de degr\u00e9 $5$.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "metadata": {}, "source": ["from sklearn.model_selection import cross_val_score\n", "from sklearn.preprocessing import PolynomialFeatures\n", "from sklearn.linear_model import LinearRegression\n", "from sklearn.pipeline import make_pipeline\n", "\n", "####### Complete this part ######## or die ####################\n", "...\n", "...\n", "###############################################################\n", "\n", "print('Le score R2 sur chacun des splits de notre k-fold:', scores)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "**Comparez le score obtenu lors de votre validation crois\u00e9e \u00e0 un plot de la fonction estim\u00e9e sur tout le jeu d'apprentissage.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "metadata": {}, "source": ["####### Complete this part ######## or die ####################\n", "...\n", "###############################################################\n", "plot(X_test, y_test, func=model.predict)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## VI. L'effet \"double descente\" (Bonus ?)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Comme vu pr\u00e9c\u00e9demment, l'effet du bruit sur l'estimateur d\u00e9pend du conditionnement de $X^TX$. Le conditionnement d'une matrice $A$ inversible est donn\u00e9 par :\n", "\n", "\n", "$$C(A)=\\lVert A^{-1}\\rVert\\lvert A\\rVert$$\n", "\n", "\n", "Il est \u00e9vident que si $A\\in{\\mathbb{R}^{1\\times 1}}^\\star$, alors $C(A)=1$. Ce n'est absolument pas vrai dans le cas g\u00e9n\u00e9ral.\n", "\n", "L'exemple ci-dessous illustre cela via la norme de Frobenius (norme Euclidienne appliqu\u00e9e \u00e0 une matrice, $\\text{Tr}(A^TA)^{0.5}$). On pr\u00e9f\u00e8rera souvent la norme d'op\u00e9rateur qui quantifie les effets d'amplification d'un vecteur $\\boldsymbol{x}$ lorsqu'on calcule $A\\boldsymbol{x}$. Cette norme d'op\u00e9rateur est directement li\u00e9e aux valeurs propres. Cependant, la norme de Frobenius est plus simple \u00e0 calculer."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["A = np.diag([1, 0.0001])\n", "print('A=\\n', A)\n", "print('C(A)=A^{-1}A=' + str(np.linalg.norm(np.linalg.inv(A))*np.linalg.norm(A)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["On remarque dans l'exemple que la matrice $A$ poss\u00e8de une toute petite valeur propre qui est responsable de cet \u00e9cart. L'exercice ci-dessous montre qu'au-del\u00e0 des consid\u00e9rations th\u00e9oriques, cela a des r\u00e9percussions importantes et totalement inattendues en r\u00e9alit\u00e9.\n", "\n", "Les simulations suivantes permettent de mettre en lumi\u00e8re cela. Elles sont construites comme d\u00e9crit ci-dessous :\n", "\n", "\n", "$$\\beta\\sim\\mathbb{U}(-2, 2)^d,\\ d\\in\\mathbb{N}^\\star$$\n", "\n", "\n", "dit autrement, on fixe un vecteur de param\u00e8tres selon une loi uniforme qui d\u00e9pend de la dimension du probl\u00e8me.\n", "Nous avons ensuite :\n", "\n", "\n", "$$x\\sim\\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{I_d}) + \\epsilon,\\ \\epsilon\\sim\\mathcal{N}(0, \\sigma^2)$$\n", "\n", "\n", "On construit ensuite un jeu de test de taille $500$ et un jeu d'apprentissage de taille variable. L'objectif ici sera d'\u00e9tudi\u00e9 l'effet de la taille du jeu d'apprentissage sur la qualit\u00e9 de notre mod\u00e8le, qualit\u00e9 que l'on aura calcul\u00e9e sur le test. Pour chaque taille de jeu de donn\u00e9es, l'exp\u00e9rience est r\u00e9p\u00e9t\u00e9e $50$ fois ($\\texttt{redo}$) afin de lisser les courbes obtenues.\n", "\n", "**<span style='color:blue'> Exercice</span>** ", "\n", "\n", "**Ex\u00e9cutez une premi\u00e8re fois le code puis jouez avec $\\texttt{noise}$ (i.e. $\\sigma$) afin de voir ce qui se passe selon la quantit\u00e9 de bruit. Essayez de d\u00e9crire rigoureusement ce que vous observez.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "\n", "####### Play with the noise #########\n", "noise = 1.\n", "#####################################\n", "\n", "d = 50\n", "redo = 50\n", "\n", "beta = np.random.uniform(-2, 2, size=(d, 1))\n", "\n", "mu = [0 for _ in range(d)]\n", "cov = np.diag([1 for _ in range(d)])\n", "\n", "test_size = 500\n", "X_test = np.random.multivariate_normal(mean=mu, cov=cov, size=test_size)\n", "y_test = np.dot(X_test, beta) + np.random.normal(0, 1, size=(test_size, 1)) * noise\n", "\n", "errors = []\n", "train_errors = []\n", "for m in range(1, 100):\n", "    error = 0\n", "    train_error = 0\n", "    for j in range(redo):\n", "        # dataset construction\n", "        X = np.random.multivariate_normal(mean=mu, cov=cov, size=m)\n", "        y = np.dot(X, beta) + np.random.normal(0, 1, size=(m, 1)) * noise\n", "        \n", "        # param estimation\n", "        beta_pinv = np.dot(np.linalg.pinv(X), y)\n", "        \n", "        # risk estimation\n", "        error += ((np.dot(X_test, beta_pinv)-y_test)**2).sum()/(test_size*redo)\n", "        train_error += ((np.dot(X, beta_pinv)-y)**2).sum()/(m*redo)\n", "    train_errors.append(train_error)\n", "    errors.append(error)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure()\n", "plt.plot([i for i in range(1, len(errors)+1)], train_errors, label=\"Train error\")\n", "plt.axvline(x=d, color='k', linewidth=2.0, linestyle='--', label='Dimension')\n", "plt.plot([i for i in range(1, len(errors)+1)], errors, label=\"Risk estimation\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure()\n", "plt.plot([i for i in range(1, len(errors)+1)], errors, label=\"Risk estimation\")\n", "plt.axvline(x=d, color='k', linewidth=2.0, linestyle='--', label='Dimension')\n", "plt.legend()\n", "plt.yscale('log')\n", "plt.show()\n", "plt.figure()\n", "plt.plot([i for i in range(1, len(train_errors)+1)], train_errors, label=\"Train error\")\n", "plt.axvline(x=d, color='k', linewidth=2.0, linestyle='--', label='Dimension')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["La ligne en pointill\u00e9 s\u00e9pare visuellement deux r\u00e9gimes diff\u00e9rents. La transition d'un r\u00e9gime \u00e0 l'autre se produit par une augmentation catastrophique de l'erreur de g\u00e9n\u00e9ralisation de notre mod\u00e8le.\n", "\n", "**<span style='color:blue'> Question</span>** ", "\n", "**Quelle particularit\u00e9 diff\u00e9rentie les deux phases ?**\n", "\n", "\n\n ----"]}, {"cell_type": "markdown", "metadata": {}, "source": ["En r\u00e9alit\u00e9, les m\u00e9thodes de *machine learning* traditionnelles se situent plut\u00f4t dans le r\u00e9gime de \"droite\". L'\u00e9tude de ce ph\u00e9nom\u00e8ne est pouss\u00e9e par les approches comme le *deep learning* qui sont souvent dans le r\u00e9gime de gauche. Comprendre ces ph\u00e9nom\u00e8nes nous permet par exemple d'\u00e9clairer les raisons du succ\u00e8s du *deep learning*."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## VII. R\u00e9gularisation"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Comme illustr\u00e9 par les quelques sc\u00e9narios pr\u00e9c\u00e9dents dont le cas catastrophique de la double descente, une certaine parcimonie est attendue par notre mod\u00e8le. On a pu notamment observer que les \"mauvaises\" fonctions du point de vue du risque de g\u00e9n\u00e9ralisation avaient une forte tendance \u00e0 osciller n'importe comment. Au lieu de laisser jouer le \"hasard\" (ou plut\u00f4t le conditionnement de $X^TX$), nous pouvons contraindre notre optimisation \u00e0 favoriser les solutions parcimonieuses ; c'est-\u00e0-dire des solutions qui n'oscillent pas n'importe comment.\n", "\n", "Intuitivement, on va choisir une solution qui minimise \u00e0 la fois le risque empirique $J(\\boldsymbol{\\beta})$, mais aussi une p\u00e9nalit\u00e9 sur la taille des \"oscillations\". En r\u00e9alit\u00e9, les oscillations sont directement contr\u00f4l\u00e9es par la norme des param\u00e8tres : un grand poids rendra notre mod\u00e8le tr\u00e8s sensible \u00e0 la moindre perturbation de la variable explicative associ\u00e9e.\n", "\n", "Nous parlons d'optimisation r\u00e9gularis\u00e9e lorsque la fonction \u00e0 optimiser s'\u00e9crit de la mani\u00e8re suivante :\n", "\n", "\n", "$$J(\\boldsymbol{\\beta})=\\frac{1}{m}\\sum_{i=1}^nr(f_{\\boldsymbol{\\beta}}(\\boldsymbol{x_i}), y_i)+\\lambda P(\\boldsymbol{\\beta})$$\n", "\n", "o\u00f9 $r:\\mathcal{Y}\\times\\mathcal{Y}\\mapsto \\mathbb{R}^+$ est notre risque \u00e9l\u00e9mentaire et $P:\\mathbb{R}^d\\mapsto\\mathbb{R}^+$ une p\u00e9nalit\u00e9 sur notre vecteur de param\u00e8tres. Plus pr\u00e9cis\u00e9ment, dans le cas de la r\u00e9gression lin\u00e9aire, nous avons :\n", "\n", "\n\n ----", "\n", "La r\u00e9gression lin\u00e9aire sans p\u00e9nalit\u00e9 s'obtient avec $\\texttt{LinearRegression}$. Celle avec p\u00e9nalit\u00e9 $\\ell_2$ s'obtient avec $\\texttt{Ridge}$, avec p\u00e9nalit\u00e9 $\\ell_1$ est $\\texttt{Lasso}$ et Elastic-Net $\\texttt{ElasticNet}$.\n", "\n", "\n\n ----", "\n", "\n", "$$r(\\hat{y}, y)=(\\hat{y}-y)^2$$\n", "\n", "\n", "et\n", "\n", "\n", "$$P(\\boldsymbol{\\beta})=\\lVert \\boldsymbol{\\beta} \\rVert,$$\n", "\n", "\n", "o\u00f9 $\\lVert \\cdot \\rVert$ est une norme quelconque. Les choix classiques sont la norme $\\ell_1$ :\n", "\n", "\n", "$$\\lVert \\boldsymbol{\\beta} \\rVert_1=\\sum_j |\\boldsymbol{\\beta}_j|$$\n", "\n", "\n", "et la norme $\\ell_2$ :\n", "\n", "\n", "$$\\lVert \\boldsymbol{\\beta} \\rVert_2 = \\sqrt{\\sum_j\\boldsymbol{\\beta}_j^2}=\\sqrt{\\boldsymbol{\\beta}^T\\boldsymbol{\\beta}}$$\n", "\n", "\n", "Une strat\u00e9gie interm\u00e9diaire consiste \u00e0 prendre la combinaison convexe des deux normes :\n", "\n", "\n", "$$P(\\boldsymbol{\\beta})=\\eta \\lVert \\boldsymbol{\\beta} \\rVert_1 + (1-\\eta) \\lVert \\boldsymbol{\\beta} \\rVert_2.$$\n", "\n", "\n", "avec $\\eta\\in\\big[0,1\\big]$. On parle alors d'*elastic-net*.\n", "\n", "Ces diff\u00e9rentes r\u00e9gularisations ne se comportent pas de la m\u00eame mani\u00e8re. Ainsi la r\u00e9gularisation $\\ell_1$, aussi appel\u00e9e Lasso, va forcer certains param\u00e8tres \u00e0 atteindre la valeur $0$. Cela permet par exemple de favoriser l'explicabilit\u00e9 de notre mod\u00e8le. En pratique, $\\ell_2$, appel\u00e9e Ridge, a tendance \u00e0 donner les meilleurs r\u00e9sultats d'un point de vue pr\u00e9dictif."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### A. Construction du jeu de donn\u00e9es"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "\n", "beta_cube = np.random.uniform(-4, 4, size=(4,1))\n", "\n", "def sample_data_cube(n, sigma=1):\n", "    X = np.random.normal(0, sigma, size=n)\n", "    noise = np.random.normal(1, 1, size=n)/2\n", "    y = beta_cube[3] * X ** 3 + beta_cube[2] * X ** 2 + beta_cube[1] * X + beta_cube[0] + noise\n", "    return X, y\n", "X, y = sample_data_cube(20)\n", "X_test, y_test = sample_data_cube(150)\n", "\n", "X = X.reshape((X.shape[0], 1))\n", "X_test = X_test.reshape((X_test.shape[0], 1))\n", "\n", "plot(X_test, y_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### B. Sans r\u00e9gularisation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import PolynomialFeatures\n", "from sklearn.linear_model import LinearRegression\n", "from sklearn.pipeline import make_pipeline"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = make_pipeline(PolynomialFeatures(10), LinearRegression())\n", "model.fit(X, y)\n", "\n", "plot(X_test, y_test, func=model.predict)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### C. Avec r\u00e9gularisation $\\ell_1$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Lorsqu'on parle de r\u00e9gresion lin\u00e9aire avec r\u00e9gularisation $\\ell_1$, on parle aussi de Lasso.\n", "\n", "**<span style='color:blue'> Exercice</span>** ", "**Testez plusieurs valeurs de $\\alpha$ ($=\\lambda$ dans notre texte). Quelle est la fonction la plus parcimonieuse que vous arrivez \u00e0 obtenir ?**\n", "\n\n ----"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.linear_model import Lasso"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = make_pipeline(PolynomialFeatures(10), Lasso(alpha=15.))\n", "model.fit(X, y)\n", "\n", "plot(X_test, y_test, func=model.predict)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('Les param\u00e8tres du mod\u00e8le sont sparses :\\n', model[1].coef_)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Vous avez du constater que selon la quantit\u00e9 de r\u00e9gularisation, les param\u00e8tres \u00e9taient plus ou moins sparse. Il se trouve qu'une fois qu'un param\u00e8tre est \u00e0 0, il le sera pour toutes les valeurs de $\\alpha$ supp\u00e9rieures. Afin d'observer visuellement, l'effet de la r\u00e9gularisation sur la sparsit\u00e9, il est possible d'afficher ce qu'on appelle les \"chemins Lasso\" ou Lasso paths."]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["hide-input"]}, "outputs": [], "source": ["from sklearn.linear_model import lars_path\n", "features = PolynomialFeatures(10)\n", "X_transformed = features.fit_transform(X)\n", "_, _, coefs = lars_path(X_transformed, y, method='lasso')\n", "\n", "xx = np.sum(np.abs(coefs.T), axis=1)\n", "xx /= xx[-1]\n", "\n", "plt.plot(xx, coefs.T)\n", "ymin, ymax = plt.ylim()\n", "# plt.vlines(xx, ymin, ymax, linestyle='dashed')\n", "plt.xlabel('|coef| / max|coef|')\n", "plt.ylabel('Coefficients')\n", "plt.title('LASSO Path')\n", "plt.axis('tight')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["A gauche se trouve le param\u00e8tre le plus parcimonieux (la plus grande valeur de $\\alpha$). Tous les param\u00e8tres y sont donc nuls. Plus la valeur de $\\alpha$ est r\u00e9duite, plus le nombre de param\u00e8tres diff\u00e9rents de $0$ augmente et leur valeur aussi."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### D. Avec r\u00e9gularisation $\\ell_2$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Lorsqu'on parle de r\u00e9gresion lin\u00e9aire avec r\u00e9gularisation $\\ell_2$, on parle aussi de Ridge.\n", "\n", "**<span style='color:blue'> Exercice</span>** ", "**Testez plusieurs valeurs de $\\alpha$ ($=\\lambda$ dans notre texte). Quelle est la fonction la plus parcimonieuse que vous arrivez \u00e0 obtenir ?**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.linear_model import Ridge"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = make_pipeline(PolynomialFeatures(10), Ridge(alpha=1000))\n", "model.fit(X, y)\n", "\n", "plot(X_test, y_test, func=model.predict)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### E. Avec r\u00e9gularisation *elastic-net*"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Lorsqu'on parle de r\u00e9gresion lin\u00e9aire avec r\u00e9gularisation *elastic-net*.\n", "\n", "**<span style='color:blue'> Exercice</span>** ", "\n", "**Testez plusieurs valeurs de $\\alpha$ ($=\\lambda$ dans notre texte). Quelle est la fonction la plus parcimonieuse que vous arrivez \u00e0 obtenir (Essayez de trouver la r\u00e9ponse en raisonnant) ?**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.linear_model import ElasticNet"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = make_pipeline(PolynomialFeatures(10), ElasticNet(alpha=20, l1_ratio=0.8))\n", "model.fit(X, y)\n", "\n", "plot(X_test, y_test, func=model.predict)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## VIII. Selection de mod\u00e8les"]}, {"cell_type": "markdown", "metadata": {}, "source": ["En pratique, nous ne pouvons pas choisir la valeur des param\u00e8tres (e.g. degr\u00e9, r\u00e9gularisation) \u00e0 l'oeil comme pr\u00e9c\u00e9demment. Il nous faut (1) un algorithme qui automatise cette t\u00e2che et (2) une strat\u00e9gie d'\u00e9valuation rigoureuse afin d'\u00e9viter les biais de confirmation (sur-apprentissage)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### A. Construction du jeu de donn\u00e9es"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "\n", "beta_cube = np.random.uniform(-4, 4, size=(4,1))\n", "\n", "def sample_data_cube(n, sigma=1):\n", "    X = np.random.normal(0, sigma, size=n)\n", "    noise = np.random.normal(1, 1, size=n)/2\n", "    y = beta_cube[3] * X ** 3 + beta_cube[2] * X ** 2 + beta_cube[1] * X + beta_cube[0] + noise\n", "    return X, y\n", "X, y = sample_data_cube(50)\n", "X_test, y_test = sample_data_cube(150)\n", "\n", "X = X.reshape((X.shape[0], 1))\n", "X_test = X_test.reshape((X_test.shape[0], 1))\n", "\n", "plot(X_test, y_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### B. Recherche exhaustive"]}, {"cell_type": "markdown", "metadata": {}, "source": ["L'algorithme de recherche par grille va exhaustivement test\u00e9 tous les param\u00e8tres donn\u00e9s. Pour chacun combinaison, une validation *k-fold* est r\u00e9alis\u00e9e. Le mod\u00e8le retenu sera celui qui aura maximis\u00e9 son score moyen lors du *k-fold*.\n", "\n", "**<span style='color:blue'> Exercice</span>** ", "\n", "**Utilisez l'objet $\\texttt{GridSearchCV}$ afin de trouver la meilleure combinaison de param\u00e8tres selon le dictionnaire d\u00e9crit ci-dessous. Toutes les combinaisons seront-elles r\u00e9ellement test\u00e9es ?**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import GridSearchCV\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.linear_model import ElasticNet, Ridge, LinearRegression, Lasso"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["param_grid = [\n", "  {'model': [LinearRegression()], \n", "   'poly__degree': [1, 2, 3, 4, 5]},\n", "  {'model': [Ridge()], \n", "   'poly__degree': [1, 2, 3, 4, 5], \n", "   'model__alpha': [0., 0.1, 0.2, 0.5, 0.8, 10., 100., 100.]},\n", "  {'model': [Lasso()], \n", "   'poly__degree': [1, 2, 3, 4, 5],\n", "   'model__alpha': [0., 0.1, 0.2, 0.5, 0.8]},\n", "  {'model': [ElasticNet()], 'poly__degree': [1, 2, 3, 4, 5],\n", "  'model__alpha': [0., 0.1, 0.2, 0.5, 0.8, 10., 100., 100.],\n", "  'model__l1_ratio': [0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.9, 1.]},\n", " ]"]}, {"cell_type": "code", "metadata": {}, "source": ["pipe = Pipeline(steps=[('poly', PolynomialFeatures(10)), ('model', LinearRegression())])\n", "####### Complete this part ######## or die ####################\n", "...\n", "...\n", "###############################################################\n", "\n", "print('Le meilleur modele est', search.best_estimator_)\n", "plot(X_test, y_test, func=search.best_estimator_.predict)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### C. Recherche al\u00e9atoire"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Une recherche exhaustive peut rapidement \u00eatre limitante. Imaginons que nous testions d\u00e9j\u00e0 $10000$ combinaisons de param\u00e8tres. Rajoutons maintenant un param\u00e8tre avec 50 modalit\u00e9s. Le nombre de combinaisons est donc multipli\u00e9 par $50$ et on monte \u00e0 $500000$ combinaisons. L'algorithme devient $50$ fois plus lent. \n", "\n", "Une strat\u00e9gie alternative est de s'appuyer sur le hasard. On peut sp\u00e9cifier a priori des distributions sur les param\u00e8tres en supposant que certaines combinaisons fourniront probablement plus de bons r\u00e9sultats que d'autres. Par d\u00e9faut, le tirage est uniforme. Cette m\u00e9thode n'est pas absurde car plusieurs combinaisons peuvent tr\u00e8s bien obtenir des r\u00e9sultats tr\u00e8s proches. L'approche al\u00e9atoire sera ainsi beaucoup plus efficaces que la recherche exhaustive pour des performances g\u00e9n\u00e9ralement assez proches.\n", "\n", "**<span style='color:blue'> Exercice</span>** ", "\n", "**Compl\u00e9tez le code ci-dessous afin de r\u00e9aliser une recherche randomis\u00e9e. Quel param\u00e8tre permet de jouer sur le nombre de tirages ?**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import RandomizedSearchCV"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["param_grid = [\n", "  {'model': [LinearRegression()], \n", "   'poly__degree': [d for d in range(1, 20)]},\n", "  {'model': [Ridge()], \n", "   'poly__degree': [d for d in range(1, 20)], \n", "   'model__alpha': [0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1., 10., 100., 100.]},\n", "  {'model': [Lasso()], \n", "   'poly__degree': [d for d in range(1, 20)],\n", "   'model__alpha': [0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.]},\n", "  {'model': [ElasticNet()], 'poly__degree': [d for d in range(1, 20)],\n", "  'model__alpha': [0.1, 0.2, 0.5, 0.8, 10., 100., 100.],\n", "  'model__l1_ratio': [0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.9, 1.]},\n", "]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pipe = Pipeline(steps=[('poly', PolynomialFeatures(10)), ('model', LinearRegression())])\n", "\n", "search = GridSearchCV(pipe, param_grid, n_jobs=-1)\n", "search.fit(X, y)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('Le meilleur modele est', search.best_estimator_)\n", "plot(X_test, y_test, func=search.best_estimator_.predict)"]}, {"cell_type": "code", "metadata": {}, "source": ["####### Complete this part ######## or die ####################\n", "...\n", "...\n", "###############################################################\n", "\n", "print('Le meilleur modele est', search.best_estimator_)\n", "plot(X_test, y_test, func=search.best_estimator_.predict)\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('Le meilleur modele est', search.best_estimator_)\n", "plot(X_test, y_test, func=search.best_estimator_.predict)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## IX. Le mot de la fin"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ce propos introductif nous a permis de toucher du doigt la notion de sur-apprentissage. Quand est-ce que le meilleur mod\u00e8le sur notre jeu d'apprentissage est suffisament bon en g\u00e9n\u00e9ral ? Quand peut-on consid\u00e9rer qu'un mod\u00e8le est suffisamment bon ? Aurions-nous pu trouver un meilleur mod\u00e8le avec une proc\u00e9dure d'apprentissage diff\u00e9rente ? "]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.2"}}, "nbformat": 4, "nbformat_minor": 4}
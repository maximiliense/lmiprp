
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Le SVM ou l‚Äôhypoth√®se max-margin ‚òïÔ∏è‚òïÔ∏è &#8212; Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Les m√©thodes ensemblistes" href="../5_ensembles/0_propos_liminaire.html" />
    <link rel="prev" title="Les m√©thodes √† noyaux" href="0_propos_liminaire.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-72WWYCKNK6"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-72WWYCKNK6');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../0_requisite/rappels_python.html">
   Rappels de Python et Numpy
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../1_what_is_ml/0_propos_liminaire.html">
   <em>
    Machine Learning
   </em>
   , initiation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/1_introduction_ml.html">
     <em>
      Machine learning
     </em>
     et mal√©diction de la dimension
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/2_regression_and_classification_trees.html">
     Les arbres de r√©gression et de classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../2_regression/0_propos_liminaire.html">
   La
   <em>
    r√©gression
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/1_linear_regression.html">
     La r√©gression lin√©aire ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/2_optimization.html">
     L‚Äôoptimisation ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/3_interpolation.html">
     Interpolation ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/4_algo_proximal_lasso.html">
     Sous-diff√©rentiel et le cas du Lasso ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/5_least_square_qr.html">
     Les moindres carr√©s via une d√©composition QR (et plus)‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/6_ridge.html">
     Une analyse de la r√©gularisation Ridge ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../3_classification/0_propos_liminaire.html">
   La
   <em>
    classification
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/1_logistic_regression.html">
     La r√©gression logistique ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/2_fonctions_proxy.html">
     Les fonctions de perte (loss function) ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/3_bayes_classifier.html">
     Le classifieur de Bayes ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/4_VC_theory.html">
     Un mod√®le formel de l‚Äôapprentissage ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è (üíÜ‚Äç‚ôÇÔ∏è)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="0_propos_liminaire.html">
   Les m√©thodes √† noyaux
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Le SVM ou l‚Äôhypoth√®se max-margin ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5_ensembles/0_propos_liminaire.html">
   Les m√©thodes
   <em>
    ensemblistes
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/1_ensembles.html">
     M√©thodes ensemblistes ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/2_bayesian_linear_regression.html">
     Bayesian linear regression ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../6_deeplearning/0_propos_liminaire.html">
   <em>
    deep learning
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/1_autodiff.html">
     La diff√©rentiation automatique et un d√©but de
     <em>
      deep learning
     </em>
     ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/2_filters_representation.html">
     Filtres et espace de repr√©sentation des r√©seaux de neurones ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/3_probabilities_calibration.html">
     Calibration des probabilit√©s et quelques notions ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/4_regularization_deep.html">
     R√©gularisation en
     <em>
      deep learning
     </em>
     ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/5_transfer_multitask.html">
     <em>
      Transfer learning
     </em>
     et apprentissage multi-t√¢ches ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/6_adversarial.html">
     Les attaques adversaires ‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../7_unsupervised/0_propos_liminaire.html">
   L‚Äôapprentissage non-supervis√©
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_unsupervised/1_principal_component_analysis.html">
     L‚ÄôAnalyse en Composantes Principales ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_unsupervised/2_em_gaussin_mixture_model.html">
     Mod√®le de M√©lange Gaussien et algorithme
     <em>
      Expectation-Maximization
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../8_set_prediction/0_propos_liminaire.html">
   Pr√©diction d‚Äôensembles
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/1_well_defined.html">
     Jeu d‚Äôapprentissage
     <em>
      set-valued
     </em>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/2_ill_defined.html">
     Jeu d‚Äôapprentissage uniquement multi-classes ‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../biblio.html">
   Bibliographie
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/4_kernel_methods/1_svm.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/maximiliense/lmiprp"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/maximiliense/lmiprp/issues/new?title=Issue%20on%20page%20%2F4_kernel_methods/1_svm.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/maximiliense/lmiprp/blob/master/Travaux Pratiques/Machine Learning/Book/4_kernel_methods/1_svm.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#i-introduction">
   I. Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ii-un-probleme-de-classification-lineaire">
   II. Un probl√®me de classification lin√©aire
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-le-primal">
     A. Le primal
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-le-dual-optionnel">
     B. Le dual (optionnel)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#c-via-une-surrogate-loss">
     C. Via une
     <em>
      surrogate loss
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iii-l-astuce-du-noyau-optionnel-suite-du-dual">
   III. L‚Äôastuce du noyau (optionnel, suite du dual)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iv-visualisation-de-la-frontiere-de-decision">
   IV. Visualisation de la fronti√®re de d√©cision
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#v-sur-de-vrais-donnees">
   V. Sur de vrais donn√©es
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#iris-dataset">
     Iris dataset
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#digits-dataset">
     Digits dataset
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#wine-dataset">
     Wine dataset
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vi-majorant-de-generalisation-pour-le-svm">
   VI. Majorant de g√©n√©ralisation pour le SVM
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="le-svm-ou-lhypothese-max-margin">
<h1>Le SVM ou l‚Äôhypoth√®se max-margin ‚òïÔ∏è‚òïÔ∏è<a class="headerlink" href="#le-svm-ou-lhypothese-max-margin" title="Permalink to this headline">¬∂</a></h1>
<div class="admonition-objectifs-de-la-sequence admonition">
<p class="admonition-title">Objectifs de la s√©quence</p>
<ul class="simple">
<li><p>Comprendre¬†:</p>
<ul>
<li><p>la notion de marge (dans les donn√©es),</p></li>
</ul>
</li>
<li><p>√ätre sensibilis√©¬†:</p>
<ul>
<li><p>√† la notion de noyaux pour les probl√®mes non lin√©aires,</p></li>
<li><p>aux aspects de g√©n√©ralisation du choix <em>max-margin</em>.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="i-introduction">
<h2>I. Introduction<a class="headerlink" href="#i-introduction" title="Permalink to this headline">¬∂</a></h2>
<p>L‚Äôhypoth√®se implicite derri√®re le <em>max-margin</em> est qu‚Äôentre deux fronti√®res de m√™me complexit√©, la plus robuste aux perturbations (dans le sens o√π si on perturbe un √©l√©ment du jeu d‚Äôapprentissage, la probabilit√© qu‚Äôil change de classe est la plus faible) est la meilleure. L‚Äôid√©e fait sens car on peut supposer que les √©chantillons nouveaux peuvent √™tre vus comme des perturbations des √©chantillons du jeu d‚Äôapprentissage.</p>
<p>La fronti√®re la plus robuste aux perturbations est celle qui maximise la distance entre le point le plus proche et elle-m√™me. C‚Äôest l‚Äôhypoth√®se du <em>max-margin</em>.</p>
<p>La figure ci-dessous illustre cette id√©e. Nous avons deux fronti√®res qui s√©parent sans faire d‚Äôerreur les deux classes. Cependant, l‚Äôune de ces fronti√®res semble mauvaise alors que l‚Äôautre maximise la marge.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/1_svm_2_0.png" src="../_images/1_svm_2_0.png" />
</div>
</div>
</div>
<div class="section" id="ii-un-probleme-de-classification-lineaire">
<h2>II. Un probl√®me de classification lin√©aire<a class="headerlink" href="#ii-un-probleme-de-classification-lineaire" title="Permalink to this headline">¬∂</a></h2>
<p>Le SVM est un classifieur lin√©aire. Soit <span class="math notranslate nohighlight">\(\mathcal{X}\subset\mathbb{R}^d\)</span> nos variables d‚Äôentr√©es et <span class="math notranslate nohighlight">\(\mathcal{Y}=\{-1,+1\}\)</span> l‚Äôespace de nos variables √† pr√©dire. Un classifieur lin√©aire s√©pare les √©l√©ments de notre jeu de donn√©es par un hyperplan. Comme vous avez pu le voir dans le TP pr√©c√©dent, un hyperplan d√©crit par le vecteur normal <span class="math notranslate nohighlight">\(w\)</span> est d√©fini par les solutions de l‚Äô√©quations suivantes :</p>
<div class="math notranslate nohighlight">
\[\langle w, x\rangle = 0\]</div>
<p>Si le produit scalaire est positif, on dira que notre √©chantillon <span class="math notranslate nohighlight">\(x\)</span> appartient √† la classe positive et inversement.</p>
<p>Un classifieur lin√©aire peut donc √™tre d√©crit de la mani√®re suivante :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
h_w:\mathcal{X}&amp;\mapsto\mathcal{Y}=\{-1,+1\}\\
x&amp;\rightarrow \text{sign}(\langle w, x\rangle)
\end{aligned}\end{split}\]</div>
<p>De la m√™me mani√®re que pour les TPs pr√©c√©dents, on peut introduire la notion de biais en rajoutant une dimension de <span class="math notranslate nohighlight">\(1\)</span> aux vecteurs <span class="math notranslate nohighlight">\(x\)</span>.</p>
<div class="admonition-petite-question-d-algebre admonition">
<p class="admonition-title">Petite question d‚Äôalg√®bre</p>
<p><strong>Trouvez le projecteur orthogonal de <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> sur l‚Äôhyperplan d√©crit par le vecteur <span class="math notranslate nohighlight">\(w\)</span>, not√© <span class="math notranslate nohighlight">\(\text{proj}_w(x)\)</span>. D√©montrez que <span class="math notranslate nohighlight">\(\forall x\in\mathcal{X},\ \text{proj}_w(x)\in\{z:\langle w, z\rangle=0\}\)</span> (autrement dit, d√©montrez que la projection de <span class="math notranslate nohighlight">\(x\)</span> sur la fronti√®re est bien sur la fronti√®re).</strong></p>
</div>
<div class="admonition-petite-question-d-algebre-2 admonition">
<p class="admonition-title">Petite question d‚Äôalg√®bre 2</p>
<p><strong>Montrer que <span class="math notranslate nohighlight">\(\text{proj}_w^2=\text{proj}_w\)</span> (le carr√© est pris dans le sens de la composition). Cela vous semble-t-il logique ?</strong></p>
</div>
<div class="section" id="a-le-primal">
<h3>A. Le primal<a class="headerlink" href="#a-le-primal" title="Permalink to this headline">¬∂</a></h3>
<p>Comme dit plus haut, on ne cherche pas n‚Äôimporte quel hyperplan, mais bien celui qui rang la marge maximale. La marge est d√©finie par la plus patite distance entre un point du jeu de donn√©es et la fronti√®re de d√©cision.</p>
<p>La distance d‚Äôun point √† la fronti√®re est donn√©e par <span class="math notranslate nohighlight">\(|\langle x_i, w\rangle|\)</span> (<span class="math notranslate nohighlight">\(w\)</span> unitaire). La quantit√© <span class="math notranslate nohighlight">\(y_i\langle x_i, w\rangle\)</span> est positive et indique la distance √† la fronti√®re si le point est bien class√© et donne la distance n√©gative si le point est mal class√©. Ainsi <span class="math notranslate nohighlight">\(\min_{i\leq m} y_i\langle x_i, w\rangle\)</span> nous donne le point la plus petite distance (n√©gative si mal class√©).</p>
<p>On souhaite donc trouver <span class="math notranslate nohighlight">\(w\)</span> tel que cette distance soit maximale (i.e. le point le plus proche est le plus loin possible de la fronti√®re) :</p>
<div class="math notranslate nohighlight">
\[\hat{w}=\text{argmax}_{w, \lVert w\rVert=1}\min_{i\leq m}y_i\langle x_i, w\rangle\]</div>
<p>Il est possible de montrer que le vecteur <span class="math notranslate nohighlight">\(w=w_0/\lVert w_0\rVert\)</span> tel que :</p>
<div class="math notranslate nohighlight">
\[w_0=\text{argmin}_{w}\lVert w\rVert^2_2,\ s.t. \forall i\leq m,\ y_i\langle x_i, w\rangle \geq 1\]</div>
<p>est solution de ce probl√®me de minimisation.</p>
<div class="caution dropdown admonition">
<p class="admonition-title">Preuve</p>
<p>Soit
<span class="math notranslate nohighlight">\(\boldsymbol{w}^\star\)</span> une solution du premier probl√®me et soit <span class="math notranslate nohighlight">\(\gamma^\star=\min_{i\leq m}y_i\langle\boldsymbol{x_i}, \boldsymbol{w^\star}\rangle\)</span>. <span class="math notranslate nohighlight">\(\gamma^\star\)</span> est donc la distance du point le plus proche de l‚Äôhyperplan de vecteur normal <span class="math notranslate nohighlight">\(\boldsymbol{w^\star}\)</span> √† ce dernier. Nous avons donc</p>
<div class="math notranslate nohighlight">
\[\forall i\leq m,\ y_i\langle \boldsymbol{x_i}, \boldsymbol{w^\star}\rangle \geq \gamma^\star,\]</div>
<p>et de mani√®re totalement √©quivalente~:</p>
<div class="math notranslate nohighlight">
\[\forall i\leq m,\ y_i\langle \boldsymbol{x_i}, \boldsymbol{w^\star}/{\gamma^\star}\rangle \geq 1\]</div>
<p>Notons ici que <span class="math notranslate nohighlight">\(\boldsymbol{w^\star}/{\gamma^\star}\)</span> satisfait bien les contraintes du probl√®mes d‚Äôoptimisation quadratique (le second probl√®me). Ainsi, il suffit de montrer qu‚Äôil n‚Äôexiste pas de meilleure solution au probl√®me quadratique, pour que nos deux probl√®mes soient √©quivalents.</p>
<p>Soit <span class="math notranslate nohighlight">\(\boldsymbol{w_0}\)</span> la solution du probl√®me quadratique avant normalisation. On a ainsi <span class="math notranslate nohighlight">\(\lVert\boldsymbol{w_0}\rVert\leq \lVert\boldsymbol{w^\star/\gamma^\star}\rVert =1/\gamma^\star\)</span>. L‚Äôin√©galit√© vient du fait que <span class="math notranslate nohighlight">\(\boldsymbol{w^\star/\gamma^\star}\)</span> satisfait les contraintes mais n‚Äôest peut-√™tre pas la meilleure solution. Soit <span class="math notranslate nohighlight">\(\hat{\boldsymbol{w}}=\boldsymbol{w_0}/\lVert\boldsymbol{w_0}\rVert\)</span>. Nous avons <span class="math notranslate nohighlight">\(\forall i\leq m\)</span>:</p>
<div class="math notranslate nohighlight">
\[y_i\langle \boldsymbol{x_i}, \hat{\boldsymbol{w}}\rangle=\frac{1}{\lVert\boldsymbol{w_0}\rVert} y_i\langle \boldsymbol{x_i}, \boldsymbol{w_0}\rangle\geq \frac{1}{\lVert\boldsymbol{w_0}\rVert}\geq \gamma^\star.\]</div>
<p>La premi√®re in√©galit√© vient de la contrainte du probl√®me d‚Äôoptimisation quadratique et la seconde que la solution du premier probl√®me satisfait elle-m√™me ces contraintes. Puisque <span class="math notranslate nohighlight">\(\lVert\hat{\boldsymbol{w}}\rVert=1\)</span> et d√©finit une marge au moins aussi grande que <span class="math notranslate nohighlight">\(\boldsymbol{w}^\star\)</span> qui par d√©finition d√©finit la plus grande marge, les solutions sont √©quivalentes.</p>
</div>
<p>Remarquez que cela fait penser √† la r√©gularisation : parmi toutes les solutions possibles, on cherche celle de norme minimale.</p>
</div>
<div class="section" id="b-le-dual-optionnel">
<h3>B. Le dual (optionnel)<a class="headerlink" href="#b-le-dual-optionnel" title="Permalink to this headline">¬∂</a></h3>
<p>Le probl√®me d‚Äôoptimisation si dessus est ce qu‚Äôon appelle un probl√®me d‚Äôoptimisation sous contrainte. Un tel probl√®me est associ√© √† ce qu‚Äôon appelle un Lagrangien :</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(w, \alpha)=\frac{1}{2}\lVert w\rVert_2^2+\sum_{i=1}^m\alpha_i(1-y_i\langle x_i, w\rangle),\ \alpha_j\geq 0, j\leq m\]</div>
<p>Notons <span class="math notranslate nohighlight">\(g(w)=\max_{\alpha,\alpha\geq 0}\mathcal{L}(w,\alpha)\)</span>. On observe assez rapidement que <span class="math notranslate nohighlight">\(g(w)=\infty\)</span> si une des contraintes n‚Äôest pas satisfaite et vaut <span class="math notranslate nohighlight">\(\lVert w\rVert^2_2/2\)</span> sinon.</p>
<p>Ainsi, minimiser <span class="math notranslate nohighlight">\(g(w)\)</span> revient √† minimiser la norme du vecteur <span class="math notranslate nohighlight">\(w\)</span> en respectant les contraintes. C‚Äôest ce qu‚Äôon appelle le <em>primal</em> qu‚Äôon note <span class="math notranslate nohighlight">\(p^\star\)</span> :</p>
<div class="math notranslate nohighlight">
\[p^\star=\min_w\max_{\alpha,\alpha\geq 0}\mathcal{L}(w,\alpha)\]</div>
<p>Le passage au dual permet d‚Äôinverser la minimisation et la maximisation. Il n‚Äôest pas √©vident de montrer que les deux probl√®mes sont √©quivalents. C‚Äôest ici le cas et on note <span class="math notranslate nohighlight">\(d^\star\)</span> le dual (on parle donc de dualit√© forte) :</p>
<div class="math notranslate nohighlight">
\[d^\star=\max_{\alpha,\alpha\geq 0}\min_w\mathcal{L}(w,\alpha).\]</div>
<div class="warning dropdown admonition">
<p class="admonition-title">D√©tails</p>
<p>o√π on peut v√©rifier qu‚Äôon a n√©cessairement <span class="math notranslate nohighlight">\(d^\star\leq p^\star\)</span>. Ainsi, dans le cadre du primal, le probl√®me est une minimisation, mais devient une maximisation dans le dual. Le gap de dualit√© donne l‚Äô√©cart entre le primal et le dual. On parle de dualit√© faible si le gap est positif et de dualit√© forte s‚Äôil est nul. Il se trouve que si <span class="math notranslate nohighlight">\(\lVert\boldsymbol{w}\rVert^2\)</span> et <span class="math notranslate nohighlight">\(\alpha_i(1 - y_i\langle \boldsymbol{x_i}, \boldsymbol{w}\rangle)\)</span> sont convexes en <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> et <span class="math notranslate nohighlight">\(\alpha_i\)</span> respectivement, et qu‚Äôil existe <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> tel que <span class="math notranslate nohighlight">\(1 - y_i\langle \boldsymbol{x_i}, \boldsymbol{w}\rangle&lt;0\)</span> (in√©galit√© stricte impliquant que <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> est dans l‚Äôenveloppe affine de l‚Äôensemble de faisabilit√©), alors nous avons une dualit√© stricte (conditions de Slater). On obtient donc~:</p>
<div class="math notranslate nohighlight">
\[p^\star=\mathcal{L}(\boldsymbol{w^\star}, \boldsymbol{\alpha^\star})=d^\star.\]</div>
</div>
<p>Quelques √©l√©ments de calculs plus loin (le minimum est un point critique, on annule les d√©riv√©es partielles, etc.), on reformule le dual de la mani√®re suivante :</p>
<div class="math notranslate nohighlight">
\[\max_{\alpha,\alpha\geq 0}\sum_i\alpha_i-\frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j\langle x_i, x_j\rangle\]</div>
<p>et</p>
<div class="math notranslate nohighlight">
\[w=\frac{1}{2}\sum_i \alpha_iy_ix_i\]</div>
<div class="warning dropdown admonition">
<p class="admonition-title">D√©tails</p>
<p>Concentrons-nous sur l‚Äôexpression duale. Pour un <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}\)</span> donn√©, <span class="math notranslate nohighlight">\(\min_{\boldsymbol{w}}\mathcal{L}(\boldsymbol{w}, \boldsymbol{\alpha})\)</span> minimise <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> relativement √† <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>. Ainsi, nous avons:</p>
<div class="math notranslate nohighlight">
\[0=\frac{\partial \mathcal{L}}{\partial\boldsymbol{w}}=\boldsymbol{w}-\sum_i\alpha_iy_i\boldsymbol{x_i}\Leftrightarrow\boldsymbol{w}=\sum_i\alpha_iy_i\boldsymbol{x_i}.\]</div>
<p>Autrement dit, nous pouvons reformuler le probl√®me dual de la mani√®re suivante:</p>
<div class="math notranslate nohighlight">
\[\max_{\boldsymbol{\alpha}, \boldsymbol{\alpha}\geq \boldsymbol{0}}\sum_i\alpha_i-\frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j\langle \boldsymbol{x_i}, \boldsymbol{x_j}\rangle.\]</div>
<p>Ce probl√®me d‚Äôoptimisation est √©quivalent au primal et consiste √† maximiser une quantit√© qui ne d√©pend plus que du produit scalaire entre les points de notre jeu de donn√©es (√† un signe pr√®s s‚Äôils appartiennent √† des classes oppos√©es).</p>
</div>
<p>Ainsi, notre mod√®le pr√©dictif prend la forme suivante :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
h:\mathcal{X}&amp;\mapsto\mathcal{Y}\\
x&amp;\rightarrow\text{sign}(\sum_i\alpha_iy_i\langle x_i, x\rangle)
\end{aligned}\end{split}\]</div>
</div>
<div class="section" id="c-via-une-surrogate-loss">
<h3>C. Via une <em>surrogate loss</em><a class="headerlink" href="#c-via-une-surrogate-loss" title="Permalink to this headline">¬∂</a></h3>
<p>Nous avons vu que la minimisation empirique √©tait difficile en g√©n√©rale. Pour cela, nous minimisions souvent une <em>surrogate loss</em>. Le SVM pr√©sent√© √† l‚Äôinstant peut √™tre reformul√© au travers d‚Äôune <em>surrogate loss</em> appel√©e la <em>hinge loss</em>¬†:</p>
<div class="math notranslate nohighlight">
\[\ell^\text{hinge}(z)=\max(0;1-z).\]</div>
<p>Le probl√®me du SVM devient alors¬†:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\omega)=\frac{1}{m}\sum_i \ell (y_i\langle \omega, x_i\rangle)+\lambda\lVert \omega\rVert_2\]</div>
<p>o√π nous avons rajout√© une p√©nalit√© √† la <em>hinge loss</em> afin de choisir le vecteur de param√®tres de norme minimale.</p>
</div>
</div>
<div class="section" id="iii-l-astuce-du-noyau-optionnel-suite-du-dual">
<h2>III. L‚Äôastuce du noyau (optionnel, suite du dual)<a class="headerlink" href="#iii-l-astuce-du-noyau-optionnel-suite-du-dual" title="Permalink to this headline">¬∂</a></h2>
<p>L‚Äôastuce du noyau d√©coule de la formation duale et notamment du fait que celle-ci n‚Äôest li√©e aux donn√©es qu‚Äôau travers du produit <span class="math notranslate nohighlight">\(y_iy_i\)</span> et du produit scalaire <span class="math notranslate nohighlight">\(\langle x_i, x_j\rangle\)</span>. Si le probl√®me de classification est non lin√©aire, il est possible de passer par une transformation non lin√©aire <span class="math notranslate nohighlight">\(\phi:\mathcal{X}\mapsto\mathcal{F}\)</span> de nos donn√©es d‚Äôentr√©es. Le probl√®me devient donc :</p>
<div class="math notranslate nohighlight">
\[\max_{\alpha,\alpha\geq 0}\sum_i\alpha_i-\frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j\langle \phi(x_i), \phi(x_j)\rangle\]</div>
<p>Sans rentrer dans les d√©tails, l‚Äôastuce du noyau vient de l‚Äôexistence de fonctions :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
k:\mathcal{X}\times\mathcal{X}&amp;\mapsto\mathbb{R}\\
x_i,x_j&amp;\rightarrow k(x_i,x_j)=\langle\phi(x_i),\phi(x_j)\rangle.
\end{aligned}\end{split}\]</div>
<p>Ces fonctions <span class="math notranslate nohighlight">\(k\)</span> ne n√©cessitent pas de projeter les <span class="math notranslate nohighlight">\(x\)</span> dans un espace de plus grande dimension et permettent d‚Äôobtenir le r√©sultat du produit scalaire directement dans l‚Äôespace d‚Äôorigine. Ainsi, on peut m√™me calculer le produit scalaire dans des espaces de dimensions infinies.</p>
<p>Par exemple le noyau :</p>
<div class="margin sidebar">
<p class="sidebar-title">Les noyaux de <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span></p>
<p>La librairie <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span> propose ses propres noyaux dont le noyau polynomial. Vous pouvez aussi proposer vos propres noyaux voire les pr√©calculer.</p>
</div>
<div class="math notranslate nohighlight">
\[k(x_i, x_j)=(\langle x_i, x_j\rangle+c)^n,\]</div>
<p>nous permet de faire une transformations polynomiales de degr√© <span class="math notranslate nohighlight">\(n\)</span> directement dans l‚Äôespace d‚Äôorigine ; on remarque que la puissance <span class="math notranslate nohighlight">\(n\)</span> est calcul√©e sur le r√©sultat du produit scalaire (<span class="math notranslate nohighlight">\(+c\)</span>) qui est donc un sclaire. Dans le cas o√π nos donn√©es seraient dans <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>, prenons la fonction de transformation polynomiale suivante:</p>
<div class="math notranslate nohighlight">
\[\phi(x)=[x_{1}^2,\sqrt{2}x_{1}x_{2}, x_2^2]^T.\]</div>
<p>Il s‚Äôagit du noyau avec <span class="math notranslate nohighlight">\(c=0\)</span> et <span class="math notranslate nohighlight">\(n=2\)</span>.</p>
<p>Un autre noyau est le <em>noyau gaussien</em> d√©finit comme:</p>
<div class="math notranslate nohighlight">
\[k(x_i, x_j)=\text{exp}\Big(-\frac{\lVert x_i-x_j\rVert^2_2}{2}\Big)\]</div>
<p>o√π les pr√©dictions d√©pendent des points dans un voisinage calcul√© par notre noyau gaussien.</p>
<p>Il se trouve que nous avons:</p>
<div class="margin sidebar">
<p class="sidebar-title">D√©veloppement en s√©rie de Taylor</p>
<p>Rappelez-vous le d√©veloppement en s√©rie de Taylor de la fonction exponentielle:</p>
<div class="math notranslate nohighlight">
\[e^x=\sum_{n=0}^\infty \frac{x^n}{n!}\]</div>
</div>
<div class="math notranslate nohighlight">
\[\text{exp}\Big(-\frac{\lVert x_i-x_j\rVert^2_2}{2}\Big)=C\sum_{l=0}^\infty \frac{\langle x_i, x_j\rangle^l}{l!}=C\sum_{l=0}^\infty \frac{k_{\text{poly}(l)}(x_i, x_j)}{l!},\]</div>
<p>o√π</p>
<div class="math notranslate nohighlight">
\[C=\text{exp}\Big(-\frac{1}{2}\lVert x_i\rVert_2^2\Big)\text{exp}\Big(-\frac{1}{2}\lVert x_j\rVert_2^2\Big).\]</div>
</div>
<div class="section" id="iv-visualisation-de-la-frontiere-de-decision">
<h2>IV. Visualisation de la fronti√®re de d√©cision<a class="headerlink" href="#iv-visualisation-de-la-frontiere-de-decision" title="Permalink to this headline">¬∂</a></h2>
<p>L‚Äôobjectif de ce premier exercice est de visualiser la fronti√®re de d√©cision d‚Äôun SVM en jouant sur un exemple simple avec les noyaux offerts par la librairie <span class="math notranslate nohighlight">\(\texttt{scikit-learn}\)</span>.</p>
<p>La visualisation suivante permet d‚Äôobserver la marge et notamment les vecteurs de supports.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/1_svm_15_0.png" src="../_images/1_svm_15_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sample_data</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">200</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">3</span> <span class="o">&lt;</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_data</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">clf</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(())</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(())</span>
    <span class="k">if</span> <span class="n">clf</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>

        <span class="n">XX</span><span class="p">,</span> <span class="n">YY</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mgrid</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="mi">1</span><span class="p">:</span><span class="mi">500</span><span class="n">j</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="mi">1</span><span class="p">:</span><span class="mi">500</span><span class="n">j</span><span class="p">]</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">XX</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">YY</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>

        <span class="c1"># Put the result into a color plot</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">XX</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">XX</span><span class="p">,</span> <span class="n">YY</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Paired</span><span class="p">,</span> <span class="n">shading</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_svm_18_0.png" src="../_images/1_svm_18_0.png" />
</div>
</div>
<p>Voici la visualisation d‚Äôun SVM avec un noyau lin√©aire (un produit scalaire <span class="math notranslate nohighlight">\(\langle\cdot,\cdot\rangle\)</span>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kernel</span> <span class="o">=</span> <span class="s1">&#39;linear&#39;</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">clf</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_svm_20_0.png" src="../_images/1_svm_20_0.png" />
</div>
</div>
<p>Comme vous avez pu le voir, si vous avez lu la section concernant le dual, le SVM permet de remplacer les comparaisons lin√©aires (i.e. le produit scalaire), par des comparaisons non-lin√©aires (i.e. produit scalaire dans un espace o√π les donn√©es sont projet√©es non lin√©airement). La librairie <span class="math notranslate nohighlight">\(\texttt{scikir-learn}\)</span> permet de jouer avec ce param√®tre.</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Jouez avec plusieurs noyaux et observez la forme de la fronti√®re de d√©cision.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="c1">###############################################################</span>
</pre></div>
</div>
<div class="admonition-question admonition">
<p class="admonition-title">Question</p>
<p><strong>Comparez la robustesse d‚Äôun SVM par rapport √† un 1NN relativement √† la dimension du probl√®me. Le SVM est-il plus ou moins robuste que le 1NN ?</strong></p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">sample_data</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">))</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">y</span><span class="o">*</span><span class="n">X</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">X</span> <span class="c1"># positive have mean mu and negative, -mu</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="o">-</span><span class="n">k</span><span class="p">))</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">noise</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>

<span class="n">scores_svm</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">redo</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">max_dim</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">first_dim</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">steps</span> <span class="o">=</span> <span class="mi">100</span>

<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">first_dim</span><span class="p">,</span> <span class="n">max_dim</span><span class="p">,</span> <span class="n">steps</span><span class="p">):</span>
    <span class="n">s_svm</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">redo</span><span class="p">):</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_data</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="n">d</span><span class="p">)</span>
        <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">sample_data</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="n">d</span><span class="p">)</span>
        
        <span class="n">model</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],)))</span>
        <span class="n">s_svm</span> <span class="o">+=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],)))</span><span class="o">/</span><span class="n">redo</span>
        
        <span class="n">c</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>
        <span class="n">c</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],)))</span>
        <span class="n">s</span> <span class="o">+=</span> <span class="n">c</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],)))</span><span class="o">/</span><span class="n">redo</span>
    <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">scores_svm</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s_svm</span><span class="p">)</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">first_dim</span><span class="p">,</span> <span class="n">max_dim</span><span class="p">,</span> <span class="n">steps</span><span class="p">)),</span> <span class="n">scores_svm</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;SVM&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">first_dim</span><span class="p">,</span> <span class="n">max_dim</span><span class="p">,</span> <span class="n">steps</span><span class="p">)),</span> <span class="n">scores</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;KNN&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_svm_25_0.png" src="../_images/1_svm_25_0.png" />
</div>
</div>
</div>
<div class="section" id="v-sur-de-vrais-donnees">
<h2>V. Sur de vrais donn√©es<a class="headerlink" href="#v-sur-de-vrais-donnees" title="Permalink to this headline">¬∂</a></h2>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Utilisez le SVM, la r√©gression logistique ou encore le KNN pour r√©soudre les probl√®mes ci-dessous.</strong></p>
</div>
<div class="section" id="iris-dataset">
<h3>Iris dataset<a class="headerlink" href="#iris-dataset" title="Permalink to this headline">¬∂</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="c1">###############################################################</span>
</pre></div>
</div>
</div>
<div class="section" id="digits-dataset">
<h3>Digits dataset<a class="headerlink" href="#digits-dataset" title="Permalink to this headline">¬∂</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="n">digit</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_digits</span><span class="p">()</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">digit</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">digit</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray_r</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Label: &#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_svm_33_0.png" src="../_images/1_svm_33_0.png" />
</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="c1">###############################################################</span>
</pre></div>
</div>
</div>
<div class="section" id="wine-dataset">
<h3>Wine dataset<a class="headerlink" href="#wine-dataset" title="Permalink to this headline">¬∂</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="n">wine</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_wine</span><span class="p">()</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">wine</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">wine</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="c1">###############################################################</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="vi-majorant-de-generalisation-pour-le-svm">
<h2>VI. Majorant de g√©n√©ralisation pour le SVM<a class="headerlink" href="#vi-majorant-de-generalisation-pour-le-svm" title="Permalink to this headline">¬∂</a></h2>
<p>Nous avons vu dans la s√©quence traitant du mod√®le formel de l‚Äôapprentissage que nous pouvions majorer l‚Äôerreur attendue du minimiseur du risque empirique en esp√©rance. Celle-ci d√©pendait de ce qu‚Äôon appelle la dimension VC. Pour un classifieur lin√©aire, la dimension VC est <span class="math notranslate nohighlight">\(d+1\)</span> (i.e. le nombre de param√®tres), o√π <span class="math notranslate nohighlight">\(d\)</span> est la dimension de nos donn√©es.</p>
<p>Il se trouve que le minimiseur empirique est l‚Äôun des classifieurs lin√©aires de <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>. Le SVM ne fait pas n‚Äôimporte quel choix. La solution sera le classifieur lin√©aire qui maximise la marge. Cela nous permet d‚Äôobtenir le majorant suivant.</p>
<div class="admonition-theoreme-majorant-de-generalisation-du-svm admonition">
<p class="admonition-title">Th√©or√®me (Majorant de g√©n√©ralisation du SVM)</p>
<p>Soit <span class="math notranslate nohighlight">\(\mathbb{P}\)</span> une distribution sur <span class="math notranslate nohighlight">\(\mathcal{X}\times\mathcal{Y}\)</span> avec <span class="math notranslate nohighlight">\(\mathcal{Y}=\{-1,+1\}\)</span> telle que <span class="math notranslate nohighlight">\(\exists w^\star\)</span>, <span class="math notranslate nohighlight">\(\lVert w^\star\rVert_2=1\)</span>, <span class="math notranslate nohighlight">\(y\langle w^\star, x\rangle+b\geq \gamma\)</span> <span class="math notranslate nohighlight">\(\forall x, y\)</span> et <span class="math notranslate nohighlight">\(\lVert x\rVert_2\leq \rho\)</span> <span class="math notranslate nohighlight">\(\forall x\)</span>. Notons <span class="math notranslate nohighlight">\(h_\text{SVM}\)</span> la solution du SVM. Soit <span class="math notranslate nohighlight">\(\delta&gt;0\)</span> Nous avons alors avec probabilit√© <span class="math notranslate nohighlight">\(1-\delta\)</span>¬†:</p>
<div class="math notranslate nohighlight">
\[L(h_\text{SVM})\leq \sqrt{\frac{4(\rho/\gamma)^2}{m}}+\sqrt{\frac{2\log(2/\delta)}{m}}\]</div>
</div>
<p>L‚Äô√©l√©ment important de ce th√©or√®me est que la capacit√© de g√©n√©ralisation du SVM ne d√©pend plus de la dimension des donn√©es mais de la marge qui s√©pare nos donn√©es. La preuve de ce th√©or√®me est disponible dans¬†:</p>
<p><em>Shalev-Shwartz, Shai, et Shai Ben-David. Understanding Machine Learning: From Theory to Algorithms. Cambridge: Cambridge University Press, 2014.</em></p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./4_kernel_methods"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="0_propos_liminaire.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Les m√©thodes √† noyaux</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../5_ensembles/0_propos_liminaire.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Les m√©thodes <em>ensemblistes</em></p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Servajean, Leveau & Chailan<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>
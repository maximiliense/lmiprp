
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Le SVM ou l’hypothèse max-margin ☕️☕️ &#8212; Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Les méthodes ensemblistes" href="../5_ensembles/0_propos_liminaire.html" />
    <link rel="prev" title="Les méthodes à noyaux" href="0_propos_liminaire.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-72WWYCKNK6"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-72WWYCKNK6');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../0_requisite/rappels_python.html">
   Rappels de Python et Numpy
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../1_what_is_ml/0_propos_liminaire.html">
   <em>
    Machine Learning
   </em>
   , initiation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/1_introduction_ml.html">
     <em>
      Machine learning
     </em>
     et malédiction de la dimension
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/2_regression_and_classification_trees.html">
     Les arbres de régression et de classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../2_regression/0_propos_liminaire.html">
   La
   <em>
    régression
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/1_linear_regression.html">
     La régression linéaire ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/2_optimization.html">
     L’optimisation ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/3_interpolation.html">
     Interpolation ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/4_algo_proximal_lasso.html">
     Sous-différentiel et le cas du Lasso ☕️☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/5_least_square_qr.html">
     Les moindres carrés via une décomposition QR (et plus)☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/6_ridge.html">
     Une analyse de la régularisation Ridge ☕️☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../3_classification/0_propos_liminaire.html">
   La
   <em>
    classification
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/1_logistic_regression.html">
     La régression logistique ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/2_fonctions_proxy.html">
     Les fonctions de perte (loss function) ☕️☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/3_bayes_classifier.html">
     Le classifieur de Bayes ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/4_VC_theory.html">
     Un modèle formel de l’apprentissage ☕️☕️☕️☕️ (💆‍♂️)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="0_propos_liminaire.html">
   Les méthodes à noyaux
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Le SVM ou l’hypothèse max-margin ☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5_ensembles/0_propos_liminaire.html">
   Les méthodes
   <em>
    ensemblistes
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/1_ensembles.html">
     Méthodes ensemblistes ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/2_bayesian_linear_regression.html">
     Bayesian linear regression ☕️☕️☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../6_deeplearning/0_propos_liminaire.html">
   <em>
    deep learning
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/1_autodiff.html">
     La différentiation automatique et un début de
     <em>
      deep learning
     </em>
     ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/2_filters_representation.html">
     Filtres et espace de représentation des réseaux de neurones ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/3_probabilities_calibration.html">
     Calibration des probabilités et quelques notions ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/4_regularization_deep.html">
     Régularisation en
     <em>
      deep learning
     </em>
     ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/5_transfer_multitask.html">
     <em>
      Transfer learning
     </em>
     et apprentissage multi-tâches ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/6_adversarial.html">
     Les attaques adversaires ☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../7_unsupervised/0_propos_liminaire.html">
   L’apprentissage non-supervisé
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_unsupervised/1_principal_component_analysis.html">
     L’Analyse en Composantes Principales ☕️☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_unsupervised/2_em_gaussin_mixture_model.html">
     Modèle de Mélange Gaussien et algorithme
     <em>
      Expectation-Maximization
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../8_set_prediction/0_propos_liminaire.html">
   Prédiction d’ensembles
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/1_well_defined.html">
     Jeu d’apprentissage
     <em>
      set-valued
     </em>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/2_ill_defined.html">
     Jeu d’apprentissage uniquement multi-classes ☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../biblio.html">
   Bibliographie
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/4_kernel_methods/1_svm.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/maximiliense/lmiprp"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/maximiliense/lmiprp/issues/new?title=Issue%20on%20page%20%2F4_kernel_methods/1_svm.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/maximiliense/lmiprp/blob/master/Travaux Pratiques/Machine Learning/Book/4_kernel_methods/1_svm.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#i-introduction">
   I. Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ii-un-probleme-de-classification-lineaire">
   II. Un problème de classification linéaire
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-le-primal">
     A. Le primal
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-le-dual-optionnel">
     B. Le dual (optionnel)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#c-via-une-surrogate-loss">
     C. Via une
     <em>
      surrogate loss
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iii-l-astuce-du-noyau-optionnel-suite-du-dual">
   III. L’astuce du noyau (optionnel, suite du dual)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iv-visualisation-de-la-frontiere-de-decision">
   IV. Visualisation de la frontière de décision
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#v-sur-de-vrais-donnees">
   V. Sur de vrais données
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#iris-dataset">
     Iris dataset
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#digits-dataset">
     Digits dataset
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#wine-dataset">
     Wine dataset
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vi-majorant-de-generalisation-pour-le-svm">
   VI. Majorant de généralisation pour le SVM
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="le-svm-ou-lhypothese-max-margin">
<h1>Le SVM ou l’hypothèse max-margin ☕️☕️<a class="headerlink" href="#le-svm-ou-lhypothese-max-margin" title="Permalink to this headline">¶</a></h1>
<div class="admonition-objectifs-de-la-sequence admonition">
<p class="admonition-title">Objectifs de la séquence</p>
<ul class="simple">
<li><p>Comprendre :</p>
<ul>
<li><p>la notion de marge (dans les données),</p></li>
</ul>
</li>
<li><p>Être sensibilisé :</p>
<ul>
<li><p>à la notion de noyaux pour les problèmes non linéaires,</p></li>
<li><p>aux aspects de généralisation du choix <em>max-margin</em>.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="i-introduction">
<h2>I. Introduction<a class="headerlink" href="#i-introduction" title="Permalink to this headline">¶</a></h2>
<p>L’hypothèse implicite derrière le <em>max-margin</em> est qu’entre deux frontières de même complexité, la plus robuste aux perturbations (dans le sens où si on perturbe un élément du jeu d’apprentissage, la probabilité qu’il change de classe est la plus faible) est la meilleure. L’idée fait sens car on peut supposer que les échantillons nouveaux peuvent être vus comme des perturbations des échantillons du jeu d’apprentissage.</p>
<p>La frontière la plus robuste aux perturbations est celle qui maximise la distance entre le point le plus proche et elle-même. C’est l’hypothèse du <em>max-margin</em>.</p>
<p>La figure ci-dessous illustre cette idée. Nous avons deux frontières qui séparent sans faire d’erreur les deux classes. Cependant, l’une de ces frontières semble mauvaise alors que l’autre maximise la marge.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/1_svm_2_0.png" src="../_images/1_svm_2_0.png" />
</div>
</div>
</div>
<div class="section" id="ii-un-probleme-de-classification-lineaire">
<h2>II. Un problème de classification linéaire<a class="headerlink" href="#ii-un-probleme-de-classification-lineaire" title="Permalink to this headline">¶</a></h2>
<p>Le SVM est un classifieur linéaire. Soit <span class="math notranslate nohighlight">\(\mathcal{X}\subset\mathbb{R}^d\)</span> nos variables d’entrées et <span class="math notranslate nohighlight">\(\mathcal{Y}=\{-1,+1\}\)</span> l’espace de nos variables à prédire. Un classifieur linéaire sépare les éléments de notre jeu de données par un hyperplan. Comme vous avez pu le voir dans le TP précédent, un hyperplan décrit par le vecteur normal <span class="math notranslate nohighlight">\(w\)</span> est défini par les solutions de l’équations suivantes :</p>
<div class="math notranslate nohighlight">
\[\langle w, x\rangle = 0\]</div>
<p>Si le produit scalaire est positif, on dira que notre échantillon <span class="math notranslate nohighlight">\(x\)</span> appartient à la classe positive et inversement.</p>
<p>Un classifieur linéaire peut donc être décrit de la manière suivante :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
h_w:\mathcal{X}&amp;\mapsto\mathcal{Y}=\{-1,+1\}\\
x&amp;\rightarrow \text{sign}(\langle w, x\rangle)
\end{aligned}\end{split}\]</div>
<p>De la même manière que pour les TPs précédents, on peut introduire la notion de biais en rajoutant une dimension de <span class="math notranslate nohighlight">\(1\)</span> aux vecteurs <span class="math notranslate nohighlight">\(x\)</span>.</p>
<div class="admonition-petite-question-d-algebre admonition">
<p class="admonition-title">Petite question d’algèbre</p>
<p><strong>Trouvez le projecteur orthogonal de <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> sur l’hyperplan décrit par le vecteur <span class="math notranslate nohighlight">\(w\)</span>, noté <span class="math notranslate nohighlight">\(\text{proj}_w(x)\)</span>. Démontrez que <span class="math notranslate nohighlight">\(\forall x\in\mathcal{X},\ \text{proj}_w(x)\in\{z:\langle w, z\rangle=0\}\)</span> (autrement dit, démontrez que la projection de <span class="math notranslate nohighlight">\(x\)</span> sur la frontière est bien sur la frontière).</strong></p>
</div>
<div class="admonition-petite-question-d-algebre-2 admonition">
<p class="admonition-title">Petite question d’algèbre 2</p>
<p><strong>Montrer que <span class="math notranslate nohighlight">\(\text{proj}_w^2=\text{proj}_w\)</span> (le carré est pris dans le sens de la composition). Cela vous semble-t-il logique ?</strong></p>
</div>
<div class="section" id="a-le-primal">
<h3>A. Le primal<a class="headerlink" href="#a-le-primal" title="Permalink to this headline">¶</a></h3>
<p>Comme dit plus haut, on ne cherche pas n’importe quel hyperplan, mais bien celui qui rang la marge maximale. La marge est définie par la plus patite distance entre un point du jeu de données et la frontière de décision.</p>
<p>La distance d’un point à la frontière est donnée par <span class="math notranslate nohighlight">\(|\langle x_i, w\rangle|\)</span> (<span class="math notranslate nohighlight">\(w\)</span> unitaire). La quantité <span class="math notranslate nohighlight">\(y_i\langle x_i, w\rangle\)</span> est positive et indique la distance à la frontière si le point est bien classé et donne la distance négative si le point est mal classé. Ainsi <span class="math notranslate nohighlight">\(\min_{i\leq m} y_i\langle x_i, w\rangle\)</span> nous donne le point la plus petite distance (négative si mal classé).</p>
<p>On souhaite donc trouver <span class="math notranslate nohighlight">\(w\)</span> tel que cette distance soit maximale (i.e. le point le plus proche est le plus loin possible de la frontière) :</p>
<div class="math notranslate nohighlight">
\[\hat{w}=\text{argmax}_{w, \lVert w\rVert=1}\min_{i\leq m}y_i\langle x_i, w\rangle\]</div>
<p>Il est possible de montrer que le vecteur <span class="math notranslate nohighlight">\(w=w_0/\lVert w_0\rVert\)</span> tel que :</p>
<div class="math notranslate nohighlight">
\[w_0=\text{argmin}_{w}\lVert w\rVert^2_2,\ s.t. \forall i\leq m,\ y_i\langle x_i, w\rangle \geq 1\]</div>
<p>est solution de ce problème de minimisation.</p>
<div class="caution dropdown admonition">
<p class="admonition-title">Preuve</p>
<p>Soit
<span class="math notranslate nohighlight">\(\boldsymbol{w}^\star\)</span> une solution du premier problème et soit <span class="math notranslate nohighlight">\(\gamma^\star=\min_{i\leq m}y_i\langle\boldsymbol{x_i}, \boldsymbol{w^\star}\rangle\)</span>. <span class="math notranslate nohighlight">\(\gamma^\star\)</span> est donc la distance du point le plus proche de l’hyperplan de vecteur normal <span class="math notranslate nohighlight">\(\boldsymbol{w^\star}\)</span> à ce dernier. Nous avons donc</p>
<div class="math notranslate nohighlight">
\[\forall i\leq m,\ y_i\langle \boldsymbol{x_i}, \boldsymbol{w^\star}\rangle \geq \gamma^\star,\]</div>
<p>et de manière totalement équivalente~:</p>
<div class="math notranslate nohighlight">
\[\forall i\leq m,\ y_i\langle \boldsymbol{x_i}, \boldsymbol{w^\star}/{\gamma^\star}\rangle \geq 1\]</div>
<p>Notons ici que <span class="math notranslate nohighlight">\(\boldsymbol{w^\star}/{\gamma^\star}\)</span> satisfait bien les contraintes du problèmes d’optimisation quadratique (le second problème). Ainsi, il suffit de montrer qu’il n’existe pas de meilleure solution au problème quadratique, pour que nos deux problèmes soient équivalents.</p>
<p>Soit <span class="math notranslate nohighlight">\(\boldsymbol{w_0}\)</span> la solution du problème quadratique avant normalisation. On a ainsi <span class="math notranslate nohighlight">\(\lVert\boldsymbol{w_0}\rVert\leq \lVert\boldsymbol{w^\star/\gamma^\star}\rVert =1/\gamma^\star\)</span>. L’inégalité vient du fait que <span class="math notranslate nohighlight">\(\boldsymbol{w^\star/\gamma^\star}\)</span> satisfait les contraintes mais n’est peut-être pas la meilleure solution. Soit <span class="math notranslate nohighlight">\(\hat{\boldsymbol{w}}=\boldsymbol{w_0}/\lVert\boldsymbol{w_0}\rVert\)</span>. Nous avons <span class="math notranslate nohighlight">\(\forall i\leq m\)</span>:</p>
<div class="math notranslate nohighlight">
\[y_i\langle \boldsymbol{x_i}, \hat{\boldsymbol{w}}\rangle=\frac{1}{\lVert\boldsymbol{w_0}\rVert} y_i\langle \boldsymbol{x_i}, \boldsymbol{w_0}\rangle\geq \frac{1}{\lVert\boldsymbol{w_0}\rVert}\geq \gamma^\star.\]</div>
<p>La première inégalité vient de la contrainte du problème d’optimisation quadratique et la seconde que la solution du premier problème satisfait elle-même ces contraintes. Puisque <span class="math notranslate nohighlight">\(\lVert\hat{\boldsymbol{w}}\rVert=1\)</span> et définit une marge au moins aussi grande que <span class="math notranslate nohighlight">\(\boldsymbol{w}^\star\)</span> qui par définition définit la plus grande marge, les solutions sont équivalentes.</p>
</div>
<p>Remarquez que cela fait penser à la régularisation : parmi toutes les solutions possibles, on cherche celle de norme minimale.</p>
</div>
<div class="section" id="b-le-dual-optionnel">
<h3>B. Le dual (optionnel)<a class="headerlink" href="#b-le-dual-optionnel" title="Permalink to this headline">¶</a></h3>
<p>Le problème d’optimisation si dessus est ce qu’on appelle un problème d’optimisation sous contrainte. Un tel problème est associé à ce qu’on appelle un Lagrangien :</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(w, \alpha)=\frac{1}{2}\lVert w\rVert_2^2+\sum_{i=1}^m\alpha_i(1-y_i\langle x_i, w\rangle),\ \alpha_j\geq 0, j\leq m\]</div>
<p>Notons <span class="math notranslate nohighlight">\(g(w)=\max_{\alpha,\alpha\geq 0}\mathcal{L}(w,\alpha)\)</span>. On observe assez rapidement que <span class="math notranslate nohighlight">\(g(w)=\infty\)</span> si une des contraintes n’est pas satisfaite et vaut <span class="math notranslate nohighlight">\(\lVert w\rVert^2_2/2\)</span> sinon.</p>
<p>Ainsi, minimiser <span class="math notranslate nohighlight">\(g(w)\)</span> revient à minimiser la norme du vecteur <span class="math notranslate nohighlight">\(w\)</span> en respectant les contraintes. C’est ce qu’on appelle le <em>primal</em> qu’on note <span class="math notranslate nohighlight">\(p^\star\)</span> :</p>
<div class="math notranslate nohighlight">
\[p^\star=\min_w\max_{\alpha,\alpha\geq 0}\mathcal{L}(w,\alpha)\]</div>
<p>Le passage au dual permet d’inverser la minimisation et la maximisation. Il n’est pas évident de montrer que les deux problèmes sont équivalents. C’est ici le cas et on note <span class="math notranslate nohighlight">\(d^\star\)</span> le dual (on parle donc de dualité forte) :</p>
<div class="math notranslate nohighlight">
\[d^\star=\max_{\alpha,\alpha\geq 0}\min_w\mathcal{L}(w,\alpha).\]</div>
<div class="warning dropdown admonition">
<p class="admonition-title">Détails</p>
<p>où on peut vérifier qu’on a nécessairement <span class="math notranslate nohighlight">\(d^\star\leq p^\star\)</span>. Ainsi, dans le cadre du primal, le problème est une minimisation, mais devient une maximisation dans le dual. Le gap de dualité donne l’écart entre le primal et le dual. On parle de dualité faible si le gap est positif et de dualité forte s’il est nul. Il se trouve que si <span class="math notranslate nohighlight">\(\lVert\boldsymbol{w}\rVert^2\)</span> et <span class="math notranslate nohighlight">\(\alpha_i(1 - y_i\langle \boldsymbol{x_i}, \boldsymbol{w}\rangle)\)</span> sont convexes en <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> et <span class="math notranslate nohighlight">\(\alpha_i\)</span> respectivement, et qu’il existe <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> tel que <span class="math notranslate nohighlight">\(1 - y_i\langle \boldsymbol{x_i}, \boldsymbol{w}\rangle&lt;0\)</span> (inégalité stricte impliquant que <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> est dans l’enveloppe affine de l’ensemble de faisabilité), alors nous avons une dualité stricte (conditions de Slater). On obtient donc~:</p>
<div class="math notranslate nohighlight">
\[p^\star=\mathcal{L}(\boldsymbol{w^\star}, \boldsymbol{\alpha^\star})=d^\star.\]</div>
</div>
<p>Quelques éléments de calculs plus loin (le minimum est un point critique, on annule les dérivées partielles, etc.), on reformule le dual de la manière suivante :</p>
<div class="math notranslate nohighlight">
\[\max_{\alpha,\alpha\geq 0}\sum_i\alpha_i-\frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j\langle x_i, x_j\rangle\]</div>
<p>et</p>
<div class="math notranslate nohighlight">
\[w=\frac{1}{2}\sum_i \alpha_iy_ix_i\]</div>
<div class="warning dropdown admonition">
<p class="admonition-title">Détails</p>
<p>Concentrons-nous sur l’expression duale. Pour un <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}\)</span> donné, <span class="math notranslate nohighlight">\(\min_{\boldsymbol{w}}\mathcal{L}(\boldsymbol{w}, \boldsymbol{\alpha})\)</span> minimise <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> relativement à <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>. Ainsi, nous avons:</p>
<div class="math notranslate nohighlight">
\[0=\frac{\partial \mathcal{L}}{\partial\boldsymbol{w}}=\boldsymbol{w}-\sum_i\alpha_iy_i\boldsymbol{x_i}\Leftrightarrow\boldsymbol{w}=\sum_i\alpha_iy_i\boldsymbol{x_i}.\]</div>
<p>Autrement dit, nous pouvons reformuler le problème dual de la manière suivante:</p>
<div class="math notranslate nohighlight">
\[\max_{\boldsymbol{\alpha}, \boldsymbol{\alpha}\geq \boldsymbol{0}}\sum_i\alpha_i-\frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j\langle \boldsymbol{x_i}, \boldsymbol{x_j}\rangle.\]</div>
<p>Ce problème d’optimisation est équivalent au primal et consiste à maximiser une quantité qui ne dépend plus que du produit scalaire entre les points de notre jeu de données (à un signe près s’ils appartiennent à des classes opposées).</p>
</div>
<p>Ainsi, notre modèle prédictif prend la forme suivante :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
h:\mathcal{X}&amp;\mapsto\mathcal{Y}\\
x&amp;\rightarrow\text{sign}(\sum_i\alpha_iy_i\langle x_i, x\rangle)
\end{aligned}\end{split}\]</div>
</div>
<div class="section" id="c-via-une-surrogate-loss">
<h3>C. Via une <em>surrogate loss</em><a class="headerlink" href="#c-via-une-surrogate-loss" title="Permalink to this headline">¶</a></h3>
<p>Nous avons vu que la minimisation empirique était difficile en générale. Pour cela, nous minimisions souvent une <em>surrogate loss</em>. Le SVM présenté à l’instant peut être reformulé au travers d’une <em>surrogate loss</em> appelée la <em>hinge loss</em> :</p>
<div class="math notranslate nohighlight">
\[\ell^\text{hinge}(z)=\max(0;1-z).\]</div>
<p>Le problème du SVM devient alors :</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\omega)=\frac{1}{m}\sum_i \ell (y_i\langle \omega, x_i\rangle)+\lambda\lVert \omega\rVert_2\]</div>
<p>où nous avons rajouté une pénalité à la <em>hinge loss</em> afin de choisir le vecteur de paramètres de norme minimale.</p>
</div>
</div>
<div class="section" id="iii-l-astuce-du-noyau-optionnel-suite-du-dual">
<h2>III. L’astuce du noyau (optionnel, suite du dual)<a class="headerlink" href="#iii-l-astuce-du-noyau-optionnel-suite-du-dual" title="Permalink to this headline">¶</a></h2>
<p>L’astuce du noyau découle de la formation duale et notamment du fait que celle-ci n’est liée aux données qu’au travers du produit <span class="math notranslate nohighlight">\(y_iy_i\)</span> et du produit scalaire <span class="math notranslate nohighlight">\(\langle x_i, x_j\rangle\)</span>. Si le problème de classification est non linéaire, il est possible de passer par une transformation non linéaire <span class="math notranslate nohighlight">\(\phi:\mathcal{X}\mapsto\mathcal{F}\)</span> de nos données d’entrées. Le problème devient donc :</p>
<div class="math notranslate nohighlight">
\[\max_{\alpha,\alpha\geq 0}\sum_i\alpha_i-\frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j\langle \phi(x_i), \phi(x_j)\rangle\]</div>
<p>Sans rentrer dans les détails, l’astuce du noyau vient de l’existence de fonctions :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
k:\mathcal{X}\times\mathcal{X}&amp;\mapsto\mathbb{R}\\
x_i,x_j&amp;\rightarrow k(x_i,x_j)=\langle\phi(x_i),\phi(x_j)\rangle.
\end{aligned}\end{split}\]</div>
<p>Ces fonctions <span class="math notranslate nohighlight">\(k\)</span> ne nécessitent pas de projeter les <span class="math notranslate nohighlight">\(x\)</span> dans un espace de plus grande dimension et permettent d’obtenir le résultat du produit scalaire directement dans l’espace d’origine. Ainsi, on peut même calculer le produit scalaire dans des espaces de dimensions infinies.</p>
<p>Par exemple le noyau :</p>
<div class="margin sidebar">
<p class="sidebar-title">Les noyaux de <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span></p>
<p>La librairie <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span> propose ses propres noyaux dont le noyau polynomial. Vous pouvez aussi proposer vos propres noyaux voire les précalculer.</p>
</div>
<div class="math notranslate nohighlight">
\[k(x_i, x_j)=(\langle x_i, x_j\rangle+c)^n,\]</div>
<p>nous permet de faire une transformations polynomiales de degré <span class="math notranslate nohighlight">\(n\)</span> directement dans l’espace d’origine ; on remarque que la puissance <span class="math notranslate nohighlight">\(n\)</span> est calculée sur le résultat du produit scalaire (<span class="math notranslate nohighlight">\(+c\)</span>) qui est donc un sclaire. Dans le cas où nos données seraient dans <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>, prenons la fonction de transformation polynomiale suivante:</p>
<div class="math notranslate nohighlight">
\[\phi(x)=[x_{1}^2,\sqrt{2}x_{1}x_{2}, x_2^2]^T.\]</div>
<p>Il s’agit du noyau avec <span class="math notranslate nohighlight">\(c=0\)</span> et <span class="math notranslate nohighlight">\(n=2\)</span>.</p>
<p>Un autre noyau est le <em>noyau gaussien</em> définit comme:</p>
<div class="math notranslate nohighlight">
\[k(x_i, x_j)=\text{exp}\Big(-\frac{\lVert x_i-x_j\rVert^2_2}{2}\Big)\]</div>
<p>où les prédictions dépendent des points dans un voisinage calculé par notre noyau gaussien.</p>
<p>Il se trouve que nous avons:</p>
<div class="margin sidebar">
<p class="sidebar-title">Développement en série de Taylor</p>
<p>Rappelez-vous le développement en série de Taylor de la fonction exponentielle:</p>
<div class="math notranslate nohighlight">
\[e^x=\sum_{n=0}^\infty \frac{x^n}{n!}\]</div>
</div>
<div class="math notranslate nohighlight">
\[\text{exp}\Big(-\frac{\lVert x_i-x_j\rVert^2_2}{2}\Big)=C\sum_{l=0}^\infty \frac{\langle x_i, x_j\rangle^l}{l!}=C\sum_{l=0}^\infty \frac{k_{\text{poly}(l)}(x_i, x_j)}{l!},\]</div>
<p>où</p>
<div class="math notranslate nohighlight">
\[C=\text{exp}\Big(-\frac{1}{2}\lVert x_i\rVert_2^2\Big)\text{exp}\Big(-\frac{1}{2}\lVert x_j\rVert_2^2\Big).\]</div>
</div>
<div class="section" id="iv-visualisation-de-la-frontiere-de-decision">
<h2>IV. Visualisation de la frontière de décision<a class="headerlink" href="#iv-visualisation-de-la-frontiere-de-decision" title="Permalink to this headline">¶</a></h2>
<p>L’objectif de ce premier exercice est de visualiser la frontière de décision d’un SVM en jouant sur un exemple simple avec les noyaux offerts par la librairie <span class="math notranslate nohighlight">\(\texttt{scikit-learn}\)</span>.</p>
<p>La visualisation suivante permet d’observer la marge et notamment les vecteurs de supports.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/1_svm_15_0.png" src="../_images/1_svm_15_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sample_data</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">200</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">3</span> <span class="o">&lt;</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_data</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">clf</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(())</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(())</span>
    <span class="k">if</span> <span class="n">clf</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>

        <span class="n">XX</span><span class="p">,</span> <span class="n">YY</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mgrid</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="mi">1</span><span class="p">:</span><span class="mi">500</span><span class="n">j</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="mi">1</span><span class="p">:</span><span class="mi">500</span><span class="n">j</span><span class="p">]</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">XX</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">YY</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>

        <span class="c1"># Put the result into a color plot</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">XX</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">XX</span><span class="p">,</span> <span class="n">YY</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Paired</span><span class="p">,</span> <span class="n">shading</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_svm_18_0.png" src="../_images/1_svm_18_0.png" />
</div>
</div>
<p>Voici la visualisation d’un SVM avec un noyau linéaire (un produit scalaire <span class="math notranslate nohighlight">\(\langle\cdot,\cdot\rangle\)</span>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kernel</span> <span class="o">=</span> <span class="s1">&#39;linear&#39;</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">clf</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_svm_20_0.png" src="../_images/1_svm_20_0.png" />
</div>
</div>
<p>Comme vous avez pu le voir, si vous avez lu la section concernant le dual, le SVM permet de remplacer les comparaisons linéaires (i.e. le produit scalaire), par des comparaisons non-linéaires (i.e. produit scalaire dans un espace où les données sont projetées non linéairement). La librairie <span class="math notranslate nohighlight">\(\texttt{scikir-learn}\)</span> permet de jouer avec ce paramètre.</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Jouez avec plusieurs noyaux et observez la forme de la frontière de décision.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="c1">###############################################################</span>
</pre></div>
</div>
<div class="admonition-question admonition">
<p class="admonition-title">Question</p>
<p><strong>Comparez la robustesse d’un SVM par rapport à un 1NN relativement à la dimension du problème. Le SVM est-il plus ou moins robuste que le 1NN ?</strong></p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">sample_data</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">))</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">y</span><span class="o">*</span><span class="n">X</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">X</span> <span class="c1"># positive have mean mu and negative, -mu</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="o">-</span><span class="n">k</span><span class="p">))</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">noise</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>

<span class="n">scores_svm</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">redo</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">max_dim</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">first_dim</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">steps</span> <span class="o">=</span> <span class="mi">100</span>

<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">first_dim</span><span class="p">,</span> <span class="n">max_dim</span><span class="p">,</span> <span class="n">steps</span><span class="p">):</span>
    <span class="n">s_svm</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">redo</span><span class="p">):</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_data</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="n">d</span><span class="p">)</span>
        <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">sample_data</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="n">d</span><span class="p">)</span>
        
        <span class="n">model</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],)))</span>
        <span class="n">s_svm</span> <span class="o">+=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],)))</span><span class="o">/</span><span class="n">redo</span>
        
        <span class="n">c</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>
        <span class="n">c</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],)))</span>
        <span class="n">s</span> <span class="o">+=</span> <span class="n">c</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],)))</span><span class="o">/</span><span class="n">redo</span>
    <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">scores_svm</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s_svm</span><span class="p">)</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">first_dim</span><span class="p">,</span> <span class="n">max_dim</span><span class="p">,</span> <span class="n">steps</span><span class="p">)),</span> <span class="n">scores_svm</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;SVM&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">first_dim</span><span class="p">,</span> <span class="n">max_dim</span><span class="p">,</span> <span class="n">steps</span><span class="p">)),</span> <span class="n">scores</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;KNN&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_svm_25_0.png" src="../_images/1_svm_25_0.png" />
</div>
</div>
</div>
<div class="section" id="v-sur-de-vrais-donnees">
<h2>V. Sur de vrais données<a class="headerlink" href="#v-sur-de-vrais-donnees" title="Permalink to this headline">¶</a></h2>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Utilisez le SVM, la régression logistique ou encore le KNN pour résoudre les problèmes ci-dessous.</strong></p>
</div>
<div class="section" id="iris-dataset">
<h3>Iris dataset<a class="headerlink" href="#iris-dataset" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="c1">###############################################################</span>
</pre></div>
</div>
</div>
<div class="section" id="digits-dataset">
<h3>Digits dataset<a class="headerlink" href="#digits-dataset" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="n">digit</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_digits</span><span class="p">()</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">digit</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">digit</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray_r</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Label: &#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_svm_33_0.png" src="../_images/1_svm_33_0.png" />
</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="c1">###############################################################</span>
</pre></div>
</div>
</div>
<div class="section" id="wine-dataset">
<h3>Wine dataset<a class="headerlink" href="#wine-dataset" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="n">wine</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_wine</span><span class="p">()</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">wine</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">wine</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="c1">###############################################################</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="vi-majorant-de-generalisation-pour-le-svm">
<h2>VI. Majorant de généralisation pour le SVM<a class="headerlink" href="#vi-majorant-de-generalisation-pour-le-svm" title="Permalink to this headline">¶</a></h2>
<p>Nous avons vu dans la séquence traitant du modèle formel de l’apprentissage que nous pouvions majorer l’erreur attendue du minimiseur du risque empirique en espérance. Celle-ci dépendait de ce qu’on appelle la dimension VC. Pour un classifieur linéaire, la dimension VC est <span class="math notranslate nohighlight">\(d+1\)</span> (i.e. le nombre de paramètres), où <span class="math notranslate nohighlight">\(d\)</span> est la dimension de nos données.</p>
<p>Il se trouve que le minimiseur empirique est l’un des classifieurs linéaires de <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>. Le SVM ne fait pas n’importe quel choix. La solution sera le classifieur linéaire qui maximise la marge. Cela nous permet d’obtenir le majorant suivant.</p>
<div class="admonition-theoreme-majorant-de-generalisation-du-svm admonition">
<p class="admonition-title">Théorème (Majorant de généralisation du SVM)</p>
<p>Soit <span class="math notranslate nohighlight">\(\mathbb{P}\)</span> une distribution sur <span class="math notranslate nohighlight">\(\mathcal{X}\times\mathcal{Y}\)</span> avec <span class="math notranslate nohighlight">\(\mathcal{Y}=\{-1,+1\}\)</span> telle que <span class="math notranslate nohighlight">\(\exists w^\star\)</span>, <span class="math notranslate nohighlight">\(\lVert w^\star\rVert_2=1\)</span>, <span class="math notranslate nohighlight">\(y\langle w^\star, x\rangle+b\geq \gamma\)</span> <span class="math notranslate nohighlight">\(\forall x, y\)</span> et <span class="math notranslate nohighlight">\(\lVert x\rVert_2\leq \rho\)</span> <span class="math notranslate nohighlight">\(\forall x\)</span>. Notons <span class="math notranslate nohighlight">\(h_\text{SVM}\)</span> la solution du SVM. Soit <span class="math notranslate nohighlight">\(\delta&gt;0\)</span> Nous avons alors avec probabilité <span class="math notranslate nohighlight">\(1-\delta\)</span> :</p>
<div class="math notranslate nohighlight">
\[L(h_\text{SVM})\leq \sqrt{\frac{4(\rho/\gamma)^2}{m}}+\sqrt{\frac{2\log(2/\delta)}{m}}\]</div>
</div>
<p>L’élément important de ce théorème est que la capacité de généralisation du SVM ne dépend plus de la dimension des données mais de la marge qui sépare nos données. La preuve de ce théorème est disponible dans :</p>
<p><em>Shalev-Shwartz, Shai, et Shai Ben-David. Understanding Machine Learning: From Theory to Algorithms. Cambridge: Cambridge University Press, 2014.</em></p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./4_kernel_methods"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="0_propos_liminaire.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Les méthodes à noyaux</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../5_ensembles/0_propos_liminaire.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Les méthodes <em>ensemblistes</em></p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Servajean, Leveau & Chailan<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>
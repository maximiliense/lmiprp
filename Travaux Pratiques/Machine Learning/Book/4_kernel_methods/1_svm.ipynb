{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["#  Le SVM ou l\u2019hypoth\u00e8se max-margin \u2615\ufe0f\u2615\ufe0f\n", "\n", "**<span style='color:blue'> Objectifs de la s\u00e9quence</span>** ", "\n", "* Comprendre&nbsp;:\n", "    * la notion de marge (dans les donn\u00e9es),\n", "* \u00catre sensibilis\u00e9&nbsp;:\n", "    * \u00e0 la notion de noyaux pour les probl\u00e8mes non lin\u00e9aires,\n", "    * aux aspects de g\u00e9n\u00e9ralisation du choix *max-margin*.\n", "    \n", "\n\n ----"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## I. Introduction\n", "L'hypoth\u00e8se implicite derri\u00e8re le *max-margin* est qu'entre deux fronti\u00e8res de m\u00eame complexit\u00e9, la plus robuste aux perturbations (dans le sens o\u00f9 si on perturbe un \u00e9l\u00e9ment du jeu d'apprentissage, la probabilit\u00e9 qu'il change de classe est la plus faible) est la meilleure. L'id\u00e9e fait sens car on peut supposer que les \u00e9chantillons nouveaux peuvent \u00eatre vus comme des perturbations des \u00e9chantillons du jeu d'apprentissage. \n", "\n", "La fronti\u00e8re la plus robuste aux perturbations est celle qui maximise la distance entre le point le plus proche et elle-m\u00eame. C'est l'hypoth\u00e8se du *max-margin*.\n", "\n", "La figure ci-dessous illustre cette id\u00e9e. Nous avons deux fronti\u00e8res qui s\u00e9parent sans faire d'erreur les deux classes. Cependant, l'une de ces fronti\u00e8res semble mauvaise alors que l'autre maximise la marge."]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["remove-input"]}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "from sklearn import svm\n", "np.random.seed(0)\n", "\n", "dataset_size = 20\n", "\n", "X = np.r_[np.random.randn(dataset_size, 2) - [2, 2], np.random.randn(dataset_size, 2) + [2, 2]]\n", "Y = [0] * dataset_size + [1] * dataset_size\n", "\n", "# figure number\n", "fignum = 1\n", "\n", "kernel = 'linear'\n", "clf = svm.SVC(kernel=kernel)\n", "clf.fit(X, Y)\n", "\n", "# get the separating hyperplane\n", "w = clf.coef_[0]\n", "a = -w[0] / w[1]\n", "xx = np.linspace(-5, 5)\n", "yy = a * xx - (clf.intercept_[0]) / w[1]\n", "\n", "# plot the parallels to the separating hyperplane that pass through the\n", "# support vectors (margin away from hyperplane in direction\n", "# perpendicular to hyperplane). This is sqrt(1+a^2) away vertically in\n", "# 2-d.\n", "margin = 1 / np.sqrt(np.sum(clf.coef_ ** 2))\n", "yy_down = yy - np.sqrt(1 + a ** 2) * margin\n", "yy_up = yy + np.sqrt(1 + a ** 2) * margin\n", "\n", "# plot the line, the points, and the nearest vectors to the plane\n", "plt.figure(figsize=(14, 8))\n", "plt.clf()\n", "plt.plot(xx, yy, 'k-', label='The decision boundary that maximizes the margin')\n", "plt.plot(xx, yy_down, 'k--')\n", "plt.plot(xx, yy_up, 'k--')\n", "\n", "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,\n", "            facecolors='none', zorder=10, edgecolors='k')\n", "\n", "plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired,\n", "            edgecolors='k')\n", "\n", "plt.axis('tight')\n", "x_min = -4.8\n", "x_max = 4.2\n", "y_min = -6\n", "y_max = 6\n", "\n", "XX, YY = np.mgrid[x_min:x_max:500j, y_min:y_max:500j]\n", "Z = clf.predict(np.c_[XX.ravel(), YY.ravel()])\n", "\n", "# Put the result into a color plot\n", "Z = Z.reshape(XX.shape)\n", "plt.pcolormesh(XX, YY, Z, cmap=plt.cm.Paired, shading='auto')\n", "\n", "x = np.linspace(-5, 5, 100)\n", "y = -(clf.coef_[0, 0]+30.5) * x / clf.coef_[0, 1]+13\n", "plt.plot(x, y, label='A decision boundary with no error')\n", "\n", "plt.xlim(x_min, x_max)\n", "plt.ylim(y_min, y_max)\n", "plt.legend()\n", "\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## II. Un probl\u00e8me de classification lin\u00e9aire"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Le SVM est un classifieur lin\u00e9aire. Soit $\\mathcal{X}\\subset\\mathbb{R}^d$ nos variables d'entr\u00e9es et $\\mathcal{Y}=\\{-1,+1\\}$ l'espace de nos variables \u00e0 pr\u00e9dire. Un classifieur lin\u00e9aire s\u00e9pare les \u00e9l\u00e9ments de notre jeu de donn\u00e9es par un hyperplan. Comme vous avez pu le voir dans le TP pr\u00e9c\u00e9dent, un hyperplan d\u00e9crit par le vecteur normal $w$ est d\u00e9fini par les solutions de l'\u00e9quations suivantes :\n", "\n", "$$\\langle w, x\\rangle = 0$$\n", "\n", "\n", "Si le produit scalaire est positif, on dira que notre \u00e9chantillon $x$ appartient \u00e0 la classe positive et inversement.\n", "\n", "Un classifieur lin\u00e9aire peut donc \u00eatre d\u00e9crit de la mani\u00e8re suivante :\n", "\n", "\n", "$$\\begin{aligned}\n", "h_w:\\mathcal{X}&\\mapsto\\mathcal{Y}=\\{-1,+1\\}\\\\\n", "x&\\rightarrow \\text{sign}(\\langle w, x\\rangle)\n", "\\end{aligned}$$\n", "\n", "\n", "De la m\u00eame mani\u00e8re que pour les TPs pr\u00e9c\u00e9dents, on peut introduire la notion de biais en rajoutant une dimension de $1$ aux vecteurs $x$.\n", "\n", "**<span style='color:blue'> Petite question d'alg\u00e8bre</span>** ", "\n", "**Trouvez le projecteur orthogonal de $\\mathcal{X}$ sur l'hyperplan d\u00e9crit par le vecteur $w$, not\u00e9 $\\text{proj}_w(x)$. D\u00e9montrez que $\\forall x\\in\\mathcal{X},\\ \\text{proj}_w(x)\\in\\{z:\\langle w, z\\rangle=0\\}$ (autrement dit, d\u00e9montrez que la projection de $x$ sur la fronti\u00e8re est bien sur la fronti\u00e8re).**\n", "\n", "\n\n ----", "\n", "\n", "**<span style='color:blue'> Petite question d'alg\u00e8bre 2</span>** ", "\n", "**Montrer que $\\text{proj}_w^2=\\text{proj}_w$ (le carr\u00e9 est pris dans le sens de la composition). Cela vous semble-t-il logique ?**\n", "\n", "\n\n ----", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### A. Le primal"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Comme dit plus haut, on ne cherche pas n'importe quel hyperplan, mais bien celui qui rang la marge maximale. La marge est d\u00e9finie par la plus patite distance entre un point du jeu de donn\u00e9es et la fronti\u00e8re de d\u00e9cision.\n", "\n", "La distance d'un point \u00e0 la fronti\u00e8re est donn\u00e9e par $|\\langle x_i, w\\rangle|$ ($w$ unitaire). La quantit\u00e9 $y_i\\langle x_i, w\\rangle$ est positive et indique la distance \u00e0 la fronti\u00e8re si le point est bien class\u00e9 et donne la distance n\u00e9gative si le point est mal class\u00e9. Ainsi $\\min_{i\\leq m} y_i\\langle x_i, w\\rangle$ nous donne le point la plus petite distance (n\u00e9gative si mal class\u00e9).\n", "\n", "On souhaite donc trouver $w$ tel que cette distance soit maximale (i.e. le point le plus proche est le plus loin possible de la fronti\u00e8re) :\n", "\n", "$$\\hat{w}=\\text{argmax}_{w, \\lVert w\\rVert=1}\\min_{i\\leq m}y_i\\langle x_i, w\\rangle$$\n", "\n", "\n", "Il est possible de montrer que le vecteur $w=w_0/\\lVert w_0\\rVert$ tel que :\n", "\n", "$$w_0=\\text{argmin}_{w}\\lVert w\\rVert^2_2,\\ s.t. \\forall i\\leq m,\\ y_i\\langle x_i, w\\rangle \\geq 1$$\n", "\n", "\n", "est solution de ce probl\u00e8me de minimisation.\n", "\n", "\n", "**<span style='color:orange'> Preuve</span>** ", "\n", "Soit \n", "$\\boldsymbol{w}^\\star$ une solution du premier probl\u00e8me et soit $\\gamma^\\star=\\min_{i\\leq m}y_i\\langle\\boldsymbol{x_i}, \\boldsymbol{w^\\star}\\rangle$. $\\gamma^\\star$ est donc la distance du point le plus proche de l'hyperplan de vecteur normal $\\boldsymbol{w^\\star}$ \u00e0 ce dernier. Nous avons donc\n", "\n", "$$\\forall i\\leq m,\\ y_i\\langle \\boldsymbol{x_i}, \\boldsymbol{w^\\star}\\rangle \\geq \\gamma^\\star,$$\n", "\n", "et de mani\u00e8re totalement \u00e9quivalente~:\n", "\n", "$$\\forall i\\leq m,\\ y_i\\langle \\boldsymbol{x_i}, \\boldsymbol{w^\\star}/{\\gamma^\\star}\\rangle \\geq 1$$\n", "\n", "Notons ici que $\\boldsymbol{w^\\star}/{\\gamma^\\star}$ satisfait bien les contraintes du probl\u00e8mes d'optimisation quadratique (le second probl\u00e8me). Ainsi, il suffit de montrer qu'il n'existe pas de meilleure solution au probl\u00e8me quadratique, pour que nos deux probl\u00e8mes soient \u00e9quivalents.\n", "\n", "Soit $\\boldsymbol{w_0}$ la solution du probl\u00e8me quadratique avant normalisation. On a ainsi $\\lVert\\boldsymbol{w_0}\\rVert\\leq \\lVert\\boldsymbol{w^\\star/\\gamma^\\star}\\rVert =1/\\gamma^\\star$. L'in\u00e9galit\u00e9 vient du fait que $\\boldsymbol{w^\\star/\\gamma^\\star}$ satisfait les contraintes mais n'est peut-\u00eatre pas la meilleure solution. Soit $\\hat{\\boldsymbol{w}}=\\boldsymbol{w_0}/\\lVert\\boldsymbol{w_0}\\rVert$. Nous avons $\\forall i\\leq m$:\n", "\n", "\n", "$$y_i\\langle \\boldsymbol{x_i}, \\hat{\\boldsymbol{w}}\\rangle=\\frac{1}{\\lVert\\boldsymbol{w_0}\\rVert} y_i\\langle \\boldsymbol{x_i}, \\boldsymbol{w_0}\\rangle\\geq \\frac{1}{\\lVert\\boldsymbol{w_0}\\rVert}\\geq \\gamma^\\star.$$\n", "\n", "La premi\u00e8re in\u00e9galit\u00e9 vient de la contrainte du probl\u00e8me d'optimisation quadratique et la seconde que la solution du premier probl\u00e8me satisfait elle-m\u00eame ces contraintes. Puisque $\\lVert\\hat{\\boldsymbol{w}}\\rVert=1$ et d\u00e9finit une marge au moins aussi grande que $\\boldsymbol{w}^\\star$ qui par d\u00e9finition d\u00e9finit la plus grande marge, les solutions sont \u00e9quivalentes.\n", "\n", "\n\n ----", "\n", "\n", "Remarquez que cela fait penser \u00e0 la r\u00e9gularisation : parmi toutes les solutions possibles, on cherche celle de norme minimale."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### B. Le dual (optionnel)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Le probl\u00e8me d'optimisation si dessus est ce qu'on appelle un probl\u00e8me d'optimisation sous contrainte. Un tel probl\u00e8me est associ\u00e9 \u00e0 ce qu'on appelle un Lagrangien :\n", "\n", "$$\\mathcal{L}(w, \\alpha)=\\frac{1}{2}\\lVert w\\rVert_2^2+\\sum_{i=1}^m\\alpha_i(1-y_i\\langle x_i, w\\rangle),\\ \\alpha_j\\geq 0, j\\leq m$$\n", "\n", "Notons $g(w)=\\max_{\\alpha,\\alpha\\geq 0}\\mathcal{L}(w,\\alpha)$. On observe assez rapidement que $g(w)=\\infty$ si une des contraintes n'est pas satisfaite et vaut $\\lVert w\\rVert^2_2/2$ sinon.\n", "\n", "Ainsi, minimiser $g(w)$ revient \u00e0 minimiser la norme du vecteur $w$ en respectant les contraintes. C'est ce qu'on appelle le *primal* qu'on note $p^\\star$ :\n", "\n", "$$p^\\star=\\min_w\\max_{\\alpha,\\alpha\\geq 0}\\mathcal{L}(w,\\alpha)$$\n", "\n", "Le passage au dual permet d'inverser la minimisation et la maximisation. Il n'est pas \u00e9vident de montrer que les deux probl\u00e8mes sont \u00e9quivalents. C'est ici le cas et on note $d^\\star$ le dual (on parle donc de dualit\u00e9 forte) :\n", "\n", "$$d^\\star=\\max_{\\alpha,\\alpha\\geq 0}\\min_w\\mathcal{L}(w,\\alpha).$$\n", "\n", "**<span style='color:orange'> D\u00e9tails</span>** ", "\n", "o\u00f9 on peut v\u00e9rifier qu'on a n\u00e9cessairement $d^\\star\\leq p^\\star$. Ainsi, dans le cadre du primal, le probl\u00e8me est une minimisation, mais devient une maximisation dans le dual. Le gap de dualit\u00e9 donne l'\u00e9cart entre le primal et le dual. On parle de dualit\u00e9 faible si le gap est positif et de dualit\u00e9 forte s'il est nul. Il se trouve que si $\\lVert\\boldsymbol{w}\\rVert^2$ et $\\alpha_i(1 - y_i\\langle \\boldsymbol{x_i}, \\boldsymbol{w}\\rangle)$ sont convexes en $\\boldsymbol{w}$ et $\\alpha_i$ respectivement, et qu'il existe $\\boldsymbol{w}$ tel que $1 - y_i\\langle \\boldsymbol{x_i}, \\boldsymbol{w}\\rangle<0$ (in\u00e9galit\u00e9 stricte impliquant que $\\boldsymbol{w}$ est dans l'enveloppe affine de l'ensemble de faisabilit\u00e9), alors nous avons une dualit\u00e9 stricte (conditions de Slater). On obtient donc~:\n", "\n", "$$p^\\star=\\mathcal{L}(\\boldsymbol{w^\\star}, \\boldsymbol{\\alpha^\\star})=d^\\star.$$\n", "\n", "\n\n ----", "\n", "Quelques \u00e9l\u00e9ments de calculs plus loin (le minimum est un point critique, on annule les d\u00e9riv\u00e9es partielles, etc.), on reformule le dual de la mani\u00e8re suivante :\n", "\n", "$$\\max_{\\alpha,\\alpha\\geq 0}\\sum_i\\alpha_i-\\frac{1}{2}\\sum_i\\sum_j\\alpha_i\\alpha_jy_iy_j\\langle x_i, x_j\\rangle$$\n", "\n", "et\n", "\n", "$$w=\\frac{1}{2}\\sum_i \\alpha_iy_ix_i$$\n", "\n", "**<span style='color:orange'> D\u00e9tails</span>** ", "\n", "Concentrons-nous sur l'expression duale. Pour un $\\boldsymbol{\\alpha}$ donn\u00e9, $\\min_{\\boldsymbol{w}}\\mathcal{L}(\\boldsymbol{w}, \\boldsymbol{\\alpha})$ minimise $\\mathcal{L}$ relativement \u00e0 $\\boldsymbol{w}$. Ainsi, nous avons:\n", "\n", "$$0=\\frac{\\partial \\mathcal{L}}{\\partial\\boldsymbol{w}}=\\boldsymbol{w}-\\sum_i\\alpha_iy_i\\boldsymbol{x_i}\\Leftrightarrow\\boldsymbol{w}=\\sum_i\\alpha_iy_i\\boldsymbol{x_i}.$$\n", "\n", "\n", "Autrement dit, nous pouvons reformuler le probl\u00e8me dual de la mani\u00e8re suivante:\n", "\n", "$$\\max_{\\boldsymbol{\\alpha}, \\boldsymbol{\\alpha}\\geq \\boldsymbol{0}}\\sum_i\\alpha_i-\\frac{1}{2}\\sum_i\\sum_j\\alpha_i\\alpha_jy_iy_j\\langle \\boldsymbol{x_i}, \\boldsymbol{x_j}\\rangle.$$\n", "\n", "Ce probl\u00e8me d'optimisation est \u00e9quivalent au primal et consiste \u00e0 maximiser une quantit\u00e9 qui ne d\u00e9pend plus que du produit scalaire entre les points de notre jeu de donn\u00e9es (\u00e0 un signe pr\u00e8s s'ils appartiennent \u00e0 des classes oppos\u00e9es).\n", "\n", "\n\n ----", "\n", "Ainsi, notre mod\u00e8le pr\u00e9dictif prend la forme suivante :\n", "\n", "\n", "$$\\begin{aligned}\n", "h:\\mathcal{X}&\\mapsto\\mathcal{Y}\\\\\n", "x&\\rightarrow\\text{sign}(\\sum_i\\alpha_iy_i\\langle x_i, x\\rangle)\n", "\\end{aligned}$$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### C. Via une *surrogate loss*\n", "\n", "Nous avons vu que la minimisation empirique \u00e9tait difficile en g\u00e9n\u00e9rale. Pour cela, nous minimisions souvent une *surrogate loss*. Le SVM pr\u00e9sent\u00e9 \u00e0 l'instant peut \u00eatre reformul\u00e9 au travers d'une *surrogate loss* appel\u00e9e la *hinge loss*&nbsp;:\n", "\n", "$$\\ell^\\text{hinge}(z)=\\max(0;1-z).$$\n", "\n", "Le probl\u00e8me du SVM devient alors&nbsp;:\n", "\n", "$$\\mathcal{L}(\\omega)=\\frac{1}{m}\\sum_i \\ell (y_i\\langle \\omega, x_i\\rangle)+\\lambda\\lVert \\omega\\rVert_2$$\n", "\n", "o\u00f9 nous avons rajout\u00e9 une p\u00e9nalit\u00e9 \u00e0 la *hinge loss* afin de choisir le vecteur de param\u00e8tres de norme minimale."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## III. L'astuce du noyau (optionnel, suite du dual)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["L'astuce du noyau d\u00e9coule de la formation duale et notamment du fait que celle-ci n'est li\u00e9e aux donn\u00e9es qu'au travers du produit $y_iy_i$ et du produit scalaire $\\langle x_i, x_j\\rangle$. Si le probl\u00e8me de classification est non lin\u00e9aire, il est possible de passer par une transformation non lin\u00e9aire $\\phi:\\mathcal{X}\\mapsto\\mathcal{F}$ de nos donn\u00e9es d'entr\u00e9es. Le probl\u00e8me devient donc :\n", "\n", "$$\\max_{\\alpha,\\alpha\\geq 0}\\sum_i\\alpha_i-\\frac{1}{2}\\sum_i\\sum_j\\alpha_i\\alpha_jy_iy_j\\langle \\phi(x_i), \\phi(x_j)\\rangle$$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Sans rentrer dans les d\u00e9tails, l'astuce du noyau vient de l'existence de fonctions :\n", "\n", "$$\\begin{aligned}\n", "k:\\mathcal{X}\\times\\mathcal{X}&\\mapsto\\mathbb{R}\\\\\n", "x_i,x_j&\\rightarrow k(x_i,x_j)=\\langle\\phi(x_i),\\phi(x_j)\\rangle.\n", "\\end{aligned}$$\n", "\n", "\n", "Ces fonctions $k$ ne n\u00e9cessitent pas de projeter les $x$ dans un espace de plus grande dimension et permettent d'obtenir le r\u00e9sultat du produit scalaire directement dans l'espace d'origine. Ainsi, on peut m\u00eame calculer le produit scalaire dans des espaces de dimensions infinies.\n", "\n", "Par exemple le noyau :\n", "\n", "$$k(x_i, x_j)=(\\langle x_i, x_j\\rangle+c)^n,$$\n", "\n", "nous permet de faire une transformations polynomiales de degr\u00e9 $n$ directement dans l'espace d'origine ; on remarque que la puissance $n$ est calcul\u00e9e sur le r\u00e9sultat du produit scalaire ($+c$) qui est donc un sclaire. Dans le cas o\u00f9 nos donn\u00e9es seraient dans $\\mathbb{R}^2$, prenons la fonction de transformation polynomiale suivante:\n", "\n", "$$\\phi(x)=[x_{1}^2,\\sqrt{2}x_{1}x_{2}, x_2^2]^T.$$\n", "\n", "Il s'agit du noyau avec $c=0$ et $n=2$.\n", "\n", "Un autre noyau est le *noyau gaussien* d\u00e9finit comme:\n", "\n", "$$k(x_i, x_j)=\\text{exp}\\Big(-\\frac{\\lVert x_i-x_j\\rVert^2_2}{2}\\Big)$$\n", "\n", "o\u00f9 les pr\u00e9dictions d\u00e9pendent des points dans un voisinage calcul\u00e9 par notre noyau gaussien.\n", "\n", "Il se trouve que nous avons:\n", "\n", "\n", "\n", "$$\\text{exp}\\Big(-\\frac{\\lVert x_i-x_j\\rVert^2_2}{2}\\Big)=C\\sum_{l=0}^\\infty \\frac{\\langle x_i, x_j\\rangle^l}{l!}=C\\sum_{l=0}^\\infty \\frac{k_{\\text{poly}(l)}(x_i, x_j)}{l!},$$\n", "\n", "o\u00f9\n", "\n", "$$C=\\text{exp}\\Big(-\\frac{1}{2}\\lVert x_i\\rVert_2^2\\Big)\\text{exp}\\Big(-\\frac{1}{2}\\lVert x_j\\rVert_2^2\\Big).$$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## IV. Visualisation de la fronti\u00e8re de d\u00e9cision"]}, {"cell_type": "markdown", "metadata": {}, "source": ["L'objectif de ce premier exercice est de visualiser la fronti\u00e8re de d\u00e9cision d'un SVM en jouant sur un exemple simple avec les noyaux offerts par la librairie $\\texttt{scikit-learn}$.\n", "\n", "La visualisation suivante permet d'observer la marge et notamment les vecteurs de supports."]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["remove-input"]}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "from sklearn import svm\n", "np.random.seed(0)\n", "\n", "dataset_size = 20\n", "\n", "X = np.r_[np.random.randn(dataset_size, 2) - [2, 2], np.random.randn(dataset_size, 2) + [2, 2]]\n", "Y = [0] * dataset_size + [1] * dataset_size\n", "\n", "# figure number\n", "fignum = 1\n", "\n", "kernel = 'linear'\n", "clf = svm.SVC(kernel=kernel)\n", "clf.fit(X, Y)\n", "\n", "# get the separating hyperplane\n", "w = clf.coef_[0]\n", "a = -w[0] / w[1]\n", "xx = np.linspace(-5, 5)\n", "yy = a * xx - (clf.intercept_[0]) / w[1]\n", "\n", "# plot the parallels to the separating hyperplane that pass through the\n", "# support vectors (margin away from hyperplane in direction\n", "# perpendicular to hyperplane). This is sqrt(1+a^2) away vertically in\n", "# 2-d.\n", "margin = 1 / np.sqrt(np.sum(clf.coef_ ** 2))\n", "yy_down = yy - np.sqrt(1 + a ** 2) * margin\n", "yy_up = yy + np.sqrt(1 + a ** 2) * margin\n", "\n", "# plot the line, the points, and the nearest vectors to the plane\n", "plt.figure(figsize=(14, 8))\n", "plt.clf()\n", "plt.plot(xx, yy, 'k-')\n", "plt.plot(xx, yy_down, 'k--')\n", "plt.plot(xx, yy_up, 'k--')\n", "\n", "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,\n", "            facecolors='none', zorder=10, edgecolors='k')\n", "\n", "plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired,\n", "            edgecolors='k')\n", "\n", "plt.axis('tight')\n", "x_min = -4.8\n", "x_max = 4.2\n", "y_min = -6\n", "y_max = 6\n", "\n", "XX, YY = np.mgrid[x_min:x_max:500j, y_min:y_max:500j]\n", "Z = clf.predict(np.c_[XX.ravel(), YY.ravel()])\n", "\n", "# Put the result into a color plot\n", "Z = Z.reshape(XX.shape)\n", "plt.pcolormesh(XX, YY, Z, cmap=plt.cm.Paired, shading='auto')\n", "\n", "plt.xlim(x_min, x_max)\n", "plt.ylim(y_min, y_max)\n", "\n", "plt.xticks(())\n", "plt.yticks(())\n", "plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def sample_data(size=200):\n", "    X = np.random.uniform(-1, 1, size=(size, 2))\n", "    y = X[:, 0]**3 < X[:, 1]\n", "    return X, y\n", "X, y = sample_data()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def plot(X, y, clf=None):\n", "    plt.figure(figsize=(14, 8))\n", "    plt.xticks(())\n", "    plt.yticks(())\n", "    if clf is not None:\n", "\n", "        XX, YY = np.mgrid[-1:1:500j, -1:1:500j]\n", "        Z = clf.predict(np.c_[XX.ravel(), YY.ravel()])\n", "\n", "        # Put the result into a color plot\n", "        Z = Z.reshape(XX.shape)\n", "        plt.pcolormesh(XX, YY, Z, cmap=plt.cm.Paired, shading='auto')\n", "\n", "    plt.xlim(-1, 1)\n", "    plt.ylim(-1, 1)\n", "\n", "    plt.scatter(X[:, 0], X[:, 1], c=y)\n", "\n", "    plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot(X, y)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Voici la visualisation d'un SVM avec un noyau lin\u00e9aire (un produit scalaire $\\langle\\cdot,\\cdot\\rangle$)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["kernel = 'linear'\n", "clf = svm.SVC(kernel=kernel)\n", "clf.fit(X, y)\n", "\n", "plot(X, y, clf)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Comme vous avez pu le voir, si vous avez lu la section concernant le dual, le SVM permet de remplacer les comparaisons lin\u00e9aires (i.e. le produit scalaire), par des comparaisons non-lin\u00e9aires (i.e. produit scalaire dans un espace o\u00f9 les donn\u00e9es sont projet\u00e9es non lin\u00e9airement). La librairie $\\texttt{scikir-learn}$ permet de jouer avec ce param\u00e8tre."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "**Jouez avec plusieurs noyaux et observez la forme de la fronti\u00e8re de d\u00e9cision.**\n", "\n\n ----"]}, {"cell_type": "code", "metadata": {}, "source": ["####### Complete this part ######## or die ####################\n", "...\n", "...\n", "...\n", "...\n", "###############################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["**<span style='color:blue'> Question</span>** ", "\n", "**Comparez la robustesse d'un SVM par rapport \u00e0 un 1NN relativement \u00e0 la dimension du probl\u00e8me. Le SVM est-il plus ou moins robuste que le 1NN ?**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.svm import SVC\n", "from sklearn.neighbors import KNeighborsClassifier\n", "import numpy as np\n", "\n", "def sample_data(n, k=3, d=3, mu=1):\n", "    y = np.random.randint(0, 2, size=(n, 1))\n", "    \n", "    X = np.random.normal(mu, 1, size=(n, k))\n", "    X = y*X-(1-y)*X # positive have mean mu and negative, -mu\n", "    noise = np.random.normal(0, 1, size=(n, d-k))\n", "    X = np.concatenate([X, noise], axis=1)\n", "    \n", "    return X, y\n", "\n", "scores_svm = []\n", "scores = []\n", "redo = 5\n", "max_dim = 5000\n", "first_dim = 10\n", "steps = 100\n", "\n", "for d in range(first_dim, max_dim, steps):\n", "    s_svm = 0\n", "    s = 0\n", "    for _ in range(redo):\n", "        X, y = sample_data(100, d=d)\n", "        X_test, y_test = sample_data(200, d=d)\n", "        \n", "        model = SVC(kernel='linear', C=1.)\n", "        model.fit(X, y.reshape((y.shape[0],)))\n", "        s_svm += model.score(X_test, y_test.reshape((y_test.shape[0],)))/redo\n", "        \n", "        c = KNeighborsClassifier()\n", "        c.fit(X, y.reshape((y.shape[0],)))\n", "        s += c.score(X_test, y_test.reshape((y_test.shape[0],)))/redo\n", "    scores.append(s)\n", "    scores_svm.append(s_svm)\n", "    \n", "plt.plot(list(range(first_dim, max_dim, steps)), scores_svm, label='SVM')\n", "plt.plot(list(range(first_dim, max_dim, steps)), scores, label='KNN')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## V. Sur de vrais donn\u00e9es"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Utilisez le SVM, la r\u00e9gression logistique ou encore le KNN pour r\u00e9soudre les probl\u00e8mes ci-dessous.**\n", "\n", "\n\n ----"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Iris dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn import datasets\n", "iris = datasets.load_iris()\n", "\n", "X = iris['data']\n", "y = iris['target']"]}, {"cell_type": "code", "metadata": {}, "source": ["####### Complete this part ######## or die ####################\n", "...\n", "...\n", "...\n", "###############################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Digits dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn import datasets\n", "digit = datasets.load_digits()\n", "\n", "X = digit['data']\n", "Y = digit['target']"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(14, 8))\n", "fig.tight_layout()\n", "for i in range(10):\n", "    plt.subplot(2, 5, i+1)\n", "    img = X[i].reshape((8, 8))\n", "    plt.gca().set_axis_off()\n", "    \n", "    plt.imshow(img, cmap=plt.cm.gray_r, interpolation='nearest')\n", "    plt.title('Label: '+str(Y[i]))\n", "plt.show()"]}, {"cell_type": "code", "metadata": {}, "source": ["####### Complete this part ######## or die ####################\n", "...\n", "...\n", "...\n", "###############################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Wine dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn import datasets\n", "wine = datasets.load_wine()\n", "\n", "X = wine['data']\n", "Y = wine['target']"]}, {"cell_type": "code", "metadata": {}, "source": ["####### Complete this part ######## or die ####################\n", "...\n", "...\n", "...\n", "###############################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## VI. Majorant de g\u00e9n\u00e9ralisation pour le SVM\n", "\n", "Nous avons vu dans la s\u00e9quence traitant du mod\u00e8le formel de l'apprentissage que nous pouvions majorer l'erreur attendue du minimiseur du risque empirique en esp\u00e9rance. Celle-ci d\u00e9pendait de ce qu'on appelle la dimension VC. Pour un classifieur lin\u00e9aire, la dimension VC est $d+1$ (i.e. le nombre de param\u00e8tres), o\u00f9 $d$ est la dimension de nos donn\u00e9es.\n", "\n", "Il se trouve que le minimiseur empirique est l'un des classifieurs lin\u00e9aires de $\\mathcal{H}$. Le SVM ne fait pas n'importe quel choix. La solution sera le classifieur lin\u00e9aire qui maximise la marge. Cela nous permet d'obtenir le majorant suivant.\n", "\n", "**<span style='color:blue'> Th\u00e9or\u00e8me (Majorant de g\u00e9n\u00e9ralisation du SVM)</span>** ", "\n", "Soit $\\mathbb{P}$ une distribution sur $\\mathcal{X}\\times\\mathcal{Y}$ avec $\\mathcal{Y}=\\{-1,+1\\}$ telle que $\\exists w^\\star$, $\\lVert w^\\star\\rVert_2=1$, $y\\langle w^\\star, x\\rangle+b\\geq \\gamma$ $\\forall x, y$ et $\\lVert x\\rVert_2\\leq \\rho$ $\\forall x$. Notons $h_\\text{SVM}$ la solution du SVM. Soit $\\delta>0$ Nous avons alors avec probabilit\u00e9 $1-\\delta$&nbsp;:\n", "\n", "$$L(h_\\text{SVM})\\leq \\sqrt{\\frac{4(\\rho/\\gamma)^2}{m}}+\\sqrt{\\frac{2\\log(2/\\delta)}{m}}$$\n", "\n", "\n\n ----", "\n", "L'\u00e9l\u00e9ment important de ce th\u00e9or\u00e8me est que la capacit\u00e9 de g\u00e9n\u00e9ralisation du SVM ne d\u00e9pend plus de la dimension des donn\u00e9es mais de la marge qui s\u00e9pare nos donn\u00e9es. La preuve de ce th\u00e9or\u00e8me est disponible dans&nbsp;:\n", "\n", "*Shalev-Shwartz, Shai, et Shai Ben-David. Understanding Machine Learning: From Theory to Algorithms. Cambridge: Cambridge University Press, 2014.*\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.2"}}, "nbformat": 4, "nbformat_minor": 2}
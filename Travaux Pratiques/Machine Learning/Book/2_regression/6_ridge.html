
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Une analyse de la r√©gularisation Ridge ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è &#8212; Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="La classification" href="../3_classification/0_propos_liminaire.html" />
    <link rel="prev" title="Les moindres carr√©s via une d√©composition QR (et plus)‚òïÔ∏è" href="5_least_square_qr.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-72WWYCKNK6"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-72WWYCKNK6');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../0_requisite/rappels_python.html">
   Rappels de Python et Numpy
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../1_what_is_ml/0_propos_liminaire.html">
   <em>
    Machine Learning
   </em>
   , initiation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/1_introduction_ml.html">
     <em>
      Machine learning
     </em>
     et mal√©diction de la dimension
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/2_regression_and_classification_trees.html">
     Les arbres de r√©gression et de classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="0_propos_liminaire.html">
   La
   <em>
    r√©gression
   </em>
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="1_linear_regression.html">
     La r√©gression lin√©aire ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="2_optimization.html">
     L‚Äôoptimisation ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3_interpolation.html">
     Interpolation ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="4_algo_proximal_lasso.html">
     Sous-diff√©rentiel et le cas du Lasso ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5_least_square_qr.html">
     Les moindres carr√©s via une d√©composition QR (et plus)‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Une analyse de la r√©gularisation Ridge ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../3_classification/0_propos_liminaire.html">
   La
   <em>
    classification
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/1_logistic_regression.html">
     La r√©gression logistique ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/2_fonctions_proxy.html">
     Les fonctions de perte (loss function) ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/3_bayes_classifier.html">
     Le classifieur de Bayes ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/4_VC_theory.html">
     Un mod√®le formel de l‚Äôapprentissage ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è (üíÜ‚Äç‚ôÇÔ∏è)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../4_kernel_methods/0_propos_liminaire.html">
   Les m√©thodes √† noyaux
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../4_kernel_methods/1_svm.html">
     Le SVM ou l‚Äôhypoth√®se max-margin ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5_ensembles/0_propos_liminaire.html">
   Les m√©thodes
   <em>
    ensemblistes
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/1_ensembles.html">
     M√©thodes ensemblistes ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/2_bayesian_linear_regression.html">
     Bayesian linear regression ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../6_deeplearning/0_propos_liminaire.html">
   <em>
    deep learning
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/1_autodiff.html">
     La diff√©rentiation automatique et un d√©but de
     <em>
      deep learning
     </em>
     ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/2_filters_representation.html">
     Filtres et espace de repr√©sentation des r√©seaux de neurones ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/3_probabilities_calibration.html">
     Calibration des probabilit√©s et quelques notions ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/4_regularization_deep.html">
     R√©gularisation en
     <em>
      deep learning
     </em>
     ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/5_transfer_multitask.html">
     <em>
      Transfer learning
     </em>
     et apprentissage multi-t√¢ches ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/6_adversarial.html">
     Les attaques adversaires ‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../7_unsupervised/0_propos_liminaire.html">
   L‚Äôapprentissage non-supervis√©
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_unsupervised/1_principal_component_analysis.html">
     L‚ÄôAnalyse en Composantes Principales ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_unsupervised/2_em_gaussin_mixture_model.html">
     Mod√®le de M√©lange Gaussien et algorithme
     <em>
      Expectation-Maximization
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../8_set_prediction/0_propos_liminaire.html">
   Pr√©diction d‚Äôensembles
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/1_well_defined.html">
     Jeu d‚Äôapprentissage
     <em>
      set-valued
     </em>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/2_ill_defined.html">
     Jeu d‚Äôapprentissage uniquement multi-classes ‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../biblio.html">
   Bibliographie
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/2_regression/6_ridge.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/maximiliense/lmiprp"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/maximiliense/lmiprp/issues/new?title=Issue%20on%20page%20%2F2_regression/6_ridge.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/maximiliense/lmiprp/blob/master/Travaux Pratiques/Machine Learning/Book/2_regression/6_ridge.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#i-ridge-une-question-de-stabilite">
   I. Ridge, une question de stabilit√©
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ii-ridge-et-la-data-augmentation">
   II. Ridge et la
   <em>
    data augmentation
   </em>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-rajouter-une-base-de-mathbb-r-n-dans-x">
     a. Rajouter une base de
     <span class="math notranslate nohighlight">
      \(\mathbb{R}^n\)
     </span>
     dans
     <span class="math notranslate nohighlight">
      \(X\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-rajouter-un-bruit-centre-en-0-de-covariance-lambda-i-n">
     b. Rajouter un bruit centr√© en
     <span class="math notranslate nohighlight">
      \(0\)
     </span>
     de covariance
     <span class="math notranslate nohighlight">
      \(\lambda I_n\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#c-enfin-de-la-vraie-data-augmentation">
     c. Enfin de la vraie
     <em>
      data augmentation
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iii-ridge-et-le-dropout">
   III. Ridge et le
   <em>
    dropout
   </em>
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="une-analyse-de-la-regularisation-ridge">
<h1>Une analyse de la r√©gularisation Ridge ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è<a class="headerlink" href="#une-analyse-de-la-regularisation-ridge" title="Permalink to this headline">¬∂</a></h1>
<div class="admonition-objectifs-de-la-sequence admonition">
<p class="admonition-title">Objectifs de la s√©quence</p>
<ul class="simple">
<li><p>Formaliser la r√©gularisation Ridge,</p></li>
<li><p>Visualiser ses effets sur la stabilit√©,</p></li>
<li><p>Comprendre pourquoi le probl√®me est mieux pos√©,</p></li>
<li><p>Concevoir des liens avec d‚Äôautres types de r√©gularisations.</p></li>
</ul>
</div>
<p>La r√©gularisation Ridge, aussi appel√©e <em>weight decay</em> en <em>deep learning</em> ou encore p√©nalit√© <span class="math notranslate nohighlight">\(\ell_2\)</span>, poss√®de des propri√©t√©s fascinantes qui (1) expliquent ses performances et (2) permettent de la relier √† d‚Äôautres strat√©gies de r√©gularisation empiriques telles que la <em>data augmentation</em>, le ou encore le <em>dropout</em>.</p>
<p>Nous nous concentrerons dans cette s√©quence sur les probl√®mes lin√©aires.</p>
<div class="section" id="i-ridge-une-question-de-stabilite">
<h2>I. Ridge, une question de stabilit√©<a class="headerlink" href="#i-ridge-une-question-de-stabilite" title="Permalink to this headline">¬∂</a></h2>
<p>Soit <span class="math notranslate nohighlight">\(X\in\mathbb{R}^{p\times n}\)</span> la matrice de nos donn√©es avec <span class="math notranslate nohighlight">\(p\)</span> individus en dimension <span class="math notranslate nohighlight">\(n\)</span>. Soit <span class="math notranslate nohighlight">\(\boldsymbol{y}\in\mathbb{p}\)</span> nos labels. Notre objectif est de trouver un vecteur <span class="math notranslate nohighlight">\(\beta\in\mathbb{R}^n\)</span> tel que la quantit√© suivante soit minimis√©e :</p>
<div class="math notranslate nohighlight">
\[\hat{\beta}=\text{argmin}_{\beta\in\mathbb{R}^n}\lVert X\beta-\boldsymbol{y}\rVert_2^2.\]</div>
<p>Nous avons pu constater que si <span class="math notranslate nohighlight">\(X^TX\)</span> √©tait inversible, ce probl√®me poss√©dait une unique solution calculable en annulant le gradient :</p>
<div class="math notranslate nohighlight">
\[\hat{\beta}=(X^TX)^{-1}X^T\boldsymbol{y}.\]</div>
<p>Lorsque <span class="math notranslate nohighlight">\(X^TX\)</span> n‚Äôest pas inversible, une solution plus g√©n√©rale (de norme minimale) est :</p>
<div class="math notranslate nohighlight">
\[\hat{\beta}=X^\dagger \boldsymbol{y},\]</div>
<p>o√π <span class="math notranslate nohighlight">\(X^\dagger\)</span> est le pseudo-inverse de Moore-Penrose. Nous avons pu constater dans les s√©quences pr√©c√©dentes que le nombre d‚Äôindividus dans <span class="math notranslate nohighlight">\(X\)</span> affectait consid√©rablement notre estimation de <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> et cela √©tait visible au travers de l‚Äôeffet <em>double descente</em>. Nous illustrons ce ph√©nom√®ne √† nouveau ci-dessous. On g√©n√®re un jeu de donn√©es synth√©tique en jouant sur le nombre d‚Äôinvididus dans notre jeu d‚Äôapprentissage.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># dimension de l&#39;espace</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">50</span>
<span class="c1"># repetition de l&#39;experience</span>
<span class="n">redo</span> <span class="o">=</span> <span class="mi">50</span>

<span class="c1"># vrai parametre utilise pour generer nos donnees</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">mu</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">)]</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">)])</span>

<span class="n">test_size</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">test_size</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">test_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">errors</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">train_errors</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
    <span class="n">error</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">train_error</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">redo</span><span class="p">):</span>
        <span class="c1"># dataset construction</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">m</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        
        <span class="c1"># param estimation</span>
        <span class="n">beta_pinv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
        
        <span class="c1"># risk estimation</span>
        <span class="n">error</span> <span class="o">+=</span> <span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">beta_pinv</span><span class="p">)</span><span class="o">-</span><span class="n">y_test</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="n">test_size</span><span class="o">*</span><span class="n">redo</span><span class="p">)</span>
        <span class="n">train_error</span> <span class="o">+=</span> <span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta_pinv</span><span class="p">)</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="n">m</span><span class="o">*</span><span class="n">redo</span><span class="p">)</span>
    <span class="n">train_errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_error</span><span class="p">)</span>
    <span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_losses</span><span class="p">(</span><span class="n">errors</span><span class="p">,</span> <span class="n">train_errors</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">errors</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Risk estimation&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">d</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Dimension&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Test error&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Dataset size&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Error&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_errors</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">train_errors</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Train error&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">d</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Dimension&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Dataset size&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Error&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Train error&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_losses</span><span class="p">(</span><span class="n">errors</span><span class="p">,</span> <span class="n">train_errors</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/6_ridge_7_0.png" src="../_images/6_ridge_7_0.png" />
</div>
</div>
<p>Nous avons donc un double (sans jeu de mots) probl√®me ici. Lorsque <span class="math notranslate nohighlight">\(X^TX\)</span> n‚Äôest pas inversible, nous devons trouver son ‚Äúinverse‚Äù parmi une infinit√© de possibilit√©s¬†: c‚Äôest la r√©gion √† gauche de la ligne vertical en pointill√©. La solution <span class="math notranslate nohighlight">\(X^\dagger\boldsymbol{y}\)</span> est celle de norme minimale, la pr√©f√©rable, mais le probl√®me reste mal pos√©. Lorsque <span class="math notranslate nohighlight">\(X^TX\)</span> est presque ou √† peine (i.e. conditionnement, d√©terminant proche de <span class="math notranslate nohighlight">\(0\)</span>) inversible les performances de g√©n√©ralisation sont particuli√®rement mauvaises.</p>
<p>Lorsque le conditionnement est mauvais, de toutes petites perturbations de <span class="math notranslate nohighlight">\(X\)</span> affectent tr√®s significativement <span class="math notranslate nohighlight">\((X^TX)^{-1}\)</span>. L‚Äôexemple suivant illustre ce probl√®me. Consid√©rons le syst√®me d‚Äô√©quations suivant :</p>
<div class="math notranslate nohighlight">
\[Ax=y\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.98</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.01</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Soit une matrice A=</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Soit une matrice A=
 [[1.98 2.  ]
 [1.   1.01]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Nos donn√©es sont x=</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> et nos labels y=</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Nos donn√©es sont x=
 [[1]
 [1]] 
 et nos labels y=
 [[3.98]
 [2.01]]
</pre></div>
</div>
</div>
</div>
<p>Imaginons maintenant (comme c‚Äôest le cas en machine learning) que nous n‚Äôobservons que <span class="math notranslate nohighlight">\(A\)</span> (la matrice qui contiendrait nos donn√©es) et <span class="math notranslate nohighlight">\(y\)</span> (les labels). Nous cherchons √† d√©terminer l‚Äôinconnue <span class="math notranslate nohighlight">\(x\)</span>. C‚Äôest tr√®s facile car ici tout est d√©terministe et <span class="math notranslate nohighlight">\(A\)</span> est bien inversible (<span class="math notranslate nohighlight">\(\text{det}(A)\neq 0\)</span>) :</p>
<div class="math notranslate nohighlight">
\[\hat{x}=A^{-1}y\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">A</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Notre pr√©diction pour x, x_hat=</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Notre pr√©diction pour x, x_hat=
 [[1.]
 [1.]]
</pre></div>
</div>
</div>
</div>
<p>Tout marche tr√®s bien. Cependant, en pratique, nos donn√©es ne sont que rarement aussi propres. Imaginons que notre collecte soit ent√¢ch√©e d‚Äôun petit peu de bruit et arrondissons <span class="math notranslate nohighlight">\(y\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_round</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s1">&#39;Notre arrondi y_round=</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">y_round</span><span class="p">,</span> 
    <span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> On remarque que l</span><span class="se">\&#39;</span><span class="s1">√©cart avec y est tr√®s petit :</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">y_round</span><span class="o">-</span><span class="n">y</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Notre arrondi y_round=
 [[4.]
 [2.]] 
 On remarque que l&#39;√©cart avec y est tr√®s petit :
 [[ 0.02]
 [-0.01]]
</pre></div>
</div>
</div>
</div>
<p>Calculons maintenant l‚Äôinconnue <span class="math notranslate nohighlight">\(x\)</span> associ√©e √† ces nouvelles observations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">A</span><span class="p">),</span> <span class="n">y_round</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Notre pr√©diction pour x, x_hat=</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Notre pr√©diction pour x, x_hat=
 [[-200.]
 [ 200.]]
</pre></div>
</div>
</div>
</div>
<p>Le r√©sultat est catastrophiquement mauvais. Une perturbation ridicule des <span class="math notranslate nohighlight">\(y\)</span> a entra√Æn√© un effet de tr√®s grande ampleur sur nos estimateurs <span class="math notranslate nohighlight">\(\hat{x}\)</span>. Ce probl√®me qui appara√Æt dans l‚Äôeffet double descente et ici est li√© au conditionnement de la matrice <span class="math notranslate nohighlight">\(X^TX\)</span> (ici la matrice <span class="math notranslate nohighlight">\(A\)</span>). Son conditionnement est le ratio de sa plus grande valeur propre et de sa plus petite valeur propre. Elles sont ici :</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eigenvalue</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">A</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">eigen_ratio</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">eigenvalue</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">eigenvalue</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Le conditionnement donne :&#39;</span><span class="p">,</span> <span class="n">eigen_ratio</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Le conditionnement donne : -44702.49997761842
</pre></div>
</div>
</div>
</div>
<p>La valeur semble d√©raisonnablement grande lorsqu‚Äôon la compare aux valeurs de nos donn√©es. L‚Äôid√©e est ainsi d‚Äôaccro√Ætre les valeurs propres lors du calcul de l‚Äôinverse :</p>
<div class="math notranslate nohighlight">
\[X^+=(X^TX+\lambda I)^{-1}X^T,\]</div>
<p>o√π <span class="math notranslate nohighlight">\(X^+\)</span> est une pseudo-inverse r√©gularis√©e. En rajoutant <span class="math notranslate nohighlight">\(\lambda\)</span> dans la diagonale, le ratio de la plus grande valeur propre avec la plus petite devient :</p>
<div class="math notranslate nohighlight">
\[\frac{\gamma_{\text{max}}+\lambda}{\gamma_{\text{min}}+\lambda},\]</div>
<p>o√π <span class="math notranslate nohighlight">\(\gamma\)</span> sont les dites valeurs propres. Reprenons notre exemple et calculons notre pseudo-inverse r√©gularis√©e :</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Proposez ci-dessous l‚Äôinverse r√©gularis√©e de <span class="math notranslate nohighlight">\(A\)</span>.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">I</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">lambda_</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="c1"># on calcule notre r√©gularisation et la pseudo inverse</span>
<span class="c1">####### Complete this part ######## or die ####################</span>
<span class="o">...</span>
<span class="c1">###############################################################</span>

<span class="n">x_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A_inv_regularized</span><span class="p">,</span> <span class="n">y_round</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Notre pr√©diction pour x, x_hat=</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">)</span>
</pre></div>
</div>
<p>On retombe presque sur la valeur initiale qu‚Äôon aurait voulu obtenir. On constatera de plus que la valeur de la r√©gularisation ‚Äú<span class="math notranslate nohighlight">\(0.1\)</span>‚Äù est faible.</p>
<p>Nous avons certes pu obtenir une pseudo-inverse r√©gularis√©e, mais notre objectif reste de minimiser une erreur de pr√©diction d‚Äôun mod√®le. Quel est le lien entre ce pseudo-inverse r√©gularis√© et notre probl√®me intial ? Il se trouve que cela revient en fait √† r√©soudre le probl√®me d‚Äôoptimisation suivant :</p>
<div class="math notranslate nohighlight">
\[\text{argmin}_{\beta\in\mathbb{R}^n}\lVert X\beta-\boldsymbol{y}\rVert_2^2,\text{ s.t. }\lVert\beta\rVert_2\leq c,\]</div>
<p>pour une constante <span class="math notranslate nohighlight">\(c&gt;0\)</span>. En passant au Lagragien, on obtient ainsi :</p>
<div class="math notranslate nohighlight">
\[\text{argmin}_{\beta\in\mathbb{R}^n}\lVert X\beta-\boldsymbol{y}\rVert_2^2+\lambda\lVert\beta\rVert_2,\]</div>
<p>pour <span class="math notranslate nohighlight">\(\lambda&gt;0\)</span>.</p>
<p>Annulons le gradient afin de montrer que la solution de ce probl√®me est-celle donn√©e plus haut.</p>
<div class="math notranslate nohighlight">
\[\nabla (\lVert X\beta-\boldsymbol{y}\rVert_2^2+\lambda\lVert\beta\rVert_2)=2X^TX\beta-2X^Ty+2\lambda\beta=2(X^TX+\lambda I)\beta-2X^T \boldsymbol{y},\]</div>
<p>En annulant, on obtient :</p>
<div class="math notranslate nohighlight">
\[\hat{\beta}=(X^TX+\lambda I)^{-1}X^T\boldsymbol{y}=X^+\boldsymbol{y}.\]</div>
<p>Si <span class="math notranslate nohighlight">\(X^TX\)</span> n‚Äô√©tait pas inversible, <span class="math notranslate nohighlight">\(X^TX+\lambda I\)</span> l‚Äôest n√©cessairement. On retrouve bien la m√™me solution. C‚Äôest bien ce probl√®me des moindres carr√©s r√©gularis√©s qu‚Äôon appelle Ridge. Reprenons maintenant l‚Äôeffet double descente !</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>D√©montrez que <span class="math notranslate nohighlight">\(X^TX+\lambda I\)</span> est n√©cessairement inversible.</strong></p>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Proposez ci-dessous la pseudo-inverse r√©gularis√©e de <span class="math notranslate nohighlight">\(X\)</span>.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">d</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">redo</span> <span class="o">=</span> <span class="mi">50</span>

<span class="n">lambda_</span><span class="o">=</span><span class="mf">0.1</span>

<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">mu</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">)]</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">)])</span>

<span class="n">test_size</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">test_size</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">test_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">errors</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">train_errors</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
    <span class="n">error</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">train_error</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">redo</span><span class="p">):</span>
        <span class="c1"># dataset construction</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">m</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        
        <span class="c1"># param estimation</span>
        <span class="c1">####### Complete this part ######## or die ####################</span>
        <span class="n">pseudo_reg_inv</span> <span class="o">=</span> <span class="o">...</span>
        <span class="c1">###############################################################</span>
        
        <span class="n">beta_pinv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">pseudo_reg_inv</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        
        <span class="c1"># risk estimation</span>
        <span class="n">error</span> <span class="o">+=</span> <span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">beta_pinv</span><span class="p">)</span><span class="o">-</span><span class="n">y_test</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="n">test_size</span><span class="o">*</span><span class="n">redo</span><span class="p">)</span>
        <span class="n">train_error</span> <span class="o">+=</span> <span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta_pinv</span><span class="p">)</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="n">m</span><span class="o">*</span><span class="n">redo</span><span class="p">)</span>
    <span class="n">train_errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_error</span><span class="p">)</span>
    <span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plot_losses</span><span class="p">(</span><span class="n">errors</span><span class="p">,</span> <span class="n">train_errors</span><span class="p">)</span>
</pre></div>
</div>
<div class="tip admonition">
<p class="admonition-title">Lien avec la pseudo-inverse</p>
<p>Nous avons vu dans la s√©quence d√©di√©e √† la r√©gression lin√©aire la pseudo-inverse de Moore-Penrose. Il se trouve que cette pseudo-inverse est √©galement la limite suivante¬†:</p>
<div class="math notranslate nohighlight">
\[\lim_{\lambda\rightarrow 0}(X^TX+\lambda I)^{-1}X^T=X^+\]</div>
</div>
<p>La r√©gularisation en rendant notre probl√®me beaucoup plus stable a permis de gommer cet effet double descente. L‚Äôinversion de la matrice <span class="math notranslate nohighlight">\(X^TX+\lambda I\)</span> est beaucoup moins sensible aux bruits pr√©sents dans la matrice <span class="math notranslate nohighlight">\(X\)</span>. Notre probl√®me g√©n√©ralise mieux lorsque la taille du jeu de donn√©es est faible voire critique (proche de la dimension).</p>
</div>
<div class="section" id="ii-ridge-et-la-data-augmentation">
<h2>II. Ridge et la <em>data augmentation</em><a class="headerlink" href="#ii-ridge-et-la-data-augmentation" title="Permalink to this headline">¬∂</a></h2>
<p>Nous allons ici √©tudier la <em>data augmentation</em> du point de vue de Ridge.</p>
<p>Les probl√®mes √©voqu√©s pr√©c√©demment peuvent se r√©soudre assez facilement en augmentant le nombre d‚Äôinvididus dans notre jeu d‚Äôapprentissage. En effet, le mauvais conditionnement de <span class="math notranslate nohighlight">\(X^TX\)</span> vient du fait que les vecteurs lignes de <span class="math notranslate nohighlight">\(X\)</span> ne sont pas g√©n√©rateurs (au sens de l‚Äôalg√®bre) de l‚Äôespace <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> ou, s‚Äôils le sont, c‚Äôest via une composante orthogonale de taille tr√®s r√©duite. Plus le nombre d‚Äôindividus dans <span class="math notranslate nohighlight">\(X\)</span> devient important, plus il est probable qu‚Äôune forte composante dans chaque dimension de l‚Äôespace existe.  Cependant, lab√©liser de nouvelles donn√©es peut √™tre extr√™mement couteux en temps de travail. Une alternative est de cr√©er des donn√©es synth√©tiques. Nous allons √©tudiers quelques strat√©gies.</p>
<p>Consid√©rons dans un premier temps le probl√®me Ridge ci-dessous :</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># on est en dimension 10</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">noise</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">lambda_</span><span class="o">=</span><span class="mf">2.</span>

<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">mu</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)])</span>

<span class="n">p</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">redo</span> <span class="o">=</span> <span class="mi">100</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">estimated_ridge_beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">redo</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">noise</span>

    <span class="n">estimated_ridge_beta</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span><span class="o">+</span><span class="n">lambda_</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n</span><span class="p">)),</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
<span class="n">estimated_ridge_beta</span><span class="o">/=</span><span class="n">redo</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;E[beta_est]=</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">estimated_ridge_beta</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>E[beta_est]=
 [[ 1.40333084]
 [-1.5793589 ]
 [-1.90095049]
 [-0.02635138]
 [-1.68807477]
 [-1.23683893]
 [-1.24566193]
 [-0.22270638]
 [ 0.69463532]
 [ 0.98917743]]
</pre></div>
</div>
</div>
</div>
<div class="section" id="a-rajouter-une-base-de-mathbb-r-n-dans-x">
<h3>a. Rajouter une base de <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> dans <span class="math notranslate nohighlight">\(X\)</span><a class="headerlink" href="#a-rajouter-une-base-de-mathbb-r-n-dans-x" title="Permalink to this headline">¬∂</a></h3>
<p>Vu que le probl√®me vient de l‚Äôabsence ou du moins de la petite taille de certaines composantes de <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> construites √† partir des vecteurs ligne de <span class="math notranslate nohighlight">\(N\)</span>, rajoutons dans <span class="math notranslate nohighlight">\(X\)</span> des individus fictifs repr√©sentants ces composantes :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\tilde{X}=\begin{pmatrix}X\\
\sqrt{\lambda}I_n\end{pmatrix},\text{ et }\tilde{y}=\begin{pmatrix}\boldsymbol{y}\\0\end{pmatrix}.\end{split}\]</div>
<p>Ici, <span class="math notranslate nohighlight">\(\sqrt{\lambda}I_n\)</span> est une base orthogonale de <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> dont la norme de chacun des vecteurs est <span class="math notranslate nohighlight">\(\sqrt{\lambda}\)</span>. Le label associ√© √† ces nouvelles donn√©es est <span class="math notranslate nohighlight">\(0\)</span>. La solution du probl√®me des moindres carr√©s est donn√©e par :</p>
<div class="math notranslate nohighlight">
\[\tilde{\beta}=(\tilde{X}^T\tilde{X})^{-1}\tilde{X}^T\tilde{y}.\]</div>
<p>On v√©rifie assez rapidement que <span class="math notranslate nohighlight">\(\tilde{X}^T\tilde{X}\)</span> revient √† faire <span class="math notranslate nohighlight">\(X^TX+\lambda I_n\)</span> (les <span class="math notranslate nohighlight">\(\sqrt{\lambda}\)</span> n‚Äôapparaisse que dans la diagonale et au carr√©) et que <span class="math notranslate nohighlight">\(\tilde{X}^T\tilde{y}=X^T\boldsymbol{y}\)</span>. Ainsi, <span class="math notranslate nohighlight">\(\tilde{\beta}=(X^TX+\lambda I_n)^{-1}X^T\boldsymbol{y}\)</span> (les <span class="math notranslate nohighlight">\(\sqrt{\lambda}\)</span> sont associ√©s au label <span class="math notranslate nohighlight">\(0\)</span>). C‚Äôest la solution du probl√®me des moindres carr√©s avec une p√©nalit√© <span class="math notranslate nohighlight">\(\ell_2\)</span> (Ridge) !</p>
<p>La p√©nalit√© revient √† nous garantir que les vecteurs lignes de <span class="math notranslate nohighlight">\(X\)</span> ‚Äúremplissent‚Äù bien les directions de l‚Äôespace <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">estimated_ridge_beta_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">redo</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">noise</span>
    
    <span class="n">X_tilde</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">lambda_</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">y_tilde</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">y</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">))],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">estimated_ridge_beta_2</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_tilde</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X_tilde</span><span class="p">)),</span> <span class="n">X_tilde</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">y_tilde</span>
    <span class="p">)</span>
<span class="n">estimated_ridge_beta_2</span><span class="o">/=</span><span class="n">redo</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;E[beta_est_2]=</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">estimated_ridge_beta_2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Difference :</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">estimated_ridge_beta</span><span class="o">-</span><span class="n">estimated_ridge_beta_2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>E[beta_est_2]=
 [[ 1.39783418]
 [-1.57652603]
 [-1.8940816 ]
 [-0.04346469]
 [-1.67366613]
 [-1.23231064]
 [-1.24552105]
 [-0.25438219]
 [ 0.6996705 ]
 [ 0.99159801]]
Difference :
 [[0.00549667]
 [0.00283288]
 [0.00686888]
 [0.01711332]
 [0.01440864]
 [0.00452829]
 [0.00014088]
 [0.03167581]
 [0.00503518]
 [0.00242058]]
</pre></div>
</div>
</div>
</div>
<p>√áa marche !</p>
</div>
<div class="section" id="b-rajouter-un-bruit-centre-en-0-de-covariance-lambda-i-n">
<h3>b. Rajouter un bruit centr√© en <span class="math notranslate nohighlight">\(0\)</span> de covariance <span class="math notranslate nohighlight">\(\lambda I_n\)</span><a class="headerlink" href="#b-rajouter-un-bruit-centre-en-0-de-covariance-lambda-i-n" title="Permalink to this headline">¬∂</a></h3>
<p>Soit <span class="math notranslate nohighlight">\(m\in\mathbb{N}^\star\)</span>. Tirons <span class="math notranslate nohighlight">\(m\)</span> vecteurs <span class="math notranslate nohighlight">\(x_r^{(i)}\)</span> selon <span class="math notranslate nohighlight">\(\mathcal{N}(0, \lambda I_n)\)</span>. Soit la matrice <span class="math notranslate nohighlight">\(X_r\)</span> construite comme suit :</p>
<div class="math notranslate nohighlight">
\[\begin{split}X_r=\begin{pmatrix}{x_r^{(1)}}^T\\ \vdots\\{x_r^{(i)}}^T\\ \vdots\\{x_r^{(m)}}^T\end{pmatrix}.\end{split}\]</div>
<p>Calculons maintenant <span class="math notranslate nohighlight">\(\frac{1}{m}X_r^TX_r\)</span>. On remarque que dans la diagonales nous avons la somme de coordonn√©es au carr√©s : c‚Äôest un estimateur de la variance (i.e. <span class="math notranslate nohighlight">\(\lambda\)</span>). Dans les autres cellules c‚Äôest la somme du produit de variables al√©atoires ind√©pendantes, c‚Äôest un estimateur de <span class="math notranslate nohighlight">\(0\)</span>. Nous avons donc : <span class="math notranslate nohighlight">\(\frac{1}{m}X_r^TX_r=\lambda I_n\)</span>. Il suffit alors de rajouter ces √©chantillons √† notre matrice <span class="math notranslate nohighlight">\(X\)</span> originale en les pond√©rant avec <span class="math notranslate nohighlight">\(\frac{1}{m}\)</span> et en leur associant le label <span class="math notranslate nohighlight">\(0\)</span> pour obtenir une approximation de Ridge.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">estimated_ridge_beta_3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">mu_tilde</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>
<span class="n">cov_tilde</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="n">lambda_</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)])</span>
<span class="n">m</span><span class="o">=</span><span class="mi">1000</span>
    
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">redo</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">noise</span>
    

    <span class="n">X_r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mu_tilde</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov_tilde</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">m</span><span class="p">)</span><span class="o">/</span><span class="n">m</span>
    <span class="n">y_r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    
    <span class="n">X_tilde</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">X_r</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">y_tilde</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">y</span><span class="p">,</span> <span class="n">y_r</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">estimated_ridge_beta_3</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_tilde</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X_tilde</span><span class="p">)),</span> <span class="n">X_tilde</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">y_tilde</span>
    <span class="p">)</span>
<span class="n">estimated_ridge_beta_3</span><span class="o">/=</span><span class="n">redo</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;E[beta_est_3]=</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">estimated_ridge_beta_3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Difference :</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">estimated_ridge_beta</span><span class="o">-</span><span class="n">estimated_ridge_beta_3</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>E[beta_est_3]=
 [[ 1.44294264]
 [-1.63815991]
 [-1.92982362]
 [-0.02571489]
 [-1.73158815]
 [-1.2745355 ]
 [-1.29607342]
 [-0.2402727 ]
 [ 0.70792636]
 [ 0.98575574]]
Difference :
 [[0.0396118 ]
 [0.058801  ]
 [0.02887313]
 [0.00063648]
 [0.04351338]
 [0.03769658]
 [0.05041149]
 [0.01756632]
 [0.01329103]
 [0.00342169]]
</pre></div>
</div>
</div>
</div>
<p>√áa marche !</p>
</div>
<div class="section" id="c-enfin-de-la-vraie-data-augmentation">
<h3>c. Enfin de la vraie <em>data augmentation</em><a class="headerlink" href="#c-enfin-de-la-vraie-data-augmentation" title="Permalink to this headline">¬∂</a></h3>
<p>Intuitivement, on s‚Äôattend √† ce qu‚Äôun nouveau <span class="math notranslate nohighlight">\(x_{\text{new}}\in\mathbb{R}^n\)</span> soit en r√©alit√© proche d‚Äôun <span class="math notranslate nohighlight">\(x\)</span>, vecteur colonne de <span class="math notranslate nohighlight">\(X\)</span> et que son label soit √©galement proche. L‚Äôid√©e va √™tre de virtuellement augmenter le nombre d‚Äôindividus dans <span class="math notranslate nohighlight">\(X, y\)</span> en perturbant les individus existants. Plus rigoureusement, cr√©eons <span class="math notranslate nohighlight">\(m\)</span> copies de nos vecteurs lignes de <span class="math notranslate nohighlight">\(X\)</span> de la mani√®re suivante : <span class="math notranslate nohighlight">\(x_{ij}^\prime=x_i+\epsilon_{ij}, i=1\ldots p, j=1\ldots m\)</span> o√π le bruit est construit de la mani√®re suivante : <span class="math notranslate nohighlight">\(\epsilon_{ij}\sim\mathcal{N}(0, \frac{\lambda}{m}I_n)\)</span>. Les labels sont quant √† eux conserv√©s.</p>
<p>Notre nouvelle matrice <span class="math notranslate nohighlight">\(\tilde{X}\)</span> ne contient cette fois-ci QUE les versions perturb√©es de nos donn√©es. On v√©rifiera qu‚Äôon obtient :</p>
<div class="math notranslate nohighlight">
\[\sum_i\sum_j x_{ij}^\prime {x_{ij}^\prime}^T=m(X^TX+\lambda I_n).\]</div>
</div>
</div>
<div class="section" id="iii-ridge-et-le-dropout">
<h2>III. Ridge et le <em>dropout</em><a class="headerlink" href="#iii-ridge-et-le-dropout" title="Permalink to this headline">¬∂</a></h2>
<p>Comme nous l‚Äôavons d√©j√† vu, une mani√®re de faire de la r√©gularisation est ce qu‚Äôon appelle le <em>dropout</em>. √âtudions cela dans le cas de l‚Äôapprentissage d‚Äôun mod√®le lin√©aire. Soit <span class="math notranslate nohighlight">\(\mathcal{X}\subseteq\mathbb{R}^d\)</span> et <span class="math notranslate nohighlight">\(\mathcal{Y}\subseteq\mathbb{R}\)</span>. L‚Äôobjectif est donc de construire un mod√®le lin√©aire de <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> dans <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> de la forme : <span class="math notranslate nohighlight">\(h:x\mapsto \langle \omega, x\rangle\)</span>, <span class="math notranslate nohighlight">\(\omega\in\mathbb{R}^d\)</span>, qui minimise l‚Äôerreur quadratique :</p>
<div class="math notranslate nohighlight">
\[L(h)=\mathbb{E}\big[(Y-h(X))^2\big],\]</div>
<p>qu‚Äôon estime via un dataset <span class="math notranslate nohighlight">\(S_n\)</span> :</p>
<div class="math notranslate nohighlight">
\[L_n(h)=\frac{1}{n}\sum_i (y_i-h(x_i))^2.\]</div>
<p>L‚Äôid√©e du <em>dropout</em> au moment de l‚Äôoptimisation (i.e. de l‚Äôapprentissage), est de consid√©rer la fonction <span class="math notranslate nohighlight">\(\phi\big(\frac{\delta\odot z}{1-p}\big)\)</span> o√π <span class="math notranslate nohighlight">\(\delta_i\)</span> est une variable al√©atoire qui suit une loi de Bernoulli de param√®tre <span class="math notranslate nohighlight">\(1-p\)</span> et est √©chantillon√©e pour chaque sample √† chaque it√©ration de l‚Äôoptimiseur et <span class="math notranslate nohighlight">\(\odot\)</span> est la multiplication √©l√©ment par √©l√©ment. Le param√®tre <span class="math notranslate nohighlight">\(p\)</span> donne l‚Äôimportance du <em>dropout</em>. √Ä <span class="math notranslate nohighlight">\(0\)</span>, il n‚Äôy a pas de r√©gularisation, proche de <span class="math notranslate nohighlight">\(1\)</span> l‚Äôapprentissage ne fonctionne pas. Dans le cas d‚Äôune op√©ration lin√©aire le biais n‚Äôest g√©n√©ralement pas touch√© par le <em>dropout</em> (tout comme pour la p√©nalit√© <span class="math notranslate nohighlight">\(\ell_2\)</span>). Nous omettrons ici les consid√©rations relatives √† ce dernier. Le facteur <span class="math notranslate nohighlight">\(1/(1-p)\)</span> implique que bien que certaines dimensions tombent √† <span class="math notranslate nohighlight">\(0\)</span> notre vecteur conserve la m√™me norme en esp√©rance. Plus rigoureusement, nous avons :</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\big[\delta_j/(1-p)\big]=1.\]</div>
<p>Le <em>dropout</em> est bien entendu ‚Äúd√©sactiv√©‚Äù une fois l‚Äôapprentissage termin√© et le mod√®le utilis√© pour faire des pr√©dictions. √âtudions l‚Äôesp√©rance du gradient de notre mod√®le :</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\Big[\frac{\partial L_n(\omega)}{\partial \omega}\Big]=-X^Ty+X^TX\beta+\frac{p}{1-p}D\beta,\]</div>
<p>o√π on a identifi√© la fonction <span class="math notranslate nohighlight">\(h\)</span> √† ses param√®tres <span class="math notranslate nohighlight">\(\omega\)</span> et o√π <span class="math notranslate nohighlight">\(D=\text{diag}(\lVert x_1\rVert_2^2, \ldots, \lVert x_n\rVert_2^2)\)</span>.</p>
<p>En annulant le gradient en esp√©rance, on obtient :</p>
<div class="math notranslate nohighlight">
\[\hat{\beta}=\big(X^TX+\frac{p}{1-p}D\big)^{-1}X^ty\]</div>
<p>qui peut √™tre vu comme une version g√©n√©ralis√©e de Ridge.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./2_regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="5_least_square_qr.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Les moindres carr√©s via une d√©composition QR (et plus)‚òïÔ∏è</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../3_classification/0_propos_liminaire.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">La <em>classification</em></p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Servajean, Leveau & Chailan<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Les moindres carrés via une décomposition QR (et plus)☕️ &#8212; Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Une analyse de la régularisation Ridge ☕️☕️☕️" href="6_ridge.html" />
    <link rel="prev" title="Sous-différentiel et le cas du Lasso ☕️☕️☕️" href="4_algo_proximal_lasso.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-72WWYCKNK6"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-72WWYCKNK6');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../0_requisite/rappels_python.html">
   Rappels de Python et Numpy
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../1_what_is_ml/0_propos_liminaire.html">
   <em>
    Machine Learning
   </em>
   , initiation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/1_introduction_ml.html">
     <em>
      Machine learning
     </em>
     et malédiction de la dimension
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/2_regression_and_classification_trees.html">
     Les arbres de régression et de classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="0_propos_liminaire.html">
   La
   <em>
    régression
   </em>
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="1_linear_regression.html">
     La régression linéaire ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="2_optimization.html">
     L’optimisation ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3_interpolation.html">
     Interpolation ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="4_algo_proximal_lasso.html">
     Sous-différentiel et le cas du Lasso ☕️☕️☕️
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Les moindres carrés via une décomposition QR (et plus)☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="6_ridge.html">
     Une analyse de la régularisation Ridge ☕️☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../3_classification/0_propos_liminaire.html">
   La
   <em>
    classification
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/1_logistic_regression.html">
     La régression logistique ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/2_fonctions_proxy.html">
     Les fonctions de perte (loss function) ☕️☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/3_bayes_classifier.html">
     Le classifieur de Bayes ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/4_VC_theory.html">
     Un modèle formel de l’apprentissage ☕️☕️☕️☕️ (💆‍♂️)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../4_kernel_methods/0_propos_liminaire.html">
   Les méthodes à noyaux
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../4_kernel_methods/1_svm.html">
     Le SVM ou l’hypothèse max-margin ☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5_ensembles/0_propos_liminaire.html">
   Les méthodes
   <em>
    ensemblistes
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/1_ensembles.html">
     Méthodes ensemblistes ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/2_bayesian_linear_regression.html">
     Bayesian linear regression ☕️☕️☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../6_deeplearning/0_propos_liminaire.html">
   <em>
    deep learning
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/1_autodiff.html">
     La différentiation automatique et un début de
     <em>
      deep learning
     </em>
     ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/2_filters_representation.html">
     Filtres et espace de représentation des réseaux de neurones ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/3_probabilities_calibration.html">
     Calibration des probabilités et quelques notions ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/4_regularization_deep.html">
     Régularisation en
     <em>
      deep learning
     </em>
     ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/5_transfer_multitask.html">
     <em>
      Transfer learning
     </em>
     et apprentissage multi-tâches ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/6_adversarial.html">
     Les attaques adversaires ☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../7_unsupervised/0_propos_liminaire.html">
   L’apprentissage non-supervisé
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_unsupervised/1_principal_component_analysis.html">
     L’Analyse en Composantes Principales ☕️☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_unsupervised/2_em_gaussin_mixture_model.html">
     Modèle de Mélange Gaussien et algorithme
     <em>
      Expectation-Maximization
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../8_set_prediction/0_propos_liminaire.html">
   Prédiction d’ensembles
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/1_well_defined.html">
     Jeu d’apprentissage
     <em>
      set-valued
     </em>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/2_ill_defined.html">
     Jeu d’apprentissage uniquement multi-classes ☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../biblio.html">
   Bibliographie
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/2_regression/5_least_square_qr.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/maximiliense/lmiprp"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/maximiliense/lmiprp/issues/new?title=Issue%20on%20page%20%2F2_regression/5_least_square_qr.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/maximiliense/lmiprp/blob/master/Travaux Pratiques/Machine Learning/Book/2_regression/5_least_square_qr.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#i-introduction">
   I. Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ii-decomposition-qr">
   II. Décomposition QR
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-procede-de-gram-schmidt">
     A. Procédé de Gram-Schmidt
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-decomposition-qr">
     B. Décomposition QR
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#c-exemple">
     C. Exemple
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iii-application-aux-moindres-carres">
   III. Application aux moindres carrés
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-notre-estimation-via-une-decomposition-qr">
     A. Notre estimation via une décomposition QR
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-exercice">
     B. Exercice
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iv-autres-decompositions-et-conclusion">
   IV. Autres décompositions et conclusion
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="les-moindres-carres-via-une-decomposition-qr-et-plus">
<h1>Les moindres carrés via une décomposition QR (et plus)☕️<a class="headerlink" href="#les-moindres-carres-via-une-decomposition-qr-et-plus" title="Permalink to this headline">¶</a></h1>
<div class="admonition-objectifs-de-la-sequence admonition">
<p class="admonition-title">Objectifs de la séquence</p>
<ul class="simple">
<li><p>Comprendre les équations normales via une décomposition QR,</p></li>
<li><p>Être sensibilisé aux problèmes liés aux approximations numériques et à leur résolution.</p></li>
</ul>
</div>
<div class="section" id="i-introduction">
<h2>I. Introduction<a class="headerlink" href="#i-introduction" title="Permalink to this headline">¶</a></h2>
<p>En <em>data science</em>, nous ne faisons pas seulement face à des modèles mathématiques que nous souhaitons programmer. Nous devons aussi tenir compte du fait que les nombres que nous manipulons ont une représentation sur la machine qui les stocke. De manière paradigmatique, considérons la fonction suivante :</p>
<div class="math notranslate nohighlight">
\[f(x)=\text{ln}\big(\text{exp}(x)\big)=x.\]</div>
<p>Les deux formulations sont parfaitement indentiques. Observons cela via <span class="math notranslate nohighlight">\(\texttt{numpy}\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="n">y_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">y_2</span> <span class="o">=</span> <span class="n">x</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_1</span><span class="p">,</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$ln(exp(x))$&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;ipython-input-2-4fd55f099922&gt;:2: RuntimeWarning: overflow encountered in exp
  y_1 = np.log(np.exp(x))
</pre></div>
</div>
<img alt="../_images/5_least_square_qr_2_1.png" src="../_images/5_least_square_qr_2_1.png" />
</div>
</div>
<p>Nous rencontrons une erreur ! Cela vient évidemment du fait que le calcul de l’exponentielle lorsque <span class="math notranslate nohighlight">\(x\)</span> devient trop grand induit un <span class="math notranslate nohighlight">\(\texttt{overflow}\)</span> que le logarithme ne peut plus interpréter. En observant <span class="math notranslate nohighlight">\(\text{ln}\big(\text{exp}(x)\big)=x\)</span>, nous avons en quelque sorte utilisé une astuce (clairement triviale ici) mathématique nous permettant d’obtenir notre résultat malgré tout. De manière similaire, nous utilisons souvent en <em>machine learning</em> la fonction <em>softmax</em>:</p>
<div class="math notranslate nohighlight">
\[\text{softmax}(x)_j=\frac{e^{x_j}}{\sum_i e^{x_i}},\]</div>
<p>notamment comme fonction de lien en <em>deep learning</em> ou en <em>régression logistique</em> afin de transformer notre vecteur de <em>logit</em> en vecteur de probabilités. Implémentons cette fonction.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Softmax [10, 12]:&#39;</span><span class="p">,</span> <span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Softmax [-10, 12]:&#39;</span><span class="p">,</span> <span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">748</span><span class="p">,</span> <span class="mi">750</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Softmax [748, 750]:&#39;</span><span class="p">,</span> <span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Softmax [10, 12]: [0.11920292 0.88079708]
Softmax [-10, 12]: [2.78946809e-10 1.00000000e+00]
Softmax [748, 750]: [nan nan]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;ipython-input-3-cdc4f3bc964f&gt;:4: RuntimeWarning: overflow encountered in exp
  return np.exp(x)/np.exp(x).sum()
&lt;ipython-input-3-cdc4f3bc964f&gt;:4: RuntimeWarning: invalid value encountered in true_divide
  return np.exp(x)/np.exp(x).sum()
</pre></div>
</div>
</div>
</div>
<p>Le calcul de l’exponentielle de <span class="math notranslate nohighlight">\(750\)</span> entraîne à nouveau un <span class="math notranslate nohighlight">\(\texttt{overflow}\)</span> et nous empêche de calculer le <em>softmax</em>. La stratégie consiste à réduire la taille du plus grand nombre que notre exponentielle devra calculer de la manière suivante :</p>
<div class="math notranslate nohighlight">
\[\text{softmax}(x)_j=\frac{e^{x_j}}{\sum_i e^{x_i}}=\frac{e^{x_j}}{\sum_i e^{x_i}}\frac{e^{-\text{max}(x)}}{e^{-\text{max}(x)}}=\frac{e^{x_j-\text{max}(x)}}{\sum_i e^{x_i-\text{max}(x)}}.\]</div>
<p>Réimplémentons notre <em>softmax</em>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Softmax [10, 12]:&#39;</span><span class="p">,</span> <span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Softmax [-10, 12]:&#39;</span><span class="p">,</span> <span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">748</span><span class="p">,</span> <span class="mi">750</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Softmax [748, 750]:&#39;</span><span class="p">,</span> <span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Softmax [10, 12]: [0.11920292 0.88079708]
Softmax [-10, 12]: [2.78946809e-10 1.00000000e+00]
Softmax [748, 750]: [0.11920292 0.88079708]
</pre></div>
</div>
</div>
</div>
<p>De très nombreuses astuces de ce type existent et sont implémentées dans les différents <em>frameworks</em>.</p>
<p>C’est exactement cela que nous voulons faire avec les moindres carrés via une décomposition QR. Calculer notre estimateur des moindres carrés est beaucoup plus stable après une décomposition QR que dans sa formulation telle que nous l’avons vue.</p>
</div>
<div class="section" id="ii-decomposition-qr">
<h2>II. Décomposition QR<a class="headerlink" href="#ii-decomposition-qr" title="Permalink to this headline">¶</a></h2>
<p>Soit une matrice <span class="math notranslate nohighlight">\(A\in\mathbb{R}^{m\times n}\)</span>. La décomposition <span class="math notranslate nohighlight">\(QR\)</span> de la matrice <span class="math notranslate nohighlight">\(A\)</span> est :</p>
<div class="math notranslate nohighlight">
\[A=QR,\]</div>
<p>où <span class="math notranslate nohighlight">\(Q\)</span> est une matrice orthogonale (par colonne si rectangulaire) et <span class="math notranslate nohighlight">\(R\)</span> une matrice diagonale supérieure. On retrouve d’ailleurs parfois l’appellation “décomposition <span class="math notranslate nohighlight">\(QU\)</span>” où <span class="math notranslate nohighlight">\(U\)</span> signifie <em>Upper triangular</em>. Une matrice orthogonale par colonne implique <span class="math notranslate nohighlight">\(Q^TQ=I\)</span> et donc <span class="math notranslate nohighlight">\(m\geq n\)</span>. En effet, si une famille de vecteurs est plus grande que la dimension de l’espace, alors elle est forcément liée.</p>
<div class="section" id="a-procede-de-gram-schmidt">
<h3>A. Procédé de Gram-Schmidt<a class="headerlink" href="#a-procede-de-gram-schmidt" title="Permalink to this headline">¶</a></h3>
<p>Il existe plusieurs stratégies permettant de réaliser cette décomposition et nous utiliserons celle basée sur le procédé (ou algorithme) de Gram-Schmidt. Soit <span class="math notranslate nohighlight">\(F=\{u_1, ..., u_n\}\)</span> une famille de vecteurs libres. Le procédé de Gram-Schmidt a pour objectif de construire une base orthonormale <span class="math notranslate nohighlight">\(B=\{e_1, ..., e_n\}\)</span> à partir de <span class="math notranslate nohighlight">\(F\)</span>.</p>
<p>L’opération de base du procédé de Gram-Schmidt est l’opérateur de projection :</p>
<div class="math notranslate nohighlight">
\[\textrm{proj}_u(v)=\Pi_u(v)=\frac{\langle v, u\rangle }{\langle u, u\rangle}u,\]</div>
<p>où le vecteur <span class="math notranslate nohighlight">\(v\)</span> est projeté orthogonalement sur <span class="math notranslate nohighlight">\(u\)</span>.</p>
<p>Le procédé itère sur l’ensemble des vecteurs de la famille <span class="math notranslate nohighlight">\(F\)</span> de la manière suivante.</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(v_1=u_1\)</span> et <span class="math notranslate nohighlight">\(e_1=v_1/\lVert v_1\rVert_2\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(v_2=u_2-\Pi_{e_1}(u_2)\)</span> et <span class="math notranslate nohighlight">\(e_2=v_2/\lVert v_2\rVert_2\)</span>,</p></li>
<li><p>…</p></li>
<li><p><span class="math notranslate nohighlight">\(v_n=u_n-\sum_{i=1}^{n-1}\Pi_{e_i}(u_n)\)</span> et <span class="math notranslate nohighlight">\(e_n=v_n/\lVert v_n\rVert_2\)</span>.</p></li>
</ol>
<p>La famille <span class="math notranslate nohighlight">\(\{e_1, ..., e_n\}\)</span> ainsi construite est une base orthonormée et engendre le même sous-espace vectoriel que la famille <span class="math notranslate nohighlight">\(F\)</span>.</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Complétez le code ci-dessous afin d’implémenter le procédé de Gram-Schmidt.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">projector</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
    <span class="c1">####### Complete this part ######## or die ####################</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="o">...</span>
    <span class="c1">###############################################################</span>

<span class="k">def</span> <span class="nf">gram_schmidt</span><span class="p">(</span><span class="n">matrix</span><span class="p">):</span>
    <span class="c1">####### Complete this part ######## or die ####################</span>
    <span class="o">...</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="o">...</span>
    <span class="c1">###############################################################</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">Q</span> <span class="o">=</span> <span class="n">gram_schmidt</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Une matrice identite :</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Q</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">Q</span><span class="p">))</span>

</pre></div>
</div>
</div>
<div class="section" id="b-decomposition-qr">
<h3>B. Décomposition QR<a class="headerlink" href="#b-decomposition-qr" title="Permalink to this headline">¶</a></h3>
<p>Soit <span class="math notranslate nohighlight">\(A\in\mathbb{R}^{m\times n}\)</span> telle que les vecteurs colonnes sont libres. Notons <span class="math notranslate nohighlight">\(\{a_1, ..., a_n\}\)</span> l’ensemble des vecteurs colonnes. Soit <span class="math notranslate nohighlight">\(\{e_1, ..., e_n\}\)</span> une base orthonormale résultant du procédé de Gram-Schmidt appliqué aux vecteurs colonnes de <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>De manière assez directe, on observe que :</p>
<div class="math notranslate nohighlight">
\[a_1=\langle a_1, e_1\rangle e_1.\]</div>
<p>Dit autrement, <span class="math notranslate nohighlight">\(a_1\)</span> est un vecteur co-linéaire à <span class="math notranslate nohighlight">\(e_1\)</span> dont la norme est <span class="math notranslate nohighlight">\(\langle e_1, a_1\rangle\)</span> (sachant que <span class="math notranslate nohighlight">\(e_1\)</span> est unitaire). Le vecteur <span class="math notranslate nohighlight">\(a_2\)</span> est un peu plus complexe à reconstruire :</p>
<div class="math notranslate nohighlight">
\[a_2=\langle a_2, e_2\rangle e_2+\langle a_2, e_1\rangle e_1.\]</div>
<p>Autrement dit, <span class="math notranslate nohighlight">\(a_2\)</span> est une combinaison linéaire de <span class="math notranslate nohighlight">\(e_1\)</span> et <span class="math notranslate nohighlight">\(e_2\)</span>, ce qui est logique puisque <span class="math notranslate nohighlight">\(e_2\)</span> est construit en retirant la composante non orthogonale à <span class="math notranslate nohighlight">\(e_1\)</span> de <span class="math notranslate nohighlight">\(a_2\)</span>.
On réitère l’opération jusqu’à <span class="math notranslate nohighlight">\(a_n=\sum_i \langle e_i, a_n\rangle e_i\)</span>.</p>
<p>En notant :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    Q=[e_1, ..., e_n]\textrm{ et }R=\begin{bmatrix}
        \langle e_1, a_1\rangle &amp; \langle e_1, a_2\rangle &amp; \langle e_1, a_3\rangle &amp; \ldots&amp;\langle e_1, a_n\rangle\\
        0 &amp; \langle e_2, a_2\rangle &amp; \langle e_2, a_3\rangle&amp; \ldots&amp;\langle e_2, a_n\rangle\\
        0 &amp;0 &amp; \langle e_3, a_3\rangle&amp; \ldots&amp;\langle e_3, a_n\rangle\\
        0 &amp; 0 &amp; 0 &amp; \ddots &amp; \vdots\\
        0 &amp; 0 &amp; 0 &amp; \ldots &amp; \langle e_n, a_n\rangle
        \end{bmatrix}
    \end{aligned}\end{split}\]</div>
<p>on retrouve bien <span class="math notranslate nohighlight">\(A=QR\)</span>.</p>
</div>
<div class="section" id="c-exemple">
<h3>C. Exemple<a class="headerlink" href="#c-exemple" title="Permalink to this headline">¶</a></h3>
<p>Soit la matrice suivante :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    A=\begin{bmatrix}
    1&amp;-1\\
    2&amp;0\\
    2&amp;2
    \end{bmatrix}=[a_1, a_2]
\end{aligned}\end{split}\]</div>
<p>Commençons la procédure de Gram-Schmidt. On note :</p>
<div class="math notranslate nohighlight">
\[v_1=a_1\textrm{ et }e_1=v_1/\lVert v_1\rVert_2=\Big[\frac{1}{3},\frac{2}{3}, \frac{2}{3}\Big]^T\]</div>
<p>Et :</p>
<div class="math notranslate nohighlight">
\[v_2=a_2-\Pi_{e_1}(a_2)\textrm{ et }e_2=v_2/\lVert v_2\rVert_2=\Big[-\frac{2}{3},-\frac{1}{3}, \frac{2}{3}\Big]^T\]</div>
<p>On vérifie assez bien que <span class="math notranslate nohighlight">\(e_1\)</span> et <span class="math notranslate nohighlight">\(e_2\)</span> sont orthogonaux et unitaires.
Calculons maintenant les produits scalaires :</p>
<div class="math notranslate nohighlight">
\[\langle e_1, a_1\rangle=3,\ \langle e_1, a_2\rangle=1\textrm{ et }\langle e_2, a_2\rangle=2\]</div>
<p>Nous avons donc</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    Q=\begin{bmatrix}
    1/3&amp;-2/3\\
    2/3&amp;-1/3\\
    2/3&amp;2/3
    \end{bmatrix}\text{ et }R=\begin{bmatrix}
    3&amp;1\\
    0&amp;2
    \end{bmatrix}
    \end{aligned}\end{split}\]</div>
<p>On vérifie facilement qu’on a bien l’égalité <span class="math notranslate nohighlight">\(A=QR\)</span>.</p>
<div class="margin sidebar">
<p class="sidebar-title">Dans <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span></p>
<p>La méthode <span class="math notranslate nohighlight">\(\texttt{np.linalg.qr}\)</span> de <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span> permet de faire une décomposition QR d’une matrice <span class="math notranslate nohighlight">\(A\)</span>.</p>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Complétez le code ci-dessous en ré-utilisant votre implémentation du procédé de Gram-Schmidt afin d’obtenir une décomposition QR.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">qr</span><span class="p">(</span><span class="n">matrix</span><span class="p">):</span>
    <span class="c1">####### Complete this part ######## or die ####################</span>
    <span class="o">...</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="o">...</span>
    <span class="c1">###############################################################</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>

<span class="n">Q</span><span class="p">,</span> <span class="n">R</span> <span class="o">=</span> <span class="n">qr</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Notre matrice initiale :</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Notre matrice initiale reconstruite :</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">R</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="iii-application-aux-moindres-carres">
<h2>III. Application aux moindres carrés<a class="headerlink" href="#iii-application-aux-moindres-carres" title="Permalink to this headline">¶</a></h2>
<div class="section" id="a-notre-estimation-via-une-decomposition-qr">
<h3>A. Notre estimation via une décomposition QR<a class="headerlink" href="#a-notre-estimation-via-une-decomposition-qr" title="Permalink to this headline">¶</a></h3>
<p>Soit <span class="math notranslate nohighlight">\(X\in\mathbb{R}^{n\times d}\)</span>, <span class="math notranslate nohighlight">\(y\in\mathbb{R}^n\)</span> et <span class="math notranslate nohighlight">\(\beta\in\mathbb{R}^d\)</span>. Notre objectif est de résoudre le problème d’optimisation suivant </p>
<div class="math notranslate nohighlight">
\[\hat{\beta}=\text{argmin}_{\beta\in\mathbb{R}^d}\lVert X\beta-y\rVert_2^2.\]</div>
<p>Nous avons déjà vu que si <span class="math notranslate nohighlight">\(X^TX\)</span> est inversible, alors nous avons la solution analytique suivante :</p>
<div class="math notranslate nohighlight">
\[\hat{\beta}=(X^TX)^{-1}X^Ty.\]</div>
<p>Considérons maintenant la décomposition <span class="math notranslate nohighlight">\(QR\)</span> de la matrice <span class="math notranslate nohighlight">\(X\)</span> (i.e. <span class="math notranslate nohighlight">\(X=QR\)</span>). Nous avons ainsi :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \hat{\beta}&amp;=(X^TX)^{-1}X^Ty\\
    &amp;=((QR)^TQR)^{-1}(QR)^Ty\\
    &amp;= ((R^TQ^TQR)^{-1}R^TQ^Ty\\
    &amp;=(R^TR)^{-1}R^TQ^Ty\\
    &amp;=R^{-1}(R^T)^{-1}R^TQ^Ty\\
    &amp;=R^{-1}Q^Ty.
\end{aligned}\end{split}\]</div>
<p>Rappellons que si <span class="math notranslate nohighlight">\(A\)</span> et <span class="math notranslate nohighlight">\(B\)</span> sont deux matrices inversibles, nous avons <span class="math notranslate nohighlight">\((AB)^{-1}=B^{-1}A^{-1}\)</span>.</p>
</div>
<div class="section" id="b-exercice">
<h3>B. Exercice<a class="headerlink" href="#b-exercice" title="Permalink to this headline">¶</a></h3>
<p>Soit la matrice suivante :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    X=\begin{bmatrix}
    1&amp;-1\\
    0&amp;10^{-5}\\
    0&amp;0
    \end{bmatrix}\textrm{ et }Y=\begin{bmatrix}
    0\\
    10^{-5}\\
    0
    \end{bmatrix}
    \end{aligned}\end{split}\]</div>
<div class="admonition-exercice-1 admonition">
<p class="admonition-title">Exercice 1</p>
<p><strong>Supposons que l’algorithme tourne sur une machine où les nombres sont arrondis après 8 décimales (i.e. si <span class="math notranslate nohighlight">\(|x|&lt;10^{-8}\)</span> alors <span class="math notranslate nohighlight">\(x:=0\)</span>). Calculez l’estimateur des moindres carrés SANS passer par une décomposition QR.</strong></p>
</div>
<div class="admonition-exercice-2 admonition">
<p class="admonition-title">Exercice 2</p>
<p><strong>Supposons que l’algorithme tourne sur une machine où les nombres sont arrondis après 8 décimales (i.e. si <span class="math notranslate nohighlight">\(|x|&lt;10^{-8}\)</span> alors <span class="math notranslate nohighlight">\(x:=0\)</span>). Calculez l’estimateur des moindres carrés AVEC une décomposition QR.</strong></p>
</div>
<div class="admonition-exercice-3 admonition">
<p class="admonition-title">Exercice 3</p>
<p><strong>Implémentez les deux stratégies précédentes en utilisant <span class="math notranslate nohighlight">\(\texttt{numpy}\)</span>. Que constatez-vous ?</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1e-5</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]])</span>

<span class="c1">####### Complete this part ######## or die ####################</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="c1">###############################################################</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Notre estimateur avec la méthode classique :</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">beta_est</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Notre estimateur avec la méthode QR :</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">beta_qr_est</span><span class="p">)</span>
</pre></div>
</div>
<p>Cette exemple simple montre déjà les avantages de la décomposition QR dans le cadre des moindres carrés. On imagine sans mal son intérêt dans des exemples beaucoup plus compliqués où les dépendances linéaires sont peut-être plus difficiles à discerner au milieu des perturbations et du bruit.</p>
</div>
</div>
<div class="section" id="iv-autres-decompositions-et-conclusion">
<h2>IV. Autres décompositions et conclusion<a class="headerlink" href="#iv-autres-decompositions-et-conclusion" title="Permalink to this headline">¶</a></h2>
<p>Soit <span class="math notranslate nohighlight">\(X\)</span> notre matrice, la solution des moindres carrés est obtenue par la pseudo-inverse de <span class="math notranslate nohighlight">\(X\)</span> (nous l’avons démontré dans la séquence de cours sur la régression linéaire) :</p>
<div class="math notranslate nohighlight">
\[\hat{\beta}=X^\dagger y.\]</div>
<p>Lorsque <span class="math notranslate nohighlight">\(X^TX\)</span> est inversible, nous pouvons obtenir cette pseudo-inverse par  :</p>
<div class="math notranslate nohighlight">
\[X^\dagger=(X^TX)^{-1}X^T,\]</div>
<p>comme nous l’avons vu au-dessus. Nous pouvons également retrouver l’expression <span class="math notranslate nohighlight">\(\hat{\beta}=(X^TX)^{-1}X^Ty=X^\dagger y\)</span> en annulant le gradient des moindres carrés. Afin de limiter les problèmes de stabilité numérique liés à <span class="math notranslate nohighlight">\((X^TX)^{-1}\)</span>, nous avons remplacé dans l’expression précédente <span class="math notranslate nohighlight">\(X\)</span> par sa décomposition <span class="math notranslate nohighlight">\(QR\)</span>.</p>
<p>Étudions maintenant les moindres carrés au travers d’une décomposition en valeurs singulières. Attention, ici l’objectif n’est pas de remplacer <span class="math notranslate nohighlight">\(X\)</span> par cette nouvelle décomposition mais d’utiliser cette dernière afin de trouver une expression de la pseudo-inverse de <span class="math notranslate nohighlight">\(X\)</span>. Une décomposition en valeurs singulières donne :</p>
<div class="math notranslate nohighlight">
\[X=U\Sigma V^T,\]</div>
<p>où <span class="math notranslate nohighlight">\(X\in\mathbb{R}^{n\times d}\)</span>, <span class="math notranslate nohighlight">\(V\)</span> est une matrice de taille <span class="math notranslate nohighlight">\(d\times d\)</span> composée d’une base orthonormale de <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span>, <span class="math notranslate nohighlight">\(U\)</span> est une matrice de taille <span class="math notranslate nohighlight">\(n\times n\)</span> contenant une base orthonormale de <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> et <span class="math notranslate nohighlight">\(\Sigma\)</span> est une matrice “diagonale” de taille <span class="math notranslate nohighlight">\(n\times d\)</span> contenant les valeurs singulières de <span class="math notranslate nohighlight">\(X\)</span> généralement triées de la plus grande à la moins grande. Les valeurs singulières sont les racines des valeurs propres de <span class="math notranslate nohighlight">\(X^TX\)</span> (celles-ci sont nécessairement positives ou nulles).</p>
<p>Soit <span class="math notranslate nohighlight">\(\Sigma^{-1}\)</span> la matrice <span class="math notranslate nohighlight">\(\Sigma\)</span> dont chaque valeur singulière différente de <span class="math notranslate nohighlight">\(0\)</span> a été inversée : <span class="math notranslate nohighlight">\(\lambda_i^{-1}=1/\lambda_i\)</span>. La pseudo-inverse de <span class="math notranslate nohighlight">\(X\)</span> est donnée par :</p>
<div class="math notranslate nohighlight">
\[X^\dagger=V\Sigma^{-1}U^T.\]</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p>Vérifier qu’il s’agit bien d’une pseudo-inverse.</p>
</div>
<div class="hint dropdown admonition">
<p class="admonition-title">Indice</p>
<p>Montrer les égalités suivantes :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
AA^\dagger A&amp;=A\text{ (appliquer }A\text{, son inverse }A^\dagger\text{ puis }A\text{ à nouveau revient à appliquer }A\text{)}\\
A^\dagger AA^\dagger&amp;=A^\dagger\text{ (c'est la même chose du point de vu de l'inverse)}\\
(AA^\dagger)^T&amp;=AA^\dagger\text{ (la transposition n'a pas d'effet)}\\
(A^\dagger A)^T&amp;=A^\dagger A\text{ (même chose que précédemment du point de vu de l'inverse)}
\end{aligned}\end{split}\]</div>
</div>
<p>Comparons dans le code ci-dessous la décomposition QR et la décomposition SVD sur un jeu de données synthétique.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">nb_elements</span> <span class="o">=</span> <span class="mi">500</span>

<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">nb_elements</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">nb_elements</span><span class="p">,</span> <span class="n">nb_elements</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">nb_elements</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">nb_elements</span><span class="p">,</span> <span class="n">nb_elements</span><span class="p">))</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">nb_elements</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># on fait notre décomposition QR</span>
<span class="n">Q</span><span class="p">,</span> <span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># on calcule notre estimateur</span>

<span class="n">beta_est</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">R</span><span class="p">),</span> <span class="n">Q</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>


<span class="n">error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta_est</span><span class="p">)</span><span class="o">-</span><span class="n">y</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">error</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">nb_elements</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Erreur sur le jeu d</span><span class="se">\&#39;</span><span class="s1">apprentissage (QR):&#39;</span><span class="p">,</span> <span class="n">mse</span><span class="p">)</span>

<span class="n">error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">beta_est</span><span class="p">)</span><span class="o">-</span><span class="n">y_test</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Erreur sur le jeu de test (QR):&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">error</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">nb_elements</span><span class="p">)))</span>

<span class="c1"># SVD decomposition</span>
<span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">VT</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1"># on inverse la matrice diagonale sauf pour ses elements nulls</span>
<span class="n">S</span><span class="p">[</span><span class="n">S</span><span class="o">!=</span><span class="mf">0.</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">S</span><span class="p">[</span><span class="n">S</span><span class="o">!=</span><span class="mf">0.</span><span class="p">]</span>
<span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>

<span class="n">beta_est</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">VT</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">S</span><span class="p">),</span> <span class="n">U</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
<span class="n">error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta_est</span><span class="p">)</span><span class="o">-</span><span class="n">y</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">error</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">nb_elements</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Erreur sur le jeu d</span><span class="se">\&#39;</span><span class="s1">apprentissage (SVD):&#39;</span><span class="p">,</span> <span class="n">mse</span><span class="p">)</span>

<span class="n">error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">beta_est</span><span class="p">)</span><span class="o">-</span><span class="n">y_test</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Erreur sur le jeu de test (SVD):&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">error</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">nb_elements</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Erreur sur le jeu d&#39;apprentissage (QR): 1.4321170199999501e-24
Erreur sur le jeu de test (QR): 486.6060026505997
Erreur sur le jeu d&#39;apprentissage (SVD): 7.531901145537263e-24
Erreur sur le jeu de test (SVD): 486.60600265068
</pre></div>
</div>
</div>
</div>
<p>Nous avons bien <span class="math notranslate nohighlight">\(X\hat{\beta} = y\)</span> (aux approximations numériques près) pour les deux décompositions. Cependant, lorsqu’on passe sur un jeu de données de test, la prédiction ne correspond plus du tout à la réalité.</p>
<p>Cela ne vient plus du problème de stabilité numérique mais du bruit qu’il y a dans les données. En effet, nous avons :</p>
<div class="math notranslate nohighlight">
\[\textbf{y}=\textbf{X}\beta + \mathbf{\epsilon},\]</div>
<p>où <span class="math notranslate nohighlight">\(\mathbf{\epsilon}\)</span> est un vecteur de bruit centré en <span class="math notranslate nohighlight">\(\mathbf{0}\)</span>. Notons <span class="math notranslate nohighlight">\(\textbf{y}_\text{pred}=\textbf{X}\beta \)</span> où <span class="math notranslate nohighlight">\(\beta\)</span> est le “vrai” vecteur de paramètres. Nous pouvons retrouver <span class="math notranslate nohighlight">\(\beta\)</span> en passant par la pseudo-inverse :</p>
<div class="math notranslate nohighlight">
\[\beta=\textbf{X}^\dagger \textbf{y}_\text{pred}.\]</div>
<p>En pratique, nous n’avons accès ni à <span class="math notranslate nohighlight">\(\beta\)</span> ni à <span class="math notranslate nohighlight">\(y_\text{pred}\)</span> et notre estimateur est :</p>
<div class="math notranslate nohighlight">
\[\hat{\beta}=\textbf{X}^\dagger \textbf{y}=\textbf{X}^\dagger (\textbf{y}_\text{pred}+\mathbf{\epsilon})=\beta + \mathbf{X}^\dagger \mathbf{\epsilon}.\]</div>
<p>Lorsque la matrice <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> est mal conditionnée (e.g. matrice aléatoire presque carrée, colonnes presque dépendantes linéairement), alors <span class="math notranslate nohighlight">\(\mathbf{X}^\dagger\)</span> aura un effet excessivement important dans certaines directions et <span class="math notranslate nohighlight">\(\mathbf{X}^\dagger \mathbf{\epsilon}\)</span> aura un impact catastrophique sur notre estimateur et nos prédictions seront mauvaises.</p>
<p>La bonne nouvelle est qu’on peut résoudre ce problème via une stratégie de régularisation comme nous pourrons le voir plus tard. Une autre stratégie de régularisation est possible via notre décomposition SVD. En effet, le mauvais conditionnement vient du ratio entre la plus grande et la plus petite valeur singulière (ou valeurs propres lorsqu’elles existent).</p>
<p>Le paramètre de régularisation, noté <span class="math notranslate nohighlight">\(\texttt{rcond}\)</span> n’est autre que le ratio entre la plus grande et la plus petite valeur singulière (différente de <span class="math notranslate nohighlight">\(0\)</span>) qu’on autorise. Toutes les valeurs singulières (ou propres) plus petites que <span class="math notranslate nohighlight">\(\texttt{rcond}\times \lambda_\text{max}\)</span> sont mises à <span class="math notranslate nohighlight">\(0\)</span>. Bien sûr, en faisant cela, nous nous écartons de la vraie pseudo-inverse. Cependant, cela permet de réduire considérablement la norme du vecteur <span class="math notranslate nohighlight">\(\mathbf{X}^\dagger\epsilon\)</span>, vecteur qui n’a que des effets indésirables.</p>
<p>Reprenons l’exemple précédent et observons les résultats.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">VT</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># on autorise un ratio de 100 entre la plus grande </span>
<span class="c1"># et la plus petite valeur singuliere</span>
<span class="n">rcond</span> <span class="o">=</span><span class="mf">1e-2</span>
<span class="n">S</span><span class="p">[</span><span class="n">S</span><span class="o">&lt;=</span><span class="n">rcond</span><span class="o">*</span><span class="n">S</span><span class="o">.</span><span class="n">max</span><span class="p">()]</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="n">S</span><span class="p">[</span><span class="n">S</span><span class="o">!=</span><span class="mf">0.</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">S</span><span class="p">[</span><span class="n">S</span><span class="o">!=</span><span class="mf">0.</span><span class="p">]</span>
<span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>

<span class="n">beta_est</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">VT</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">S</span><span class="p">),</span> <span class="n">U</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
<span class="n">error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta_est</span><span class="p">)</span><span class="o">-</span><span class="n">y</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">error</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">nb_elements</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Erreur sur le jeu d</span><span class="se">\&#39;</span><span class="s1">apprentissage (SVD avec filtre):&#39;</span><span class="p">,</span> <span class="n">mse</span><span class="p">)</span>

<span class="n">error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">beta_est</span><span class="p">)</span><span class="o">-</span><span class="n">y_test</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Erreur sur le jeu de test (SVD avec filtre):&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">error</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">nb_elements</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Erreur sur le jeu d&#39;apprentissage (SVD avec filtre): 0.27668371335748604
Erreur sur le jeu de test (SVD avec filtre): 3.5522910930652705
</pre></div>
</div>
</div>
</div>
<p>On observe donc que notre nouvel estimateur est beaucoup plus robuste et permet d’obtenir de biens meilleurs résultats sur le test. Cependant, nous n’avons effectivement plus une vraie pseudo-inverse et l’erreur sur le train n’est plus négligeable.</p>
<p>La méthode <span class="math notranslate nohighlight">\(\texttt{np.linalg.pinv}\)</span> utilise <span class="math notranslate nohighlight">\(\texttt{np.linalg.svd}\)</span> et possède un paramètre <span class="math notranslate nohighlight">\(\texttt{rcond}\)</span> fixé par défaut à <span class="math notranslate nohighlight">\(10^{-15}\)</span> qui permet de gérer cette régularisation.</p>
<p>En règle général, lorsque la matrice <span class="math notranslate nohighlight">\(X^TX\)</span> est bien inversible les solutions <span class="math notranslate nohighlight">\((X^TX)^{-1}X^T\)</span>, QR ou SVD sont comparables en termes de résultats. Lorsqu’on se rapproche d’un déterminant nul, la méthode <span class="math notranslate nohighlight">\((X^TX)^{-1}X^T\)</span> est la première à devenir instable, suivi de <span class="math notranslate nohighlight">\(QR\)</span>. La méthode basée sur <span class="math notranslate nohighlight">\(SVD\)</span> permet d’obtenir une pseudo-inverse qui fonctionne même si <span class="math notranslate nohighlight">\(X^TX\)</span> possède un déterminant nul.</p>
<p>Cela fait qu’en pratique, sur certains problèmes très mal conditionnés, la solution de <span class="math notranslate nohighlight">\(\texttt{LinearRegression}\)</span> ou via <span class="math notranslate nohighlight">\(\texttt{pinv}\)</span> ou via <span class="math notranslate nohighlight">\(\texttt{SVD}\)</span> possèdent de bonnes performances en test.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./2_regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="4_algo_proximal_lasso.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Sous-différentiel et le cas du Lasso ☕️☕️☕️</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="6_ridge.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Une analyse de la régularisation Ridge ☕️☕️☕️</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Servajean, Leveau & Chailan<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>
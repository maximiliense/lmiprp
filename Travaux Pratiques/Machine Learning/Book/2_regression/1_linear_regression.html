
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>La régression linéaire ☕️ &#8212; Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="L’optimisation ☕️☕️" href="2_optimization.html" />
    <link rel="prev" title="La régression" href="0_propos_liminaire.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-72WWYCKNK6"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-72WWYCKNK6');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../0_requisite/rappels_python.html">
   Rappels de Python et Numpy
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../1_what_is_ml/0_propos_liminaire.html">
   <em>
    Machine Learning
   </em>
   , initiation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/1_introduction_ml.html">
     <em>
      Machine learning
     </em>
     et malédiction de la dimension
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/2_regression_and_classification_trees.html">
     Les arbres de régression et de classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="0_propos_liminaire.html">
   La
   <em>
    régression
   </em>
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     La régression linéaire ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="2_optimization.html">
     L’optimisation ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3_interpolation.html">
     Interpolation ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="4_algo_proximal_lasso.html">
     Sous-différentiel et le cas du Lasso ☕️☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5_least_square_qr.html">
     Les moindres carrés via une décomposition QR (et plus)☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="6_ridge.html">
     Une analyse de la régularisation Ridge ☕️☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../3_classification/0_propos_liminaire.html">
   La
   <em>
    classification
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/1_logistic_regression.html">
     La régression logistique ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/2_fonctions_proxy.html">
     Les fonctions de perte (loss function) ☕️☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/3_bayes_classifier.html">
     Le classifieur de Bayes ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/4_VC_theory.html">
     Un modèle formel de l’apprentissage ☕️☕️☕️☕️ (💆‍♂️)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../4_kernel_methods/0_propos_liminaire.html">
   Les méthodes à noyaux
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../4_kernel_methods/1_svm.html">
     Le SVM ou l’hypothèse max-margin ☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5_ensembles/0_propos_liminaire.html">
   Les méthodes
   <em>
    ensemblistes
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/1_ensembles.html">
     Méthodes ensemblistes ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/2_bayesian_linear_regression.html">
     Bayesian linear regression ☕️☕️☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../6_deeplearning/0_propos_liminaire.html">
   <em>
    deep learning
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/1_autodiff.html">
     La différentiation automatique et un début de
     <em>
      deep learning
     </em>
     ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/2_filters_representation.html">
     Filtres et espace de représentation des réseaux de neurones ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/3_probabilities_calibration.html">
     Calibration des probabilités et quelques notions ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/4_regularization_deep.html">
     Régularisation en
     <em>
      deep learning
     </em>
     ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/5_transfer_multitask.html">
     <em>
      Transfer learning
     </em>
     et apprentissage multi-tâches ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/6_adversarial.html">
     Les attaques adversaires ☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../7_unsupervised/0_propos_liminaire.html">
   L’apprentissage non-supervisé
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_unsupervised/1_principal_component_analysis.html">
     L’Analyse en Composantes Principales ☕️☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_unsupervised/2_em_gaussin_mixture_model.html">
     Modèle de Mélange Gaussien et algorithme
     <em>
      Expectation-Maximization
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../8_set_prediction/0_propos_liminaire.html">
   Prédiction d’ensembles
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/1_well_defined.html">
     Jeu d’apprentissage
     <em>
      set-valued
     </em>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/2_ill_defined.html">
     Jeu d’apprentissage uniquement multi-classes ☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../biblio.html">
   Bibliographie
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/2_regression/1_linear_regression.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/maximiliense/lmiprp"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/maximiliense/lmiprp/issues/new?title=Issue%20on%20page%20%2F2_regression/1_linear_regression.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/maximiliense/lmiprp/blob/master/Travaux Pratiques/Machine Learning/Book/2_regression/1_linear_regression.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#i-introduction">
   I. Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ii-construction-d-un-jeu-de-donnees">
   II. Construction d’un jeu de données
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iii-du-modele-statistique-a-l-optimisation">
   III. Du modèle statistique à l’optimisation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-la-fonction-objectif">
     A. La fonction objectif
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-optimisation-par-descente-de-gradient">
     B. Optimisation par Descente de gradient
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#c-a-vous-de-jouer">
     C. À vous de jouer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#d-l-algorithme-de-descente-de-gradient">
     D. L’algorithme de descente de gradient
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#e-les-equations-normales-de-la-regression-lineaire-la-solution-par-pseudo-inverse">
     E. Les équations normales de la régression linéaire : la solution par pseudo-inverse
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#f-a-vous-de-jouer">
     F. À vous de jouer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#g-avec-sklearn">
     G. Avec sklearn
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iv-features-variables-explicatives-transformees">
   IV. Features - Variables explicatives transformées
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-construction-du-jeu-de-donnees-polynomial">
     A. Construction du jeu de données polynomial
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-solution-par-pseudo-inverse">
     B. Solution par pseudo-inverse
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#c-solution-sklearn">
     C. Solution Sklearn
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#v-la-validation-croisee-et-le-dilemme-biais-variance">
   V. La validation croisée et le dilemme biais-variance
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-construction-du-jeu-de-donnees">
     A. Construction du jeu de données
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-optimiser-une-fonction-est-il-suffisant-pour-parler-d-apprentissage">
     B. Optimiser une fonction est-il suffisant pour parler d’apprentissage ?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vi-l-effet-double-descente-bonus">
   VI. L’effet “double descente” (Bonus ?)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vii-regularisation">
   VII. Régularisation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     A. Construction du jeu de données
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-sans-regularisation">
     B. Sans régularisation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#c-avec-regularisation-ell-1">
     C. Avec régularisation
     <span class="math notranslate nohighlight">
      \(\ell_1\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#d-avec-regularisation-ell-2">
     D. Avec régularisation
     <span class="math notranslate nohighlight">
      \(\ell_2\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#e-avec-regularisation-elastic-net">
     E. Avec régularisation
     <em>
      elastic-net
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#viii-selection-de-modeles">
   VIII. Selection de modèles
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     A. Construction du jeu de données
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-recherche-exhaustive">
     B. Recherche exhaustive
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#c-recherche-aleatoire">
     C. Recherche aléatoire
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ix-le-mot-de-la-fin">
   IX. Le mot de la fin
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="la-regression-lineaire">
<h1>La régression linéaire ☕️<a class="headerlink" href="#la-regression-lineaire" title="Permalink to this headline">¶</a></h1>
<div class="admonition-objectifs-de-la-sequence admonition">
<p class="admonition-title">Objectifs de la séquence</p>
<ul class="simple">
<li><p>Concevoir :</p>
<ul>
<li><p>la régression linéaire d’un point de vue prédictif,</p></li>
<li><p>la régression linéaire au travers d’un problème d’optimisation.</p></li>
</ul>
</li>
<li><p>Être capable :</p>
<ul>
<li><p>d’implémenter un algorithme de descente de gradient,</p></li>
<li><p>de transformer les variables d’entrée pour rendre le modèle non linéaire,</p></li>
<li><p>d’utiliser la librairie <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span>.</p></li>
</ul>
</li>
<li><p>De s’initier à la notion de régularisation et de sélection de variables.</p></li>
</ul>
</div>
<div class="section" id="i-introduction">
<h2>I. Introduction<a class="headerlink" href="#i-introduction" title="Permalink to this headline">¶</a></h2>
<p>La régression linéaire est un modèle cherchant à établir un lien linéaire entre des données d’observation et des données à prédire. Plus concrètement, les données observées sont décrites par un vecteur <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^d\)</span> et la variable à prédire, par une quantité scalaire (un réel) <span class="math notranslate nohighlight">\(y \in \mathbb{R}\)</span>. On notera <span class="math notranslate nohighlight">\(\mathbf{x}, y\sim \mathbb{P}\)</span> la loi jointe du couple (par un abus de langage important, <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> et <span class="math notranslate nohighlight">\(y\)</span> expriment à la fois une variable aléatoire et sa réalisation) avec <span class="math notranslate nohighlight">\(\mu\)</span> la marginale de <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> et le lien s’exprime sous le format suivant :</p>
<div class="math notranslate nohighlight">
\[y = \beta_0 +  x_1 \beta_1 + x_2 \beta_2 + x_3 \beta_3 + ... + x_d \beta_d +\epsilon = \beta_0 + \sum_i^d x_i \beta_i+\epsilon,\ \epsilon\sim\mathcal{N}(0, \sigma)\]</div>
<p>que l’on peut aussi écrire en notation vectorielle :</p>
<div class="math notranslate nohighlight">
\[y = \beta_0  + \langle \boldsymbol{\beta}, \mathbf{x} \rangle +\epsilon,\ \epsilon\sim\mathcal{N}(0, \sigma)\]</div>
<p>où <span class="math notranslate nohighlight">\(\boldsymbol{\beta} \in \mathbb{R}^d\)</span> et <span class="math notranslate nohighlight">\(\beta_0\in\mathbb{R}\)</span> correspondent respectivement au vecteur et au scalaire contenant les paramètres du “vrai” modèle qui défini le lien entre les données et que l’on va vouloir apprendre pour prédire la bonne valeur de <span class="math notranslate nohighlight">\(y\)</span> en fonction du vecteur <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. Le modèle linéaire ne peut prédire la variable <span class="math notranslate nohighlight">\(y\)</span> qu’à un bruit <span class="math notranslate nohighlight">\(\epsilon\)</span> près. Une fois ces paramètres appris par notre algorithme d’apprentissage, on pourra utiliser la fonction de prédiction <span class="math notranslate nohighlight">\(f_{\boldsymbol{\beta}}(\mathbf{x}): \mathbb{R}^d \rightarrow \mathbb{R}\)</span> apprise pour prédire la valeur <span class="math notranslate nohighlight">\(y_{new}\)</span> associée à un nouveau vecteur <span class="math notranslate nohighlight">\(\mathbf{x_{new}}\)</span> que l’on n’a pas encore observé :</p>
<div class="math notranslate nohighlight">
\[\hat{y}_{new} = f_{\boldsymbol{\beta}}(\boldsymbol{x_{new}}) = \beta_0  + \langle \boldsymbol{\beta}, \boldsymbol{x_{new}} \rangle\]</div>
<p>Pour simplifier les calculs et les notations, on préfère que la fonction de prédiction puisse se calculer à partir d’une notation complètement vectorielle. C’est ce que l’on fait en pratique, en ajoutant une composante supplémentaire <span class="math notranslate nohighlight">\(x_0\)</span> au vecteur <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> égale à <span class="math notranslate nohighlight">\(1\)</span> :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    \mathbf{x} &amp;= \begin{bmatrix}
          1 \\
           x_{1} \\
           \vdots \\
           x_{d}
         \end{bmatrix},
\end{align*}\end{split}\]</div>
<p>de sorte à ce que la fonction de prédiction linéaire puisse s’exprimer simplement sous la forme du produit scalaire:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    f_{\boldsymbol{\beta}}(\mathbf{x}) &amp;= \langle \boldsymbol{\beta}, \mathbf{x} \rangle &amp;=
          \begin{bmatrix}
           \beta_{0} \\           
           \vdots \\
           \beta_{d}
          \end{bmatrix}^T
          \begin{bmatrix}
           1 \\
           \vdots \\
           x_{d}
         \end{bmatrix} &amp;= \sum_{i=0}^d x_i \beta_i=\langle \boldsymbol{x}, \boldsymbol{\beta}\rangle_{\mathbb{R}^{d+1}}
\end{aligned}\end{split}\]</div>
<p>où cette fois <span class="math notranslate nohighlight">\(\boldsymbol{\beta} \in \mathbb{R}^{d+1}\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{x} \in \mathbb{R}^{d+1}\)</span> et <span class="math notranslate nohighlight">\(\langle \cdot, \cdot\rangle_{\mathbb{R}^{d+1}}\)</span> est le produit scalaire dans <span class="math notranslate nohighlight">\(\mathbb{R}^{d+1}\)</span>. Le but d’un algorithme d’apprentissage sera de trouver un estimateur <span class="math notranslate nohighlight">\(\boldsymbol{\hat{\beta}}\)</span> de <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> à partir d’un ensemble fini de <span class="math notranslate nohighlight">\(n\)</span> exemples d’apprentissage <span class="math notranslate nohighlight">\((\boldsymbol{x}, \langle \boldsymbol{\beta}, \boldsymbol{x} \rangle + \epsilon) \in \mathbb{R}^2\)</span> préalablement collectés. On notera <span class="math notranslate nohighlight">\(\mathcal{S}=\{(\boldsymbol{x_i}, y_i)\}_{i\leq n}\)</span> le jeu de données.</p>
<div class="margin sidebar">
<p class="sidebar-title">Modèle prédictif, descriptif</p>
<p>Les modèles prédictifs cherchent à réaliser une prédiction relative à une tâche quelconque après avoir pu observer une donnée <span class="math notranslate nohighlight">\(x\)</span>. Le modèle étant bien entendu “appris” à partir d’un jeu de données d’apprentissage.</p>
<p>Les modèles descriptifs cherchent à quantifier les liens entre les variables explicatives et la variable à expliquer.</p>
<p>Que ce soit pour une analyse prédictive ou une analyse descriptive, nous pouvons utiliser un modèle linéaire. Cependant, dans le cadre d’une analyse prédictive, le critère clé sera vraiment la qualité de la prédiction sur des <em><strong>données nouvelles</strong></em>.</p>
</div>
<p>Nous commencerons par implémenter le cas simple d’une régréssion linéaire à une seule variable d’entrée et une seule variable de sortie qui pourra donc s’écrire sous la forme :</p>
<div class="math notranslate nohighlight">
\[\hat{y} = f_{\boldsymbol{\beta}}(\mathbf{x}) = \beta_0  + \beta_1 x\]</div>
<p>C’est à dire une fonction affine dont on pourra afficher la représentation graphique (une droite) sur une figure en 2 dimensions. Par la suite vous aurez donc à implémenter le calcul de la fonction de coût du modèle sur l’ensemble d’apprentissage, le calcul du gradient de cette fonction de coût ainsi que l’algorithme de descente de gradient qui, à partir du gradient, permet d’obtenir le vecteur <span class="math notranslate nohighlight">\(\hat{\beta}\)</span>.</p>
<p>La seconde partie de ce notebook étendra ces notions à des concepts plus compliqués.</p>
</div>
<div class="section" id="ii-construction-d-un-jeu-de-donnees">
<h2>II. Construction d’un jeu de données<a class="headerlink" href="#ii-construction-d-un-jeu-de-donnees" title="Permalink to this headline">¶</a></h2>
<p>Commençons tout d’abord par simuler notre jeu de données avec le modèle génératif suivant  :</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{x} \sim \mathcal{N}(\mu, \sigma) \in \mathbb{R}\]</div>
<p>ou <span class="math notranslate nohighlight">\(\sigma\)</span> correspond à la variance de la variable explicative. Nous choissons une règle arbitraire pour générer aléatoirement les paramètres du “vrai” modèle :</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\beta} \sim \mathbb{U}(-1, 1)^2 \in \mathbb{R}^2\]</div>
<p>où <span class="math notranslate nohighlight">\(\mathbb{U}^2\)</span> est la loi uniforme dans <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>. Enfin, le bruit est construit de la manière suivante :</p>
<div class="math notranslate nohighlight">
\[\epsilon \sim \mathcal{N}(0, 1).\]</div>
<p>Chaque exemple d’apprentissage correspond donc à un couple de réels <span class="math notranslate nohighlight">\((x_j, y_j = \beta_0  + \beta_1 x_j + \epsilon) \in \mathbb{R}^2\)</span>. Le code ci dessous construit et affiche le jeux de données ainsi que la représentation graphique de <span class="math notranslate nohighlight">\(f(x)=\beta_1x+\beta_0\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># on simule le vecteur de parametre</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># on construit un jeu de donnees de 10 points selon la methode </span>
<span class="c1"># decrite ci-dessus.</span>
<span class="k">def</span> <span class="nf">sample_data</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">add_noise</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">add_noise</span><span class="o">*</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">beta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">noise</span> <span class="o">+</span> <span class="n">beta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_data</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">add_noise</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># jouer avec le bruit</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># configuration generale de matplotlib</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">12.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>

<span class="c1"># plot de la fonction</span>
<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">ymin_</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="n">eps</span>
    <span class="n">ymax_</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="n">eps</span>
    <span class="n">min_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="n">eps</span>
    <span class="n">max_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="n">eps</span>
    <span class="k">if</span> <span class="n">beta</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">min_</span><span class="p">,</span> <span class="n">max_</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">y_</span> <span class="o">=</span> <span class="n">beta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_</span> <span class="o">+</span> <span class="n">beta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span> <span class="n">y_</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">func</span> <span class="o">=</span> <span class="p">[(</span><span class="n">func</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)]</span> <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">func</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">list</span> <span class="k">else</span> <span class="n">func</span>
            <span class="n">disp_legend</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">func</span><span class="p">:</span>
                <span class="n">y_</span> <span class="o">=</span> <span class="n">t</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x_</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">x_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)))</span>
                <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span> <span class="n">y_</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">t</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">disp_legend</span> <span class="o">=</span> <span class="n">disp_legend</span> <span class="ow">or</span> <span class="n">t</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="s1">&#39;&#39;</span>
            <span class="k">if</span> <span class="n">disp_legend</span><span class="p">:</span>
                <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="n">min_</span><span class="p">,</span> <span class="n">max_</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="n">ymin_</span><span class="p">,</span> <span class="n">ymax_</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    
<span class="c1"># on plot le dataset precedent</span>
<span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_5_0.png" src="../_images/1_linear_regression_5_0.png" />
</div>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Que se passe-t-il si le bruit <span class="math notranslate nohighlight">\(\epsilon\)</span> est nul ? Quelle est alors la méthode la plus rapide pour trouver les paramètres du vrai modèle ?</strong></p>
</div>
<div class="tip admonition">
<p class="admonition-title">Interpolation</p>
<p>Lorsque le bruit disparait, la droite passe exactement par tous les points (à moins que l’hypothèse <span class="math notranslate nohighlight">\(y=\langle\beta, x\rangle\)</span> soit fausse). On parle alors d’interpolation.</p>
</div>
</div>
<div class="section" id="iii-du-modele-statistique-a-l-optimisation">
<h2>III. Du modèle statistique à l’optimisation<a class="headerlink" href="#iii-du-modele-statistique-a-l-optimisation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="a-la-fonction-objectif">
<h3>A. La fonction objectif<a class="headerlink" href="#a-la-fonction-objectif" title="Permalink to this headline">¶</a></h3>
<p>Nous souhaitons en pratique trouver un paramêtre <span class="math notranslate nohighlight">\(\boldsymbol{\hat{\beta}}\)</span> qui minimise le risque du modèle, c’est-à-dire la quantité d’erreur en espérance de n’importe quel modèle <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>. On notera <span class="math notranslate nohighlight">\(\boldsymbol{\beta}^\star\)</span> le “vrai” modèle, soit celui qui minimise le risque en espérance. Pour la régression linéaire, on peut définir ce risque comme :</p>
<div class="math notranslate nohighlight">
\[R(\boldsymbol{\beta}) = \mathbb{E}_{X\times Y}\Big[ (f_{\boldsymbol{\beta}}(\mathbf{X}) - Y)^2 \Big].\]</div>
<p>On ne sait pas calculer cette fonction car on ne connaît pas la distribution jointe et où qu’on ne sait pas calculer l’intégrale. Cependant, on peut en avoir un estimateur via un jeu de données <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>, où <span class="math notranslate nohighlight">\(\mathcal{S} = \Big\{ \big(\boldsymbol{x_j}, y_j \big) \Big\}_{j\leq n}\)</span> est un jeu de données composé de <span class="math notranslate nohighlight">\(n\)</span> points indépendants et identiquement distribués selon le modèle génératif décrit précédement.</p>
<p>A défaut d’avoir accès au risque (i.e. à l’erreur en espérance), on peut utiliser une autre quantité qui consiste en la somme des carrés des erreurs de prédictions pour chaque exemple d’apprentissage, c’est <strong>le risque emprique</strong> :</p>
<div class="math notranslate nohighlight">
\[J(\boldsymbol{\beta}) = R_{emp}(\boldsymbol{\beta}) = \frac{1}{n}\sum_j^n (f_{\boldsymbol{\beta}}(x_j) - y_j)^2\]</div>
<p>où <span class="math notranslate nohighlight">\(f_{\boldsymbol{\beta}}(x_j) = \beta_0  + \beta_1 x_j\)</span>. On montre assez facilement que pour un <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> quelconque :</p>
<div class="math notranslate nohighlight">
\[R(\boldsymbol{\beta})=\mathbb{E}_{\mathcal{S \sim \mathbb{P}_S}}\big[J(\boldsymbol{\beta})\big],\]</div>
<p>Notons que minimiser ce risque empirique revient à chercher le maximum de vraisemblance du modèle statistique. Effectivement, avec l’hypothèse gaussienne, la vraissemblance de n’importe quel modèle de paramètres <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> pour un jeu de données <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> peut s’écrire :</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\mathcal{S}}(\boldsymbol{\beta}) \propto \prod_{\boldsymbol{x}\times y\in\mathcal{S}} \exp\Bigg(-\frac{\big(f_{\boldsymbol{\beta}}(\mathbf{x}) - y\big)^2}{2}\Bigg)\]</div>
<p>Le paramètre maximisant la vraissamblance est aussi celui minimisant la log-vraissamblance négative :</p>
<div class="math notranslate nohighlight">
\[- \text{log} \Big( \mathcal{L}_{\mathcal{S}}(\boldsymbol{\beta})\Big) = \sum_{\boldsymbol{x}\times y\in\mathcal{S}}\frac{\big(f_{\boldsymbol{\beta}}(\mathbf{x}) - y\big)^2}{2}\propto\sum_{\boldsymbol{x}\times y\in\mathcal{S}}\big(f_{\boldsymbol{\beta}}(\mathbf{x}) - y\big)^2\]</div>
<p>N’ayant accès au véritable risque, on cherche <span class="math notranslate nohighlight">\(\boldsymbol{\hat{\beta}}\)</span> tel que :</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\hat{\beta}} = \text{argmin}_{\boldsymbol{\beta}} \Big[ - \log \Big( \mathcal{L}_{\mathcal{S}}(\boldsymbol{\beta})\Big) \Big]\]</div>
<p>Minimiser le risque emprique se traduit donc naturellement par un problème d’optimisation de la fonction de coût <span class="math notranslate nohighlight">\(J(\boldsymbol{\beta}) : \mathbb{R}^2 \rightarrow \mathbb{R}\)</span> (<span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span> dans notre exemple courant, <span class="math notranslate nohighlight">\(\mathbb{R}^{d+1}\)</span> dans le cas général) par rapport à <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>.</p>
<p>En pratique et pour des raisons de simplicité, on ne minimise pas <span class="math notranslate nohighlight">\(\sum_{\boldsymbol{x}\times y\in\mathcal{S}}\big(f_{\boldsymbol{\beta}}(\mathbf{x}) - y\big)^2\)</span> mais :</p>
<div class="math notranslate nohighlight">
\[J(\boldsymbol{\beta}) = \frac{1}{2n}\sum_{\boldsymbol{x}\times y\in\mathcal{S}} (f_{\boldsymbol{\beta}}(x) - y)^2\]</div>
<p>Le résultat est bien évidemment le même. La division par <span class="math notranslate nohighlight">\(2\)</span> est là pour simplifier l’expresion du gradient que l’on calculera et la division par <span class="math notranslate nohighlight">\(n\)</span> permet de rendre la norme du gradient indépendente de la taille de notre jeu de données. C’est une propriété importante pour l’algorithme de descente de gradient dont la taille des déplacements affecte sa stabilité.</p>
<p><strong>Note - Notation vectorielle de la régression linéaire :</strong> On peut aussi exprimer ce calcul avec une équation en notation vectorielle. Pour cela, on exprime dans un premier temps le résultat de la fonction de prédiction en notation vectorielle (il s’agit de la prédiction pour tout notre jeu de données) :</p>
<div class="math notranslate nohighlight">
\[f_{\boldsymbol{\beta}}(\mathbf{X}) = \mathbf{X}\boldsymbol{\beta}\in\mathbb{R}^n\]</div>
<p>où <span class="math notranslate nohighlight">\(\boldsymbol{\beta} \in \mathbb{R}^{d+1}\)</span> est une matrice de dimensions <span class="math notranslate nohighlight">\((d+1)\times 1\)</span> (en <span class="math notranslate nohighlight">\(\texttt{numpy}\)</span>, la dimension <span class="math notranslate nohighlight">\(1\)</span> est importante) et <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> est une matrice de dimensions <span class="math notranslate nohighlight">\(n\times (d+1)\)</span> dont les <span class="math notranslate nohighlight">\(n\)</span> vecteurs lignes correspondent aux vecteurs d’apprentissage d’entrée. Dans notre cas (celui de la régression linéaire à <span class="math notranslate nohighlight">\(1\)</span> variable) la matrice prend la forme suivante :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbf{X} = 
\begin{pmatrix} 
1 &amp; x_{1} \\
\vdots &amp; \vdots\\
1 &amp; x_{j} \\
\vdots &amp; \vdots\\
1 &amp; x_{n} 
\end{pmatrix},\ \boldsymbol{\beta}=
\begin{bmatrix}
\beta_{0} \\           
\beta_{1}
\end{bmatrix}
\end{aligned}\end{split}\]</div>
<p>La fonction de coût peut ainsi s’exprimer :</p>
<div class="math notranslate nohighlight">
\[J(\boldsymbol{\beta}) = \frac{1}{2n} (\mathbf{X}\boldsymbol{\beta} - \mathbf{y})^T(\mathbf{X}\boldsymbol{\beta} - \mathbf{y})\]</div>
<p>que l’on peut réécrire :</p>
<div class="math notranslate nohighlight">
\[J(\boldsymbol{\beta}) = \frac{1}{2n} (\mathbf{\hat{y}} - \mathbf{y})^T(\mathbf{\hat{y}} - \mathbf{y}) =  \frac{1}{2n} ||\mathbf{\hat{y}} - \mathbf{y}||_2^2\]</div>
<p>où <span class="math notranslate nohighlight">\(\mathbf{y} \in  \mathbb{R}^n\)</span> est le vecteur dont chacune des composantes <span class="math notranslate nohighlight">\(y_j\)</span> sont les valeurs à prédire à partir de leur <span class="math notranslate nohighlight">\(x_j\)</span> correspondant, et <span class="math notranslate nohighlight">\(\hat{y} \in  \mathbb{R}^n\)</span> correspond aux valeurs prédites par le modèle. On note ici que la fonction objectif à optimiser peut se calculer aisément en utilisant la norme euclidienne au carré du vecteur d’erreur.</p>
</div>
<div class="section" id="b-optimisation-par-descente-de-gradient">
<h3>B. Optimisation par Descente de gradient<a class="headerlink" href="#b-optimisation-par-descente-de-gradient" title="Permalink to this headline">¶</a></h3>
<div class="dropdown admonition">
<p class="admonition-title">Le gradient est orthogonal aux lignes de niveau</p>
<p>Soit <span class="math notranslate nohighlight">\(c:\mathbb{R}^+\mapsto\mathbb{R}^d\)</span> un arc paramétré qui suit une ligne de niveau de <span class="math notranslate nohighlight">\(f\)</span> (un arc paramétré prend en argument le “temps” et retourne une coordonnée dans l’espace). Si <span class="math notranslate nohighlight">\(c\)</span> suit une ligne de niveau, nous avons donc :</p>
<div class="math notranslate nohighlight">
\[f(c(t))=f(c(0))=\text{const}.\]</div>
<p>Cela implique que nous ayons aussi :</p>
<div class="math notranslate nohighlight">
\[(f(c(t)))^\prime=\langle \nabla f(c(t)), c^\prime(t)\rangle=0,\]</div>
<p>où <span class="math notranslate nohighlight">\(\nabla f(c(t))\)</span> est le gradient en <span class="math notranslate nohighlight">\(c(t)\)</span> et <span class="math notranslate nohighlight">\(c^\prime(t)\)</span> donne la direction de l’arc paramétré (i.e. de la ligne de niveau) en <span class="math notranslate nohighlight">\(c(t)\)</span>. Les deux sont bien ainsi orthogonaux.</p>
</div>
<p>La descente de gradient est une méthode d’optimisation numérique permettant de trouver les valeurs des paramètres qui minimisent une fonction. Dans notre cas, nous voulons minimiser l’erreur de prédiction moyenne de notre modèle, fonction définie précédemment. Cette méthode d’optimisation consiste à calculer le gradient de notre fonction objectif par rapport aux paramètres courant du modèle et de les déplacer par un “petit” pas dans la direction opposée au gradient (i.e. le gradient donne la plus forte croissance et son opposé la plus forte décroissance).</p>
<p><strong>Définition générale du gradient d’une fonction à plusieurs variables :</strong> Il s’agit simplement du vecteur contenant les dérivées partielles de la fonction, c-à-d les dérivées de la fonction par rapport à chaque variable indépendamment des autres :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\nabla_{\boldsymbol{\beta}} J(\boldsymbol{\beta}) = \frac{\partial J(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} = 
\begin{bmatrix}
\frac{\partial J(\beta)}{\partial \beta_0}\\
\frac{\partial J(\beta)}{\partial \beta_1}\\
 \vdots \\
\frac{\partial J(\beta)}{\partial \beta_d}
\end{bmatrix}
\end{aligned}\end{split}\]</div>
<div class="dropdown admonition">
<p class="admonition-title">Le gradient est la plus forte pente</p>
<p>Soit <span class="math notranslate nohighlight">\(c:\mathbb{R}^+\mapsto\mathbb{R}^d\)</span> un arc paramétré et soit <span class="math notranslate nohighlight">\(f:\mathbb{R}^d\mapsto\mathbb{R}\)</span>. Nous étudions l’évolution de <span class="math notranslate nohighlight">\(f\)</span> le long de <span class="math notranslate nohighlight">\(c\)</span> :</p>
<div class="math notranslate nohighlight">
\[f(c(t)).\]</div>
<p>L’accroissement de <span class="math notranslate nohighlight">\(f\)</span> le long de <span class="math notranslate nohighlight">\(c(t)\)</span> est donné par la dérivée :</p>
<div class="math notranslate nohighlight">
\[(f(c(t))^\prime=\langle \nabla f(c(t)), c^\prime(t)\rangle.\]</div>
<p>Nous avons par Cauchy-Schwartz :</p>
<div class="math notranslate nohighlight">
\[\lVert\langle \nabla f(c), c^\prime\rangle\rVert \leq \lVert\nabla f(c)\rVert\lVert c^\prime\rVert,\]</div>
<p>où le gradient et l’arc sont évalués en <span class="math notranslate nohighlight">\(t\)</span>. L’égalité est atteinte lorsque les vecteurs sont colinéaires. La plus forte pente est donc la direction du gradient.</p>
</div>
<p>En descente de gradient, la mise à jour de chaque paramètre <span class="math notranslate nohighlight">\(\beta_j\)</span> du modèle à l’itération <span class="math notranslate nohighlight">\(t\)</span> se fait donc avec la règle suivante :</p>
<div class="math notranslate nohighlight">
\[\beta_j^{(t+1)} = \beta_j^{(t)} - \rho  \frac{\partial J(\beta^{(t)})}{\partial \beta_j}\]</div>
<p>ou bien, en notation vectorielle :</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \rho  \nabla_{\boldsymbol{\beta}} J(\boldsymbol{\beta})^{(t)}\]</div>
<p>où <span class="math notranslate nohighlight">\(\rho\)</span> est le learning rate (pas d’apprentissage). Un pas d’apprentissage <span class="math notranslate nohighlight">\(\rho\)</span> trop petit nous fera nous déplacer trop lentement et trop grand rendra l’optimisation instable.</p>
</div>
<div class="section" id="c-a-vous-de-jouer">
<h3>C. À vous de jouer<a class="headerlink" href="#c-a-vous-de-jouer" title="Permalink to this headline">¶</a></h3>
<div class="admonition-question-1 admonition">
<p class="admonition-title">Question 1</p>
<p><strong>Complétez la méthode <span class="math notranslate nohighlight">\(\texttt{val}\)</span> de l’objet <span class="math notranslate nohighlight">\(\texttt{LeastSquare}\)</span> ci-dessous.</strong></p>
</div>
<div class="admonition-question-2 admonition">
<p class="admonition-title">Question 2</p>
<p><strong>Calculez les dérivées partielles <span class="math notranslate nohighlight">\(\partial J(\beta)/\partial \beta_0\)</span> et <span class="math notranslate nohighlight">\(\partial J(\beta)/\partial \beta_1\)</span> de la fonction de coût de notre modèle de régréssion linéaire. Complétez la méthode <span class="math notranslate nohighlight">\(\texttt{grad}\)</span> de l’objet <span class="math notranslate nohighlight">\(\texttt{LeastSquare}\)</span> ci dessous.</strong></p>
</div>
<div class="hint dropdown admonition">
<p class="admonition-title">Indices</p>
<p>Rappellez vous que la dérivée d’une composition de fonctions s’écrit <span class="math notranslate nohighlight">\((g \circ f)^\prime (x) = f^\prime(x) g^\prime(f(x))\)</span> et que la fonction de coût de notre modèle s’écrit :</p>
<div class="math notranslate nohighlight">
\[J(\boldsymbol{\beta}) = \frac{1}{2n}\sum_j^n g(f_{\boldsymbol{\beta}}(x_j) - y_j)\]</div>
<p>avec <span class="math notranslate nohighlight">\(g(z) = z ^ 2\)</span> et <span class="math notranslate nohighlight">\(f_{\boldsymbol{\beta}}(x_j) = \beta_0  + \beta_1 x_j\)</span>.</p>
</div>
<div class="admonition-question-3-dur admonition">
<p class="admonition-title">Question 3 (dur)</p>
<p><strong>Calculez le gradient de la fonction <span class="math notranslate nohighlight">\(J(\boldsymbol{\beta})\)</span> en utilisant les dérivées matricielles. Complétez la méthode <span class="math notranslate nohighlight">\(\texttt{grad}\)</span> de l’objet <span class="math notranslate nohighlight">\(\texttt{LeastSquare}\)</span> avec le gradient en notation vectorielle.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LeastSquare</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">LeastSquare</span><span class="o">.</span><span class="n">_format_ndarray</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">LeastSquare</span><span class="o">.</span><span class="n">_format_ndarray</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pos</span> <span class="o">=</span> <span class="mi">0</span>
        
    <span class="k">def</span> <span class="nf">_format_ndarray</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
        <span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="k">else</span> <span class="n">arr</span>
        <span class="k">return</span> <span class="n">arr</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">arr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">arr</span>
    
    <span class="k">def</span> <span class="nf">val</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">LeastSquare</span><span class="o">.</span><span class="n">_format_ndarray</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
        <span class="c1">####### Complete this part ######## or die ####################</span>
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="c1">###############################################################</span>
        <span class="k">return</span> <span class="n">val</span>
    
    <span class="k">def</span> <span class="nf">_shuffle</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">batch_size</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span> <span class="k">else</span> <span class="n">batch_size</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_pos</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">_pos</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_pos</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pos</span><span class="o">+</span><span class="n">batch_size</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pos</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_shuffle</span><span class="p">()</span>
            
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

        <span class="n">beta</span> <span class="o">=</span> <span class="n">LeastSquare</span><span class="o">.</span><span class="n">_format_ndarray</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
        
        <span class="c1">####### Complete this part ######## or die ####################</span>
        <span class="o">...</span>
        <span class="c1">###############################################################</span>
        <span class="k">return</span> <span class="n">grad</span>
    

<span class="n">l</span> <span class="o">=</span> <span class="n">LeastSquare</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;La valeur de la loss pour le vrai parametre est&#39;</span><span class="p">,</span> <span class="n">l</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">beta</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;La valeur du gradient pour le vrai parametre est</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">l</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">beta</span><span class="p">))</span>
<span class="n">plot_loss_contour</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">three_dim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plot_loss_contour</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">three_dim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>Attention, pour des raisons de temps de calcul, l’estimation du gradient n’est pas tout le temps faite sur tout le jeu de données mais sur une partie de celui-ci. Un estimateur calculé de cette manière là aura en espérance la même valeur qu’un gradient calculé sur toutes les données. On appelle généralement Descente de Gradient Stochastique ou SGD une approche qui ne fait qu’estimer le gradient à partir d’un batch de données.</p>
<div class="admonition-question admonition">
<p class="admonition-title">Question</p>
<p><strong>Saurez-vous retrouver dans le code ci-dessus ce qui permet de jouer sur la taille du batch lors du calcul du gradient ?</strong></p>
</div>
</div>
<div class="section" id="d-l-algorithme-de-descente-de-gradient">
<h3>D. L’algorithme de descente de gradient<a class="headerlink" href="#d-l-algorithme-de-descente-de-gradient" title="Permalink to this headline">¶</a></h3>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Complétez le code de descente de gradient.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GradientDescent</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="n">init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">LeastSquare</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.005</span><span class="p">,</span> <span class="n">nb_iterations</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">init</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">param_trace</span> <span class="o">=</span> <span class="p">[</span><span class="n">beta</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
        <span class="n">loss_trace</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">beta</span><span class="p">)]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_iterations</span><span class="p">):</span>
            <span class="c1">####### Complete this part ######## or die ####################</span>
            <span class="o">...</span>
            <span class="c1">###############################################################</span>
            <span class="n">param_trace</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">beta</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">loss_trace</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">beta</span><span class="p">))</span>
            
        <span class="k">return</span> <span class="n">param_trace</span><span class="p">,</span> <span class="n">loss_trace</span>
        
<span class="n">gd</span> <span class="o">=</span> <span class="n">GradientDescent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">param_trace</span><span class="p">,</span> <span class="n">loss_trace</span> <span class="o">=</span> <span class="n">gd</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">nb_iterations</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>

<span class="n">param_trace</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">param_trace</span><span class="p">)</span>
<span class="n">loss_trace</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">loss_trace</span><span class="p">)</span>
<span class="n">loss_trace</span> <span class="o">=</span> <span class="n">loss_trace</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">loss_trace</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">xyz</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">param_trace</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">loss_trace</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plot_loss_contour</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">param_trace</span><span class="o">=</span><span class="n">xyz</span><span class="p">,</span> <span class="n">three_dim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plot_loss_contour</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">param_trace</span><span class="o">=</span><span class="n">param_trace</span><span class="p">,</span> <span class="n">three_dim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition-convergence-de-la-descente-de-gradient admonition">
<p class="admonition-title">Convergence de la descente de gradient</p>
<p>Notons <span class="math notranslate nohighlight">\(\beta^\star\)</span> une solution du problème d’optimisation ci-dessus. Si le pas d’apprentissage est assez petit, alors il est possible de démontrer que l’algorithme de descente de gradient converge nécessairement vers <span class="math notranslate nohighlight">\(J(\beta^\star)\)</span> à une vitesse proportionnelle à <span class="math notranslate nohighlight">\(1/k\)</span> où <span class="math notranslate nohighlight">\(k\)</span> est le nombre d’itérations. La séquence <em><strong>optimisation</strong></em> de ce cours démontrera rigoureusement cela.</p>
</div>
<div class="admonition-stochastic-gradient-descent admonition">
<p class="admonition-title">Stochastic Gradient Descent</p>
<p>La propriété d’orthogonalité par rapport aux lignes de niveau de la fonction de coût est elle conservée dans ce cas ? Pourquoi ? Ne suit-on pourtant toujours pas le gradient ? Que pouvez vous dire sur la nature et la “vitesse” de convergence vers le minimum de la fonction ? Réfléchissez d’un point de vue calculatoire sur ce qui se passe sur des tailles d’échantillons très grandes ?</p>
</div>
</div>
<div class="section" id="e-les-equations-normales-de-la-regression-lineaire-la-solution-par-pseudo-inverse">
<h3>E. Les équations normales de la régression linéaire : la solution par pseudo-inverse<a class="headerlink" href="#e-les-equations-normales-de-la-regression-lineaire-la-solution-par-pseudo-inverse" title="Permalink to this headline">¶</a></h3>
<p>Comme calculé plus haut, l’expression du gradient est donnée par <span class="math notranslate nohighlight">\((X^TX\boldsymbol{\beta}-X^T\boldsymbol{y})/n\)</span>.  La fonction <span class="math notranslate nohighlight">\(J\)</span> étant coercive et convexe, elle admet au moins un minimum local/global. Les points critiques sont donnés en annulant le gradient :</p>
<div class="math notranslate nohighlight">
\[X^TX\boldsymbol{\beta}-X^T\boldsymbol{y} = 0 \Leftrightarrow X^TX\boldsymbol{\beta}=X^t\boldsymbol{y}.\]</div>
<p>Il s’agit des équations dites “normales”. Tout vecteur <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> solution de ces équations est donc nécessairement un minimiseur de <span class="math notranslate nohighlight">\(J(\boldsymbol{\beta})\)</span>.</p>
<p><strong>Dans le cas standard (i.e. sur-déterminé)</strong> où chaque variable explicative est linéairement indépendante des autres et où le nombre d’échantillons de notre jeu de données est supérieur ou égal à la dimension du problème considéré, la matrice <span class="math notranslate nohighlight">\(X^TX\)</span> est inversible (i.e. <span class="math notranslate nohighlight">\(\text{det}(X^TX)\neq 0\)</span>). Dit autrement, il existe une unique solution aux équations normales donnée par :</p>
<div class="math notranslate nohighlight">
\[\hat{\boldsymbol{\beta}}=(X^TX)^{-1}X^T\boldsymbol{y}.\]</div>
<p>On appelle <span class="math notranslate nohighlight">\(X^\dagger = (X^TX)^{-1}X^T\)</span>  pseudo-inverse de <span class="math notranslate nohighlight">\(X\)</span> (ou inverse généralisée) et la solution analytique à notre problème est donnée par <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}=X^\dagger \boldsymbol{y}\)</span>.</p>
<div class="tip dropdown admonition">
<p class="admonition-title">L’effet du bruit</p>
<p>Soit <span class="math notranslate nohighlight">\(\boldsymbol{y}=X\boldsymbol{\beta}+\eta\)</span> où on utilise <span class="math notranslate nohighlight">\(\eta\)</span> plutôt que <span class="math notranslate nohighlight">\(\epsilon\)</span> pour différentier la réalisation effective du bruit de la variable aléatoire. Notre estimateur est donc :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}\hat{\beta}&amp;=X^\dagger y = X^\dagger(X\boldsymbol{\beta} + \eta)\\ &amp;= (X^\dagger X)\boldsymbol{\beta} + X^\dagger\eta.\end{aligned}\end{split}\]</div>
<p>On observe, par propriété de la pseudo-inverse, que la première contribution est la projection orthogonale du vrai modèle sur l’espace des vecteurs ligne de <span class="math notranslate nohighlight">\(X\)</span>. Il est donc une combinaison linéaire des vecteurs que l’on voit pendant l’apprentissage ! La deuxième contribution est l’effet du bruit sur la solution optimale. Nous discuterons plus loin de ces contributions et d’effets étranges qui peuvent se produire notament quand la matrice <span class="math notranslate nohighlight">\(X\)</span> est mal conditionnée (le ratio entre la plus grande valeur propre de <span class="math notranslate nohighlight">\(X^TX\)</span> et sa plus petite valeur propre est très grand).</p>
</div>
<p><strong>Dans le cas non standard (i.e. sous-déterminé)</strong> où certaines variables peuvent être des combinaisons linéaires d’autres variables (inutile en pratique) ou si le nombre d’échantillons est inférieur à la dimension, <span class="math notranslate nohighlight">\(X^TX\)</span> n’est plus inversible. Dans ce cas de figure, il existe une infinité de solutions aux équations normales (i.e. une infinité de minimiseurs). Dans ce cas, la solution de norme minimale est donnée par :</p>
<div class="math notranslate nohighlight">
\[\hat{\beta}=X^\dagger y = X^T(XX^T)^{-1}y.\]</div>
<p><strong>Concernant le cas général (i.e. sous et sur-déterminé),.</strong> E. H. Moore (1920), A. Bjerhammar (1951) et R. Penrose (1955) proposent indépendamment une expression générale de <span class="math notranslate nohighlight">\(X^\dagger\)</span> appelée pseudo-inverse de Moore-Penrose et calculable à partir d’une décomposition en valeur singulière, notée <span class="math notranslate nohighlight">\(X^\dagger\)</span>. Celle-ci coïncide bien sûr avec l’expression standard lorsqu’elle existe. On obtient donc une expression analytique générale, solution des équations normales :</p>
<div class="math notranslate nohighlight">
\[\hat{\boldsymbol{\beta}}=X^\dagger\boldsymbol{y},\]</div>
<p>où <span class="math notranslate nohighlight">\(X^\dagger\)</span> est le pseudo-inverse de Moore-Penrose. Celle-ci est obtenue par une décomposition en valeurs singulières <span class="math notranslate nohighlight">\(X=U\Sigma V^T\)</span> de la manière suivante : <span class="math notranslate nohighlight">\(X^\dagger = V\Sigma^{-1}U^T\)</span> où <span class="math notranslate nohighlight">\(\Sigma^{-1}\)</span> est la matrice <span class="math notranslate nohighlight">\(\Sigma\)</span> où nous avons inversés les valeurs singulières non nulles. La séquence de cours “Les moindres carrés via une décomposition QR (et plus)” détaille cela.</p>
<div class="tip dropdown admonition">
<p class="admonition-title">Un système d’équations</p>
<p>En réalité, minimiser les moindres carrés revient à résoudre un système d’équation :</p>
<div class="math notranslate nohighlight">
\[Ax=y\text{ ou }Ax\approx y.\]</div>
<p>Si le problème possède autant d’équation que d’inconnues, alors le problème est bien posé et admet une solution. Si le nombre d’équations (linéairement indépendantes) est supérieur au nombre d’inconnues, alors le problème est sur-déterminé et on ne peut résoudre que <span class="math notranslate nohighlight">\(Ax\approx y\)</span>. Enfin, si le nombre d’équations est inférieur au nombre d’inconnues, alors il existe une infinité de solutions et on choisira celle de norme minimale.</p>
</div>
<p><em><strong>Quelques précisions d’algèbre</strong></em>  : finalement, quel est le lien entre une pseudo-inverse et l’inverse classique. Soit une application linéaire <span class="math notranslate nohighlight">\(A:\mathbb{R}^n\mapsto\mathbb{R}^n\)</span> représentée par une matrice <span class="math notranslate nohighlight">\(A\in\mathbb{R}^{n\times n}\)</span>. On appelle inverse de <span class="math notranslate nohighlight">\(A\)</span> l’unique matrice, notée <span class="math notranslate nohighlight">\(A^{-1}\)</span>, telle que  <span class="math notranslate nohighlight">\(A^{-1}A=\text{Id}\)</span>. Dans le cas inversible, l’inverse de <span class="math notranslate nohighlight">\(A^{-1}\)</span> est donc de manière évidente <span class="math notranslate nohighlight">\(A\)</span>. Cela revient à transformer un vecteur <span class="math notranslate nohighlight">\(x\in\mathbb{R}^n\)</span> par <span class="math notranslate nohighlight">\(A\)</span> puis à annuler sa transformation par <span class="math notranslate nohighlight">\(A^{-1}\)</span>. L’inverse n’existe cependant pas toujours. Ainsi, par exemple, si <span class="math notranslate nohighlight">\(\text{ker}(A)\neq \{\boldsymbol{0}\}\)</span> (i.e. le noyau ne se résume pas à l’élément nul, nous avons <span class="math notranslate nohighlight">\(\forall x\in\mathbb{R}^n,\ u\in\text{ker}(A)\)</span> que <span class="math notranslate nohighlight">\(A(x+u)=Ax\)</span>. Finalement l’inverse de <span class="math notranslate nohighlight">\(Ax\)</span> est-il <span class="math notranslate nohighlight">\(x\)</span> ou <span class="math notranslate nohighlight">\(x+u\)</span> ?</p>
<p>Reprenons le cas de la pseudo-inverse. Quelques propriétés qui peuvent sembler évidentes émergent :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
AA^\dagger A&amp;=A\text{ (appliquer }A\text{, son inverse }A^\dagger\text{ puis }A\text{ à nouveau revient à appliquer }A\text{)}\\
A^\dagger AA^\dagger&amp;=A^\dagger\text{ (c'est la même chose du point de vu de l'inverse)}\\
(AA^\dagger)^T&amp;=AA^\dagger\text{ (la transposition n'a pas d'effet)}\\
(A^\dagger A)^T&amp;=A^\dagger A\text{ (même chose que précédemment du point de vu de l'inverse)}
\end{align*}\end{split}\]</div>
<p>La pseudo inverse est l’unique matrice <span class="math notranslate nohighlight">\(A^\dagger\)</span> satisfaisant les propriétés précédentes. Dans le cas où <span class="math notranslate nohighlight">\(A\)</span> est inversible, on a alors <span class="math notranslate nohighlight">\(A^\dagger=A^{-1}\)</span>. Intuitivement, l’idée est de ne considérer “que” les éléments qui ne sont pas dans le noyaux. Ainsi <span class="math notranslate nohighlight">\(\text{Im}(A)=\text{Ker}(A^\dagger)^\perp\)</span> et inversement.</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Vérifier que les deux inverses proposées dans les cas sur-déterminé et sous-déterminé vérifient bien les égalités précédentes.</strong></p>
</div>
<p>La question à laquelle nous pouvons maintenant essayer de répondre est en quoi une pseudo-inverse permet d’obtenir une solution acceptable au problème des moindres carrés. Idéalement, nous aurions :</p>
<div class="math notranslate nohighlight">
\[Ax = y \text{ généralement noté }X\beta=y.\]</div>
<p>En pratique, il n’existe pas nécessairement de vecteur <span class="math notranslate nohighlight">\(x\)</span> tel que cette égalité soit satisfaite. Dit autrement, <span class="math notranslate nohighlight">\(y\)</span> n’est pas dans l’image de <span class="math notranslate nohighlight">\(A\)</span>, notée <span class="math notranslate nohighlight">\(\text{Im}(A)\)</span>. Nous cherchons ainsi à trouver <span class="math notranslate nohighlight">\(x\)</span> tel que <span class="math notranslate nohighlight">\(\lVert Ax-y\rVert_2\)</span> est minimisé. Notons <span class="math notranslate nohighlight">\(K(A)\)</span> le noyau de <span class="math notranslate nohighlight">\(A\)</span>. Considérons maintenant la proposition suivante :</p>
<div class="admonition-proposition admonition">
<p class="admonition-title">Proposition</p>
<div class="math notranslate nohighlight">
\[\text{proj}_{\text{Im}(A)}=AA^\dagger \text{ (projection orthogonale sur l'image de }A\text{)}\]</div>
<div class="math notranslate nohighlight">
\[\text{proj}_{\text{K}(A^T)}=I-AA^\dagger\]</div>
<div class="math notranslate nohighlight">
\[\text{proj}_{\text{Im}(A^T)}=A^\dagger A\]</div>
<div class="math notranslate nohighlight">
\[\text{proj}_{\text{K}(A)}=I-A^\dagger A\]</div>
</div>
<div class="caution dropdown admonition">
<p class="admonition-title">Preuve</p>
<p>Démontrons la première égalité.</p>
<p>Nous avons <span class="math notranslate nohighlight">\((AA^\dagger)^T=AA^\dagger\)</span> et notre application est donc symétrique. De plus,</p>
<div class="math notranslate nohighlight">
\[(AA^\dagger)(AA^\dagger)=(AA^\dagger A)A^\dagger = AA^\dagger\]</div>
<p>et notre application est donc <a class="reference external" href="https://fr.wikipedia.org/wiki/Idempotence">idempotente</a>. L’idempotence implique que <span class="math notranslate nohighlight">\(\text{proj}_{\text{Im}(X)}\)</span> est un projecteur (non nécessairement orthogonal).</p>
<p>Soit <span class="math notranslate nohighlight">\(y\in\text{Im}(A)\)</span>. Il existe donc <span class="math notranslate nohighlight">\(x\)</span> tel que <span class="math notranslate nohighlight">\(y=Ax\)</span>. Nous avons donc :</p>
<div class="math notranslate nohighlight">
\[\text{proj}_{\text{Im}(A)}(y)=AA^\dagger y=AA^\dagger A x=Ax=y.\]</div>
<p>Ainsi, si <span class="math notranslate nohighlight">\(y\in\text{Im}(A)\)</span>, le projecteur le projette sur lui-même. Considérons maintenant <span class="math notranslate nohighlight">\(z\in \text{Im}(A)^\perp\)</span>. Dit autrement, <span class="math notranslate nohighlight">\((Ax)^Tz=0=x^TA^Tz,\ \forall x\)</span> impliquant <span class="math notranslate nohighlight">\(A^Tz=0\)</span>.
Nous avons donc :</p>
<div class="math notranslate nohighlight">
\[\text{proj}_{\text{Im}(A)}(z)=AA^\dagger z = (AA^\dagger)^T z = (A^\dagger)^TA^Tz = 0.\]</div>
<p>Tous les éléments orthogonaux à l’image de <span class="math notranslate nohighlight">\(A\)</span> sont projetés sur <span class="math notranslate nohighlight">\(0\)</span>. Nous avons donc bien un projecteur orthogonal.</p>
</div>
<p>Cela nous permet de rédiger la proposition suivante :</p>
<div class="admonition-proposition admonition">
<p class="admonition-title">Proposition</p>
<p>La solution de norme minimale de <span class="math notranslate nohighlight">\(\lVert Ax-y\rVert_2\)</span> est donnée par :</p>
<div class="math notranslate nohighlight">
\[x=A^\dagger y\]</div>
</div>
<div class="caution dropdown admonition">
<p class="admonition-title">Preuve</p>
<p>Comme nous l’avons vu, en pratique l’équation :</p>
<div class="math notranslate nohighlight">
\[Ax= y\]</div>
<p>n’admet pas de solution et nous devons considérer celle qui minimise l’erreur au carré (i.e. la norme euclidienne). Le vecteur de résidus qui minimisera l’erreur est celui qui sera orthogonal à l’image de <span class="math notranslate nohighlight">\(A\)</span> (la plus courte distance, Pythagore tout ça). Dit autrement, on cherche à résoudre le système d’équations suivant :</p>
<div class="math notranslate nohighlight">
\[Ax=\text{proj}_{\text{Im}(A)}(y)=AA^\dagger y.\]</div>
<p>Si <span class="math notranslate nohighlight">\(y\)</span> est déjà dans l’image et qu’une solution existe, alors c’est cela ne change rien puisque <span class="math notranslate nohighlight">\(\text{proj}_{\text{Im}(A)}(y)=y\)</span>, sinon on cherche la solution la plus proche. Nous avons donc :</p>
<div class="math notranslate nohighlight">
\[Ax=AA^\dagger y\Leftrightarrow A(x-A^\dagger y)=0\]</div>
<p>On constate ainsi que <span class="math notranslate nohighlight">\((x-A^\dagger y)\in K(A)\)</span>. N’importe quel élément <span class="math notranslate nohighlight">\(x\)</span> tel que <span class="math notranslate nohighlight">\(x-A^\dagger y\)</span> est dans le noyau de <span class="math notranslate nohighlight">\(A\)</span> satisfait l’équation ci-dessus :</p>
<div class="math notranslate nohighlight">
\[x-A^\dagger y = \text{proj}_{\text{K}(A)}(w)=(I-A^\dagger A)w\]</div>
<p>où <span class="math notranslate nohighlight">\(w\)</span> est choisi arbitrairement. Une solution générale pour <span class="math notranslate nohighlight">\(x\)</span> est donc :</p>
<div class="math notranslate nohighlight">
\[x=A^\dagger y + (I-A^\dagger A)w,\]</div>
<p>où on remarque que les deux termes sont des vecteurs orthogonaux. Ainsi, si on privilégie le vecteur de norme minimale, nous obtenons <span class="math notranslate nohighlight">\(x=A^\dagger y\)</span> et la pseudo-inverse est une méthode acceptable pour déterminer la solution des moindres carrés.</p>
</div>
</div>
<div class="section" id="f-a-vous-de-jouer">
<h3>F. À vous de jouer<a class="headerlink" href="#f-a-vous-de-jouer" title="Permalink to this headline">¶</a></h3>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Calculez la solution du problème de régression linéaire en utilisant la pseudo-inverse de Moore-Penrose proposée par <span class="math notranslate nohighlight">\(\texttt{numpy}\)</span> via <span class="math notranslate nohighlight">\(\texttt{np.linalg.pinv}\)</span>.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">matplotlib</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">12.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">)</span>
<span class="c1"># descente de gradient</span>

<span class="c1"># descente de gradient sans stochasticite</span>
<span class="n">param_trace</span> <span class="p">,</span> <span class="n">loss_trace</span> <span class="o">=</span> <span class="n">gd</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.04</span><span class="p">,</span> <span class="n">nb_iterations</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span> 

<span class="c1">####### Complete this part ######## or die ####################</span>
<span class="c1"># solution par pseudo inverse</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="c1">###############################################################</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;La loss pour la solution par pseudo-inverse est&#39;</span><span class="p">,</span> <span class="n">l</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">beta_pinv</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;La loss pour la solution obtenue par descente de gradient est&#39;</span><span class="p">,</span> <span class="n">loss_trace</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">beta_pinv</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition-remarques-et-question admonition">
<p class="admonition-title">Remarques et question</p>
<p><strong>On remarque ici que la valeur de la loss atteinte par GD est plus haute que celle atteinte par la solution de la pseudo-inverse. A votre avis pourquoi ? Augmentez le nombre d’itérations de GD. Que constatez vous ? Est-ce étonnant par rapport à votre brillante démonstration sur GD dans la section précédente ? Au passage, on pourrait s’amuser à montrer qu’avec l’initialisation <span class="math notranslate nohighlight">\(\beta=\boldsymbol{0}\)</span>, chaque step reste bien dans l’espace engendré par les vecteurs lignes de X. Qui veut passer au tableau ?</strong></p>
</div>
</div>
<div class="section" id="g-avec-sklearn">
<h3>G. Avec sklearn<a class="headerlink" href="#g-avec-sklearn" title="Permalink to this headline">¶</a></h3>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Proposez une régression linéaire sur le même problème en utilisant <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span>.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="c1">####### Complete this part ######## or die ####################</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="c1">###############################################################</span>
<span class="n">coef</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="n">coef</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;La loss pour la solution obtenue par Sklearn est&#39;</span><span class="p">,</span> <span class="n">l</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">coef</span><span class="p">))</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">coef</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="iv-features-variables-explicatives-transformees">
<h2>IV. Features - Variables explicatives transformées<a class="headerlink" href="#iv-features-variables-explicatives-transformees" title="Permalink to this headline">¶</a></h2>
<p>Dans beaucoup de problèmes réels, la variable à expliquer n’est pas une simple combinaison linéaire des variables explicatives. Cela peut-être une dépendence non linéaire (e.g. quadratique), ou des dépendences croisées entre nos variables explicatives. La stratégie permettant d’aborder cette problématique consiste à transformer notre vecteur <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> en rajoutant par exemple des transformations quadratiques et à optimiser notre modèle linéaire sur le vecteur transformé. Afin de simplifier les notations, nous allons volontairement omettre le biais <span class="math notranslate nohighlight">\(\beta_0\)</span> de nos notations.</p>
<p>Construire nos <em>features</em> consiste à chercher une fonction <span class="math notranslate nohighlight">\(\phi:\mathbb{R}^d\mapsto\mathbb{R}^p\)</span> qui transforme non-linéairement nos variables explicatives initiales.</p>
<p>Le problème se reformule ainsi de la manière suivante :</p>
<div class="math notranslate nohighlight">
\[\hat{y}=\langle \phi(\boldsymbol{x}), \boldsymbol{\beta}\rangle\]</div>
<p>Il suffit donc de transformer nos variables explicatives par <span class="math notranslate nohighlight">\(\phi\)</span> et de considérer le résultat comme nos nouvelles variables explicatives.</p>
<div class="section" id="a-construction-du-jeu-de-donnees-polynomial">
<h3>A. Construction du jeu de données polynomial<a class="headerlink" href="#a-construction-du-jeu-de-donnees-polynomial" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># vrais parametres</span>
<span class="n">beta_cube</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">sample_data_cube</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">noise</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">150</span><span class="p">)</span>

<span class="c1">#affichage du polynome</span>
<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_38_0.png" src="../_images/1_linear_regression_38_0.png" />
</div>
</div>
</div>
<div class="section" id="b-solution-par-pseudo-inverse">
<h3>B. Solution par pseudo-inverse<a class="headerlink" href="#b-solution-par-pseudo-inverse" title="Permalink to this headline">¶</a></h3>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Complétez le code ci-dessous en utilisant une solution par pseudo-inverse via <span class="math notranslate nohighlight">\(\texttt{numpy}\)</span>.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Polynomial</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">deg</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">deg</span> <span class="o">=</span> <span class="n">deg</span>

    <span class="k">def</span> <span class="nf">_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1"># here we transform the input into a polynomial</span>
        <span class="c1">####### Complete this part ######## or die ####################</span>
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="k">return</span> <span class="o">...</span>
        <span class="c1">###############################################################</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="c1">####### Complete this part ######## or die ####################</span>
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="c1">###############################################################</span>
        
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;You must fit the model first&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">X_transformed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_transformed</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">errors</span> <span class="o">=</span> <span class="p">(</span><span class="n">prediction</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span><span class="mi">2</span>
        <span class="k">return</span> <span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="n">errors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
<span class="c1"># vraie solution</span>
<span class="n">real_model</span> <span class="o">=</span> <span class="n">Polynomial</span><span class="p">(</span><span class="n">deg</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">real_model</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta_cube</span>
</pre></div>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Le plot est affiché avec un jeu dit de test. Il s’agit d’un ensemble de points qui n’ont pas été utilisés lors de notre apprentissage (par pseudo-inverse). Le jeu d’apprentissaget et de test n’ont souvent pas la même taille.</strong></p>
<p><strong>Jouez avec le degré du polynôme que vous manipulez et observez le résultat. Que constatez-vous ?</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deg</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Polynomial</span><span class="p">(</span><span class="n">deg</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="p">[(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">,</span> <span class="s1">&#39;Our model&#39;</span><span class="p">),</span> <span class="p">(</span><span class="n">real_model</span><span class="o">.</span><span class="n">predict</span><span class="p">,</span> <span class="s1">&#39;Real model&#39;</span><span class="p">)])</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Empirical risk: &#39;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
<div class="admonition-remarque-et-question admonition">
<p class="admonition-title">Remarque et question</p>
<p><strong>Le risque empirique est celui calculé directement sur les données utilisées lors du calcul du pseudo-inverse. Que constatez-vous par rapport à ce dernier lorsque vous jouez avec le degré du polynôme ?</strong></p>
<p><strong>Est-il un bon indicateur du véritable risque de généralisation ? Autrement dit, est-il un bon indicateur de la qualité du polynôme obtenu.</strong></p>
</div>
</div>
<div class="section" id="c-solution-sklearn">
<h3>C. Solution Sklearn<a class="headerlink" href="#c-solution-sklearn" title="Permalink to this headline">¶</a></h3>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Proposez la même solution polynomiale via <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span>. Choisissez le même degré qu’utilisé au-dessus et comparez les résultats.</strong></p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
</pre></div>
</div>
</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="c1">###############################################################</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="p">[(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">,</span> <span class="s1">&#39;sklearn&#39;</span><span class="p">),</span> <span class="p">(</span><span class="n">real_model</span><span class="o">.</span><span class="n">predict</span><span class="p">,</span> <span class="s1">&#39;Real model&#39;</span><span class="p">)])</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="v-la-validation-croisee-et-le-dilemme-biais-variance">
<h2>V. La validation croisée et le dilemme biais-variance<a class="headerlink" href="#v-la-validation-croisee-et-le-dilemme-biais-variance" title="Permalink to this headline">¶</a></h2>
<p>Vous avez pu constater qu’en fonction du degré du polynôme choisi dans l’exercice précédent, le modèle obtenu était plus ou moins loin de la solution idéale. De plus, le risque empirique s’est montré être un piètre estimateur de la qualité de notre solution estimée.</p>
<p>En réalité, le risque empirique est un estimateur sans biais du risque de généralisation pour un vecteur de paramètres quelconque. Ce n’est plus vrai si on choisit la solution estimée via notre optimisation. Dit autrement :</p>
<div class="math notranslate nohighlight">
\[R(\boldsymbol{\beta})=\mathbb{E}_{\mathcal{S}}\Big[J(\boldsymbol{\beta})\Big],\text{ }\boldsymbol{\beta}\text{ quelconque, et }R(\text{argmin}_{\boldsymbol{\beta}}J(\boldsymbol{\beta}))\neq \mathbb{E}_{\mathcal{S}}\Big[\text{argmin}_{\boldsymbol{\beta}}J(\boldsymbol{\beta})\Big]\]</div>
<hr class="docutils" />
<p>Nous pouvons décomposer l’erreur attendue en briques élémentaires qui nous permettront d’en saisir l’origine. Reprenons l’erreur suivante :</p>
<div class="math notranslate nohighlight">
\[R(\boldsymbol{\hat{\beta}})=\mathbb{E}\big[J(\boldsymbol{\hat{\beta}})\big]=\mathbb{E}\big[(y-\hat{f}(x))^2\big],\]</div>
<p>où <span class="math notranslate nohighlight">\(y=f(x)+\epsilon\)</span>, <span class="math notranslate nohighlight">\(\epsilon\sim \mathcal{N}(0, \sigma^2)\)</span> avec <span class="math notranslate nohighlight">\(f\)</span> la “vraie” fonction et <span class="math notranslate nohighlight">\(\hat{f}\)</span> notre estimateur de celle-ci. Nous avons ainsi :</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\mathbb{E}\big[(y-\hat{f}(x))^2\big]&amp;=\mathbb{E}\big[y^2\big]-2\mathbb{E}\big[y\hat{f}(x)\big]+\mathbb{E}\big[\hat{f}(x)^2\big].
\end{aligned}\]</div>
<p>Remarquons tout d’abord :</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\big[\hat{f}(x)^2\big]=\text{Var}\big(\hat{f}(x)\big)+\mathbb{E}\big[\hat{f}(x)\big]^2,\]</div>
<p>ainsi que :</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\big[y^2\big]=\text{Var}\big(y\big)+\mathbb{E}\big[y\big]^2,\]</div>
<p>où :</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\big[y\big]=\mathbb{E}\big[f(x)+\epsilon\big]=\mathbb{E}\big[f(x)\big]=f(x),\]</div>
<p>et enfin :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{E}\big[y\hat{f}(x)\big]&amp;=\mathbb{E}\big[(f(x)+\epsilon)\hat{f}(x)\big]\\
&amp;=\mathbb{E}\big[f(x)\hat{f}(x)\big]+\mathbb{E}\big[\epsilon\hat{f}(x)\big]\\
&amp;=f(x)\mathbb{E}\big[\hat{f}(x)\big]+\mathbb{E}\big[\epsilon\big]\mathbb{E}\big[\hat{f}(x)\big]\\
&amp;=f(x)\mathbb{E}\big[\hat{f}(x)\big].
\end{aligned}\end{split}\]</div>
<p>En regroupant tout ensemble, nous avons donc :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{E}\big[(y-\hat{f}(x))^2\big]&amp;=\text{Var}\big(y\big)+f(x)^2-2f(x)\mathbb{E}\big[\hat{f}(x)\big]+\text{Var}\big(\hat{f}(x)\big)+\mathbb{E}\big[\hat{f}(x)\big]^2\\
&amp;=\sigma^2+\text{Var}\big(\hat{f}(x)\big)+\mathbb{E}\big[f(x)-\hat{f}(x)\big]^2\\
&amp;=\sigma^2+\text{Var}\big(\hat{f}\big)+\text{bias}(\hat{f})^2.\end{aligned}\end{split}\]</div>
<p>On observe donc que l’erreur de notre modèle se décompose théoriquement en trois éléments :</p>
<ul class="simple">
<li><p><strong>Erreur irréductible :</strong> <span class="math notranslate nohighlight">\(\sigma^2\)</span>. Quand bien même nous connaîtrions le véritable processus générateur de nos données, notre erreur quadratique serait toujours de <span class="math notranslate nohighlight">\(\sigma^2\)</span>,</p></li>
<li><p><strong>Variance :</strong> <span class="math notranslate nohighlight">\(\text{Var}\big(\hat{f}\big)\)</span>. Elle décrit la fluctuation de notre apprentissage autour de son espérance,</p></li>
<li><p><strong>Biais (au carré) :</strong> <span class="math notranslate nohighlight">\(\text{bias}(\hat{f})^2\)</span>. Il nous donne l’écart en espérance entre notre estimateur et le véritable processus générateur.</p></li>
</ul>
<p>Toute la difficulté vient du fait qu’il est très facile de réduire le biais en augmentant la complexité de notre modèle entraînant par la même une augmentation de la variance…</p>
<hr class="docutils" />
<p>Cela implique de mettre en place une procédure expérimentale permettant d’évaluer la qualité de notre modèle.</p>
<div class="section" id="a-construction-du-jeu-de-donnees">
<h3>A. Construction du jeu de données<a class="headerlink" href="#a-construction-du-jeu-de-donnees" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">beta_cube</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">sample_data_cube</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">noise</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">150</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_52_0.png" src="../_images/1_linear_regression_52_0.png" />
</div>
</div>
</div>
<div class="section" id="b-optimiser-une-fonction-est-il-suffisant-pour-parler-d-apprentissage">
<h3>B. Optimiser une fonction est-il suffisant pour parler d’apprentissage ?<a class="headerlink" href="#b-optimiser-une-fonction-est-il-suffisant-pour-parler-d-apprentissage" title="Permalink to this headline">¶</a></h3>
<p>Il existe deux stratégies d’évaluation sans biais de la qualité de notre modèle :</p>
<ul class="simple">
<li><p>La validation non croisée où une partie de notre jeu de donnée est cachée pendant l’apprentissage puis utilisée afin d’évaluer les performances du modèle. Il s’agit du découpage train/test. Cette stratégie est un estimateur sans biais de la qualité de notre modèle mais possède une variance plus forte que la validation croisée. Elle peut-être particulièrement utile lorsque le coût d’apprentissage d’un modèle est très élevé (e.g. <em>deep learning</em>)</p></li>
<li><p>La validation croisée où notre jeu de données est divisé en <em>k</em> parties (on parle aussi de <em>k-fold</em>). Évidemment, <span class="math notranslate nohighlight">\(k\in\{2, ..., n\}\)</span> où <span class="math notranslate nohighlight">\(n\)</span> est la taille du jeu de données. Chacune des parties jouera successivement le rôle de jeu de test pendant que les <span class="math notranslate nohighlight">\(k-1\)</span> autres parties serviront à calculer notre modèle. Le résultat de cette procédure est un vecteur de <span class="math notranslate nohighlight">\(k\)</span> scores dont on peut calculer la moyenne, la variance, etc.</p></li>
</ul>
<p>On peut illustrer la méthode des <em>k-folds</em> via l’exemple suivant :</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\text{Appartient au train set: } \color{red}{\boxed{}}&amp;\text{ et appartient au test set: }\color{green}{\boxed{}}
\end{align}\]</div>
<div class="math notranslate nohighlight">
\[\begin{align}
\text{Step 1: }\color{green}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align}\]</div>
<div class="math notranslate nohighlight">
\[\begin{align}
\text{Step 2: }\color{red}{\boxed{}}\color{green}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align}\]</div>
<div class="math notranslate nohighlight">
\[\begin{align}
\text{Step 3: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{green}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align}\]</div>
<div class="math notranslate nohighlight">
\[\begin{align}
\text{Step 4: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{green}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align}\]</div>
<div class="math notranslate nohighlight">
\[\begin{align}
\text{Step 5: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{green}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align}\]</div>
<div class="math notranslate nohighlight">
\[\begin{align}
\text{Step 6: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{green}{\boxed{}}\color{red}{\boxed{}}
\end{align}\]</div>
<div class="math notranslate nohighlight">
\[\begin{align}
\text{Step 7: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{green}{\boxed{}}
\end{align}\]</div>
<p>La méthode <span class="math notranslate nohighlight">\(\texttt{cross_val_score}\)</span> de <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span> permet de réaliser cette procédure. On pourra renseigner le paramètre <span class="math notranslate nohighlight">\(\texttt{cv}\)</span> qui indique le nombre <span class="math notranslate nohighlight">\(k\)</span> et le paramètre <span class="math notranslate nohighlight">\(\texttt{scoring}\)</span> qui donne la métrique que l’on souhaite calculer.</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Proposez un <em>5-fold</em> avec la métrique <span class="math notranslate nohighlight">\(R^2\)</span> que vous appliquerez à une régression polynomiale de degré <span class="math notranslate nohighlight">\(5\)</span>.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>

<span class="c1">####### Complete this part ######## or die ####################</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="c1">###############################################################</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Le score R2 sur chacun des splits de notre k-fold:&#39;</span><span class="p">,</span> <span class="n">scores</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Comparez le score obtenu lors de votre validation croisée à un plot de la fonction estimée sur tout le jeu d’apprentissage.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="o">...</span>
<span class="c1">###############################################################</span>
<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="vi-l-effet-double-descente-bonus">
<h2>VI. L’effet “double descente” (Bonus ?)<a class="headerlink" href="#vi-l-effet-double-descente-bonus" title="Permalink to this headline">¶</a></h2>
<p>Comme vu précédemment, l’effet du bruit sur l’estimateur dépend du conditionnement de <span class="math notranslate nohighlight">\(X^TX\)</span>. Le conditionnement d’une matrice <span class="math notranslate nohighlight">\(A\)</span> inversible est donné par :</p>
<div class="math notranslate nohighlight">
\[C(A)=\lVert A^{-1}\rVert\lvert A\rVert\]</div>
<p>Il est évident que si <span class="math notranslate nohighlight">\(A\in{\mathbb{R}^{1\times 1}}^\star\)</span>, alors <span class="math notranslate nohighlight">\(C(A)=1\)</span>. Ce n’est absolument pas vrai dans le cas général.</p>
<p>L’exemple ci-dessous illustre cela via la norme de Frobenius (norme Euclidienne appliquée à une matrice, <span class="math notranslate nohighlight">\(\text{Tr}(A^TA)^{0.5}\)</span>). On préfèrera souvent la norme d’opérateur qui quantifie les effets d’amplification d’un vecteur <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> lorsqu’on calcule <span class="math notranslate nohighlight">\(A\boldsymbol{x}\)</span>. Cette norme d’opérateur est directement liée aux valeurs propres. Cependant, la norme de Frobenius est plus simple à calculer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;A=</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;C(A)=A^{-1}A=&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">A</span><span class="p">))</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>A=
 [[1.e+00 0.e+00]
 [0.e+00 1.e-04]]
C(A)=A^{-1}A=10000.000100000001
</pre></div>
</div>
</div>
</div>
<p>On remarque dans l’exemple que la matrice <span class="math notranslate nohighlight">\(A\)</span> possède une toute petite valeur propre qui est responsable de cet écart. L’exercice ci-dessous montre qu’au-delà des considérations théoriques, cela a des répercussions importantes et totalement inattendues en réalité.</p>
<p>Les simulations suivantes permettent de mettre en lumière cela. Elles sont construites comme décrit ci-dessous :</p>
<div class="math notranslate nohighlight">
\[\beta\sim\mathbb{U}(-2, 2)^d,\ d\in\mathbb{N}^\star\]</div>
<p>dit autrement, on fixe un vecteur de paramètres selon une loi uniforme qui dépend de la dimension du problème.
Nous avons ensuite :</p>
<div class="math notranslate nohighlight">
\[x\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I_d}) + \epsilon,\ \epsilon\sim\mathcal{N}(0, \sigma^2)\]</div>
<p>On construit ensuite un jeu de test de taille <span class="math notranslate nohighlight">\(500\)</span> et un jeu d’apprentissage de taille variable. L’objectif ici sera d’étudié l’effet de la taille du jeu d’apprentissage sur la qualité de notre modèle, qualité que l’on aura calculée sur le test. Pour chaque taille de jeu de données, l’expérience est répétée <span class="math notranslate nohighlight">\(50\)</span> fois (<span class="math notranslate nohighlight">\(\texttt{redo}\)</span>) afin de lisser les courbes obtenues.</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Exécutez une première fois le code puis jouez avec <span class="math notranslate nohighlight">\(\texttt{noise}\)</span> (i.e. <span class="math notranslate nohighlight">\(\sigma\)</span>) afin de voir ce qui se passe selon la quantité de bruit. Essayez de décrire rigoureusement ce que vous observez.</strong></p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1">####### Play with the noise #########</span>
<span class="n">noise</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="c1">#####################################</span>

<span class="n">d</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">redo</span> <span class="o">=</span> <span class="mi">50</span>

<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">mu</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">)]</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">)])</span>

<span class="n">test_size</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">test_size</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">test_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">noise</span>

<span class="n">errors</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">train_errors</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
    <span class="n">error</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">train_error</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">redo</span><span class="p">):</span>
        <span class="c1"># dataset construction</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">m</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">noise</span>
        
        <span class="c1"># param estimation</span>
        <span class="n">beta_pinv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
        
        <span class="c1"># risk estimation</span>
        <span class="n">error</span> <span class="o">+=</span> <span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">beta_pinv</span><span class="p">)</span><span class="o">-</span><span class="n">y_test</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="n">test_size</span><span class="o">*</span><span class="n">redo</span><span class="p">)</span>
        <span class="n">train_error</span> <span class="o">+=</span> <span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta_pinv</span><span class="p">)</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="n">m</span><span class="o">*</span><span class="n">redo</span><span class="p">)</span>
    <span class="n">train_errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_error</span><span class="p">)</span>
    <span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">train_errors</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Train error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">d</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Dimension&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">errors</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Risk estimation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_64_0.png" src="../_images/1_linear_regression_64_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">errors</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Risk estimation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">d</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Dimension&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_errors</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">train_errors</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Train error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">d</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Dimension&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_65_0.png" src="../_images/1_linear_regression_65_0.png" />
<img alt="../_images/1_linear_regression_65_1.png" src="../_images/1_linear_regression_65_1.png" />
</div>
</div>
<p>La ligne en pointillé sépare visuellement deux régimes différents. La transition d’un régime à l’autre se produit par une augmentation catastrophique de l’erreur de généralisation de notre modèle.</p>
<div class="admonition-question admonition">
<p class="admonition-title">Question</p>
<p><strong>Quelle particularité différentie les deux phases ?</strong></p>
</div>
<p>En réalité, les méthodes de <em>machine learning</em> traditionnelles se situent plutôt dans le régime de “droite”. L’étude de ce phénomène est poussée par les approches comme le <em>deep learning</em> qui sont souvent dans le régime de gauche. Comprendre ces phénomènes nous permet par exemple d’éclairer les raisons du succès du <em>deep learning</em>.</p>
</div>
<div class="section" id="vii-regularisation">
<h2>VII. Régularisation<a class="headerlink" href="#vii-regularisation" title="Permalink to this headline">¶</a></h2>
<p>Comme illustré par les quelques scénarios précédents dont le cas catastrophique de la double descente, une certaine parcimonie est attendue par notre modèle. On a pu notamment observer que les “mauvaises” fonctions du point de vue du risque de généralisation avaient une forte tendance à osciller n’importe comment. Au lieu de laisser jouer le “hasard” (ou plutôt le conditionnement de <span class="math notranslate nohighlight">\(X^TX\)</span>), nous pouvons contraindre notre optimisation à favoriser les solutions parcimonieuses ; c’est-à-dire des solutions qui n’oscillent pas n’importe comment.</p>
<p>Intuitivement, on va choisir une solution qui minimise à la fois le risque empirique <span class="math notranslate nohighlight">\(J(\boldsymbol{\beta})\)</span>, mais aussi une pénalité sur la taille des “oscillations”. En réalité, les oscillations sont directement contrôlées par la norme des paramètres : un grand poids rendra notre modèle très sensible à la moindre perturbation de la variable explicative associée.</p>
<p>Nous parlons d’optimisation régularisée lorsque la fonction à optimiser s’écrit de la manière suivante :</p>
<div class="math notranslate nohighlight">
\[J(\boldsymbol{\beta})=\frac{1}{m}\sum_{i=1}^nr(f_{\boldsymbol{\beta}}(\boldsymbol{x_i}), y_i)+\lambda P(\boldsymbol{\beta})\]</div>
<p>où <span class="math notranslate nohighlight">\(r:\mathcal{Y}\times\mathcal{Y}\mapsto \mathbb{R}^+\)</span> est notre risque élémentaire et <span class="math notranslate nohighlight">\(P:\mathbb{R}^d\mapsto\mathbb{R}^+\)</span> une pénalité sur notre vecteur de paramètres. Plus précisément, dans le cas de la régression linéaire, nous avons :</p>
<div class="margin sidebar">
<p class="sidebar-title">Dans <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span></p>
<p>La régression linéaire sans pénalité s’obtient avec <span class="math notranslate nohighlight">\(\texttt{LinearRegression}\)</span>. Celle avec pénalité <span class="math notranslate nohighlight">\(\ell_2\)</span> s’obtient avec <span class="math notranslate nohighlight">\(\texttt{Ridge}\)</span>, avec pénalité <span class="math notranslate nohighlight">\(\ell_1\)</span> est <span class="math notranslate nohighlight">\(\texttt{Lasso}\)</span> et Elastic-Net <span class="math notranslate nohighlight">\(\texttt{ElasticNet}\)</span>.</p>
</div>
<div class="math notranslate nohighlight">
\[r(\hat{y}, y)=(\hat{y}-y)^2\]</div>
<p>et</p>
<div class="math notranslate nohighlight">
\[P(\boldsymbol{\beta})=\lVert \boldsymbol{\beta} \rVert,\]</div>
<p>où <span class="math notranslate nohighlight">\(\lVert \cdot \rVert\)</span> est une norme quelconque. Les choix classiques sont la norme <span class="math notranslate nohighlight">\(\ell_1\)</span> :</p>
<div class="math notranslate nohighlight">
\[\lVert \boldsymbol{\beta} \rVert_1=\sum_j |\boldsymbol{\beta}_j|\]</div>
<p>et la norme <span class="math notranslate nohighlight">\(\ell_2\)</span> :</p>
<div class="math notranslate nohighlight">
\[\lVert \boldsymbol{\beta} \rVert_2 = \sqrt{\sum_j\boldsymbol{\beta}_j^2}=\sqrt{\boldsymbol{\beta}^T\boldsymbol{\beta}}\]</div>
<p>Une stratégie intermédiaire consiste à prendre la combinaison convexe des deux normes :</p>
<div class="math notranslate nohighlight">
\[P(\boldsymbol{\beta})=\eta \lVert \boldsymbol{\beta} \rVert_1 + (1-\eta) \lVert \boldsymbol{\beta} \rVert_2.\]</div>
<p>avec <span class="math notranslate nohighlight">\(\eta\in\big[0,1\big]\)</span>. On parle alors d’<em>elastic-net</em>.</p>
<p>Ces différentes régularisations ne se comportent pas de la même manière. Ainsi la régularisation <span class="math notranslate nohighlight">\(\ell_1\)</span>, aussi appelée Lasso, va forcer certains paramètres à atteindre la valeur <span class="math notranslate nohighlight">\(0\)</span>. Cela permet par exemple de favoriser l’explicabilité de notre modèle. En pratique, <span class="math notranslate nohighlight">\(\ell_2\)</span>, appelée Ridge, a tendance à donner les meilleurs résultats d’un point de vue prédictif.</p>
<div class="section" id="id1">
<h3>A. Construction du jeu de données<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">beta_cube</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">sample_data_cube</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">noise</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">150</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_71_0.png" src="../_images/1_linear_regression_71_0.png" />
</div>
</div>
</div>
<div class="section" id="b-sans-regularisation">
<h3>B. Sans régularisation<a class="headerlink" href="#b-sans-regularisation" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">LinearRegression</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_74_0.png" src="../_images/1_linear_regression_74_0.png" />
</div>
</div>
</div>
<div class="section" id="c-avec-regularisation-ell-1">
<h3>C. Avec régularisation <span class="math notranslate nohighlight">\(\ell_1\)</span><a class="headerlink" href="#c-avec-regularisation-ell-1" title="Permalink to this headline">¶</a></h3>
<p>Lorsqu’on parle de régresion linéaire avec régularisation <span class="math notranslate nohighlight">\(\ell_1\)</span>, on parle aussi de Lasso.</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Testez plusieurs valeurs de <span class="math notranslate nohighlight">\(\alpha\)</span> (<span class="math notranslate nohighlight">\(=\lambda\)</span> dans notre texte). Quelle est la fonction la plus parcimonieuse que vous arrivez à obtenir ?</strong></p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">15.</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_78_0.png" src="../_images/1_linear_regression_78_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Les paramètres du modèle sont sparses :</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">model</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Les paramètres du modèle sont sparses :
 [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00901764]
</pre></div>
</div>
</div>
</div>
<p>Vous avez du constater que selon la quantité de régularisation, les paramètres étaient plus ou moins sparse. Il se trouve qu’une fois qu’un paramètre est à 0, il le sera pour toutes les valeurs de <span class="math notranslate nohighlight">\(\alpha\)</span> suppérieures. Afin d’observer visuellement, l’effet de la régularisation sur la sparsité, il est possible d’afficher ce qu’on appelle les “chemins Lasso” ou Lasso paths.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">lars_path</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">X_transformed</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">coefs</span> <span class="o">=</span> <span class="n">lars_path</span><span class="p">(</span><span class="n">X_transformed</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;lasso&#39;</span><span class="p">)</span>

<span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">coefs</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">xx</span> <span class="o">/=</span> <span class="n">xx</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">coefs</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">ymin</span><span class="p">,</span> <span class="n">ymax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">()</span>
<span class="c1"># plt.vlines(xx, ymin, ymax, linestyle=&#39;dashed&#39;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;|coef| / max|coef|&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Coefficients&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;LASSO Path&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_81_0.png" src="../_images/1_linear_regression_81_0.png" />
</div>
</div>
<p>A gauche se trouve le paramètre le plus parcimonieux (la plus grande valeur de <span class="math notranslate nohighlight">\(\alpha\)</span>). Tous les paramètres y sont donc nuls. Plus la valeur de <span class="math notranslate nohighlight">\(\alpha\)</span> est réduite, plus le nombre de paramètres différents de <span class="math notranslate nohighlight">\(0\)</span> augmente et leur valeur aussi.</p>
</div>
<div class="section" id="d-avec-regularisation-ell-2">
<h3>D. Avec régularisation <span class="math notranslate nohighlight">\(\ell_2\)</span><a class="headerlink" href="#d-avec-regularisation-ell-2" title="Permalink to this headline">¶</a></h3>
<p>Lorsqu’on parle de régresion linéaire avec régularisation <span class="math notranslate nohighlight">\(\ell_2\)</span>, on parle aussi de Ridge.</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Testez plusieurs valeurs de <span class="math notranslate nohighlight">\(\alpha\)</span> (<span class="math notranslate nohighlight">\(=\lambda\)</span> dans notre texte). Quelle est la fonction la plus parcimonieuse que vous arrivez à obtenir ?</strong></p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1000</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_86_0.png" src="../_images/1_linear_regression_86_0.png" />
</div>
</div>
</div>
<div class="section" id="e-avec-regularisation-elastic-net">
<h3>E. Avec régularisation <em>elastic-net</em><a class="headerlink" href="#e-avec-regularisation-elastic-net" title="Permalink to this headline">¶</a></h3>
<p>Lorsqu’on parle de régresion linéaire avec régularisation <em>elastic-net</em>.</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Testez plusieurs valeurs de <span class="math notranslate nohighlight">\(\alpha\)</span> (<span class="math notranslate nohighlight">\(=\lambda\)</span> dans notre texte). Quelle est la fonction la plus parcimonieuse que vous arrivez à obtenir (Essayez de trouver la réponse en raisonnant) ?</strong></p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNet</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">ElasticNet</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.8</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_90_0.png" src="../_images/1_linear_regression_90_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="viii-selection-de-modeles">
<h2>VIII. Selection de modèles<a class="headerlink" href="#viii-selection-de-modeles" title="Permalink to this headline">¶</a></h2>
<p>En pratique, nous ne pouvons pas choisir la valeur des paramètres (e.g. degré, régularisation) à l’oeil comme précédemment. Il nous faut (1) un algorithme qui automatise cette tâche et (2) une stratégie d’évaluation rigoureuse afin d’éviter les biais de confirmation (sur-apprentissage).</p>
<div class="section" id="id2">
<h3>A. Construction du jeu de données<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">beta_cube</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">sample_data_cube</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">noise</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">150</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_94_0.png" src="../_images/1_linear_regression_94_0.png" />
</div>
</div>
</div>
<div class="section" id="b-recherche-exhaustive">
<h3>B. Recherche exhaustive<a class="headerlink" href="#b-recherche-exhaustive" title="Permalink to this headline">¶</a></h3>
<p>L’algorithme de recherche par grille va exhaustivement testé tous les paramètres donnés. Pour chacun combinaison, une validation <em>k-fold</em> est réalisée. Le modèle retenu sera celui qui aura maximisé son score moyen lors du <em>k-fold</em>.</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Utilisez l’objet <span class="math notranslate nohighlight">\(\texttt{GridSearchCV}\)</span> afin de trouver la meilleure combinaison de paramètres selon le dictionnaire décrit ci-dessous. Toutes les combinaisons seront-elles réellement testées ?</strong></p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNet</span><span class="p">,</span> <span class="n">Ridge</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">,</span> <span class="n">Lasso</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">[</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">LinearRegression</span><span class="p">()],</span> 
   <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]},</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">Ridge</span><span class="p">()],</span> 
   <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> 
   <span class="s1">&#39;model__alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">]},</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">Lasso</span><span class="p">()],</span> 
   <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
   <span class="s1">&#39;model__alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]},</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">ElasticNet</span><span class="p">()],</span> <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
  <span class="s1">&#39;model__alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">],</span>
  <span class="s1">&#39;model__l1_ratio&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]},</span>
 <span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">10</span><span class="p">)),</span> <span class="p">(</span><span class="s1">&#39;model&#39;</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">())])</span>
<span class="c1">####### Complete this part ######## or die ####################</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="c1">###############################################################</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Le meilleur modele est&#39;</span><span class="p">,</span> <span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="c-recherche-aleatoire">
<h3>C. Recherche aléatoire<a class="headerlink" href="#c-recherche-aleatoire" title="Permalink to this headline">¶</a></h3>
<p>Une recherche exhaustive peut rapidement être limitante. Imaginons que nous testions déjà <span class="math notranslate nohighlight">\(10000\)</span> combinaisons de paramètres. Rajoutons maintenant un paramètre avec 50 modalités. Le nombre de combinaisons est donc multiplié par <span class="math notranslate nohighlight">\(50\)</span> et on monte à <span class="math notranslate nohighlight">\(500000\)</span> combinaisons. L’algorithme devient <span class="math notranslate nohighlight">\(50\)</span> fois plus lent.</p>
<p>Une stratégie alternative est de s’appuyer sur le hasard. On peut spécifier a priori des distributions sur les paramètres en supposant que certaines combinaisons fourniront probablement plus de bons résultats que d’autres. Par défaut, le tirage est uniforme. Cette méthode n’est pas absurde car plusieurs combinaisons peuvent très bien obtenir des résultats très proches. L’approche aléatoire sera ainsi beaucoup plus efficaces que la recherche exhaustive pour des performances généralement assez proches.</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Complétez le code ci-dessous afin de réaliser une recherche randomisée. Quel paramètre permet de jouer sur le nombre de tirages ?</strong></p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">[</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">LinearRegression</span><span class="p">()],</span> 
   <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)]},</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">Ridge</span><span class="p">()],</span> 
   <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)],</span> 
   <span class="s1">&#39;model__alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">]},</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">Lasso</span><span class="p">()],</span> 
   <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)],</span>
   <span class="s1">&#39;model__alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]},</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">ElasticNet</span><span class="p">()],</span> <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)],</span>
  <span class="s1">&#39;model__alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">],</span>
  <span class="s1">&#39;model__l1_ratio&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]},</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">10</span><span class="p">)),</span> <span class="p">(</span><span class="s1">&#39;model&#39;</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">())])</span>

<span class="n">search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipe</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GridSearchCV(estimator=Pipeline(steps=[(&#39;poly&#39;, PolynomialFeatures(degree=10)),
                                       (&#39;model&#39;, LinearRegression())]),
             n_jobs=-1,
             param_grid=[{&#39;model&#39;: [LinearRegression()],
                          &#39;poly__degree&#39;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
                                           12, 13, 14, 15, 16, 17, 18, 19]},
                         {&#39;model&#39;: [Ridge()],
                          &#39;model__alpha&#39;: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6,
                                           0.7, 0.8, 0.9, 1.0, 10.0, 100.0,
                                           100.0],
                          &#39;poly__degree&#39;: [1, 2, 3, 4, 5,...
                                           12, 13, 14, 15, 16, 17, 18, 19]},
                         {&#39;model&#39;: [Lasso()],
                          &#39;model__alpha&#39;: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6,
                                           0.7, 0.8, 0.9, 1.0],
                          &#39;poly__degree&#39;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
                                           12, 13, 14, 15, 16, 17, 18, 19]},
                         {&#39;model&#39;: [ElasticNet()],
                          &#39;model__alpha&#39;: [0.1, 0.2, 0.5, 0.8, 10.0, 100.0,
                                           100.0],
                          &#39;model__l1_ratio&#39;: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6,
                                              0.9, 1.0],
                          &#39;poly__degree&#39;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
                                           12, 13, 14, 15, 16, 17, 18, 19]}])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Le meilleur modele est&#39;</span><span class="p">,</span> <span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Le meilleur modele est Pipeline(steps=[(&#39;poly&#39;, PolynomialFeatures(degree=3)),
                (&#39;model&#39;, LinearRegression())])
</pre></div>
</div>
<img alt="../_images/1_linear_regression_105_1.png" src="../_images/1_linear_regression_105_1.png" />
</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="c1">###############################################################</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Le meilleur modele est&#39;</span><span class="p">,</span> <span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Le meilleur modele est&#39;</span><span class="p">,</span> <span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Le meilleur modele est Pipeline(steps=[(&#39;poly&#39;, PolynomialFeatures(degree=3)),
                (&#39;model&#39;, LinearRegression())])
</pre></div>
</div>
<img alt="../_images/1_linear_regression_107_1.png" src="../_images/1_linear_regression_107_1.png" />
</div>
</div>
</div>
</div>
<div class="section" id="ix-le-mot-de-la-fin">
<h2>IX. Le mot de la fin<a class="headerlink" href="#ix-le-mot-de-la-fin" title="Permalink to this headline">¶</a></h2>
<p>Ce propos introductif nous a permis de toucher du doigt la notion de sur-apprentissage. Quand est-ce que le meilleur modèle sur notre jeu d’apprentissage est suffisament bon en général ? Quand peut-on considérer qu’un modèle est suffisamment bon ? Aurions-nous pu trouver un meilleur modèle avec une procédure d’apprentissage différente ?</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./2_regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="0_propos_liminaire.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">La <em>régression</em></p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="2_optimization.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">L’optimisation ☕️☕️</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Servajean, Leveau & Chailan<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>La r√©gression lin√©aire ‚òïÔ∏è &#8212; Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="L‚Äôoptimisation ‚òïÔ∏è‚òïÔ∏è" href="2_optimization.html" />
    <link rel="prev" title="La r√©gression" href="0_propos_liminaire.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-72WWYCKNK6"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-72WWYCKNK6');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../0_requisite/rappels_python.html">
   Rappels de Python et Numpy
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../1_what_is_ml/0_propos_liminaire.html">
   <em>
    Machine Learning
   </em>
   , initiation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/1_introduction_ml.html">
     <em>
      Machine learning
     </em>
     et mal√©diction de la dimension
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/2_regression_and_classification_trees.html">
     Les arbres de r√©gression et de classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="0_propos_liminaire.html">
   La
   <em>
    r√©gression
   </em>
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     La r√©gression lin√©aire ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="2_optimization.html">
     L‚Äôoptimisation ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3_interpolation.html">
     Interpolation ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="4_algo_proximal_lasso.html">
     Sous-diff√©rentiel et le cas du Lasso ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5_least_square_qr.html">
     Les moindres carr√©s via une d√©composition QR (et plus)‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="6_ridge.html">
     Une analyse de la r√©gularisation Ridge ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../3_classification/0_propos_liminaire.html">
   La
   <em>
    classification
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/1_logistic_regression.html">
     La r√©gression logistique ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/2_fonctions_proxy.html">
     Les fonctions de perte (loss function) ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/3_bayes_classifier.html">
     Le classifieur de Bayes ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/4_VC_theory.html">
     Un mod√®le formel de l‚Äôapprentissage ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è (üíÜ‚Äç‚ôÇÔ∏è)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../4_kernel_methods/0_propos_liminaire.html">
   Les m√©thodes √† noyaux
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../4_kernel_methods/1_svm.html">
     Le SVM ou l‚Äôhypoth√®se max-margin ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5_ensembles/0_propos_liminaire.html">
   Les m√©thodes
   <em>
    ensemblistes
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/1_ensembles.html">
     M√©thodes ensemblistes ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/2_bayesian_linear_regression.html">
     Bayesian linear regression ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../6_deeplearning/0_propos_liminaire.html">
   <em>
    deep learning
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/1_autodiff.html">
     La diff√©rentiation automatique et un d√©but de
     <em>
      deep learning
     </em>
     ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/2_filters_representation.html">
     Filtres et espace de repr√©sentation des r√©seaux de neurones ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/3_probabilities_calibration.html">
     Calibration des probabilit√©s et quelques notions ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/4_regularization_deep.html">
     R√©gularisation en
     <em>
      deep learning
     </em>
     ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/5_transfer_multitask.html">
     <em>
      Transfer learning
     </em>
     et apprentissage multi-t√¢ches ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/6_adversarial.html">
     Les attaques adversaires ‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../7_unsupervised/0_propos_liminaire.html">
   L‚Äôapprentissage non-supervis√©
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_unsupervised/1_principal_component_analysis.html">
     L‚ÄôAnalyse en Composantes Principales ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_unsupervised/2_em_gaussin_mixture_model.html">
     Mod√®le de M√©lange Gaussien et algorithme
     <em>
      Expectation-Maximization
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../8_set_prediction/0_propos_liminaire.html">
   Pr√©diction d‚Äôensembles
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/1_well_defined.html">
     Jeu d‚Äôapprentissage
     <em>
      set-valued
     </em>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/2_ill_defined.html">
     Jeu d‚Äôapprentissage uniquement multi-classes ‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../biblio.html">
   Bibliographie
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/2_regression/1_linear_regression.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/maximiliense/lmiprp"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/maximiliense/lmiprp/issues/new?title=Issue%20on%20page%20%2F2_regression/1_linear_regression.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/maximiliense/lmiprp/blob/master/Travaux Pratiques/Machine Learning/Book/2_regression/1_linear_regression.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#i-introduction">
   I. Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ii-construction-d-un-jeu-de-donnees">
   II. Construction d‚Äôun jeu de donn√©es
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iii-du-modele-statistique-a-l-optimisation">
   III. Du mod√®le statistique √† l‚Äôoptimisation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-la-fonction-objectif">
     A. La fonction objectif
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-optimisation-par-descente-de-gradient">
     B. Optimisation par Descente de gradient
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#c-a-vous-de-jouer">
     C. √Ä vous de jouer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#d-l-algorithme-de-descente-de-gradient">
     D. L‚Äôalgorithme de descente de gradient
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#e-les-equations-normales-de-la-regression-lineaire-la-solution-par-pseudo-inverse">
     E. Les √©quations normales de la r√©gression lin√©aire : la solution par pseudo-inverse
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#f-a-vous-de-jouer">
     F. √Ä vous de jouer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#g-avec-sklearn">
     G. Avec sklearn
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iv-features-variables-explicatives-transformees">
   IV. Features - Variables explicatives transform√©es
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-construction-du-jeu-de-donnees-polynomial">
     A. Construction du jeu de donn√©es polynomial
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-solution-par-pseudo-inverse">
     B. Solution par pseudo-inverse
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#c-solution-sklearn">
     C. Solution Sklearn
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#v-la-validation-croisee-et-le-dilemme-biais-variance">
   V. La validation crois√©e et le dilemme biais-variance
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-construction-du-jeu-de-donnees">
     A. Construction du jeu de donn√©es
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-optimiser-une-fonction-est-il-suffisant-pour-parler-d-apprentissage">
     B. Optimiser une fonction est-il suffisant pour parler d‚Äôapprentissage ?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vi-l-effet-double-descente-bonus">
   VI. L‚Äôeffet ‚Äúdouble descente‚Äù (Bonus ?)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vii-regularisation">
   VII. R√©gularisation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     A. Construction du jeu de donn√©es
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-sans-regularisation">
     B. Sans r√©gularisation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#c-avec-regularisation-ell-1">
     C. Avec r√©gularisation
     <span class="math notranslate nohighlight">
      \(\ell_1\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#d-avec-regularisation-ell-2">
     D. Avec r√©gularisation
     <span class="math notranslate nohighlight">
      \(\ell_2\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#e-avec-regularisation-elastic-net">
     E. Avec r√©gularisation
     <em>
      elastic-net
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#viii-selection-de-modeles">
   VIII. Selection de mod√®les
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     A. Construction du jeu de donn√©es
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-recherche-exhaustive">
     B. Recherche exhaustive
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#c-recherche-aleatoire">
     C. Recherche al√©atoire
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ix-le-mot-de-la-fin">
   IX. Le mot de la fin
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="la-regression-lineaire">
<h1>La r√©gression lin√©aire ‚òïÔ∏è<a class="headerlink" href="#la-regression-lineaire" title="Permalink to this headline">¬∂</a></h1>
<div class="admonition-objectifs-de-la-sequence admonition">
<p class="admonition-title">Objectifs de la s√©quence</p>
<ul class="simple">
<li><p>Concevoir¬†:</p>
<ul>
<li><p>la r√©gression lin√©aire d‚Äôun point de vue pr√©dictif,</p></li>
<li><p>la r√©gression lin√©aire au travers d‚Äôun probl√®me d‚Äôoptimisation.</p></li>
</ul>
</li>
<li><p>√ätre capable¬†:</p>
<ul>
<li><p>d‚Äôimpl√©menter un algorithme de descente de gradient,</p></li>
<li><p>de transformer les variables d‚Äôentr√©e pour rendre le mod√®le non lin√©aire,</p></li>
<li><p>d‚Äôutiliser la librairie <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span>.</p></li>
</ul>
</li>
<li><p>De s‚Äôinitier √† la notion de r√©gularisation et de s√©lection de variables.</p></li>
</ul>
</div>
<div class="section" id="i-introduction">
<h2>I. Introduction<a class="headerlink" href="#i-introduction" title="Permalink to this headline">¬∂</a></h2>
<p>La r√©gression lin√©aire est un mod√®le cherchant √† √©tablir un lien lin√©aire entre des donn√©es d‚Äôobservation et des donn√©es √† pr√©dire. Plus concr√®tement, les donn√©es observ√©es sont d√©crites par un vecteur <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^d\)</span> et la variable √† pr√©dire, par une quantit√© scalaire (un r√©el) <span class="math notranslate nohighlight">\(y \in \mathbb{R}\)</span>. On notera <span class="math notranslate nohighlight">\(\mathbf{x}, y\sim \mathbb{P}\)</span> la loi jointe du couple (par un abus de langage important, <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> et <span class="math notranslate nohighlight">\(y\)</span> expriment √† la fois une variable al√©atoire et sa r√©alisation) avec <span class="math notranslate nohighlight">\(\mu\)</span> la marginale de <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> et le lien s‚Äôexprime sous le format suivant¬†:</p>
<div class="math notranslate nohighlight">
\[y = \beta_0 +  x_1 \beta_1 + x_2 \beta_2 + x_3 \beta_3 + ... + x_d \beta_d +\epsilon = \beta_0 + \sum_i^d x_i \beta_i+\epsilon,\ \epsilon\sim\mathcal{N}(0, \sigma)\]</div>
<p>que l‚Äôon peut aussi √©crire en notation vectorielle¬†:</p>
<div class="math notranslate nohighlight">
\[y = \beta_0  + \langle \boldsymbol{\beta}, \mathbf{x} \rangle +\epsilon,\ \epsilon\sim\mathcal{N}(0, \sigma)\]</div>
<p>o√π <span class="math notranslate nohighlight">\(\boldsymbol{\beta} \in \mathbb{R}^d\)</span> et <span class="math notranslate nohighlight">\(\beta_0\in\mathbb{R}\)</span> correspondent respectivement au vecteur et au scalaire contenant les param√®tres du ‚Äúvrai‚Äù mod√®le qui d√©fini le lien entre les donn√©es et que l‚Äôon va vouloir apprendre pour pr√©dire la bonne valeur de <span class="math notranslate nohighlight">\(y\)</span> en fonction du vecteur <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. Le mod√®le lin√©aire ne peut pr√©dire la variable <span class="math notranslate nohighlight">\(y\)</span> qu‚Äô√† un bruit <span class="math notranslate nohighlight">\(\epsilon\)</span> pr√®s. Une fois ces param√®tres appris par notre algorithme d‚Äôapprentissage, on pourra utiliser la fonction de pr√©diction <span class="math notranslate nohighlight">\(f_{\boldsymbol{\beta}}(\mathbf{x}): \mathbb{R}^d \rightarrow \mathbb{R}\)</span> apprise pour pr√©dire la valeur <span class="math notranslate nohighlight">\(y_{new}\)</span> associ√©e √† un nouveau vecteur <span class="math notranslate nohighlight">\(\mathbf{x_{new}}\)</span> que l‚Äôon n‚Äôa pas encore observ√©¬†:</p>
<div class="math notranslate nohighlight">
\[\hat{y}_{new} = f_{\boldsymbol{\beta}}(\boldsymbol{x_{new}}) = \beta_0  + \langle \boldsymbol{\beta}, \boldsymbol{x_{new}} \rangle\]</div>
<p>Pour simplifier les calculs et les notations, on pr√©f√®re que la fonction de pr√©diction puisse se calculer √† partir d‚Äôune notation compl√®tement vectorielle. C‚Äôest ce que l‚Äôon fait en pratique, en ajoutant une composante suppl√©mentaire <span class="math notranslate nohighlight">\(x_0\)</span> au vecteur <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> √©gale √† <span class="math notranslate nohighlight">\(1\)</span>¬†:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    \mathbf{x} &amp;= \begin{bmatrix}
          1 \\
           x_{1} \\
           \vdots \\
           x_{d}
         \end{bmatrix},
\end{align*}\end{split}\]</div>
<p>de sorte √† ce que la fonction de pr√©diction lin√©aire puisse s‚Äôexprimer simplement sous la forme du produit scalaire:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    f_{\boldsymbol{\beta}}(\mathbf{x}) &amp;= \langle \boldsymbol{\beta}, \mathbf{x} \rangle &amp;=
          \begin{bmatrix}
           \beta_{0} \\           
           \vdots \\
           \beta_{d}
          \end{bmatrix}^T
          \begin{bmatrix}
           1 \\
           \vdots \\
           x_{d}
         \end{bmatrix} &amp;= \sum_{i=0}^d x_i \beta_i=\langle \boldsymbol{x}, \boldsymbol{\beta}\rangle_{\mathbb{R}^{d+1}}
\end{aligned}\end{split}\]</div>
<p>o√π cette fois <span class="math notranslate nohighlight">\(\boldsymbol{\beta} \in \mathbb{R}^{d+1}\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{x} \in \mathbb{R}^{d+1}\)</span> et <span class="math notranslate nohighlight">\(\langle \cdot, \cdot\rangle_{\mathbb{R}^{d+1}}\)</span> est le produit scalaire dans <span class="math notranslate nohighlight">\(\mathbb{R}^{d+1}\)</span>. Le but d‚Äôun algorithme d‚Äôapprentissage sera de trouver un estimateur <span class="math notranslate nohighlight">\(\boldsymbol{\hat{\beta}}\)</span> de <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> √† partir d‚Äôun ensemble fini de <span class="math notranslate nohighlight">\(n\)</span> exemples d‚Äôapprentissage <span class="math notranslate nohighlight">\((\boldsymbol{x}, \langle \boldsymbol{\beta}, \boldsymbol{x} \rangle + \epsilon) \in \mathbb{R}^2\)</span> pr√©alablement collect√©s. On notera <span class="math notranslate nohighlight">\(\mathcal{S}=\{(\boldsymbol{x_i}, y_i)\}_{i\leq n}\)</span> le jeu de donn√©es.</p>
<div class="margin sidebar">
<p class="sidebar-title">Mod√®le pr√©dictif, descriptif</p>
<p>Les mod√®les pr√©dictifs cherchent √† r√©aliser une pr√©diction relative √† une t√¢che quelconque apr√®s avoir pu observer une donn√©e <span class="math notranslate nohighlight">\(x\)</span>. Le mod√®le √©tant bien entendu ‚Äúappris‚Äù √† partir d‚Äôun jeu de donn√©es d‚Äôapprentissage.</p>
<p>Les mod√®les descriptifs cherchent √† quantifier les liens entre les variables explicatives et la variable √† expliquer.</p>
<p>Que ce soit pour une analyse pr√©dictive ou une analyse descriptive, nous pouvons utiliser un mod√®le lin√©aire. Cependant, dans le cadre d‚Äôune analyse pr√©dictive, le crit√®re cl√© sera vraiment la qualit√© de la pr√©diction sur des <em><strong>donn√©es nouvelles</strong></em>.</p>
</div>
<p>Nous commencerons par impl√©menter le cas simple d‚Äôune r√©gr√©ssion lin√©aire √† une seule variable d‚Äôentr√©e et une seule variable de sortie qui pourra donc s‚Äô√©crire sous la forme¬†:</p>
<div class="math notranslate nohighlight">
\[\hat{y} = f_{\boldsymbol{\beta}}(\mathbf{x}) = \beta_0  + \beta_1 x\]</div>
<p>C‚Äôest √† dire une fonction affine dont on pourra afficher la repr√©sentation graphique (une droite) sur une figure en 2 dimensions. Par la suite vous aurez donc √† impl√©menter le calcul de la fonction de co√ªt du mod√®le sur l‚Äôensemble d‚Äôapprentissage, le calcul du gradient de cette fonction de co√ªt ainsi que l‚Äôalgorithme de descente de gradient qui, √† partir du gradient, permet d‚Äôobtenir le vecteur <span class="math notranslate nohighlight">\(\hat{\beta}\)</span>.</p>
<p>La seconde partie de ce notebook √©tendra ces notions √† des concepts plus compliqu√©s.</p>
</div>
<div class="section" id="ii-construction-d-un-jeu-de-donnees">
<h2>II. Construction d‚Äôun jeu de donn√©es<a class="headerlink" href="#ii-construction-d-un-jeu-de-donnees" title="Permalink to this headline">¬∂</a></h2>
<p>Commen√ßons tout d‚Äôabord par simuler notre jeu de donn√©es avec le mod√®le g√©n√©ratif suivant¬† :</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{x} \sim \mathcal{N}(\mu, \sigma) \in \mathbb{R}\]</div>
<p>ou <span class="math notranslate nohighlight">\(\sigma\)</span> correspond √† la variance de la variable explicative. Nous choissons une r√®gle arbitraire pour g√©n√©rer al√©atoirement les param√®tres du ‚Äúvrai‚Äù mod√®le¬†:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\beta} \sim \mathbb{U}(-1, 1)^2 \in \mathbb{R}^2\]</div>
<p>o√π <span class="math notranslate nohighlight">\(\mathbb{U}^2\)</span> est la loi uniforme dans <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>. Enfin, le bruit est construit de la mani√®re suivante¬†:</p>
<div class="math notranslate nohighlight">
\[\epsilon \sim \mathcal{N}(0, 1).\]</div>
<p>Chaque exemple d‚Äôapprentissage correspond donc √† un couple de r√©els <span class="math notranslate nohighlight">\((x_j, y_j = \beta_0  + \beta_1 x_j + \epsilon) \in \mathbb{R}^2\)</span>. Le code ci dessous construit et affiche le jeux de donn√©es ainsi que la repr√©sentation graphique de <span class="math notranslate nohighlight">\(f(x)=\beta_1x+\beta_0\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># on simule le vecteur de parametre</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># on construit un jeu de donnees de 10 points selon la methode </span>
<span class="c1"># decrite ci-dessus.</span>
<span class="k">def</span> <span class="nf">sample_data</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">add_noise</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">add_noise</span><span class="o">*</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">beta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">noise</span> <span class="o">+</span> <span class="n">beta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_data</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">add_noise</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># jouer avec le bruit</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># configuration generale de matplotlib</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">12.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>

<span class="c1"># plot de la fonction</span>
<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">ymin_</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="n">eps</span>
    <span class="n">ymax_</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="n">eps</span>
    <span class="n">min_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="n">eps</span>
    <span class="n">max_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="n">eps</span>
    <span class="k">if</span> <span class="n">beta</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">min_</span><span class="p">,</span> <span class="n">max_</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">y_</span> <span class="o">=</span> <span class="n">beta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_</span> <span class="o">+</span> <span class="n">beta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span> <span class="n">y_</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">func</span> <span class="o">=</span> <span class="p">[(</span><span class="n">func</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)]</span> <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">func</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">list</span> <span class="k">else</span> <span class="n">func</span>
            <span class="n">disp_legend</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">func</span><span class="p">:</span>
                <span class="n">y_</span> <span class="o">=</span> <span class="n">t</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x_</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">x_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)))</span>
                <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span> <span class="n">y_</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">t</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">disp_legend</span> <span class="o">=</span> <span class="n">disp_legend</span> <span class="ow">or</span> <span class="n">t</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="s1">&#39;&#39;</span>
            <span class="k">if</span> <span class="n">disp_legend</span><span class="p">:</span>
                <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="n">min_</span><span class="p">,</span> <span class="n">max_</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="n">ymin_</span><span class="p">,</span> <span class="n">ymax_</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    
<span class="c1"># on plot le dataset precedent</span>
<span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_5_0.png" src="../_images/1_linear_regression_5_0.png" />
</div>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Que se passe-t-il si le bruit <span class="math notranslate nohighlight">\(\epsilon\)</span> est nul ? Quelle est alors la m√©thode la plus rapide pour trouver les param√®tres du vrai mod√®le ?</strong></p>
</div>
<div class="tip admonition">
<p class="admonition-title">Interpolation</p>
<p>Lorsque le bruit disparait, la droite passe exactement par tous les points (√† moins que l‚Äôhypoth√®se <span class="math notranslate nohighlight">\(y=\langle\beta, x\rangle\)</span> soit fausse). On parle alors d‚Äôinterpolation.</p>
</div>
</div>
<div class="section" id="iii-du-modele-statistique-a-l-optimisation">
<h2>III. Du mod√®le statistique √† l‚Äôoptimisation<a class="headerlink" href="#iii-du-modele-statistique-a-l-optimisation" title="Permalink to this headline">¬∂</a></h2>
<div class="section" id="a-la-fonction-objectif">
<h3>A. La fonction objectif<a class="headerlink" href="#a-la-fonction-objectif" title="Permalink to this headline">¬∂</a></h3>
<p>Nous souhaitons en pratique trouver un param√™tre <span class="math notranslate nohighlight">\(\boldsymbol{\hat{\beta}}\)</span> qui minimise le risque du mod√®le, c‚Äôest-√†-dire la quantit√© d‚Äôerreur en esp√©rance de n‚Äôimporte quel mod√®le <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>. On notera <span class="math notranslate nohighlight">\(\boldsymbol{\beta}^\star\)</span> le ‚Äúvrai‚Äù mod√®le, soit celui qui minimise le risque en esp√©rance. Pour la r√©gression lin√©aire, on peut d√©finir ce risque comme¬†:</p>
<div class="math notranslate nohighlight">
\[R(\boldsymbol{\beta}) = \mathbb{E}_{X\times Y}\Big[ (f_{\boldsymbol{\beta}}(\mathbf{X}) - Y)^2 \Big].\]</div>
<p>On ne sait pas calculer cette fonction car on ne conna√Æt pas la distribution jointe et o√π qu‚Äôon ne sait pas calculer l‚Äôint√©grale. Cependant, on peut en avoir un estimateur via un jeu de donn√©es <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>, o√π <span class="math notranslate nohighlight">\(\mathcal{S} = \Big\{ \big(\boldsymbol{x_j}, y_j \big) \Big\}_{j\leq n}\)</span> est un jeu de donn√©es compos√© de <span class="math notranslate nohighlight">\(n\)</span> points ind√©pendants et identiquement distribu√©s selon le mod√®le g√©n√©ratif d√©crit pr√©c√©dement.</p>
<p>A d√©faut d‚Äôavoir acc√®s au risque (i.e. √† l‚Äôerreur en esp√©rance), on peut utiliser une autre quantit√© qui consiste en la somme des carr√©s des erreurs de pr√©dictions pour chaque exemple d‚Äôapprentissage, c‚Äôest <strong>le risque emprique</strong>¬†:</p>
<div class="math notranslate nohighlight">
\[J(\boldsymbol{\beta}) = R_{emp}(\boldsymbol{\beta}) = \frac{1}{n}\sum_j^n (f_{\boldsymbol{\beta}}(x_j) - y_j)^2\]</div>
<p>o√π <span class="math notranslate nohighlight">\(f_{\boldsymbol{\beta}}(x_j) = \beta_0  + \beta_1 x_j\)</span>. On montre assez facilement que pour un <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> quelconque¬†:</p>
<div class="math notranslate nohighlight">
\[R(\boldsymbol{\beta})=\mathbb{E}_{\mathcal{S \sim \mathbb{P}_S}}\big[J(\boldsymbol{\beta})\big],\]</div>
<p>Notons que minimiser ce risque empirique revient √† chercher le maximum de vraisemblance du mod√®le statistique. Effectivement, avec l‚Äôhypoth√®se gaussienne, la vraissemblance de n‚Äôimporte quel mod√®le de param√®tres <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> pour un jeu de donn√©es <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> peut s‚Äô√©crire¬†:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\mathcal{S}}(\boldsymbol{\beta}) \propto \prod_{\boldsymbol{x}\times y\in\mathcal{S}} \exp\Bigg(-\frac{\big(f_{\boldsymbol{\beta}}(\mathbf{x}) - y\big)^2}{2}\Bigg)\]</div>
<p>Le param√®tre maximisant la vraissamblance est aussi celui minimisant la log-vraissamblance n√©gative¬†:</p>
<div class="math notranslate nohighlight">
\[- \text{log} \Big( \mathcal{L}_{\mathcal{S}}(\boldsymbol{\beta})\Big) = \sum_{\boldsymbol{x}\times y\in\mathcal{S}}\frac{\big(f_{\boldsymbol{\beta}}(\mathbf{x}) - y\big)^2}{2}\propto\sum_{\boldsymbol{x}\times y\in\mathcal{S}}\big(f_{\boldsymbol{\beta}}(\mathbf{x}) - y\big)^2\]</div>
<p>N‚Äôayant acc√®s au v√©ritable risque, on cherche <span class="math notranslate nohighlight">\(\boldsymbol{\hat{\beta}}\)</span> tel que¬†:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\hat{\beta}} = \text{argmin}_{\boldsymbol{\beta}} \Big[ - \log \Big( \mathcal{L}_{\mathcal{S}}(\boldsymbol{\beta})\Big) \Big]\]</div>
<p>Minimiser le risque emprique se traduit donc naturellement par un probl√®me d‚Äôoptimisation de la fonction de co√ªt <span class="math notranslate nohighlight">\(J(\boldsymbol{\beta}) : \mathbb{R}^2 \rightarrow \mathbb{R}\)</span> (<span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span> dans notre exemple courant, <span class="math notranslate nohighlight">\(\mathbb{R}^{d+1}\)</span> dans le cas g√©n√©ral) par rapport √† <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>.</p>
<p>En pratique et pour des raisons de simplicit√©, on ne minimise pas <span class="math notranslate nohighlight">\(\sum_{\boldsymbol{x}\times y\in\mathcal{S}}\big(f_{\boldsymbol{\beta}}(\mathbf{x}) - y\big)^2\)</span> mais¬†:</p>
<div class="math notranslate nohighlight">
\[J(\boldsymbol{\beta}) = \frac{1}{2n}\sum_{\boldsymbol{x}\times y\in\mathcal{S}} (f_{\boldsymbol{\beta}}(x) - y)^2\]</div>
<p>Le r√©sultat est bien √©videmment le m√™me. La division par <span class="math notranslate nohighlight">\(2\)</span> est l√† pour simplifier l‚Äôexpresion du gradient que l‚Äôon calculera et la division par <span class="math notranslate nohighlight">\(n\)</span> permet de rendre la norme du gradient ind√©pendente de la taille de notre jeu de donn√©es. C‚Äôest une propri√©t√© importante pour l‚Äôalgorithme de descente de gradient dont la taille des d√©placements affecte sa stabilit√©.</p>
<p><strong>Note - Notation vectorielle de la r√©gression lin√©aire :</strong> On peut aussi exprimer ce calcul avec une √©quation en notation vectorielle. Pour cela, on exprime dans un premier temps le r√©sultat de la fonction de pr√©diction en notation vectorielle (il s‚Äôagit de la pr√©diction pour tout notre jeu de donn√©es)¬†:</p>
<div class="math notranslate nohighlight">
\[f_{\boldsymbol{\beta}}(\mathbf{X}) = \mathbf{X}\boldsymbol{\beta}\in\mathbb{R}^n\]</div>
<p>o√π <span class="math notranslate nohighlight">\(\boldsymbol{\beta} \in \mathbb{R}^{d+1}\)</span> est une matrice de dimensions <span class="math notranslate nohighlight">\((d+1)\times 1\)</span> (en <span class="math notranslate nohighlight">\(\texttt{numpy}\)</span>, la dimension <span class="math notranslate nohighlight">\(1\)</span> est importante) et <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> est une matrice de dimensions <span class="math notranslate nohighlight">\(n\times (d+1)\)</span> dont les <span class="math notranslate nohighlight">\(n\)</span> vecteurs lignes correspondent aux vecteurs d‚Äôapprentissage d‚Äôentr√©e. Dans notre cas (celui de la r√©gression lin√©aire √† <span class="math notranslate nohighlight">\(1\)</span> variable) la matrice prend la forme suivante¬†:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbf{X} = 
\begin{pmatrix} 
1 &amp; x_{1} \\
\vdots &amp; \vdots\\
1 &amp; x_{j} \\
\vdots &amp; \vdots\\
1 &amp; x_{n} 
\end{pmatrix},\ \boldsymbol{\beta}=
\begin{bmatrix}
\beta_{0} \\           
\beta_{1}
\end{bmatrix}
\end{aligned}\end{split}\]</div>
<p>La fonction de co√ªt peut ainsi s‚Äôexprimer¬†:</p>
<div class="math notranslate nohighlight">
\[J(\boldsymbol{\beta}) = \frac{1}{2n} (\mathbf{X}\boldsymbol{\beta} - \mathbf{y})^T(\mathbf{X}\boldsymbol{\beta} - \mathbf{y})\]</div>
<p>que l‚Äôon peut r√©√©crire¬†:</p>
<div class="math notranslate nohighlight">
\[J(\boldsymbol{\beta}) = \frac{1}{2n} (\mathbf{\hat{y}} - \mathbf{y})^T(\mathbf{\hat{y}} - \mathbf{y}) =  \frac{1}{2n} ||\mathbf{\hat{y}} - \mathbf{y}||_2^2\]</div>
<p>o√π <span class="math notranslate nohighlight">\(\mathbf{y} \in  \mathbb{R}^n\)</span> est le vecteur dont chacune des composantes <span class="math notranslate nohighlight">\(y_j\)</span> sont les valeurs √† pr√©dire √† partir de leur <span class="math notranslate nohighlight">\(x_j\)</span> correspondant, et <span class="math notranslate nohighlight">\(\hat{y} \in  \mathbb{R}^n\)</span> correspond aux valeurs pr√©dites par le mod√®le. On note ici que la fonction objectif √† optimiser peut se calculer ais√©ment en utilisant la norme euclidienne au carr√© du vecteur d‚Äôerreur.</p>
</div>
<div class="section" id="b-optimisation-par-descente-de-gradient">
<h3>B. Optimisation par Descente de gradient<a class="headerlink" href="#b-optimisation-par-descente-de-gradient" title="Permalink to this headline">¬∂</a></h3>
<div class="dropdown admonition">
<p class="admonition-title">Le gradient est orthogonal aux lignes de niveau</p>
<p>Soit <span class="math notranslate nohighlight">\(c:\mathbb{R}^+\mapsto\mathbb{R}^d\)</span> un arc param√©tr√© qui suit une ligne de niveau de <span class="math notranslate nohighlight">\(f\)</span> (un arc param√©tr√© prend en argument le ‚Äútemps‚Äù et retourne une coordonn√©e dans l‚Äôespace). Si <span class="math notranslate nohighlight">\(c\)</span> suit une ligne de niveau, nous avons donc¬†:</p>
<div class="math notranslate nohighlight">
\[f(c(t))=f(c(0))=\text{const}.\]</div>
<p>Cela implique que nous ayons aussi¬†:</p>
<div class="math notranslate nohighlight">
\[(f(c(t)))^\prime=\langle \nabla f(c(t)), c^\prime(t)\rangle=0,\]</div>
<p>o√π <span class="math notranslate nohighlight">\(\nabla f(c(t))\)</span> est le gradient en <span class="math notranslate nohighlight">\(c(t)\)</span> et <span class="math notranslate nohighlight">\(c^\prime(t)\)</span> donne la direction de l‚Äôarc param√©tr√© (i.e. de la ligne de niveau) en <span class="math notranslate nohighlight">\(c(t)\)</span>. Les deux sont bien ainsi orthogonaux.</p>
</div>
<p>La descente de gradient est une m√©thode d‚Äôoptimisation num√©rique permettant de trouver les valeurs des param√®tres qui minimisent une fonction. Dans notre cas, nous voulons minimiser l‚Äôerreur de pr√©diction moyenne de notre mod√®le, fonction d√©finie pr√©c√©demment. Cette m√©thode d‚Äôoptimisation consiste √† calculer le gradient de notre fonction objectif par rapport aux param√®tres courant du mod√®le et de les d√©placer par un ‚Äúpetit‚Äù pas dans la direction oppos√©e au gradient (i.e. le gradient donne la plus forte croissance et son oppos√© la plus forte d√©croissance).</p>
<p><strong>D√©finition g√©n√©rale du gradient d‚Äôune fonction √† plusieurs variables¬†:</strong> Il s‚Äôagit simplement du vecteur contenant les d√©riv√©es partielles de la fonction, c-√†-d les d√©riv√©es de la fonction par rapport √† chaque variable ind√©pendamment des autres¬†:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\nabla_{\boldsymbol{\beta}} J(\boldsymbol{\beta}) = \frac{\partial J(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} = 
\begin{bmatrix}
\frac{\partial J(\beta)}{\partial \beta_0}\\
\frac{\partial J(\beta)}{\partial \beta_1}\\
 \vdots \\
\frac{\partial J(\beta)}{\partial \beta_d}
\end{bmatrix}
\end{aligned}\end{split}\]</div>
<div class="dropdown admonition">
<p class="admonition-title">Le gradient est la plus forte pente</p>
<p>Soit <span class="math notranslate nohighlight">\(c:\mathbb{R}^+\mapsto\mathbb{R}^d\)</span> un arc param√©tr√© et soit <span class="math notranslate nohighlight">\(f:\mathbb{R}^d\mapsto\mathbb{R}\)</span>. Nous √©tudions l‚Äô√©volution de <span class="math notranslate nohighlight">\(f\)</span> le long de <span class="math notranslate nohighlight">\(c\)</span>¬†:</p>
<div class="math notranslate nohighlight">
\[f(c(t)).\]</div>
<p>L‚Äôaccroissement de <span class="math notranslate nohighlight">\(f\)</span> le long de <span class="math notranslate nohighlight">\(c(t)\)</span> est donn√© par la d√©riv√©e¬†:</p>
<div class="math notranslate nohighlight">
\[(f(c(t))^\prime=\langle \nabla f(c(t)), c^\prime(t)\rangle.\]</div>
<p>Nous avons par Cauchy-Schwartz¬†:</p>
<div class="math notranslate nohighlight">
\[\lVert\langle \nabla f(c), c^\prime\rangle\rVert \leq \lVert\nabla f(c)\rVert\lVert c^\prime\rVert,\]</div>
<p>o√π le gradient et l‚Äôarc sont √©valu√©s en <span class="math notranslate nohighlight">\(t\)</span>. L‚Äô√©galit√© est atteinte lorsque les vecteurs sont colin√©aires. La plus forte pente est donc la direction du gradient.</p>
</div>
<p>En descente de gradient, la mise √† jour de chaque param√®tre <span class="math notranslate nohighlight">\(\beta_j\)</span> du mod√®le √† l‚Äôit√©ration <span class="math notranslate nohighlight">\(t\)</span> se fait donc avec la r√®gle suivante¬†:</p>
<div class="math notranslate nohighlight">
\[\beta_j^{(t+1)} = \beta_j^{(t)} - \rho  \frac{\partial J(\beta^{(t)})}{\partial \beta_j}\]</div>
<p>ou bien, en notation vectorielle¬†:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \rho  \nabla_{\boldsymbol{\beta}} J(\boldsymbol{\beta})^{(t)}\]</div>
<p>o√π <span class="math notranslate nohighlight">\(\rho\)</span> est le learning rate (pas d‚Äôapprentissage). Un pas d‚Äôapprentissage <span class="math notranslate nohighlight">\(\rho\)</span> trop petit nous fera nous d√©placer trop lentement et trop grand rendra l‚Äôoptimisation instable.</p>
</div>
<div class="section" id="c-a-vous-de-jouer">
<h3>C. √Ä vous de jouer<a class="headerlink" href="#c-a-vous-de-jouer" title="Permalink to this headline">¬∂</a></h3>
<div class="admonition-question-1 admonition">
<p class="admonition-title">Question 1</p>
<p><strong>Compl√©tez la m√©thode <span class="math notranslate nohighlight">\(\texttt{val}\)</span> de l‚Äôobjet <span class="math notranslate nohighlight">\(\texttt{LeastSquare}\)</span> ci-dessous.</strong></p>
</div>
<div class="admonition-question-2 admonition">
<p class="admonition-title">Question 2</p>
<p><strong>Calculez les d√©riv√©es partielles <span class="math notranslate nohighlight">\(\partial J(\beta)/\partial \beta_0\)</span> et <span class="math notranslate nohighlight">\(\partial J(\beta)/\partial \beta_1\)</span> de la fonction de co√ªt de notre mod√®le de r√©gr√©ssion lin√©aire. Compl√©tez la m√©thode <span class="math notranslate nohighlight">\(\texttt{grad}\)</span> de l‚Äôobjet <span class="math notranslate nohighlight">\(\texttt{LeastSquare}\)</span> ci dessous.</strong></p>
</div>
<div class="hint dropdown admonition">
<p class="admonition-title">Indices</p>
<p>Rappellez vous que la d√©riv√©e d‚Äôune composition de fonctions s‚Äô√©crit <span class="math notranslate nohighlight">\((g \circ f)^\prime (x) = f^\prime(x) g^\prime(f(x))\)</span> et que la fonction de co√ªt de notre mod√®le s‚Äô√©crit¬†:</p>
<div class="math notranslate nohighlight">
\[J(\boldsymbol{\beta}) = \frac{1}{2n}\sum_j^n g(f_{\boldsymbol{\beta}}(x_j) - y_j)\]</div>
<p>avec <span class="math notranslate nohighlight">\(g(z) = z ^ 2\)</span> et <span class="math notranslate nohighlight">\(f_{\boldsymbol{\beta}}(x_j) = \beta_0  + \beta_1 x_j\)</span>.</p>
</div>
<div class="admonition-question-3-dur admonition">
<p class="admonition-title">Question 3 (dur)</p>
<p><strong>Calculez le gradient de la fonction <span class="math notranslate nohighlight">\(J(\boldsymbol{\beta})\)</span> en utilisant les d√©riv√©es matricielles. Compl√©tez la m√©thode <span class="math notranslate nohighlight">\(\texttt{grad}\)</span> de l‚Äôobjet <span class="math notranslate nohighlight">\(\texttt{LeastSquare}\)</span> avec le gradient en notation vectorielle.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LeastSquare</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">LeastSquare</span><span class="o">.</span><span class="n">_format_ndarray</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">LeastSquare</span><span class="o">.</span><span class="n">_format_ndarray</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pos</span> <span class="o">=</span> <span class="mi">0</span>
        
    <span class="k">def</span> <span class="nf">_format_ndarray</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
        <span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="k">else</span> <span class="n">arr</span>
        <span class="k">return</span> <span class="n">arr</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">arr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">arr</span>
    
    <span class="k">def</span> <span class="nf">val</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">LeastSquare</span><span class="o">.</span><span class="n">_format_ndarray</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
        <span class="c1">####### Complete this part ######## or die ####################</span>
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="c1">###############################################################</span>
        <span class="k">return</span> <span class="n">val</span>
    
    <span class="k">def</span> <span class="nf">_shuffle</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">batch_size</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span> <span class="k">else</span> <span class="n">batch_size</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_pos</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">_pos</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_pos</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pos</span><span class="o">+</span><span class="n">batch_size</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pos</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_shuffle</span><span class="p">()</span>
            
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

        <span class="n">beta</span> <span class="o">=</span> <span class="n">LeastSquare</span><span class="o">.</span><span class="n">_format_ndarray</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
        
        <span class="c1">####### Complete this part ######## or die ####################</span>
        <span class="o">...</span>
        <span class="c1">###############################################################</span>
        <span class="k">return</span> <span class="n">grad</span>
    

<span class="n">l</span> <span class="o">=</span> <span class="n">LeastSquare</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;La valeur de la loss pour le vrai parametre est&#39;</span><span class="p">,</span> <span class="n">l</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">beta</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;La valeur du gradient pour le vrai parametre est</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">l</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">beta</span><span class="p">))</span>
<span class="n">plot_loss_contour</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">three_dim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plot_loss_contour</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">three_dim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>Attention, pour des raisons de temps de calcul, l‚Äôestimation du gradient n‚Äôest pas tout le temps faite sur tout le jeu de donn√©es mais sur une partie de celui-ci. Un estimateur calcul√© de cette mani√®re l√† aura en esp√©rance la m√™me valeur qu‚Äôun gradient calcul√© sur toutes les donn√©es. On appelle g√©n√©ralement Descente de Gradient Stochastique ou SGD une approche qui ne fait qu‚Äôestimer le gradient √† partir d‚Äôun batch de donn√©es.</p>
<div class="admonition-question admonition">
<p class="admonition-title">Question</p>
<p><strong>Saurez-vous retrouver dans le code ci-dessus ce qui permet de jouer sur la taille du batch lors du calcul du gradient ?</strong></p>
</div>
</div>
<div class="section" id="d-l-algorithme-de-descente-de-gradient">
<h3>D. L‚Äôalgorithme de descente de gradient<a class="headerlink" href="#d-l-algorithme-de-descente-de-gradient" title="Permalink to this headline">¬∂</a></h3>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Compl√©tez le code de descente de gradient.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GradientDescent</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="n">init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">LeastSquare</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.005</span><span class="p">,</span> <span class="n">nb_iterations</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">init</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">param_trace</span> <span class="o">=</span> <span class="p">[</span><span class="n">beta</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
        <span class="n">loss_trace</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">beta</span><span class="p">)]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_iterations</span><span class="p">):</span>
            <span class="c1">####### Complete this part ######## or die ####################</span>
            <span class="o">...</span>
            <span class="c1">###############################################################</span>
            <span class="n">param_trace</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">beta</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">loss_trace</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">beta</span><span class="p">))</span>
            
        <span class="k">return</span> <span class="n">param_trace</span><span class="p">,</span> <span class="n">loss_trace</span>
        
<span class="n">gd</span> <span class="o">=</span> <span class="n">GradientDescent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">param_trace</span><span class="p">,</span> <span class="n">loss_trace</span> <span class="o">=</span> <span class="n">gd</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">nb_iterations</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>

<span class="n">param_trace</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">param_trace</span><span class="p">)</span>
<span class="n">loss_trace</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">loss_trace</span><span class="p">)</span>
<span class="n">loss_trace</span> <span class="o">=</span> <span class="n">loss_trace</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">loss_trace</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">xyz</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">param_trace</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">loss_trace</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plot_loss_contour</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">param_trace</span><span class="o">=</span><span class="n">xyz</span><span class="p">,</span> <span class="n">three_dim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plot_loss_contour</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">param_trace</span><span class="o">=</span><span class="n">param_trace</span><span class="p">,</span> <span class="n">three_dim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition-convergence-de-la-descente-de-gradient admonition">
<p class="admonition-title">Convergence de la descente de gradient</p>
<p>Notons <span class="math notranslate nohighlight">\(\beta^\star\)</span> une solution du probl√®me d‚Äôoptimisation ci-dessus. Si le pas d‚Äôapprentissage est assez petit, alors il est possible de d√©montrer que l‚Äôalgorithme de descente de gradient converge n√©cessairement vers <span class="math notranslate nohighlight">\(J(\beta^\star)\)</span> √† une vitesse proportionnelle √† <span class="math notranslate nohighlight">\(1/k\)</span> o√π <span class="math notranslate nohighlight">\(k\)</span> est le nombre d‚Äôit√©rations. La s√©quence <em><strong>optimisation</strong></em> de ce cours d√©montrera rigoureusement cela.</p>
</div>
<div class="admonition-stochastic-gradient-descent admonition">
<p class="admonition-title">Stochastic Gradient Descent</p>
<p>La propri√©t√© d‚Äôorthogonalit√© par rapport aux lignes de niveau de la fonction de co√ªt est elle conserv√©e dans ce cas ? Pourquoi ? Ne suit-on pourtant toujours pas le gradient ? Que pouvez vous dire sur la nature et la ‚Äúvitesse‚Äù de convergence vers le minimum de la fonction ? R√©fl√©chissez d‚Äôun point de vue calculatoire sur ce qui se passe sur des tailles d‚Äô√©chantillons tr√®s grandes ?</p>
</div>
</div>
<div class="section" id="e-les-equations-normales-de-la-regression-lineaire-la-solution-par-pseudo-inverse">
<h3>E. Les √©quations normales de la r√©gression lin√©aire : la solution par pseudo-inverse<a class="headerlink" href="#e-les-equations-normales-de-la-regression-lineaire-la-solution-par-pseudo-inverse" title="Permalink to this headline">¬∂</a></h3>
<p>Comme calcul√© plus haut, l‚Äôexpression du gradient est donn√©e par <span class="math notranslate nohighlight">\((X^TX\boldsymbol{\beta}-X^T\boldsymbol{y})/n\)</span>.  La fonction <span class="math notranslate nohighlight">\(J\)</span> √©tant coercive et convexe, elle admet au moins un minimum local/global. Les points critiques sont donn√©s en annulant le gradient¬†:</p>
<div class="math notranslate nohighlight">
\[X^TX\boldsymbol{\beta}-X^T\boldsymbol{y} = 0 \Leftrightarrow X^TX\boldsymbol{\beta}=X^t\boldsymbol{y}.\]</div>
<p>Il s‚Äôagit des √©quations dites ‚Äúnormales‚Äù. Tout vecteur <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> solution de ces √©quations est donc n√©cessairement un minimiseur de <span class="math notranslate nohighlight">\(J(\boldsymbol{\beta})\)</span>.</p>
<p><strong>Dans le cas standard (i.e. sur-d√©termin√©)</strong> o√π chaque variable explicative est lin√©airement ind√©pendante des autres et o√π le nombre d‚Äô√©chantillons de notre jeu de donn√©es est sup√©rieur ou √©gal √† la dimension du probl√®me consid√©r√©, la matrice <span class="math notranslate nohighlight">\(X^TX\)</span> est inversible (i.e. <span class="math notranslate nohighlight">\(\text{det}(X^TX)\neq 0\)</span>). Dit autrement, il existe une unique solution aux √©quations normales donn√©e par¬†:</p>
<div class="math notranslate nohighlight">
\[\hat{\boldsymbol{\beta}}=(X^TX)^{-1}X^T\boldsymbol{y}.\]</div>
<p>On appelle <span class="math notranslate nohighlight">\(X^\dagger = (X^TX)^{-1}X^T\)</span>  pseudo-inverse de <span class="math notranslate nohighlight">\(X\)</span> (ou inverse g√©n√©ralis√©e) et la solution analytique √† notre probl√®me est donn√©e par <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}=X^\dagger \boldsymbol{y}\)</span>.</p>
<div class="tip dropdown admonition">
<p class="admonition-title">L‚Äôeffet du bruit</p>
<p>Soit <span class="math notranslate nohighlight">\(\boldsymbol{y}=X\boldsymbol{\beta}+\eta\)</span> o√π on utilise <span class="math notranslate nohighlight">\(\eta\)</span> plut√¥t que <span class="math notranslate nohighlight">\(\epsilon\)</span> pour diff√©rentier la r√©alisation effective du bruit de la variable al√©atoire. Notre estimateur est donc¬†:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}\hat{\beta}&amp;=X^\dagger y = X^\dagger(X\boldsymbol{\beta} + \eta)\\ &amp;= (X^\dagger X)\boldsymbol{\beta} + X^\dagger\eta.\end{aligned}\end{split}\]</div>
<p>On observe, par propri√©t√© de la pseudo-inverse, que la premi√®re contribution est la projection orthogonale du vrai mod√®le sur l‚Äôespace des vecteurs ligne de <span class="math notranslate nohighlight">\(X\)</span>. Il est donc une combinaison lin√©aire des vecteurs que l‚Äôon voit pendant l‚Äôapprentissage ! La deuxi√®me contribution est l‚Äôeffet du bruit sur la solution optimale. Nous discuterons plus loin de ces contributions et d‚Äôeffets √©tranges qui peuvent se produire notament quand la matrice <span class="math notranslate nohighlight">\(X\)</span> est mal conditionn√©e (le ratio entre la plus grande valeur propre de <span class="math notranslate nohighlight">\(X^TX\)</span> et sa plus petite valeur propre est tr√®s grand).</p>
</div>
<p><strong>Dans le cas non standard (i.e. sous-d√©termin√©)</strong> o√π certaines variables peuvent √™tre des combinaisons lin√©aires d‚Äôautres variables (inutile en pratique) ou si le nombre d‚Äô√©chantillons est inf√©rieur √† la dimension, <span class="math notranslate nohighlight">\(X^TX\)</span> n‚Äôest plus inversible. Dans ce cas de figure, il existe une infinit√© de solutions aux √©quations normales (i.e. une infinit√© de minimiseurs). Dans ce cas, la solution de norme minimale est donn√©e par¬†:</p>
<div class="math notranslate nohighlight">
\[\hat{\beta}=X^\dagger y = X^T(XX^T)^{-1}y.\]</div>
<p><strong>Concernant le cas g√©n√©ral (i.e. sous et sur-d√©termin√©),.</strong> E. H. Moore (1920), A. Bjerhammar (1951) et R. Penrose (1955) proposent ind√©pendamment une expression g√©n√©rale de <span class="math notranslate nohighlight">\(X^\dagger\)</span> appel√©e pseudo-inverse de Moore-Penrose et calculable √† partir d‚Äôune d√©composition en valeur singuli√®re, not√©e <span class="math notranslate nohighlight">\(X^\dagger\)</span>. Celle-ci co√Øncide bien s√ªr avec l‚Äôexpression standard lorsqu‚Äôelle existe. On obtient donc une expression analytique g√©n√©rale, solution des √©quations normales¬†:</p>
<div class="math notranslate nohighlight">
\[\hat{\boldsymbol{\beta}}=X^\dagger\boldsymbol{y},\]</div>
<p>o√π <span class="math notranslate nohighlight">\(X^\dagger\)</span> est le pseudo-inverse de Moore-Penrose. Celle-ci est obtenue par une d√©composition en valeurs singuli√®res <span class="math notranslate nohighlight">\(X=U\Sigma V^T\)</span> de la mani√®re suivante¬†: <span class="math notranslate nohighlight">\(X^\dagger = V\Sigma^{-1}U^T\)</span> o√π <span class="math notranslate nohighlight">\(\Sigma^{-1}\)</span> est la matrice <span class="math notranslate nohighlight">\(\Sigma\)</span> o√π nous avons invers√©s les valeurs singuli√®res non nulles. La s√©quence de cours ‚ÄúLes moindres carr√©s via une d√©composition QR (et plus)‚Äù d√©taille cela.</p>
<div class="tip dropdown admonition">
<p class="admonition-title">Un syst√®me d‚Äô√©quations</p>
<p>En r√©alit√©, minimiser les moindres carr√©s revient √† r√©soudre un syst√®me d‚Äô√©quation¬†:</p>
<div class="math notranslate nohighlight">
\[Ax=y\text{ ou }Ax\approx y.\]</div>
<p>Si le probl√®me poss√®de autant d‚Äô√©quation que d‚Äôinconnues, alors le probl√®me est bien pos√© et admet une solution. Si le nombre d‚Äô√©quations (lin√©airement ind√©pendantes) est sup√©rieur au nombre d‚Äôinconnues, alors le probl√®me est sur-d√©termin√© et on ne peut r√©soudre que <span class="math notranslate nohighlight">\(Ax\approx y\)</span>. Enfin, si le nombre d‚Äô√©quations est inf√©rieur au nombre d‚Äôinconnues, alors il existe une infinit√© de solutions et on choisira celle de norme minimale.</p>
</div>
<p><em><strong>Quelques pr√©cisions d‚Äôalg√®bre</strong></em>¬† : finalement, quel est le lien entre une pseudo-inverse et l‚Äôinverse classique. Soit une application lin√©aire <span class="math notranslate nohighlight">\(A:\mathbb{R}^n\mapsto\mathbb{R}^n\)</span> repr√©sent√©e par une matrice <span class="math notranslate nohighlight">\(A\in\mathbb{R}^{n\times n}\)</span>. On appelle inverse de <span class="math notranslate nohighlight">\(A\)</span> l‚Äôunique matrice, not√©e <span class="math notranslate nohighlight">\(A^{-1}\)</span>, telle que  <span class="math notranslate nohighlight">\(A^{-1}A=\text{Id}\)</span>. Dans le cas inversible, l‚Äôinverse de <span class="math notranslate nohighlight">\(A^{-1}\)</span> est donc de mani√®re √©vidente <span class="math notranslate nohighlight">\(A\)</span>. Cela revient √† transformer un vecteur <span class="math notranslate nohighlight">\(x\in\mathbb{R}^n\)</span> par <span class="math notranslate nohighlight">\(A\)</span> puis √† annuler sa transformation par <span class="math notranslate nohighlight">\(A^{-1}\)</span>. L‚Äôinverse n‚Äôexiste cependant pas toujours. Ainsi, par exemple, si <span class="math notranslate nohighlight">\(\text{ker}(A)\neq \{\boldsymbol{0}\}\)</span> (i.e. le noyau ne se r√©sume pas √† l‚Äô√©l√©ment nul, nous avons <span class="math notranslate nohighlight">\(\forall x\in\mathbb{R}^n,\ u\in\text{ker}(A)\)</span> que <span class="math notranslate nohighlight">\(A(x+u)=Ax\)</span>. Finalement l‚Äôinverse de <span class="math notranslate nohighlight">\(Ax\)</span> est-il <span class="math notranslate nohighlight">\(x\)</span> ou <span class="math notranslate nohighlight">\(x+u\)</span> ?</p>
<p>Reprenons le cas de la pseudo-inverse. Quelques propri√©t√©s qui peuvent sembler √©videntes √©mergent¬†:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
AA^\dagger A&amp;=A\text{ (appliquer }A\text{, son inverse }A^\dagger\text{ puis }A\text{ √† nouveau revient √† appliquer }A\text{)}\\
A^\dagger AA^\dagger&amp;=A^\dagger\text{ (c'est la m√™me chose du point de vu de l'inverse)}\\
(AA^\dagger)^T&amp;=AA^\dagger\text{ (la transposition n'a pas d'effet)}\\
(A^\dagger A)^T&amp;=A^\dagger A\text{ (m√™me chose que pr√©c√©demment du point de vu de l'inverse)}
\end{align*}\end{split}\]</div>
<p>La pseudo inverse est l‚Äôunique matrice <span class="math notranslate nohighlight">\(A^\dagger\)</span> satisfaisant les propri√©t√©s pr√©c√©dentes. Dans le cas o√π <span class="math notranslate nohighlight">\(A\)</span> est inversible, on a alors <span class="math notranslate nohighlight">\(A^\dagger=A^{-1}\)</span>. Intuitivement, l‚Äôid√©e est de ne consid√©rer ‚Äúque‚Äù les √©l√©ments qui ne sont pas dans le noyaux. Ainsi <span class="math notranslate nohighlight">\(\text{Im}(A)=\text{Ker}(A^\dagger)^\perp\)</span> et inversement.</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>V√©rifier que les deux inverses propos√©es dans les cas sur-d√©termin√© et sous-d√©termin√© v√©rifient bien les √©galit√©s pr√©c√©dentes.</strong></p>
</div>
<p>La question √† laquelle nous pouvons maintenant essayer de r√©pondre est en quoi une pseudo-inverse permet d‚Äôobtenir une solution acceptable au probl√®me des moindres carr√©s. Id√©alement, nous aurions¬†:</p>
<div class="math notranslate nohighlight">
\[Ax = y \text{ g√©n√©ralement not√© }X\beta=y.\]</div>
<p>En pratique, il n‚Äôexiste pas n√©cessairement de vecteur <span class="math notranslate nohighlight">\(x\)</span> tel que cette √©galit√© soit satisfaite. Dit autrement, <span class="math notranslate nohighlight">\(y\)</span> n‚Äôest pas dans l‚Äôimage de <span class="math notranslate nohighlight">\(A\)</span>, not√©e <span class="math notranslate nohighlight">\(\text{Im}(A)\)</span>. Nous cherchons ainsi √† trouver <span class="math notranslate nohighlight">\(x\)</span> tel que <span class="math notranslate nohighlight">\(\lVert Ax-y\rVert_2\)</span> est minimis√©. Notons <span class="math notranslate nohighlight">\(K(A)\)</span> le noyau de <span class="math notranslate nohighlight">\(A\)</span>. Consid√©rons maintenant la proposition suivante¬†:</p>
<div class="admonition-proposition admonition">
<p class="admonition-title">Proposition</p>
<div class="math notranslate nohighlight">
\[\text{proj}_{\text{Im}(A)}=AA^\dagger \text{ (projection orthogonale sur l'image de }A\text{)}\]</div>
<div class="math notranslate nohighlight">
\[\text{proj}_{\text{K}(A^T)}=I-AA^\dagger\]</div>
<div class="math notranslate nohighlight">
\[\text{proj}_{\text{Im}(A^T)}=A^\dagger A\]</div>
<div class="math notranslate nohighlight">
\[\text{proj}_{\text{K}(A)}=I-A^\dagger A\]</div>
</div>
<div class="caution dropdown admonition">
<p class="admonition-title">Preuve</p>
<p>D√©montrons la premi√®re √©galit√©.</p>
<p>Nous avons <span class="math notranslate nohighlight">\((AA^\dagger)^T=AA^\dagger\)</span> et notre application est donc sym√©trique. De plus,</p>
<div class="math notranslate nohighlight">
\[(AA^\dagger)(AA^\dagger)=(AA^\dagger A)A^\dagger = AA^\dagger\]</div>
<p>et notre application est donc <a class="reference external" href="https://fr.wikipedia.org/wiki/Idempotence">idempotente</a>. L‚Äôidempotence implique que <span class="math notranslate nohighlight">\(\text{proj}_{\text{Im}(X)}\)</span> est un projecteur (non n√©cessairement orthogonal).</p>
<p>Soit <span class="math notranslate nohighlight">\(y\in\text{Im}(A)\)</span>. Il existe donc <span class="math notranslate nohighlight">\(x\)</span> tel que <span class="math notranslate nohighlight">\(y=Ax\)</span>. Nous avons donc¬†:</p>
<div class="math notranslate nohighlight">
\[\text{proj}_{\text{Im}(A)}(y)=AA^\dagger y=AA^\dagger A x=Ax=y.\]</div>
<p>Ainsi, si <span class="math notranslate nohighlight">\(y\in\text{Im}(A)\)</span>, le projecteur le projette sur lui-m√™me. Consid√©rons maintenant <span class="math notranslate nohighlight">\(z\in \text{Im}(A)^\perp\)</span>. Dit autrement, <span class="math notranslate nohighlight">\((Ax)^Tz=0=x^TA^Tz,\ \forall x\)</span> impliquant <span class="math notranslate nohighlight">\(A^Tz=0\)</span>.
Nous avons donc¬†:</p>
<div class="math notranslate nohighlight">
\[\text{proj}_{\text{Im}(A)}(z)=AA^\dagger z = (AA^\dagger)^T z = (A^\dagger)^TA^Tz = 0.\]</div>
<p>Tous les √©l√©ments orthogonaux √† l‚Äôimage de <span class="math notranslate nohighlight">\(A\)</span> sont projet√©s sur <span class="math notranslate nohighlight">\(0\)</span>. Nous avons donc bien un projecteur orthogonal.</p>
</div>
<p>Cela nous permet de r√©diger la proposition suivante¬†:</p>
<div class="admonition-proposition admonition">
<p class="admonition-title">Proposition</p>
<p>La solution de norme minimale de <span class="math notranslate nohighlight">\(\lVert Ax-y\rVert_2\)</span> est donn√©e par¬†:</p>
<div class="math notranslate nohighlight">
\[x=A^\dagger y\]</div>
</div>
<div class="caution dropdown admonition">
<p class="admonition-title">Preuve</p>
<p>Comme nous l‚Äôavons vu, en pratique l‚Äô√©quation¬†:</p>
<div class="math notranslate nohighlight">
\[Ax= y\]</div>
<p>n‚Äôadmet pas de solution et nous devons consid√©rer celle qui minimise l‚Äôerreur au carr√© (i.e. la norme euclidienne). Le vecteur de r√©sidus qui minimisera l‚Äôerreur est celui qui sera orthogonal √† l‚Äôimage de <span class="math notranslate nohighlight">\(A\)</span> (la plus courte distance, Pythagore tout √ßa). Dit autrement, on cherche √† r√©soudre le syst√®me d‚Äô√©quations suivant¬†:</p>
<div class="math notranslate nohighlight">
\[Ax=\text{proj}_{\text{Im}(A)}(y)=AA^\dagger y.\]</div>
<p>Si <span class="math notranslate nohighlight">\(y\)</span> est d√©j√† dans l‚Äôimage et qu‚Äôune solution existe, alors c‚Äôest cela ne change rien puisque <span class="math notranslate nohighlight">\(\text{proj}_{\text{Im}(A)}(y)=y\)</span>, sinon on cherche la solution la plus proche. Nous avons donc¬†:</p>
<div class="math notranslate nohighlight">
\[Ax=AA^\dagger y\Leftrightarrow A(x-A^\dagger y)=0\]</div>
<p>On constate ainsi que <span class="math notranslate nohighlight">\((x-A^\dagger y)\in K(A)\)</span>. N‚Äôimporte quel √©l√©ment <span class="math notranslate nohighlight">\(x\)</span> tel que <span class="math notranslate nohighlight">\(x-A^\dagger y\)</span> est dans le noyau de <span class="math notranslate nohighlight">\(A\)</span> satisfait l‚Äô√©quation ci-dessus¬†:</p>
<div class="math notranslate nohighlight">
\[x-A^\dagger y = \text{proj}_{\text{K}(A)}(w)=(I-A^\dagger A)w\]</div>
<p>o√π <span class="math notranslate nohighlight">\(w\)</span> est choisi arbitrairement. Une solution g√©n√©rale pour <span class="math notranslate nohighlight">\(x\)</span> est donc¬†:</p>
<div class="math notranslate nohighlight">
\[x=A^\dagger y + (I-A^\dagger A)w,\]</div>
<p>o√π on remarque que les deux termes sont des vecteurs orthogonaux. Ainsi, si on privil√©gie le vecteur de norme minimale, nous obtenons <span class="math notranslate nohighlight">\(x=A^\dagger y\)</span> et la pseudo-inverse est une m√©thode acceptable pour d√©terminer la solution des moindres carr√©s.</p>
</div>
</div>
<div class="section" id="f-a-vous-de-jouer">
<h3>F. √Ä vous de jouer<a class="headerlink" href="#f-a-vous-de-jouer" title="Permalink to this headline">¬∂</a></h3>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Calculez la solution du probl√®me de r√©gression lin√©aire en utilisant la pseudo-inverse de Moore-Penrose propos√©e par <span class="math notranslate nohighlight">\(\texttt{numpy}\)</span> via <span class="math notranslate nohighlight">\(\texttt{np.linalg.pinv}\)</span>.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">matplotlib</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">12.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">)</span>
<span class="c1"># descente de gradient</span>

<span class="c1"># descente de gradient sans stochasticite</span>
<span class="n">param_trace</span> <span class="p">,</span> <span class="n">loss_trace</span> <span class="o">=</span> <span class="n">gd</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.04</span><span class="p">,</span> <span class="n">nb_iterations</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span> 

<span class="c1">####### Complete this part ######## or die ####################</span>
<span class="c1"># solution par pseudo inverse</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="c1">###############################################################</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;La loss pour la solution par pseudo-inverse est&#39;</span><span class="p">,</span> <span class="n">l</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">beta_pinv</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;La loss pour la solution obtenue par descente de gradient est&#39;</span><span class="p">,</span> <span class="n">loss_trace</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">beta_pinv</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition-remarques-et-question admonition">
<p class="admonition-title">Remarques et question</p>
<p><strong>On remarque ici que la valeur de la loss atteinte par GD est plus haute que celle atteinte par la solution de la pseudo-inverse. A votre avis pourquoi ? Augmentez le nombre d‚Äôit√©rations de GD. Que constatez vous ? Est-ce √©tonnant par rapport √† votre brillante d√©monstration sur GD dans la section pr√©c√©dente ? Au passage, on pourrait s‚Äôamuser √† montrer qu‚Äôavec l‚Äôinitialisation <span class="math notranslate nohighlight">\(\beta=\boldsymbol{0}\)</span>, chaque step reste bien dans l‚Äôespace engendr√© par les vecteurs lignes de X. Qui veut passer au tableau ?</strong></p>
</div>
</div>
<div class="section" id="g-avec-sklearn">
<h3>G. Avec sklearn<a class="headerlink" href="#g-avec-sklearn" title="Permalink to this headline">¬∂</a></h3>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Proposez une r√©gression lin√©aire sur le m√™me probl√®me en utilisant <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span>.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="c1">####### Complete this part ######## or die ####################</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="c1">###############################################################</span>
<span class="n">coef</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="n">coef</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;La loss pour la solution obtenue par Sklearn est&#39;</span><span class="p">,</span> <span class="n">l</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">coef</span><span class="p">))</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">coef</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="iv-features-variables-explicatives-transformees">
<h2>IV. Features - Variables explicatives transform√©es<a class="headerlink" href="#iv-features-variables-explicatives-transformees" title="Permalink to this headline">¬∂</a></h2>
<p>Dans beaucoup de probl√®mes r√©els, la variable √† expliquer n‚Äôest pas une simple combinaison lin√©aire des variables explicatives. Cela peut-√™tre une d√©pendence non lin√©aire (e.g. quadratique), ou des d√©pendences crois√©es entre nos variables explicatives. La strat√©gie permettant d‚Äôaborder cette probl√©matique consiste √† transformer notre vecteur <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> en rajoutant par exemple des transformations quadratiques et √† optimiser notre mod√®le lin√©aire sur le vecteur transform√©. Afin de simplifier les notations, nous allons volontairement omettre le biais <span class="math notranslate nohighlight">\(\beta_0\)</span> de nos notations.</p>
<p>Construire nos <em>features</em> consiste √† chercher une fonction <span class="math notranslate nohighlight">\(\phi:\mathbb{R}^d\mapsto\mathbb{R}^p\)</span> qui transforme non-lin√©airement nos variables explicatives initiales.</p>
<p>Le probl√®me se reformule ainsi de la mani√®re suivante :</p>
<div class="math notranslate nohighlight">
\[\hat{y}=\langle \phi(\boldsymbol{x}), \boldsymbol{\beta}\rangle\]</div>
<p>Il suffit donc de transformer nos variables explicatives par <span class="math notranslate nohighlight">\(\phi\)</span> et de consid√©rer le r√©sultat comme nos nouvelles variables explicatives.</p>
<div class="section" id="a-construction-du-jeu-de-donnees-polynomial">
<h3>A. Construction du jeu de donn√©es polynomial<a class="headerlink" href="#a-construction-du-jeu-de-donnees-polynomial" title="Permalink to this headline">¬∂</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># vrais parametres</span>
<span class="n">beta_cube</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">sample_data_cube</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">noise</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">150</span><span class="p">)</span>

<span class="c1">#affichage du polynome</span>
<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_38_0.png" src="../_images/1_linear_regression_38_0.png" />
</div>
</div>
</div>
<div class="section" id="b-solution-par-pseudo-inverse">
<h3>B. Solution par pseudo-inverse<a class="headerlink" href="#b-solution-par-pseudo-inverse" title="Permalink to this headline">¬∂</a></h3>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Compl√©tez le code ci-dessous en utilisant une solution par pseudo-inverse via <span class="math notranslate nohighlight">\(\texttt{numpy}\)</span>.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Polynomial</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">deg</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">deg</span> <span class="o">=</span> <span class="n">deg</span>

    <span class="k">def</span> <span class="nf">_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1"># here we transform the input into a polynomial</span>
        <span class="c1">####### Complete this part ######## or die ####################</span>
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="k">return</span> <span class="o">...</span>
        <span class="c1">###############################################################</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="c1">####### Complete this part ######## or die ####################</span>
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="c1">###############################################################</span>
        
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;You must fit the model first&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">X_transformed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_transformed</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">errors</span> <span class="o">=</span> <span class="p">(</span><span class="n">prediction</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span><span class="mi">2</span>
        <span class="k">return</span> <span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="n">errors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
<span class="c1"># vraie solution</span>
<span class="n">real_model</span> <span class="o">=</span> <span class="n">Polynomial</span><span class="p">(</span><span class="n">deg</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">real_model</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta_cube</span>
</pre></div>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Le plot est affich√© avec un jeu dit de test. Il s‚Äôagit d‚Äôun ensemble de points qui n‚Äôont pas √©t√© utilis√©s lors de notre apprentissage (par pseudo-inverse). Le jeu d‚Äôapprentissaget et de test n‚Äôont souvent pas la m√™me taille.</strong></p>
<p><strong>Jouez avec le degr√© du polyn√¥me que vous manipulez et observez le r√©sultat. Que constatez-vous ?</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deg</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Polynomial</span><span class="p">(</span><span class="n">deg</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="p">[(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">,</span> <span class="s1">&#39;Our model&#39;</span><span class="p">),</span> <span class="p">(</span><span class="n">real_model</span><span class="o">.</span><span class="n">predict</span><span class="p">,</span> <span class="s1">&#39;Real model&#39;</span><span class="p">)])</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Empirical risk: &#39;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
<div class="admonition-remarque-et-question admonition">
<p class="admonition-title">Remarque et question</p>
<p><strong>Le risque empirique est celui calcul√© directement sur les donn√©es utilis√©es lors du calcul du pseudo-inverse. Que constatez-vous par rapport √† ce dernier lorsque vous jouez avec le degr√© du polyn√¥me ?</strong></p>
<p><strong>Est-il un bon indicateur du v√©ritable risque de g√©n√©ralisation ? Autrement dit, est-il un bon indicateur de la qualit√© du polyn√¥me obtenu.</strong></p>
</div>
</div>
<div class="section" id="c-solution-sklearn">
<h3>C. Solution Sklearn<a class="headerlink" href="#c-solution-sklearn" title="Permalink to this headline">¬∂</a></h3>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Proposez la m√™me solution polynomiale via <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span>. Choisissez le m√™me degr√© qu‚Äôutilis√© au-dessus et comparez les r√©sultats.</strong></p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
</pre></div>
</div>
</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="c1">###############################################################</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="p">[(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">,</span> <span class="s1">&#39;sklearn&#39;</span><span class="p">),</span> <span class="p">(</span><span class="n">real_model</span><span class="o">.</span><span class="n">predict</span><span class="p">,</span> <span class="s1">&#39;Real model&#39;</span><span class="p">)])</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="v-la-validation-croisee-et-le-dilemme-biais-variance">
<h2>V. La validation crois√©e et le dilemme biais-variance<a class="headerlink" href="#v-la-validation-croisee-et-le-dilemme-biais-variance" title="Permalink to this headline">¬∂</a></h2>
<p>Vous avez pu constater qu‚Äôen fonction du degr√© du polyn√¥me choisi dans l‚Äôexercice pr√©c√©dent, le mod√®le obtenu √©tait plus ou moins loin de la solution id√©ale. De plus, le risque empirique s‚Äôest montr√© √™tre un pi√®tre estimateur de la qualit√© de notre solution estim√©e.</p>
<p>En r√©alit√©, le risque empirique est un estimateur sans biais du risque de g√©n√©ralisation pour un vecteur de param√®tres quelconque. Ce n‚Äôest plus vrai si on choisit la solution estim√©e via notre optimisation. Dit autrement¬†:</p>
<div class="math notranslate nohighlight">
\[R(\boldsymbol{\beta})=\mathbb{E}_{\mathcal{S}}\Big[J(\boldsymbol{\beta})\Big],\text{ }\boldsymbol{\beta}\text{ quelconque, et }R(\text{argmin}_{\boldsymbol{\beta}}J(\boldsymbol{\beta}))\neq \mathbb{E}_{\mathcal{S}}\Big[\text{argmin}_{\boldsymbol{\beta}}J(\boldsymbol{\beta})\Big]\]</div>
<hr class="docutils" />
<p>Nous pouvons d√©composer l‚Äôerreur attendue en briques √©l√©mentaires qui nous permettront d‚Äôen saisir l‚Äôorigine. Reprenons l‚Äôerreur suivante¬†:</p>
<div class="math notranslate nohighlight">
\[R(\boldsymbol{\hat{\beta}})=\mathbb{E}\big[J(\boldsymbol{\hat{\beta}})\big]=\mathbb{E}\big[(y-\hat{f}(x))^2\big],\]</div>
<p>o√π <span class="math notranslate nohighlight">\(y=f(x)+\epsilon\)</span>, <span class="math notranslate nohighlight">\(\epsilon\sim \mathcal{N}(0, \sigma^2)\)</span> avec <span class="math notranslate nohighlight">\(f\)</span> la ‚Äúvraie‚Äù fonction et <span class="math notranslate nohighlight">\(\hat{f}\)</span> notre estimateur de celle-ci. Nous avons ainsi¬†:</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\mathbb{E}\big[(y-\hat{f}(x))^2\big]&amp;=\mathbb{E}\big[y^2\big]-2\mathbb{E}\big[y\hat{f}(x)\big]+\mathbb{E}\big[\hat{f}(x)^2\big].
\end{aligned}\]</div>
<p>Remarquons tout d‚Äôabord¬†:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\big[\hat{f}(x)^2\big]=\text{Var}\big(\hat{f}(x)\big)+\mathbb{E}\big[\hat{f}(x)\big]^2,\]</div>
<p>ainsi que¬†:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\big[y^2\big]=\text{Var}\big(y\big)+\mathbb{E}\big[y\big]^2,\]</div>
<p>o√π¬†:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\big[y\big]=\mathbb{E}\big[f(x)+\epsilon\big]=\mathbb{E}\big[f(x)\big]=f(x),\]</div>
<p>et enfin¬†:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{E}\big[y\hat{f}(x)\big]&amp;=\mathbb{E}\big[(f(x)+\epsilon)\hat{f}(x)\big]\\
&amp;=\mathbb{E}\big[f(x)\hat{f}(x)\big]+\mathbb{E}\big[\epsilon\hat{f}(x)\big]\\
&amp;=f(x)\mathbb{E}\big[\hat{f}(x)\big]+\mathbb{E}\big[\epsilon\big]\mathbb{E}\big[\hat{f}(x)\big]\\
&amp;=f(x)\mathbb{E}\big[\hat{f}(x)\big].
\end{aligned}\end{split}\]</div>
<p>En regroupant tout ensemble, nous avons donc¬†:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{E}\big[(y-\hat{f}(x))^2\big]&amp;=\text{Var}\big(y\big)+f(x)^2-2f(x)\mathbb{E}\big[\hat{f}(x)\big]+\text{Var}\big(\hat{f}(x)\big)+\mathbb{E}\big[\hat{f}(x)\big]^2\\
&amp;=\sigma^2+\text{Var}\big(\hat{f}(x)\big)+\mathbb{E}\big[f(x)-\hat{f}(x)\big]^2\\
&amp;=\sigma^2+\text{Var}\big(\hat{f}\big)+\text{bias}(\hat{f})^2.\end{aligned}\end{split}\]</div>
<p>On observe donc que l‚Äôerreur de notre mod√®le se d√©compose th√©oriquement en trois √©l√©ments¬†:</p>
<ul class="simple">
<li><p><strong>Erreur irr√©ductible :</strong> <span class="math notranslate nohighlight">\(\sigma^2\)</span>. Quand bien m√™me nous conna√Ætrions le v√©ritable processus g√©n√©rateur de nos donn√©es, notre erreur quadratique serait toujours de <span class="math notranslate nohighlight">\(\sigma^2\)</span>,</p></li>
<li><p><strong>Variance :</strong> <span class="math notranslate nohighlight">\(\text{Var}\big(\hat{f}\big)\)</span>. Elle d√©crit la fluctuation de notre apprentissage autour de son esp√©rance,</p></li>
<li><p><strong>Biais (au carr√©) :</strong> <span class="math notranslate nohighlight">\(\text{bias}(\hat{f})^2\)</span>. Il nous donne l‚Äô√©cart en esp√©rance entre notre estimateur et le v√©ritable processus g√©n√©rateur.</p></li>
</ul>
<p>Toute la difficult√© vient du fait qu‚Äôil est tr√®s facile de r√©duire le biais en augmentant la complexit√© de notre mod√®le entra√Ænant par la m√™me une augmentation de la variance‚Ä¶</p>
<hr class="docutils" />
<p>Cela implique de mettre en place une proc√©dure exp√©rimentale permettant d‚Äô√©valuer la qualit√© de notre mod√®le.</p>
<div class="section" id="a-construction-du-jeu-de-donnees">
<h3>A. Construction du jeu de donn√©es<a class="headerlink" href="#a-construction-du-jeu-de-donnees" title="Permalink to this headline">¬∂</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">beta_cube</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">sample_data_cube</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">noise</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">150</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_52_0.png" src="../_images/1_linear_regression_52_0.png" />
</div>
</div>
</div>
<div class="section" id="b-optimiser-une-fonction-est-il-suffisant-pour-parler-d-apprentissage">
<h3>B. Optimiser une fonction est-il suffisant pour parler d‚Äôapprentissage ?<a class="headerlink" href="#b-optimiser-une-fonction-est-il-suffisant-pour-parler-d-apprentissage" title="Permalink to this headline">¬∂</a></h3>
<p>Il existe deux strat√©gies d‚Äô√©valuation sans biais de la qualit√© de notre mod√®le :</p>
<ul class="simple">
<li><p>La validation non crois√©e o√π une partie de notre jeu de donn√©e est cach√©e pendant l‚Äôapprentissage puis utilis√©e afin d‚Äô√©valuer les performances du mod√®le. Il s‚Äôagit du d√©coupage train/test. Cette strat√©gie est un estimateur sans biais de la qualit√© de notre mod√®le mais poss√®de une variance plus forte que la validation crois√©e. Elle peut-√™tre particuli√®rement utile lorsque le co√ªt d‚Äôapprentissage d‚Äôun mod√®le est tr√®s √©lev√© (e.g. <em>deep learning</em>)</p></li>
<li><p>La validation crois√©e o√π notre jeu de donn√©es est divis√© en <em>k</em> parties (on parle aussi de <em>k-fold</em>). √âvidemment, <span class="math notranslate nohighlight">\(k\in\{2, ..., n\}\)</span> o√π <span class="math notranslate nohighlight">\(n\)</span> est la taille du jeu de donn√©es. Chacune des parties jouera successivement le r√¥le de jeu de test pendant que les <span class="math notranslate nohighlight">\(k-1\)</span> autres parties serviront √† calculer notre mod√®le. Le r√©sultat de cette proc√©dure est un vecteur de <span class="math notranslate nohighlight">\(k\)</span> scores dont on peut calculer la moyenne, la variance, etc.</p></li>
</ul>
<p>On peut illustrer la m√©thode des <em>k-folds</em> via l‚Äôexemple suivant :</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\text{Appartient au train set: } \color{red}{\boxed{}}&amp;\text{ et appartient au test set: }\color{green}{\boxed{}}
\end{align}\]</div>
<div class="math notranslate nohighlight">
\[\begin{align}
\text{Step 1: }\color{green}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align}\]</div>
<div class="math notranslate nohighlight">
\[\begin{align}
\text{Step 2: }\color{red}{\boxed{}}\color{green}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align}\]</div>
<div class="math notranslate nohighlight">
\[\begin{align}
\text{Step 3: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{green}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align}\]</div>
<div class="math notranslate nohighlight">
\[\begin{align}
\text{Step 4: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{green}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align}\]</div>
<div class="math notranslate nohighlight">
\[\begin{align}
\text{Step 5: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{green}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align}\]</div>
<div class="math notranslate nohighlight">
\[\begin{align}
\text{Step 6: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{green}{\boxed{}}\color{red}{\boxed{}}
\end{align}\]</div>
<div class="math notranslate nohighlight">
\[\begin{align}
\text{Step 7: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{green}{\boxed{}}
\end{align}\]</div>
<p>La m√©thode <span class="math notranslate nohighlight">\(\texttt{cross_val_score}\)</span> de <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span> permet de r√©aliser cette proc√©dure. On pourra renseigner le param√®tre <span class="math notranslate nohighlight">\(\texttt{cv}\)</span> qui indique le nombre <span class="math notranslate nohighlight">\(k\)</span> et le param√®tre <span class="math notranslate nohighlight">\(\texttt{scoring}\)</span> qui donne la m√©trique que l‚Äôon souhaite calculer.</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Proposez un <em>5-fold</em> avec la m√©trique <span class="math notranslate nohighlight">\(R^2\)</span> que vous appliquerez √† une r√©gression polynomiale de degr√© <span class="math notranslate nohighlight">\(5\)</span>.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>

<span class="c1">####### Complete this part ######## or die ####################</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="c1">###############################################################</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Le score R2 sur chacun des splits de notre k-fold:&#39;</span><span class="p">,</span> <span class="n">scores</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Comparez le score obtenu lors de votre validation crois√©e √† un plot de la fonction estim√©e sur tout le jeu d‚Äôapprentissage.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="o">...</span>
<span class="c1">###############################################################</span>
<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="vi-l-effet-double-descente-bonus">
<h2>VI. L‚Äôeffet ‚Äúdouble descente‚Äù (Bonus ?)<a class="headerlink" href="#vi-l-effet-double-descente-bonus" title="Permalink to this headline">¬∂</a></h2>
<p>Comme vu pr√©c√©demment, l‚Äôeffet du bruit sur l‚Äôestimateur d√©pend du conditionnement de <span class="math notranslate nohighlight">\(X^TX\)</span>. Le conditionnement d‚Äôune matrice <span class="math notranslate nohighlight">\(A\)</span> inversible est donn√© par :</p>
<div class="math notranslate nohighlight">
\[C(A)=\lVert A^{-1}\rVert\lvert A\rVert\]</div>
<p>Il est √©vident que si <span class="math notranslate nohighlight">\(A\in{\mathbb{R}^{1\times 1}}^\star\)</span>, alors <span class="math notranslate nohighlight">\(C(A)=1\)</span>. Ce n‚Äôest absolument pas vrai dans le cas g√©n√©ral.</p>
<p>L‚Äôexemple ci-dessous illustre cela via la norme de Frobenius (norme Euclidienne appliqu√©e √† une matrice, <span class="math notranslate nohighlight">\(\text{Tr}(A^TA)^{0.5}\)</span>). On pr√©f√®rera souvent la norme d‚Äôop√©rateur qui quantifie les effets d‚Äôamplification d‚Äôun vecteur <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> lorsqu‚Äôon calcule <span class="math notranslate nohighlight">\(A\boldsymbol{x}\)</span>. Cette norme d‚Äôop√©rateur est directement li√©e aux valeurs propres. Cependant, la norme de Frobenius est plus simple √† calculer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;A=</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;C(A)=A^{-1}A=&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">A</span><span class="p">))</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>A=
 [[1.e+00 0.e+00]
 [0.e+00 1.e-04]]
C(A)=A^{-1}A=10000.000100000001
</pre></div>
</div>
</div>
</div>
<p>On remarque dans l‚Äôexemple que la matrice <span class="math notranslate nohighlight">\(A\)</span> poss√®de une toute petite valeur propre qui est responsable de cet √©cart. L‚Äôexercice ci-dessous montre qu‚Äôau-del√† des consid√©rations th√©oriques, cela a des r√©percussions importantes et totalement inattendues en r√©alit√©.</p>
<p>Les simulations suivantes permettent de mettre en lumi√®re cela. Elles sont construites comme d√©crit ci-dessous :</p>
<div class="math notranslate nohighlight">
\[\beta\sim\mathbb{U}(-2, 2)^d,\ d\in\mathbb{N}^\star\]</div>
<p>dit autrement, on fixe un vecteur de param√®tres selon une loi uniforme qui d√©pend de la dimension du probl√®me.
Nous avons ensuite :</p>
<div class="math notranslate nohighlight">
\[x\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I_d}) + \epsilon,\ \epsilon\sim\mathcal{N}(0, \sigma^2)\]</div>
<p>On construit ensuite un jeu de test de taille <span class="math notranslate nohighlight">\(500\)</span> et un jeu d‚Äôapprentissage de taille variable. L‚Äôobjectif ici sera d‚Äô√©tudi√© l‚Äôeffet de la taille du jeu d‚Äôapprentissage sur la qualit√© de notre mod√®le, qualit√© que l‚Äôon aura calcul√©e sur le test. Pour chaque taille de jeu de donn√©es, l‚Äôexp√©rience est r√©p√©t√©e <span class="math notranslate nohighlight">\(50\)</span> fois (<span class="math notranslate nohighlight">\(\texttt{redo}\)</span>) afin de lisser les courbes obtenues.</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Ex√©cutez une premi√®re fois le code puis jouez avec <span class="math notranslate nohighlight">\(\texttt{noise}\)</span> (i.e. <span class="math notranslate nohighlight">\(\sigma\)</span>) afin de voir ce qui se passe selon la quantit√© de bruit. Essayez de d√©crire rigoureusement ce que vous observez.</strong></p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1">####### Play with the noise #########</span>
<span class="n">noise</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="c1">#####################################</span>

<span class="n">d</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">redo</span> <span class="o">=</span> <span class="mi">50</span>

<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">mu</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">)]</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">)])</span>

<span class="n">test_size</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">test_size</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">test_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">noise</span>

<span class="n">errors</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">train_errors</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
    <span class="n">error</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">train_error</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">redo</span><span class="p">):</span>
        <span class="c1"># dataset construction</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">m</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">noise</span>
        
        <span class="c1"># param estimation</span>
        <span class="n">beta_pinv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
        
        <span class="c1"># risk estimation</span>
        <span class="n">error</span> <span class="o">+=</span> <span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">beta_pinv</span><span class="p">)</span><span class="o">-</span><span class="n">y_test</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="n">test_size</span><span class="o">*</span><span class="n">redo</span><span class="p">)</span>
        <span class="n">train_error</span> <span class="o">+=</span> <span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta_pinv</span><span class="p">)</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="n">m</span><span class="o">*</span><span class="n">redo</span><span class="p">)</span>
    <span class="n">train_errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_error</span><span class="p">)</span>
    <span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">train_errors</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Train error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">d</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Dimension&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">errors</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Risk estimation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_64_0.png" src="../_images/1_linear_regression_64_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">errors</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Risk estimation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">d</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Dimension&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_errors</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">train_errors</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Train error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">d</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Dimension&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_65_0.png" src="../_images/1_linear_regression_65_0.png" />
<img alt="../_images/1_linear_regression_65_1.png" src="../_images/1_linear_regression_65_1.png" />
</div>
</div>
<p>La ligne en pointill√© s√©pare visuellement deux r√©gimes diff√©rents. La transition d‚Äôun r√©gime √† l‚Äôautre se produit par une augmentation catastrophique de l‚Äôerreur de g√©n√©ralisation de notre mod√®le.</p>
<div class="admonition-question admonition">
<p class="admonition-title">Question</p>
<p><strong>Quelle particularit√© diff√©rentie les deux phases ?</strong></p>
</div>
<p>En r√©alit√©, les m√©thodes de <em>machine learning</em> traditionnelles se situent plut√¥t dans le r√©gime de ‚Äúdroite‚Äù. L‚Äô√©tude de ce ph√©nom√®ne est pouss√©e par les approches comme le <em>deep learning</em> qui sont souvent dans le r√©gime de gauche. Comprendre ces ph√©nom√®nes nous permet par exemple d‚Äô√©clairer les raisons du succ√®s du <em>deep learning</em>.</p>
</div>
<div class="section" id="vii-regularisation">
<h2>VII. R√©gularisation<a class="headerlink" href="#vii-regularisation" title="Permalink to this headline">¬∂</a></h2>
<p>Comme illustr√© par les quelques sc√©narios pr√©c√©dents dont le cas catastrophique de la double descente, une certaine parcimonie est attendue par notre mod√®le. On a pu notamment observer que les ‚Äúmauvaises‚Äù fonctions du point de vue du risque de g√©n√©ralisation avaient une forte tendance √† osciller n‚Äôimporte comment. Au lieu de laisser jouer le ‚Äúhasard‚Äù (ou plut√¥t le conditionnement de <span class="math notranslate nohighlight">\(X^TX\)</span>), nous pouvons contraindre notre optimisation √† favoriser les solutions parcimonieuses ; c‚Äôest-√†-dire des solutions qui n‚Äôoscillent pas n‚Äôimporte comment.</p>
<p>Intuitivement, on va choisir une solution qui minimise √† la fois le risque empirique <span class="math notranslate nohighlight">\(J(\boldsymbol{\beta})\)</span>, mais aussi une p√©nalit√© sur la taille des ‚Äúoscillations‚Äù. En r√©alit√©, les oscillations sont directement contr√¥l√©es par la norme des param√®tres : un grand poids rendra notre mod√®le tr√®s sensible √† la moindre perturbation de la variable explicative associ√©e.</p>
<p>Nous parlons d‚Äôoptimisation r√©gularis√©e lorsque la fonction √† optimiser s‚Äô√©crit de la mani√®re suivante :</p>
<div class="math notranslate nohighlight">
\[J(\boldsymbol{\beta})=\frac{1}{m}\sum_{i=1}^nr(f_{\boldsymbol{\beta}}(\boldsymbol{x_i}), y_i)+\lambda P(\boldsymbol{\beta})\]</div>
<p>o√π <span class="math notranslate nohighlight">\(r:\mathcal{Y}\times\mathcal{Y}\mapsto \mathbb{R}^+\)</span> est notre risque √©l√©mentaire et <span class="math notranslate nohighlight">\(P:\mathbb{R}^d\mapsto\mathbb{R}^+\)</span> une p√©nalit√© sur notre vecteur de param√®tres. Plus pr√©cis√©ment, dans le cas de la r√©gression lin√©aire, nous avons :</p>
<div class="margin sidebar">
<p class="sidebar-title">Dans <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span></p>
<p>La r√©gression lin√©aire sans p√©nalit√© s‚Äôobtient avec <span class="math notranslate nohighlight">\(\texttt{LinearRegression}\)</span>. Celle avec p√©nalit√© <span class="math notranslate nohighlight">\(\ell_2\)</span> s‚Äôobtient avec <span class="math notranslate nohighlight">\(\texttt{Ridge}\)</span>, avec p√©nalit√© <span class="math notranslate nohighlight">\(\ell_1\)</span> est <span class="math notranslate nohighlight">\(\texttt{Lasso}\)</span> et Elastic-Net <span class="math notranslate nohighlight">\(\texttt{ElasticNet}\)</span>.</p>
</div>
<div class="math notranslate nohighlight">
\[r(\hat{y}, y)=(\hat{y}-y)^2\]</div>
<p>et</p>
<div class="math notranslate nohighlight">
\[P(\boldsymbol{\beta})=\lVert \boldsymbol{\beta} \rVert,\]</div>
<p>o√π <span class="math notranslate nohighlight">\(\lVert \cdot \rVert\)</span> est une norme quelconque. Les choix classiques sont la norme <span class="math notranslate nohighlight">\(\ell_1\)</span> :</p>
<div class="math notranslate nohighlight">
\[\lVert \boldsymbol{\beta} \rVert_1=\sum_j |\boldsymbol{\beta}_j|\]</div>
<p>et la norme <span class="math notranslate nohighlight">\(\ell_2\)</span> :</p>
<div class="math notranslate nohighlight">
\[\lVert \boldsymbol{\beta} \rVert_2 = \sqrt{\sum_j\boldsymbol{\beta}_j^2}=\sqrt{\boldsymbol{\beta}^T\boldsymbol{\beta}}\]</div>
<p>Une strat√©gie interm√©diaire consiste √† prendre la combinaison convexe des deux normes :</p>
<div class="math notranslate nohighlight">
\[P(\boldsymbol{\beta})=\eta \lVert \boldsymbol{\beta} \rVert_1 + (1-\eta) \lVert \boldsymbol{\beta} \rVert_2.\]</div>
<p>avec <span class="math notranslate nohighlight">\(\eta\in\big[0,1\big]\)</span>. On parle alors d‚Äô<em>elastic-net</em>.</p>
<p>Ces diff√©rentes r√©gularisations ne se comportent pas de la m√™me mani√®re. Ainsi la r√©gularisation <span class="math notranslate nohighlight">\(\ell_1\)</span>, aussi appel√©e Lasso, va forcer certains param√®tres √† atteindre la valeur <span class="math notranslate nohighlight">\(0\)</span>. Cela permet par exemple de favoriser l‚Äôexplicabilit√© de notre mod√®le. En pratique, <span class="math notranslate nohighlight">\(\ell_2\)</span>, appel√©e Ridge, a tendance √† donner les meilleurs r√©sultats d‚Äôun point de vue pr√©dictif.</p>
<div class="section" id="id1">
<h3>A. Construction du jeu de donn√©es<a class="headerlink" href="#id1" title="Permalink to this headline">¬∂</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">beta_cube</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">sample_data_cube</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">noise</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">150</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_71_0.png" src="../_images/1_linear_regression_71_0.png" />
</div>
</div>
</div>
<div class="section" id="b-sans-regularisation">
<h3>B. Sans r√©gularisation<a class="headerlink" href="#b-sans-regularisation" title="Permalink to this headline">¬∂</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">LinearRegression</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_74_0.png" src="../_images/1_linear_regression_74_0.png" />
</div>
</div>
</div>
<div class="section" id="c-avec-regularisation-ell-1">
<h3>C. Avec r√©gularisation <span class="math notranslate nohighlight">\(\ell_1\)</span><a class="headerlink" href="#c-avec-regularisation-ell-1" title="Permalink to this headline">¬∂</a></h3>
<p>Lorsqu‚Äôon parle de r√©gresion lin√©aire avec r√©gularisation <span class="math notranslate nohighlight">\(\ell_1\)</span>, on parle aussi de Lasso.</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Testez plusieurs valeurs de <span class="math notranslate nohighlight">\(\alpha\)</span> (<span class="math notranslate nohighlight">\(=\lambda\)</span> dans notre texte). Quelle est la fonction la plus parcimonieuse que vous arrivez √† obtenir ?</strong></p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">15.</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_78_0.png" src="../_images/1_linear_regression_78_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Les param√®tres du mod√®le sont sparses :</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">model</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Les param√®tres du mod√®le sont sparses :
 [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00901764]
</pre></div>
</div>
</div>
</div>
<p>Vous avez du constater que selon la quantit√© de r√©gularisation, les param√®tres √©taient plus ou moins sparse. Il se trouve qu‚Äôune fois qu‚Äôun param√®tre est √† 0, il le sera pour toutes les valeurs de <span class="math notranslate nohighlight">\(\alpha\)</span> supp√©rieures. Afin d‚Äôobserver visuellement, l‚Äôeffet de la r√©gularisation sur la sparsit√©, il est possible d‚Äôafficher ce qu‚Äôon appelle les ‚Äúchemins Lasso‚Äù ou Lasso paths.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">lars_path</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">X_transformed</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">coefs</span> <span class="o">=</span> <span class="n">lars_path</span><span class="p">(</span><span class="n">X_transformed</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;lasso&#39;</span><span class="p">)</span>

<span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">coefs</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">xx</span> <span class="o">/=</span> <span class="n">xx</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">coefs</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">ymin</span><span class="p">,</span> <span class="n">ymax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">()</span>
<span class="c1"># plt.vlines(xx, ymin, ymax, linestyle=&#39;dashed&#39;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;|coef| / max|coef|&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Coefficients&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;LASSO Path&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_81_0.png" src="../_images/1_linear_regression_81_0.png" />
</div>
</div>
<p>A gauche se trouve le param√®tre le plus parcimonieux (la plus grande valeur de <span class="math notranslate nohighlight">\(\alpha\)</span>). Tous les param√®tres y sont donc nuls. Plus la valeur de <span class="math notranslate nohighlight">\(\alpha\)</span> est r√©duite, plus le nombre de param√®tres diff√©rents de <span class="math notranslate nohighlight">\(0\)</span> augmente et leur valeur aussi.</p>
</div>
<div class="section" id="d-avec-regularisation-ell-2">
<h3>D. Avec r√©gularisation <span class="math notranslate nohighlight">\(\ell_2\)</span><a class="headerlink" href="#d-avec-regularisation-ell-2" title="Permalink to this headline">¬∂</a></h3>
<p>Lorsqu‚Äôon parle de r√©gresion lin√©aire avec r√©gularisation <span class="math notranslate nohighlight">\(\ell_2\)</span>, on parle aussi de Ridge.</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Testez plusieurs valeurs de <span class="math notranslate nohighlight">\(\alpha\)</span> (<span class="math notranslate nohighlight">\(=\lambda\)</span> dans notre texte). Quelle est la fonction la plus parcimonieuse que vous arrivez √† obtenir ?</strong></p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1000</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_86_0.png" src="../_images/1_linear_regression_86_0.png" />
</div>
</div>
</div>
<div class="section" id="e-avec-regularisation-elastic-net">
<h3>E. Avec r√©gularisation <em>elastic-net</em><a class="headerlink" href="#e-avec-regularisation-elastic-net" title="Permalink to this headline">¬∂</a></h3>
<p>Lorsqu‚Äôon parle de r√©gresion lin√©aire avec r√©gularisation <em>elastic-net</em>.</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Testez plusieurs valeurs de <span class="math notranslate nohighlight">\(\alpha\)</span> (<span class="math notranslate nohighlight">\(=\lambda\)</span> dans notre texte). Quelle est la fonction la plus parcimonieuse que vous arrivez √† obtenir (Essayez de trouver la r√©ponse en raisonnant) ?</strong></p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNet</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">ElasticNet</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.8</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_90_0.png" src="../_images/1_linear_regression_90_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="viii-selection-de-modeles">
<h2>VIII. Selection de mod√®les<a class="headerlink" href="#viii-selection-de-modeles" title="Permalink to this headline">¬∂</a></h2>
<p>En pratique, nous ne pouvons pas choisir la valeur des param√®tres (e.g. degr√©, r√©gularisation) √† l‚Äôoeil comme pr√©c√©demment. Il nous faut (1) un algorithme qui automatise cette t√¢che et (2) une strat√©gie d‚Äô√©valuation rigoureuse afin d‚Äô√©viter les biais de confirmation (sur-apprentissage).</p>
<div class="section" id="id2">
<h3>A. Construction du jeu de donn√©es<a class="headerlink" href="#id2" title="Permalink to this headline">¬∂</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">beta_cube</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">sample_data_cube</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">noise</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">150</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_94_0.png" src="../_images/1_linear_regression_94_0.png" />
</div>
</div>
</div>
<div class="section" id="b-recherche-exhaustive">
<h3>B. Recherche exhaustive<a class="headerlink" href="#b-recherche-exhaustive" title="Permalink to this headline">¬∂</a></h3>
<p>L‚Äôalgorithme de recherche par grille va exhaustivement test√© tous les param√®tres donn√©s. Pour chacun combinaison, une validation <em>k-fold</em> est r√©alis√©e. Le mod√®le retenu sera celui qui aura maximis√© son score moyen lors du <em>k-fold</em>.</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Utilisez l‚Äôobjet <span class="math notranslate nohighlight">\(\texttt{GridSearchCV}\)</span> afin de trouver la meilleure combinaison de param√®tres selon le dictionnaire d√©crit ci-dessous. Toutes les combinaisons seront-elles r√©ellement test√©es ?</strong></p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNet</span><span class="p">,</span> <span class="n">Ridge</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">,</span> <span class="n">Lasso</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">[</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">LinearRegression</span><span class="p">()],</span> 
   <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]},</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">Ridge</span><span class="p">()],</span> 
   <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> 
   <span class="s1">&#39;model__alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">]},</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">Lasso</span><span class="p">()],</span> 
   <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
   <span class="s1">&#39;model__alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]},</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">ElasticNet</span><span class="p">()],</span> <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
  <span class="s1">&#39;model__alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">],</span>
  <span class="s1">&#39;model__l1_ratio&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]},</span>
 <span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">10</span><span class="p">)),</span> <span class="p">(</span><span class="s1">&#39;model&#39;</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">())])</span>
<span class="c1">####### Complete this part ######## or die ####################</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="c1">###############################################################</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Le meilleur modele est&#39;</span><span class="p">,</span> <span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="c-recherche-aleatoire">
<h3>C. Recherche al√©atoire<a class="headerlink" href="#c-recherche-aleatoire" title="Permalink to this headline">¬∂</a></h3>
<p>Une recherche exhaustive peut rapidement √™tre limitante. Imaginons que nous testions d√©j√† <span class="math notranslate nohighlight">\(10000\)</span> combinaisons de param√®tres. Rajoutons maintenant un param√®tre avec 50 modalit√©s. Le nombre de combinaisons est donc multipli√© par <span class="math notranslate nohighlight">\(50\)</span> et on monte √† <span class="math notranslate nohighlight">\(500000\)</span> combinaisons. L‚Äôalgorithme devient <span class="math notranslate nohighlight">\(50\)</span> fois plus lent.</p>
<p>Une strat√©gie alternative est de s‚Äôappuyer sur le hasard. On peut sp√©cifier a priori des distributions sur les param√®tres en supposant que certaines combinaisons fourniront probablement plus de bons r√©sultats que d‚Äôautres. Par d√©faut, le tirage est uniforme. Cette m√©thode n‚Äôest pas absurde car plusieurs combinaisons peuvent tr√®s bien obtenir des r√©sultats tr√®s proches. L‚Äôapproche al√©atoire sera ainsi beaucoup plus efficaces que la recherche exhaustive pour des performances g√©n√©ralement assez proches.</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Compl√©tez le code ci-dessous afin de r√©aliser une recherche randomis√©e. Quel param√®tre permet de jouer sur le nombre de tirages ?</strong></p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">[</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">LinearRegression</span><span class="p">()],</span> 
   <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)]},</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">Ridge</span><span class="p">()],</span> 
   <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)],</span> 
   <span class="s1">&#39;model__alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">]},</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">Lasso</span><span class="p">()],</span> 
   <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)],</span>
   <span class="s1">&#39;model__alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]},</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">ElasticNet</span><span class="p">()],</span> <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)],</span>
  <span class="s1">&#39;model__alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">],</span>
  <span class="s1">&#39;model__l1_ratio&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]},</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">10</span><span class="p">)),</span> <span class="p">(</span><span class="s1">&#39;model&#39;</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">())])</span>

<span class="n">search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipe</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GridSearchCV(estimator=Pipeline(steps=[(&#39;poly&#39;, PolynomialFeatures(degree=10)),
                                       (&#39;model&#39;, LinearRegression())]),
             n_jobs=-1,
             param_grid=[{&#39;model&#39;: [LinearRegression()],
                          &#39;poly__degree&#39;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
                                           12, 13, 14, 15, 16, 17, 18, 19]},
                         {&#39;model&#39;: [Ridge()],
                          &#39;model__alpha&#39;: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6,
                                           0.7, 0.8, 0.9, 1.0, 10.0, 100.0,
                                           100.0],
                          &#39;poly__degree&#39;: [1, 2, 3, 4, 5,...
                                           12, 13, 14, 15, 16, 17, 18, 19]},
                         {&#39;model&#39;: [Lasso()],
                          &#39;model__alpha&#39;: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6,
                                           0.7, 0.8, 0.9, 1.0],
                          &#39;poly__degree&#39;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
                                           12, 13, 14, 15, 16, 17, 18, 19]},
                         {&#39;model&#39;: [ElasticNet()],
                          &#39;model__alpha&#39;: [0.1, 0.2, 0.5, 0.8, 10.0, 100.0,
                                           100.0],
                          &#39;model__l1_ratio&#39;: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6,
                                              0.9, 1.0],
                          &#39;poly__degree&#39;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
                                           12, 13, 14, 15, 16, 17, 18, 19]}])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Le meilleur modele est&#39;</span><span class="p">,</span> <span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Le meilleur modele est Pipeline(steps=[(&#39;poly&#39;, PolynomialFeatures(degree=3)),
                (&#39;model&#39;, LinearRegression())])
</pre></div>
</div>
<img alt="../_images/1_linear_regression_105_1.png" src="../_images/1_linear_regression_105_1.png" />
</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="c1">###############################################################</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Le meilleur modele est&#39;</span><span class="p">,</span> <span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Le meilleur modele est&#39;</span><span class="p">,</span> <span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Le meilleur modele est Pipeline(steps=[(&#39;poly&#39;, PolynomialFeatures(degree=3)),
                (&#39;model&#39;, LinearRegression())])
</pre></div>
</div>
<img alt="../_images/1_linear_regression_107_1.png" src="../_images/1_linear_regression_107_1.png" />
</div>
</div>
</div>
</div>
<div class="section" id="ix-le-mot-de-la-fin">
<h2>IX. Le mot de la fin<a class="headerlink" href="#ix-le-mot-de-la-fin" title="Permalink to this headline">¬∂</a></h2>
<p>Ce propos introductif nous a permis de toucher du doigt la notion de sur-apprentissage. Quand est-ce que le meilleur mod√®le sur notre jeu d‚Äôapprentissage est suffisament bon en g√©n√©ral ? Quand peut-on consid√©rer qu‚Äôun mod√®le est suffisamment bon ? Aurions-nous pu trouver un meilleur mod√®le avec une proc√©dure d‚Äôapprentissage diff√©rente ?</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./2_regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="0_propos_liminaire.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">La <em>r√©gression</em></p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="2_optimization.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">L‚Äôoptimisation ‚òïÔ∏è‚òïÔ∏è</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Servajean, Leveau & Chailan<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>
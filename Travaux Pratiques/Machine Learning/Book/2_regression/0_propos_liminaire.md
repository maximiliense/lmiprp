La *régression*
=======================

Les problèmes de *régression* font référence aux cas où notre espace cible $\mathcal{Y}$ est l'ensemble des réels $\mathbb{R}$. Ce chapitre se construit de la manière suivante&nbsp;:

1. **Régression linéaire** - La régression linéaire sera introduite de manière approfondie. Nous étudierons tout d'abord son formalisme au travers de la fonction objectif idéale que nous souhaitons optimiser. Nous montrerons la fonction empirique minimisée, l'algorithme de descente de gradient, le calcul de ce dernier et nous aborderons des réflexions liées à son fonctionnement. Nous montrerons également les solutions analytiques au travers des équations normales. Nous parlerons de transformation des variables afin d'aller au-delà du cas linéaire ainsi que de validation croisée. Nous évoquerons un effet original&nbsp;: l'effet double descente. Enfin, nous terminerons sur la régularisation de la régression linéaire et sur la sélection de modèles.
2. **L'optimisation** - Cette séquence se concentre un peu plus sur les algorithmes d'optimisation. Nous approfondirons l'étude de l’algorithme de descente de gradient qui est standard en *machine learning*. Nous le coderons et nous démontrerons sa convergence. Nous proposerons une alternative où chaque pas d'optimisation à une taille optimale. Nous introduirons la méthode de Newton qui s'appuie sur les développement limités de notre fonction objectif. Enfin, nous étudierons la descente de coordonnées.
3. **L'interpolation** - Cette séquence évoquera la notion d'interpolation. On parle d'interpolation lorsqu'on cherche à passer par tous les points de notre jeu de données. Cela se fait de manière plus ou moins compliquée avec des résultats plus ou moins fins et des effets plus ou moins inattendus (on pensera à l'effet double descente). Cela sera l'occasion de remettre sur le devant de la scène l'idée de régularisation.
4. **Sous-différentiel et le cas du Lasso** - Comment optimiser lorsque notre fonction objectif n'est pas totalement différentiable ? Nous répondrons à cette question avec l'algorithme d'optimisation *proximal* que nous illustrerons dans le cas de la régression linéaire régularisée $\ell_1$, dite Lasso.
5. **Les moindres carrés via une décomposition QR** - Une fois les différents éléments formellement définis, nous pouvons les implémenter. Cependant, tout ne se passe pas tout le temps comme prévu. Notre code entraîne des erreurs de calcul numériques. Cela vient du fait que notre ordinateur ne représente pas tous les nombres avec une précisions infinies et parfois, cela empêche totalement l'algorithme de fonctionner. Tout n'est pas perdu et des astuces existent ! Après quelques motivations, des astuces simples et un rappel de la décomposition QR, nous montrerons comment l’estimateur des moindres carrés devient plus stable lorsqu'il est calculé via une décomposition QR.
6.  **Une analyse de la régularisation Ridge** - Cette séquence s'écarte des réseaux de neurones et propose un approfondissement de la régularisation Ridge où on cherche à pénaliser la norme $\ell_2$ de notre vecteur de paramètres.

{"cells": [{"cell_type": "markdown", "id": "urban-envelope", "metadata": {}, "source": ["# Sous-diff\u00e9rentiel et le cas du Lasso \u2615\ufe0f\u2615\ufe0f\u2615\ufe0f\n", "\n", "**<span style='color:blue'> Objectifs de la s\u00e9quence</span>** ", "\n", "* \u00catre sensibilis\u00e9&nbsp;:\n", "    * \u00c0 la notion d'optimisation via l'algorithme *proximal*,\n", "    * \u00c0 la notion de sous-gradient.\n", "* \u00catre capable d'impl\u00e9menter \u00e0 la main l'algorithme *proximal*.\n", "\n", "\n\n ----"]}, {"cell_type": "markdown", "id": "norman-robinson", "metadata": {}, "source": ["## Introduction "]}, {"cell_type": "markdown", "id": "addressed-truck", "metadata": {}, "source": ["Soit $X\\in\\mathbb{R}^{n\\times d}$ et $\\boldsymbol{y}=\\mathbb{R}^n$. De mani\u00e8re assez directe, le probl\u00e8me des moindres carr\u00e9s se formule de la mani\u00e8re suivante :\n", "\n", "$$\\beta^\\star=\\text{argmin}_{\\beta\\in\\mathbb{R}^d}\\lVert X\\beta-\\boldsymbol{y}\\rVert_2^2.$$\n", "\n", "Supposons que $X^TX$ soit inversible. Alors, en annulant le gradient, on obtient :\n", "\n", "$$\\beta^\\star=(X^TX)^{-1}X^T\\boldsymbol{y}.$$\n", "\n", "Si nous avions souhait\u00e9 minimiser notre fonction objectif via une descente de gradient (par exemple dans le cas o\u00f9 $X^TX$ n'est pas inversible ou si le co\u00fbt de son inversion est trop important), alors le gradient \u00e0 utiliser est :\n", "\n", "$$\\nabla (\\lVert X\\beta-\\boldsymbol{y}\\rVert_2)=2X^TX-2X^t\\boldsymbol{y}.$$\n", "\n", "Cependant, comme nous l'avons vu dans une s\u00e9quence pr\u00e9c\u00e9dente, il est parfois pr\u00e9f\u00e9rable de \"r\u00e9gulariser\" ce probl\u00e8me d'optimisation de mani\u00e8re \u00e0 obtenir un vecteur $\\beta^\\star$ g\u00e9n\u00e9ralisant mieux \u00e0 de nouveaux exemples d'apprentissage en machine learning ou offrant plus de stabilit\u00e9. Le cas de la r\u00e9gularisation $\\lVert\\cdot\\rVert_1$ dit $\\ell_1$, qu'on appelle Lasso, est int\u00e9ressant car il permet de faire de la s\u00e9lection de variables. En effet, le vecteur $\\beta^\\star$ obtenu apr\u00e8s minimisation est sparse.\n", "\n", "Rappelons que la norme $\\ell_1$ se d\u00e9finie de la mani\u00e8re suivante :\n", "\n", "$$\\lVert x \\rVert_1=\\sum_i |x_i|.$$\n", "\n", "Le probl\u00e8me r\u00e9gularis\u00e9 peut donc se reformuler de la mani\u00e8re suivante :\n", "\n", "$$\\beta^\\star=\\text{argmin}_{\\beta\\in\\mathbb{R}^d}\\lVert X\\beta-\\boldsymbol{y}\\rVert_2^2+\\lambda \\lVert \\beta\\rVert_1,$$\n", "\n", "o\u00f9 $\\lambda\\geq0$ contr\u00f4le la quantit\u00e9 de r\u00e9gularisation.\n", "\n", "Le probl\u00e8me ici est que $\\lVert \\beta\\rVert_1$ n'est pas diff\u00e9rentiable partout - la valeur absolu n'est pas d\u00e9rivable en $0$. On aurait pu se dire que ce n'est pas un probl\u00e8me et que notre descente de gradient aurait \u00e0 coup s\u00fbr \"saut\u00e9\" les points non-diff\u00e9rentiables, mais ces derniers s'av\u00e8rent justement \u00eatre solution de notre probl\u00e8me : c'est le c\u00f4t\u00e9 sparse. Nous allons, dans cette s\u00e9quence introduire quelques \u00e9l\u00e9ments permettant d'aborder ce probl\u00e8me : le sous-diff\u00e9rentiel.\n", "\n", "\n", "\u00c0 la fin de cette s\u00e9quence, nous d\u00e9riverons un algorithme permettant d'obtenir la solution du probl\u00e8me d'optimisation Lasso !"]}, {"cell_type": "markdown", "id": "threaded-baptist", "metadata": {}, "source": ["## I. Quelques bases"]}, {"cell_type": "markdown", "id": "swedish-mainland", "metadata": {}, "source": ["Nous allons ici introduire quelques \u00e9l\u00e9ments math\u00e9matiques permettant de d\u00e9river les algorithmes d'optimisation voulus. Tout d'abord, soit $f:\\mathbb{R}\\mapsto\\mathbb{R}$ une fonction d\u00e9rivable, alors sa tangente en $a$ a pour \u00e9quation :\n", "\n", "$$t(x)=f(a)+f^\\prime(a)(x-a).$$\n", "\n", "Si $f$ est convexe, alors $f(y)\\geq t(x),\\ \\forall x, y\\in\\mathbb{R}$. Rappelons que la convexit\u00e9 d'une fonction $f$ nous indique que $\\forall x,y\\in\\mathbb{R},\\ \\lambda\\in [0, 1]$, on a :\n", "\n", "$$f(\\lambda x+(1-\\lambda)y)\\leq \\lambda f(x)+(1-\\lambda)f(y).$$\n", "\n", "### La sous-d\u00e9riv\u00e9e\n", "L'id\u00e9e de la sous-d\u00e9riv\u00e9e est de pouvoir g\u00e9n\u00e9raliser l'id\u00e9e que la d\u00e9riv\u00e9e contr\u00f4le la tangente dans le cas d'une fonction convexe. Soit $f$ une fonction convexe, $s$ est une sous d\u00e9riv\u00e9e de $f$ en $a$ si :\n", "\n", "$$f(x) \\geq f(a)+s(x-a),\\ \\forall x\\in\\mathbb{R}.$$\n", "\n", "La sous-d\u00e9riv\u00e9e n'est pas forc\u00e9ment unique en un point $a$ et on appelle sous-diff\u00e9rentiel l'ensemble des sous-d\u00e9riv\u00e9s qu'on note $\\partial f(a)$. Cependant, si $f$ est diff\u00e9rentiable en $a$, alors $\\partial f(a)=\\{f^\\prime(a)\\}$. On observe assez rapidement que si $f$ est concave au voisinage de $a$, alors elle n'admet pas de sous-d\u00e9riv\u00e9e : $\\partial f(x)=\\emptyset$.\n", "\n", "La figure suivante illustre quelques sous-d\u00e9riv\u00e9es pour les fonctions $f(x)=|x|$ et $g(x)=x^2$."]}, {"cell_type": "code", "execution_count": null, "id": "another-frank", "metadata": {"tags": ["remove-input"]}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "x = np.linspace(-2, 2, 101)\n", "\n", "\n", "y = np.abs(x)\n", "sub_gradient = np.stack([0.9*x, 0.3*x, -0.3*x, -0.7*x])\n", "\n", "x = np.tile(x, (4, 1))\n", "\n", "plt.figure(figsize=(16.0, 8.0))\n", "plt.subplot(1, 2, 1)\n", "plt.plot(x[0], y, label='$f(x)=|x|$')\n", "lines = plt.plot(x.T, sub_gradient.T, \n", "                 label=r'$t_s(x)=\\langle s, x\\rangle$, $s$ sous-d\u00e9riv\u00e9e', color='gray', alpha=0.5)\n", "plt.setp(lines[1:], label=\"_\")\n", "\n", "plt.ylim(-2, 2)\n", "\n", "plt.gca().spines['left'].set_position('center')\n", "plt.gca().spines['right'].set_color('none')\n", "plt.gca().spines['bottom'].set_position('center')\n", "plt.gca().spines['top'].set_color('none')\n", "plt.gca().xaxis.set_ticks_position('bottom')\n", "plt.gca().yaxis.set_ticks_position('left')\n", "\n", "plt.legend()\n", "plt.subplot(1, 2, 2)\n", "plt.ylim(-2, 2)\n", "plt.plot(x[0], x[0]**2, label='$g(x)=x^2$')\n", "y = 0.5**2 + 2*0.5*(x[0]-0.5)\n", "plt.plot(x[0], y, color='gray', alpha=0.5, label='Tangente')\n", "plt.gca().spines['left'].set_position('center')\n", "plt.gca().spines['right'].set_color('none')\n", "plt.gca().spines['bottom'].set_position('center')\n", "plt.gca().spines['top'].set_color('none')\n", "plt.gca().xaxis.set_ticks_position('bottom')\n", "plt.gca().yaxis.set_ticks_position('left')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "worth-scanning", "metadata": {}, "source": ["On observe bien l'infinit\u00e9 des sous-d\u00e9riv\u00e9es de la valeur absolue en $0$ et l'unicit\u00e9 en tout point pour la fonction $x^2$."]}, {"cell_type": "markdown", "id": "visible-bumper", "metadata": {}, "source": ["Un certain nombre de r\u00e8gles de calcul usuelles se g\u00e9n\u00e9ralisent assez bien \u00e0 ce nouveau concept. Soit $f, g$ deux fonctions et un scalaire $\\alpha$, nous avons:\n", "\n", "$$\\partial (\\alpha(f+g))=\\alpha\\partial f+\\alpha \\partial g.$$\n", "\n", "Soit $f$ une fonction d\u00e9rivable et soit le probl\u00e8me d'optimisation suivant :\n", "\n", "$$x^\\star=\\text{argmin}_{x\\in\\mathbb{R}}f(x).$$\n", "\n", "La condition d'optimalit\u00e9 du premier ordre, dit de Fermat, nous donne l'\u00e9quivalence suivante :\n", "\n", "$$x^\\star\\text{ est un minimum local}\\Leftrightarrow f^\\prime(x^\\star)=0.$$\n", "\n", "Si $f$ est convexe, alors $x^\\star$ est un minimum global. Qu'en est-il dans le cas o\u00f9 $f$ n'est pas d\u00e9rivable en $x^\\star$ mais sous-diff\u00e9rentiable ? Supposons $f$ convexe. On a :\n", "\n", "$$x^\\star\\text{ est un minimum global}\\Leftrightarrow 0\\in\\partial f(x^\\star).$$"]}, {"cell_type": "markdown", "id": "dental-windsor", "metadata": {}, "source": ["Lorsqu'on consid\u00e8re des fonctions de $\\mathbb{R}^n$ dans $\\mathbb{R}$, alors les principes pr\u00e9c\u00e9dents se g\u00e9n\u00e9ralisent. On parle alors de sous-gradient."]}, {"cell_type": "markdown", "id": "healthy-search", "metadata": {}, "source": ["## II. L'algorithme d'optimisation proximal et son application au Lasso"]}, {"cell_type": "markdown", "id": "essential-germany", "metadata": {}, "source": ["Reprenons le probl\u00e8me d'optimisation r\u00e9gularis\u00e9 suivant :\n", "\n", "$$\\beta^\\star=\\text{argmin}_{\\beta\\in\\mathbb{R}^d}\\lVert X\\beta-\\boldsymbol{y}\\rVert_2+\\lambda \\lVert \\beta\\rVert_1 = f(\\beta)+g(\\beta),$$\n", "\n", "o\u00f9 $f$ et $g$ sont des fonctions convexes, $f$ une fonction diff\u00e9rentiable.\n", "\n", "Une strat\u00e9gie d'optimisation de ce probl\u00e8me est ce qu'on appelle l'algorithme *proximal*. Ce dernier d\u00e9coupe chaque pas d'optimisation en deux \u00e9tapes. La premi\u00e8re consiste \u00e0 optimiser $f$ en suivant son gradient (comme dans la descente de gradient). La seconde \u00e9tape consiste \u00e0 \"optimiser $g$ dans le voisinage du point pr\u00e9c\u00e9demment obtenu\" :\n", "\n", "$$\\text{pas d'optimisation : }\\begin{cases}u^{(t+1)}&=\\beta^{(t)}-\\gamma\\nabla f(\\beta^{(t)})\\\\\n", "\\beta^{(t+1)}&=\\textbf{prox}_{\\gamma g}(u^{(t+1)})\\end{cases}$$\n", "\n", "Cette algorithme s'appuie sur un op\u00e9rateur qu'on appelle \"l'op\u00e9rateur proximal\" :\n", "\n", "$$\\textbf{prox}_{\\gamma g}(u)=\\text{argmin}_y\\big\\{g(y)+\\frac{1}{2\\gamma}\\lVert y-u\\rVert_2^2\\big\\}.$$\n", "\n", "On cherche \u00e0 minimiser $g$ depuis un point $u$ en gardant la distance $\\ell_2$ au carr\u00e9 faible.\n", "\n", "**La p\u00e9nalit\u00e9 $\\ell_1$ du Lasso et l'op\u00e9rateur proximal :** Soit $g(x)=\\lambda \\lVert x\\rVert_1$. On cherche donc \u00e0 trouver un point d'annulation de la sous-diff\u00e9rentielle de la fonction objectif de notre op\u00e9rateur proximal. On a donc :\n", "\n", "$$\\textbf{prox}_{\\gamma g}(u)=\\textbf{prox}_{\\gamma \\lambda \\lVert \\cdot\\rVert_1}(u),$$\n", "\n", "et en calculant la diff\u00e9rentielle, nous obtenons :\n", "\n", "$$\\partial(\\lVert y\\rVert_1+\\frac{1}{2\\lambda\\gamma}\\lVert y-u\\rVert_2^2)=\\partial \\lVert y\\rVert_1+\\frac{1}{2\\lambda\\gamma}\\partial \\lVert y-u \\rVert_2^2=\\partial \\lVert y\\rVert_1+\\Big\\{\\frac{y-u}{\\lambda\\gamma}\\Big\\}.$$\n", "\n", "Le probl\u00e8me est s\u00e9parable et peut \u00eatre consid\u00e9r\u00e9 coordonn\u00e9e par coordonn\u00e9e :\n", "\n", "$$(\\partial \\lVert y\\rVert_1)_i=\\begin{cases}1&\\text{ si }y_i>0\\\\ [-1, 1]&\\text{ si }y_i=0\\\\ -1&\\text{ sinon.}\\end{cases}$$\n", "\n", "En combinant les deux sous-diff\u00e9rentielles, et en traitant les deux probl\u00e8mes coordonn\u00e9e par coordonn\u00e9e, nous obtenons :\n", "\n", "$$\\partial(\\lVert y\\rVert_1+\\frac{1}{2\\lambda\\gamma}\\lVert y-u\\rVert_2^2)=\\begin{cases}1+\\frac{y_i-u_i}{\\lambda\\gamma}&\\text{ si }y_i>0\\\\ \\Big[-1+\\frac{y_i-u_i}{\\lambda\\gamma}, 1+\\frac{y_i-u_i}{\\lambda\\gamma}\\Big]&\\text{ si }y_i=0\\\\ -1+\\frac{y_i-u_i}{\\lambda\\gamma}&\\text{ si }y<0.\\end{cases}$$\n", "\n", "Notre objectif est de trouver $y_i$ tel que $0\\in\\partial(\\lVert y\\rVert_1+\\frac{1}{2\\lambda\\gamma}\\lVert y-u\\rVert_2^2)$. Trois sc\u00e9narios sont possibles. Tout d'abord :\n", "\n", "$$0=1+\\frac{y_i-u_i}{\\lambda\\gamma}\\text{ et }y_i>0\\Leftrightarrow y_i=u_i-\\lambda\\gamma\\text{ et }u_i>\\lambda\\gamma.$$\n", "\n", "Les deux autres sc\u00e9narios se construisent exactement de la m\u00eame mani\u00e8re. On obtient au final :\n", "\n", "$$\\textbf{prox}_{\\gamma \\lambda \\lVert \\cdot\\rVert_1}(u)_i=\\begin{cases}u_i-\\lambda\\gamma&\\text{ si }u_i>\\lambda\\gamma\\\\ u_i+\\lambda\\gamma&\\text{ si } u_i<-\\lambda\\gamma\\\\ 0&\\text{ si }u_i\\in[-\\lambda\\gamma,\\lambda\\gamma].\\end{cases}$$\n", "\n", "**La fonction objectif non r\u00e9gularis\u00e9e du Lasso :** Soit $f(\\beta)=\\lVert X\\beta-y\\rVert_2^2$. On a comme nous l'avons d\u00e9j\u00e0 vu plusieurs fois :\n", "\n", "$$\\nabla f(\\beta)=(X^TX)\\beta-X^Ty.$$\n", "\n", "**L'algorithme proximal appliqu\u00e9 au Lasso :** Soit le probl\u00e8me Lasso suivant : \n", "\n", "$$\\beta^\\star=\\text{argmin}_{\\beta\\in\\mathbb{R}^d}\\lVert X\\beta-\\boldsymbol{y}\\rVert_2+\\lambda \\lVert \\beta\\rVert_1 = f(\\beta)+g(\\beta).$$\n", "\n", "L'algorithme proximal est ainsi un algorithme d'optimisation qui r\u00e9alise une suite de pas d'optimisation. Un pas d'optimisation est construit de la mani\u00e8re suivante : \n", "\n", "$$\\text{pas d'optimisation : }\\begin{cases}u^{(t+1)}&=\\beta^{(t)}-\\gamma\\nabla f(\\beta^{(t)})\\\\\n", "\\beta^{(t+1)}&=\\textbf{prox}_{\\gamma \\lambda \\lVert \\cdot\\rVert_1}(u^{(t+1)})\\end{cases}$$"]}, {"cell_type": "markdown", "id": "dietary-fourth", "metadata": {}, "source": ["## III. mise en pratique"]}, {"cell_type": "markdown", "id": "boolean-breach", "metadata": {}, "source": ["### Construction du jeu de donn\u00e9es"]}, {"cell_type": "code", "execution_count": null, "id": "removed-palmer", "metadata": {}, "outputs": [], "source": ["real_beta = np.array([[8.], [-1.]])\n", "\n", "def construct_dataset(n):\n", "    global real_beta\n", "    X = np.random.normal(0, 1, size=(n, 2))\n", "    X[:, 0] += 2\n", "    y = np.dot(X, real_beta) + np.random.normal(0, 1, size=(n, 1))\n", "    return X, y\n", "X, y = construct_dataset(5)\n", "lambda_ = 2"]}, {"cell_type": "markdown", "id": "medical-liverpool", "metadata": {}, "source": ["### Les fonctions \u00e0 optimiser"]}, {"cell_type": "markdown", "id": "every-checkout", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "**Donnez le code permettant \u00e9valuer la formule :**\n", "\n", "$$\\frac{1}{2}\\lVert X\\beta-y \\rVert_2^2.$$\n", "\n", "**On appellera cette fonction $\\texttt{main}\\_\\texttt{objective}$.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "id": "practical-dressing", "metadata": {}, "source": ["def main_objective(X, y, beta):\n", "    ####### Complete this part ######## or die ####################\n", "    ...\n", "    return ...\n", "    ###############################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "focal-amazon", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Donnez le code permettant \u00e9valuer la formule :**\n", "\n", "$$\\lambda\\lVert \\beta\\rVert_1.$$\n", "\n", "**On appellera cette fonction $\\texttt{penalty}$.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "id": "selected-bloom", "metadata": {}, "source": ["def penality(beta, lambda_):\n", "    ####### Complete this part ######## or die ####################\n", "    ...\n", "    return ...\n", "    ###############################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "valued-gabriel", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**En utilisant les deux fonctions pr\u00e9c\u00e9dentes, donnez le code permettant d'\u00e9valuer la formule :**\n", "\n", "$$\\frac{1}{2}\\lVert X\\beta-y \\rVert_2^2+\\lambda\\lVert \\beta\\rVert_1.$$\n", "\n", "**On appellera cette fonction $\\texttt{loss}$.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "id": "composed-writing", "metadata": {}, "source": ["def loss(X, y, beta, lambda_):\n", "    ####### Complete this part ######## or die ####################\n", "    ...\n", "    return ...\n", "    ###############################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "ranking-screw", "metadata": {}, "source": ["Le code suivant permet d'afficher la fonction ou ses composantes qu'on cherche \u00e0 optimiser."]}, {"cell_type": "code", "execution_count": null, "id": "civic-rubber", "metadata": {"tags": ["hide-cell"]}, "outputs": [], "source": ["def plot(title, obj='loss', param_trace=None, lambda_=lambda_):\n", "    plt.figure(figsize=(12.0, 8.0))\n", "    ax = plt.gca()\n", "\n", "    delta = 0.1\n", "    x_range = np.arange(-20.0, 20.0+delta, delta)\n", "    y_range = np.arange(-20.0, 20.0+delta, delta)\n", "    XX, YY = np.meshgrid(x_range, y_range)\n", "\n", "    ZZ = np.zeros(XX.shape)\n", "    for i in range(XX.shape[0]):\n", "        for j in range(XX.shape[1]):\n", "            if obj == 'loss':\n", "                ZZ[i, j] = np.sqrt(loss(X, y, np.array([[XX[i, j]], [YY[i, j]]]), lambda_))\n", "            elif obj =='penalty':\n", "                ZZ[i, j] = penality(np.array([[XX[i, j]], [YY[i, j]]]), lambda_)\n", "            elif obj == 'objective':\n", "                ZZ[i, j] = np.sqrt(main_objective(X, y, np.array([[XX[i, j]], [YY[i, j]]])))\n", "                \n", "\n", "    CS = ax.contour(XX, YY, ZZ)\n", "    ax.clabel(CS, inline=True, fontsize=10)\n", "    # ax.scatter(real_beta[0, :], real_beta[1, :])\n", "    ax.set_title(title)\n", "    ax.axhline(0, lw=0.5, color='red') # x = 0\n", "    ax.axvline(0, lw=0.5, color='red') # y = 0\n", "    lines = None\n", "    if param_trace is not None:\n", "        lines = ax.plot(param_trace[:, 0], param_trace[:, 1], color='blue')\n", "    plt.show()"]}, {"cell_type": "markdown", "id": "level-pearl", "metadata": {}, "source": ["On observe clairemant dans l'affichage ci-dessous la non diff\u00e9rentiabilit\u00e9 de $\\lVert x\\rVert_1$ en certains endroits."]}, {"cell_type": "code", "id": "ongoing-company", "metadata": {}, "source": ["plot(obj='penalty', title='$\\ell_1$ penalty ($\\\\lambda||\\\\beta||_1$)')\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "preliminary-forest", "metadata": {}, "source": ["\u00c0 l'inverse, notre objectif principal est clairement \"smooth\"."]}, {"cell_type": "code", "id": "indonesian-reference", "metadata": {}, "source": ["plot(obj='objective', title='$\\\\frac{1}{2}||X\\\\beta-y||_2^2$')\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "republican-encyclopedia", "metadata": {}, "source": ["Malheureusement, la combinaison des deux rend notre fonction non diff\u00e9rentiable en certains endroits."]}, {"cell_type": "code", "id": "sixth-ordinance", "metadata": {}, "source": ["plot(obj='loss', title='$\\\\frac{1}{2}||X\\\\beta-y||_2^2+\\lambda||\\\\beta||_1$')\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "professional-plane", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**L'algorithme d'optimisation proximal s'appuie sur deux op\u00e9rations. Le gradient et l'operateur proximal. Compl\u00e9tez les deux fonctions associ\u00e9es ci-dessous.**\n", "\n", "**Ensuite, proposez la m\u00e9thode d'optimisation de notre classe $\\texttt{ProximalOptimization}$.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "id": "temporal-coating", "metadata": {}, "source": ["class ProximalOptimization(object):\n", "    def __init__(self, X, y):\n", "        self.X = X\n", "        self.y = y\n", "        \n", "    def gradient(self, beta):\n", "        ####### Complete this part ######## or die ####################\n", "        return ...\n", "        ###############################################################\n", "    \n", "    def gradient_descent_step(self, learning_rate, beta):\n", "        ####### Complete this part ######## or die ####################\n", "        ...\n", "        return ...\n", "        ###############################################################\n", "    \n", "    def proxy_step(self, learning_rate, lambda_, u):\n", "        ####### Complete this part ######## or die ####################\n", "        ...\n", "        return ...\n", "        ###############################################################\n", "        \n", "    def optimize(self, learning_rate, beta, lambda_, nb_iterations):\n", "        ####### Complete this part ######## or die ####################\n", "        ...\n", "        return ...\n", "        ###############################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "aware-blood", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Jouez avec les diff\u00e9rents param\u00e8tres afin d'en observer les effets.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "id": "523e98ba", "metadata": {}, "source": ["lambda_= 12\n", "beta = np.array([[-1], [19]])\n", "optim = ProximalOptimization(X, y)\n", "param_trace = optim.optimize(0.01, beta, lambda_, 1000)\n", "plot(\n", "    obj='loss', title='Optimizing $\\\\frac{1}{2}||X\\\\beta-y||_2^2+\\lambda||\\\\beta||_1$', \n", "    param_trace=param_trace\n", ")\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "pediatric-stations", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Qu'observez-vous lorsque notre chemin d'optimisation passe trop proche d'un axe ?**\n", "\n", "\n\n ----"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.2"}}, "nbformat": 4, "nbformat_minor": 5}
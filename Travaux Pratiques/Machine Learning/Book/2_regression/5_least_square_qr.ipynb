{"cells": [{"cell_type": "markdown", "id": "95cf9aa9", "metadata": {}, "source": ["# Les moindres carr\u00e9s via une d\u00e9composition QR (et plus)\u2615\ufe0f\n", "\n", "**<span style='color:blue'> Objectifs de la s\u00e9quence</span>** ", "\n", "* Comprendre les \u00e9quations normales via une d\u00e9composition QR,\n", "* \u00catre sensibilis\u00e9 aux probl\u00e8mes li\u00e9s aux approximations num\u00e9riques et \u00e0 leur r\u00e9solution.\n", "\n", "\n\n ----", "\n", "## I. Introduction\n", "\n", "En *data science*, nous ne faisons pas seulement face \u00e0 des mod\u00e8les math\u00e9matiques que nous souhaitons programmer. Nous devons aussi tenir compte du fait que les nombres que nous manipulons ont une repr\u00e9sentation sur la machine qui les stocke. De mani\u00e8re paradigmatique, consid\u00e9rons la fonction suivante&nbsp;:\n", "\n", "$$f(x)=\\text{ln}\\big(\\text{exp}(x)\\big)=x.$$\n", "\n", "Les deux formulations sont parfaitement indentiques. Observons cela via $\\texttt{numpy}$"]}, {"cell_type": "code", "execution_count": null, "id": "a7ad126a", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "code", "execution_count": null, "id": "da9e0441", "metadata": {}, "outputs": [], "source": ["x = np.linspace(0, 1000, 500)\n", "y_1 = np.log(np.exp(x))\n", "y_2 = x\n", "\n", "plt.figure(figsize=(12, 8))\n", "\n", "plt.plot(x, y_2, label=r'$x$')\n", "plt.plot(x, y_1, '--', label=r'$ln(exp(x))$')\n", "\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "dc717b0e", "metadata": {}, "source": ["Nous rencontrons une erreur ! Cela vient \u00e9videmment du fait que le calcul de l'exponentielle lorsque $x$ devient trop grand induit un $\\texttt{overflow}$ que le logarithme ne peut plus interpr\u00e9ter. En observant $\\text{ln}\\big(\\text{exp}(x)\\big)=x$, nous avons en quelque sorte utilis\u00e9 une astuce (clairement triviale ici) math\u00e9matique nous permettant d'obtenir notre r\u00e9sultat malgr\u00e9 tout. De mani\u00e8re similaire, nous utilisons souvent en *machine learning* la fonction *softmax*:\n", "\n", "$$\\text{softmax}(x)_j=\\frac{e^{x_j}}{\\sum_i e^{x_i}},$$\n", "\n", "notamment comme fonction de lien en *deep learning* ou en *r\u00e9gression logistique* afin de transformer notre vecteur de *logit* en vecteur de probabilit\u00e9s. Impl\u00e9mentons cette fonction."]}, {"cell_type": "code", "execution_count": null, "id": "ed93107e", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "\n", "def softmax(x):\n", "    return np.exp(x)/np.exp(x).sum()\n", "\n", "x = [10, 12]\n", "print('Softmax [10, 12]:', softmax(x))\n", "\n", "x = [-10, 12]\n", "print('Softmax [-10, 12]:', softmax(x))\n", "\n", "x = [748, 750]\n", "print('Softmax [748, 750]:', softmax(x))\n"]}, {"cell_type": "markdown", "id": "2e80f050", "metadata": {}, "source": ["Le calcul de l'exponentielle de $750$ entra\u00eene \u00e0 nouveau un $\\texttt{overflow}$ et nous emp\u00eache de calculer le *softmax*. La strat\u00e9gie consiste \u00e0 r\u00e9duire la taille du plus grand nombre que notre exponentielle devra calculer de la mani\u00e8re suivante&nbsp;:\n", "\n", "$$\\text{softmax}(x)_j=\\frac{e^{x_j}}{\\sum_i e^{x_i}}=\\frac{e^{x_j}}{\\sum_i e^{x_i}}\\frac{e^{-\\text{max}(x)}}{e^{-\\text{max}(x)}}=\\frac{e^{x_j-\\text{max}(x)}}{\\sum_i e^{x_i-\\text{max}(x)}}.$$\n", "\n", "R\u00e9impl\u00e9mentons notre *softmax*."]}, {"cell_type": "code", "execution_count": null, "id": "0d0f04f0", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "\n", "def softmax(x):\n", "    return np.exp(x-np.max(x))/np.exp(x-np.max(x)).sum()\n", "\n", "x = [10, 12]\n", "print('Softmax [10, 12]:', softmax(x))\n", "\n", "x = [-10, 12]\n", "print('Softmax [-10, 12]:', softmax(x))\n", "\n", "x = [748, 750]\n", "print('Softmax [748, 750]:', softmax(x))\n", "\n"]}, {"cell_type": "markdown", "id": "2bac52c9", "metadata": {}, "source": ["De tr\u00e8s nombreuses astuces de ce type existent et sont impl\u00e9ment\u00e9es dans les diff\u00e9rents *frameworks*.\n", "\n", "C'est exactement cela que nous voulons faire avec les moindres carr\u00e9s via une d\u00e9composition QR. Calculer notre estimateur des moindres carr\u00e9s est beaucoup plus stable apr\u00e8s une d\u00e9composition QR que dans sa formulation telle que nous l'avons vue."]}, {"cell_type": "markdown", "id": "ffc25fd2", "metadata": {}, "source": ["## II. D\u00e9composition QR\n", "Soit une matrice $A\\in\\mathbb{R}^{m\\times n}$. La d\u00e9composition $QR$ de la matrice $A$ est&nbsp;:\n", "\n", "$$A=QR,$$\n", "\n", "o\u00f9 $Q$ est une matrice orthogonale (par colonne si rectangulaire) et $R$ une matrice diagonale sup\u00e9rieure. On retrouve d'ailleurs parfois l'appellation \"d\u00e9composition $QU$\" o\u00f9 $U$ signifie *Upper triangular*. Une matrice orthogonale par colonne implique $Q^TQ=I$ et donc $m\\geq n$. En effet, si une famille de vecteurs est plus grande que la dimension de l'espace, alors elle est forc\u00e9ment li\u00e9e.\n", "\n", "### A. Proc\u00e9d\u00e9 de Gram-Schmidt\n", "Il existe plusieurs strat\u00e9gies permettant de r\u00e9aliser cette d\u00e9composition et nous utiliserons celle bas\u00e9e sur le proc\u00e9d\u00e9 (ou algorithme) de Gram-Schmidt. Soit $F=\\{u_1, ..., u_n\\}$ une famille de vecteurs libres. Le proc\u00e9d\u00e9 de Gram-Schmidt a pour objectif de construire une base orthonormale $B=\\{e_1, ..., e_n\\}$ \u00e0 partir de $F$.\n", "\n", "L'op\u00e9ration de base du proc\u00e9d\u00e9 de Gram-Schmidt est l'op\u00e9rateur de projection&nbsp;:\n", "\n", "$$\\textrm{proj}_u(v)=\\Pi_u(v)=\\frac{\\langle v, u\\rangle }{\\langle u, u\\rangle}u,$$\n", "\n", "o\u00f9 le vecteur $v$ est projet\u00e9 orthogonalement sur $u$.\n", "\n", "Le proc\u00e9d\u00e9 it\u00e8re sur l'ensemble des vecteurs de la famille $F$ de la mani\u00e8re suivante.\n", "\n", "\n", "1.  $v_1=u_1$ et $e_1=v_1/\\lVert v_1\\rVert_2$,\n", "2.  $v_2=u_2-\\Pi_{e_1}(u_2)$ et $e_2=v_2/\\lVert v_2\\rVert_2$,\n", "3.  ...\n", "4.  $v_n=u_n-\\sum_{i=1}^{n-1}\\Pi_{e_i}(u_n)$ et $e_n=v_n/\\lVert v_n\\rVert_2$.\n", "\n", "\n", "La famille $\\{e_1, ..., e_n\\}$ ainsi construite est une base orthonorm\u00e9e et engendre le m\u00eame sous-espace vectoriel que la famille $F$.\n", "\n", "**<span style='color:blue'> Exercice</span>** ", "\n", "**Compl\u00e9tez le code ci-dessous afin d'impl\u00e9menter le proc\u00e9d\u00e9 de Gram-Schmidt.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "id": "c2d2280c", "metadata": {}, "source": ["import numpy as np\n", "\n", "def projector(u, v):\n", "    ####### Complete this part ######## or die ####################\n", "    ...\n", "    return ...\n", "    ###############################################################\n", "\n", "def gram_schmidt(matrix):\n", "    ####### Complete this part ######## or die ####################\n", "    ...\n", "    ...\n", "    return ...\n", "    ###############################################################\n", "\n", "A = np.random.random((4, 4))\n", "\n", "Q = gram_schmidt(A)\n", "\n", "print('Une matrice identite :\\n', np.dot(Q.T, Q))\n", "\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "023b4988", "metadata": {}, "source": ["### B. D\u00e9composition QR\n", "\n", "Soit $A\\in\\mathbb{R}^{m\\times n}$ telle que les vecteurs colonnes sont libres. Notons $\\{a_1, ..., a_n\\}$ l'ensemble des vecteurs colonnes. Soit $\\{e_1, ..., e_n\\}$ une base orthonormale r\u00e9sultant du proc\u00e9d\u00e9 de Gram-Schmidt appliqu\u00e9 aux vecteurs colonnes de $A$.\n", "\n", "De mani\u00e8re assez directe, on observe que&nbsp;:\n", "\n", "$$a_1=\\langle a_1, e_1\\rangle e_1.$$ \n", "\n", "Dit autrement, $a_1$ est un vecteur co-lin\u00e9aire \u00e0 $e_1$ dont la norme est $\\langle e_1, a_1\\rangle$ (sachant que $e_1$ est unitaire). Le vecteur $a_2$ est un peu plus complexe \u00e0 reconstruire&nbsp;: \n", "\n", "$$a_2=\\langle a_2, e_2\\rangle e_2+\\langle a_2, e_1\\rangle e_1.$$ \n", "\n", "Autrement dit, $a_2$ est une combinaison lin\u00e9aire de $e_1$ et $e_2$, ce qui est logique puisque $e_2$ est construit en retirant la composante non orthogonale \u00e0 $e_1$ de $a_2$.\n", "On r\u00e9it\u00e8re l'op\u00e9ration jusqu'\u00e0 $a_n=\\sum_i \\langle e_i, a_n\\rangle e_i$.\n", "\n", "En notant&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "    Q=[e_1, ..., e_n]\\textrm{ et }R=\\begin{bmatrix}\n", "        \\langle e_1, a_1\\rangle & \\langle e_1, a_2\\rangle & \\langle e_1, a_3\\rangle & \\ldots&\\langle e_1, a_n\\rangle\\\\\n", "        0 & \\langle e_2, a_2\\rangle & \\langle e_2, a_3\\rangle& \\ldots&\\langle e_2, a_n\\rangle\\\\\n", "        0 &0 & \\langle e_3, a_3\\rangle& \\ldots&\\langle e_3, a_n\\rangle\\\\\n", "        0 & 0 & 0 & \\ddots & \\vdots\\\\\n", "        0 & 0 & 0 & \\ldots & \\langle e_n, a_n\\rangle\n", "        \\end{bmatrix}\n", "    \\end{aligned}$$\n", "    \n", "on retrouve bien $A=QR$.\n", "\n", "### C. Exemple\n", "\n", "Soit la matrice suivante&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "    A=\\begin{bmatrix}\n", "    1&-1\\\\\n", "    2&0\\\\\n", "    2&2\n", "    \\end{bmatrix}=[a_1, a_2]\n", "\\end{aligned}$$\n", "\n", "Commen\u00e7ons la proc\u00e9dure de Gram-Schmidt. On note&nbsp;:\n", "\n", "$$v_1=a_1\\textrm{ et }e_1=v_1/\\lVert v_1\\rVert_2=\\Big[\\frac{1}{3},\\frac{2}{3}, \\frac{2}{3}\\Big]^T$$\n", "\n", "Et&nbsp;:\n", "\n", "$$v_2=a_2-\\Pi_{e_1}(a_2)\\textrm{ et }e_2=v_2/\\lVert v_2\\rVert_2=\\Big[-\\frac{2}{3},-\\frac{1}{3}, \\frac{2}{3}\\Big]^T$$\n", "\n", "On v\u00e9rifie assez bien que $e_1$ et $e_2$ sont orthogonaux et unitaires.\n", "Calculons maintenant les produits scalaires&nbsp;:\n", "\n", "$$\\langle e_1, a_1\\rangle=3,\\ \\langle e_1, a_2\\rangle=1\\textrm{ et }\\langle e_2, a_2\\rangle=2$$\n", "\n", "Nous avons donc\n", "\n", "$$\\begin{aligned}\n", "    Q=\\begin{bmatrix}\n", "    1/3&-2/3\\\\\n", "    2/3&-1/3\\\\\n", "    2/3&2/3\n", "    \\end{bmatrix}\\text{ et }R=\\begin{bmatrix}\n", "    3&1\\\\\n", "    0&2\n", "    \\end{bmatrix}\n", "    \\end{aligned}$$\n", "    \n", "On v\u00e9rifie facilement qu'on a bien l'\u00e9galit\u00e9 $A=QR$.\n", "\n", "\n", "**<span style='color:blue'> Exercice</span>** ", "\n", "**Compl\u00e9tez le code ci-dessous en r\u00e9-utilisant votre impl\u00e9mentation du proc\u00e9d\u00e9 de Gram-Schmidt afin d'obtenir une d\u00e9composition QR.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "id": "61995e47", "metadata": {}, "source": ["def qr(matrix):\n", "    ####### Complete this part ######## or die ####################\n", "    ...\n", "    ...\n", "    return ...\n", "    ###############################################################\n", "\n", "A = np.array([[1, -1], [2, 0], [2, 2]])\n", "\n", "Q, R = qr(A)\n", "print('Notre matrice initiale :\\n', A)\n", "print('Notre matrice initiale reconstruite :\\n', np.dot(Q, R))\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "9c099bd2", "metadata": {}, "source": ["## III. Application aux moindres carr\u00e9s\n", "### A. Notre estimation via une d\u00e9composition QR\n", "\n", "Soit $X\\in\\mathbb{R}^{n\\times d}$, $y\\in\\mathbb{R}^n$ et $\\beta\\in\\mathbb{R}^d$. Notre objectif est de r\u00e9soudre le probl\u00e8me d'optimisation suivant&nbsp;\n", "\n", "$$\\hat{\\beta}=\\text{argmin}_{\\beta\\in\\mathbb{R}^d}\\lVert X\\beta-y\\rVert_2^2.$$\n", "\n", "Nous avons d\u00e9j\u00e0 vu que si $X^TX$ est inversible, alors nous avons la solution analytique suivante&nbsp;:\n", "\n", "$$\\hat{\\beta}=(X^TX)^{-1}X^Ty.$$\n", "\n", "Consid\u00e9rons maintenant la d\u00e9composition $QR$ de la matrice $X$ (i.e. $X=QR$). Nous avons ainsi&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "    \\hat{\\beta}&=(X^TX)^{-1}X^Ty\\\\\n", "    &=((QR)^TQR)^{-1}(QR)^Ty\\\\\n", "    &= ((R^TQ^TQR)^{-1}R^TQ^Ty\\\\\n", "    &=(R^TR)^{-1}R^TQ^Ty\\\\\n", "    &=R^{-1}(R^T)^{-1}R^TQ^Ty\\\\\n", "    &=R^{-1}Q^Ty.\n", "\\end{aligned}$$\n", "\n", "Rappellons que si $A$ et $B$ sont deux matrices inversibles, nous avons $(AB)^{-1}=B^{-1}A^{-1}$.\n", "\n", "### B. Exercice\n", "\n", "Soit la matrice suivante&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "    X=\\begin{bmatrix}\n", "    1&-1\\\\\n", "    0&10^{-5}\\\\\n", "    0&0\n", "    \\end{bmatrix}\\textrm{ et }Y=\\begin{bmatrix}\n", "    0\\\\\n", "    10^{-5}\\\\\n", "    0\n", "    \\end{bmatrix}\n", "    \\end{aligned}$$\n", "    \n", "\n", "**<span style='color:blue'> Exercice 1</span>** ", "\n", "**Supposons que l'algorithme tourne sur une machine o\u00f9 les nombres sont arrondis apr\u00e8s 8 d\u00e9cimales (i.e. si $|x|<10^{-8}$ alors $x:=0$). Calculez l'estimateur des moindres carr\u00e9s SANS passer par une d\u00e9composition QR.**\n", "\n", "\n\n ----", "\n", "**<span style='color:blue'> Exercice 2</span>** ", "\n", "**Supposons que l'algorithme tourne sur une machine o\u00f9 les nombres sont arrondis apr\u00e8s 8 d\u00e9cimales (i.e. si $|x|<10^{-8}$ alors $x:=0$). Calculez l'estimateur des moindres carr\u00e9s AVEC une d\u00e9composition QR.**\n", "\n", "\n\n ----", "\n", "\n", "**<span style='color:blue'> Exercice 3</span>** ", "\n", "**Impl\u00e9mentez les deux strat\u00e9gies pr\u00e9c\u00e9dentes en utilisant $\\texttt{numpy}$. Que constatez-vous ?**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "id": "505b74e5", "metadata": {}, "source": ["import numpy as np\n", "\n", "X = np.array([[1, -1], [0, 1e-5], [0, 0]])\n", "y = np.array([[0], [1e-5], [0]])\n", "\n", "####### Complete this part ######## or die ####################\n", "...\n", "...\n", "...\n", "###############################################################\n", "\n", "print(\"Notre estimateur avec la m\u00e9thode classique :\\n\", beta_est)\n", "print(\"Notre estimateur avec la m\u00e9thode QR :\\n\", beta_qr_est)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "c3e475c7", "metadata": {}, "source": ["Cette exemple simple montre d\u00e9j\u00e0 les avantages de la d\u00e9composition QR dans le cadre des moindres carr\u00e9s. On imagine sans mal son int\u00e9r\u00eat dans des exemples beaucoup plus compliqu\u00e9s o\u00f9 les d\u00e9pendances lin\u00e9aires sont peut-\u00eatre plus difficiles \u00e0 discerner au milieu des perturbations et du bruit."]}, {"cell_type": "markdown", "id": "127ea778", "metadata": {}, "source": ["## IV. Autres d\u00e9compositions et conclusion"]}, {"cell_type": "markdown", "id": "05c9d14b", "metadata": {}, "source": ["Soit $X$ notre matrice, la solution des moindres carr\u00e9s est obtenue par la pseudo-inverse de $X$ (nous l'avons d\u00e9montr\u00e9 dans la s\u00e9quence de cours sur la r\u00e9gression lin\u00e9aire)&nbsp;:\n", "\n", "$$\\hat{\\beta}=X^\\dagger y.$$\n", "\n", "Lorsque $X^TX$ est inversible, nous pouvons obtenir cette pseudo-inverse par &nbsp;:\n", "\n", "$$X^\\dagger=(X^TX)^{-1}X^T,$$\n", "\n", "comme nous l'avons vu au-dessus. Nous pouvons \u00e9galement retrouver l'expression $\\hat{\\beta}=(X^TX)^{-1}X^Ty=X^\\dagger y$ en annulant le gradient des moindres carr\u00e9s. Afin de limiter les probl\u00e8mes de stabilit\u00e9 num\u00e9rique li\u00e9s \u00e0 $(X^TX)^{-1}$, nous avons remplac\u00e9 dans l'expression pr\u00e9c\u00e9dente $X$ par sa d\u00e9composition $QR$. \n", "\n", "\u00c9tudions maintenant les moindres carr\u00e9s au travers d'une d\u00e9composition en valeurs singuli\u00e8res. Attention, ici l'objectif n'est pas de remplacer $X$ par cette nouvelle d\u00e9composition mais d'utiliser cette derni\u00e8re afin de trouver une expression de la pseudo-inverse de $X$. Une d\u00e9composition en valeurs singuli\u00e8res donne&nbsp;:\n", "\n", "$$X=U\\Sigma V^T,$$\n", "\n", "o\u00f9 $X\\in\\mathbb{R}^{n\\times d}$, $V$ est une matrice de taille $d\\times d$ compos\u00e9e d'une base orthonormale de $\\mathbb{R}^d$, $U$ est une matrice de taille $n\\times n$ contenant une base orthonormale de $\\mathbb{R}^n$ et $\\Sigma$ est une matrice \"diagonale\" de taille $n\\times d$ contenant les valeurs singuli\u00e8res de $X$ g\u00e9n\u00e9ralement tri\u00e9es de la plus grande \u00e0 la moins grande. Les valeurs singuli\u00e8res sont les racines des valeurs propres de $X^TX$ (celles-ci sont n\u00e9cessairement positives ou nulles).\n", "\n", "Soit $\\Sigma^{-1}$ la matrice $\\Sigma$ dont chaque valeur singuli\u00e8re diff\u00e9rente de $0$ a \u00e9t\u00e9 invers\u00e9e&nbsp;: $\\lambda_i^{-1}=1/\\lambda_i$. La pseudo-inverse de $X$ est donn\u00e9e par&nbsp;:\n", "\n", "$$X^\\dagger=V\\Sigma^{-1}U^T.$$\n", "\n", "**<span style='color:blue'> Exercice</span>** ", "\n", "V\u00e9rifier qu'il s'agit bien d'une pseudo-inverse.\n", "\n", "\n\n ----", "\n", "**<span style='color:green'> Indice</span>** ", "\n", "Montrer les \u00e9galit\u00e9s suivantes&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "AA^\\dagger A&=A\\text{ (appliquer }A\\text{, son inverse }A^\\dagger\\text{ puis }A\\text{ \u00e0 nouveau revient \u00e0 appliquer }A\\text{)}\\\\\n", "A^\\dagger AA^\\dagger&=A^\\dagger\\text{ (c'est la m\u00eame chose du point de vu de l'inverse)}\\\\\n", "(AA^\\dagger)^T&=AA^\\dagger\\text{ (la transposition n'a pas d'effet)}\\\\\n", "(A^\\dagger A)^T&=A^\\dagger A\\text{ (m\u00eame chose que pr\u00e9c\u00e9demment du point de vu de l'inverse)}\n", "\\end{aligned}$$\n", "\n", "\n", "\n\n ----", "\n"]}, {"cell_type": "markdown", "id": "d6f7f080", "metadata": {}, "source": ["Comparons dans le code ci-dessous la d\u00e9composition QR et la d\u00e9composition SVD sur un jeu de donn\u00e9es synth\u00e9tique."]}, {"cell_type": "code", "execution_count": 1, "id": "f49c625c", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "\n", "nb_elements = 500\n", "\n", "beta = np.random.random((nb_elements, 1))\n", "\n", "X = np.random.random((nb_elements, nb_elements))\n", "y = np.dot(X, beta)+np.random.normal(size=(nb_elements, 1))\n", "\n", "X_test = np.random.random((nb_elements, nb_elements))\n", "y_test = np.dot(X_test, beta)+np.random.normal(size=(nb_elements, 1))"]}, {"cell_type": "code", "execution_count": 2, "id": "a555f9ff", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Erreur sur le jeu d'apprentissage (QR): 4.18312667468882e-25\n", "Erreur sur le jeu de test (QR): 1058.2756812250134\n", "Erreur sur le jeu d'apprentissage (SVD): 1.2829829794638883e-24\n", "Erreur sur le jeu de test (SVD): 1058.275681225051\n"]}], "source": ["# on fait notre d\u00e9composition QR\n", "Q, R = np.linalg.qr(X)\n", "\n", "# on calcule notre estimateur\n", "\n", "beta_est = np.dot(np.dot(np.linalg.inv(R), Q.T), y)\n", "\n", "\n", "error = np.dot(X, beta_est)-y\n", "mse = np.squeeze(np.dot(error.T, error)/float(nb_elements))\n", "print('Erreur sur le jeu d\\'apprentissage (QR):', mse)\n", "\n", "error = np.dot(X_test, beta_est)-y_test\n", "print('Erreur sur le jeu de test (QR):', np.squeeze(np.dot(error.T, error)/float(nb_elements)))\n", "\n", "# SVD decomposition\n", "U, S, VT = np.linalg.svd(X, full_matrices=False)\n", "# on inverse la matrice diagonale sauf pour ses elements nulls\n", "S[S!=0.] = 1/S[S!=0.]\n", "S = np.diag(S)\n", "\n", "beta_est = np.dot(np.dot(np.dot(VT.T, S), U.T), y)\n", "error = np.dot(X, beta_est)-y\n", "mse = np.squeeze(np.dot(error.T, error)/float(nb_elements))\n", "print('Erreur sur le jeu d\\'apprentissage (SVD):', mse)\n", "\n", "error = np.dot(X_test, beta_est)-y_test\n", "print('Erreur sur le jeu de test (SVD):', np.squeeze(np.dot(error.T, error)/float(nb_elements)))"]}, {"cell_type": "markdown", "id": "83a476d5", "metadata": {}, "source": ["Nous avons bien $X\\hat{\\beta} = y$ (aux approximations num\u00e9riques pr\u00e8s) pour les deux d\u00e9compositions. Cependant, lorsqu'on passe sur un jeu de donn\u00e9es de test, la pr\u00e9diction ne correspond plus du tout \u00e0 la r\u00e9alit\u00e9.\n", "\n", "Cela ne vient plus du probl\u00e8me de stabilit\u00e9 num\u00e9rique mais du bruit qu'il y a dans les donn\u00e9es. En effet, nous avons&nbsp;:\n", "\n", "$$\\textbf{y}=\\textbf{X}\\beta + \\mathbf{\\epsilon},$$\n", "\n", "o\u00f9 $\\mathbf{\\epsilon}$ est un vecteur de bruit centr\u00e9 en $\\mathbf{0}$. Notons $\\textbf{y}_\\text{pred}=\\textbf{X}\\beta $ o\u00f9 $\\beta$ est le \"vrai\" vecteur de param\u00e8tres. Nous pouvons retrouver $\\beta$ en passant par la pseudo-inverse&nbsp;:\n", "\n", "$$\\beta=\\textbf{X}^\\dagger \\textbf{y}_\\text{pred}.$$\n", "\n", "En pratique, nous n'avons acc\u00e8s ni \u00e0 $\\beta$ ni \u00e0 $y_\\text{pred}$ et notre estimateur est&nbsp;:\n", "\n", "$$\\hat{\\beta}=\\textbf{X}^\\dagger \\textbf{y}=\\textbf{X}^\\dagger (\\textbf{y}_\\text{pred}+\\mathbf{\\epsilon})=\\beta + \\mathbf{X}^\\dagger \\mathbf{\\epsilon}.$$\n", "\n", "\n", "Lorsque la matrice $\\mathbf{X}$ est mal conditionn\u00e9e (e.g. matrice al\u00e9atoire presque carr\u00e9e, colonnes presque d\u00e9pendantes lin\u00e9airement), alors $\\mathbf{X}^\\dagger$ aura un effet excessivement important dans certaines directions et $\\mathbf{X}^\\dagger \\mathbf{\\epsilon}$ aura un impact catastrophique sur notre estimateur et nos pr\u00e9dictions seront mauvaises."]}, {"cell_type": "markdown", "id": "e974187e", "metadata": {}, "source": ["La bonne nouvelle est qu'on peut r\u00e9soudre ce probl\u00e8me via une strat\u00e9gie de r\u00e9gularisation comme nous pourrons le voir plus tard. Une autre strat\u00e9gie de r\u00e9gularisation est possible via notre d\u00e9composition SVD. En effet, le mauvais conditionnement vient du ratio entre la plus grande et la plus petite valeur singuli\u00e8re (ou valeurs propres lorsqu'elles existent).\n", "\n", "Le param\u00e8tre de r\u00e9gularisation, not\u00e9 $\\texttt{rcond}$ n'est autre que le ratio entre la plus grande et la plus petite valeur singuli\u00e8re (diff\u00e9rente de $0$) qu'on autorise. Toutes les valeurs singuli\u00e8res (ou propres) plus petites que $\\texttt{rcond}\\times \\lambda_\\text{max}$ sont mises \u00e0 $0$. Bien s\u00fbr, en faisant cela, nous nous \u00e9cartons de la vraie pseudo-inverse. Cependant, cela permet de r\u00e9duire consid\u00e9rablement la norme du vecteur $\\mathbf{X}^\\dagger\\epsilon$, vecteur qui n'a que des effets ind\u00e9sirables.\n", "\n", "Reprenons l'exemple pr\u00e9c\u00e9dent et observons les r\u00e9sultats."]}, {"cell_type": "code", "execution_count": 3, "id": "b2cbfb87", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Erreur sur le jeu d'apprentissage (SVD avec filtre): 0.24827151296985273\n", "Erreur sur le jeu de test (SVD avec filtre): 2.8234517739450564\n"]}], "source": ["U, S, VT = np.linalg.svd(X, full_matrices=False)\n", "\n", "# on autorise un ratio de 100 entre la plus grande \n", "# et la plus petite valeur singuliere\n", "rcond =1e-2\n", "S[S<=rcond*S.max()] = 0.\n", "S[S!=0.] = 1/S[S!=0.]\n", "S = np.diag(S)\n", "\n", "beta_est = np.dot(np.dot(np.dot(VT.T, S), U.T), y)\n", "error = np.dot(X, beta_est)-y\n", "mse = np.squeeze(np.dot(error.T, error)/float(nb_elements))\n", "print('Erreur sur le jeu d\\'apprentissage (SVD avec filtre):', mse)\n", "\n", "error = np.dot(X_test, beta_est)-y_test\n", "print('Erreur sur le jeu de test (SVD avec filtre):', np.squeeze(np.dot(error.T, error)/float(nb_elements)))"]}, {"cell_type": "markdown", "id": "4dbd383a", "metadata": {}, "source": ["On observe donc que notre nouvel estimateur est beaucoup plus robuste et permet d'obtenir de biens meilleurs r\u00e9sultats sur le test. Cependant, nous n'avons effectivement plus une vraie pseudo-inverse et l'erreur sur le train n'est plus n\u00e9gligeable.\n", "\n", "La m\u00e9thode $\\texttt{np.linalg.pinv}$ utilise $\\texttt{np.linalg.svd}$ et poss\u00e8de un param\u00e8tre $\\texttt{rcond}$ fix\u00e9 par d\u00e9faut \u00e0 $10^{-15}$ qui permet de g\u00e9rer cette r\u00e9gularisation. \n", "\n", "En r\u00e8gle g\u00e9n\u00e9ral, lorsque la matrice $X^TX$ est bien inversible les solutions $(X^TX)^{-1}X^T$, QR ou SVD sont comparables en termes de r\u00e9sultats. Lorsqu'on se rapproche d'un d\u00e9terminant nul, la m\u00e9thode $(X^TX)^{-1}X^T$ est la premi\u00e8re \u00e0 devenir instable, suivi de $QR$. La m\u00e9thode bas\u00e9e sur $SVD$ permet d'obtenir une pseudo-inverse qui fonctionne m\u00eame si $X^TX$ poss\u00e8de un d\u00e9terminant nul.\n", "\n", "Cela fait qu'en pratique, sur certains probl\u00e8mes tr\u00e8s mal conditionn\u00e9s, la solution de $\\texttt{LinearRegression}$ ou via $\\texttt{pinv}$ ou via $\\texttt{SVD}$ poss\u00e8dent de bonnes performances en test."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.2"}}, "nbformat": 4, "nbformat_minor": 5}
{"cells": [{"cell_type": "markdown", "id": "hired-grammar", "metadata": {}, "source": ["# R\u00e9gularisation en *deep learning* \u2615\ufe0f\u2615\ufe0f\n", "\n", "**<span style='color:blue'> Objectifs de la s\u00e9quence</span>** ", "\n", "* \u00catre sensibilis\u00e9&nbsp;:\n", "    * aux diff\u00e9rentes strat\u00e9gies de r\u00e9gularisation en *deep learning*\n", "* \u00catre capable de&nbsp;:\n", "    * d'impl\u00e9menter ces diff\u00e9rentes strat\u00e9gies avec $\\texttt{pytorch}$.\n", "\n", "\n\n ----"]}, {"cell_type": "markdown", "id": "illegal-capture", "metadata": {}, "source": ["Les techniques de r\u00e9gularisation sont fondamentales en *machine learning*. Elles nous permettent de consid\u00e9rer des classes de fonctions complexes tout en favorisant certaines solutions dont on sait qu'elles auront tendances \u00e0 \u00eatre plus stables et \u00e0 mieux g\u00e9n\u00e9raliser \u00e0 de nouvelles donn\u00e9es. Les r\u00e9gularisations peuvent \u00eatre explicites comme au travers de l'usage d'une p\u00e9nalit\u00e9 $\\ell_1$ ou $\\ell_2$ comme nous avons pu le voir. Elles peuvent \u00eatres implicites en r\u00e9sultant par exemple d'un effet de bord de l'optimiseur comme nous le verrons plus bas."]}, {"cell_type": "markdown", "id": "coated-distributor", "metadata": {}, "source": ["Le *deep learning* offre tout un ensemble de strat\u00e9gies qu'on ne retrouve pas syst\u00e9matiquement ailleurs. Cela est d\u00fb \u00e0 la structure m\u00eame de ces fonctions mais aussi au types de t\u00e2ches, d'optimiseurs et de donn\u00e9es qu'elles permettent de traiter avec succ\u00e8s. Cette s\u00e9quence est donc consacr\u00e9e aux m\u00e9thodes de r\u00e9gularisations utilis\u00e9es en *deep learning*."]}, {"cell_type": "markdown", "id": "southwest-surfing", "metadata": {}, "source": ["Toute cette s\u00e9quence consid\u00e9rera $\\mathcal{X}$ comme \u00e9tant l'espace des donn\u00e9es d'entr\u00e9e, $\\mathcal{Y}$ celui de nos labels et $S_n=\\{(x_i, y_i)\\}_{i\\leq n}$ notre jeu de donn\u00e9es d'apprentissage et $T_n=\\{(x_i, y_i)\\}_{i\\leq m}$ notre jeu de validation."]}, {"cell_type": "markdown", "id": "olive-commercial", "metadata": {}, "source": ["## I. R\u00e9gularisation de la norme du vecteur de param\u00e8tres"]}, {"cell_type": "markdown", "id": "skilled-filter", "metadata": {}, "source": ["De la m\u00eame mani\u00e8re que pour d'autres approches de *machine learning*, les r\u00e9seaux de neurones en tant que mod\u00e8le param\u00e9trique peut \u00eatre p\u00e9nalis\u00e9 au cours de l'apprentissage. Notons $L(\\theta, S_n)$ la *loss* \u00e0 optimiser sur notre jeu de donn\u00e9es $S_n$ o\u00f9 $\\theta$ est une param\u00e9trisation donn\u00e9e de notre mod\u00e8le. Nous d\u00e9finissons notre *loss* r\u00e9gularis\u00e9e de mani\u00e8re tr\u00e8s standard de la mani\u00e8re suivante :\n", "\n", "$$\\tilde{L}(\\theta, S_n)=L(\\theta, S_n)+\\lambda P(\\theta),$$\n", "\n", "o\u00f9 $\\lambda > 0$ contr\u00f4le la quantit\u00e9 de r\u00e9gularisation souhait\u00e9e. \n", "### A. La r\u00e9gularisation $\\ell_2$, *weight decay* ou encore *Ridge*\n", "La r\u00e9gularisation $\\ell_2$ est peut-\u00eatre la r\u00e9gularisation explicite la plus transversalement adopt\u00e9e. Dans ce cas de figure la p\u00e9nalit\u00e9 n'est autre que la norme $\\ell_2$ de notre vecteur de param\u00e8tres :\n", "\n", "$$P(\\theta)=\\frac{1}{2}\\lVert\\theta\\rVert_2^2,$$\n", "\n", "o\u00f9 le $1/2$ n'a aucun int\u00e9r\u00eat autre que calculatoire. Une s\u00e9quence compl\u00e8te sera d\u00e9di\u00e9e \u00e0 cette r\u00e9gularisation. Pour l'instant, observons intuitivement l'effet de cette derni\u00e8re.\n", "\n", "$$\\begin{aligned}\n", "\\nabla \\tilde{L}(\\theta, S_n)&=\\nabla L(\\theta, S_n)+ \\nabla\\frac{\\lambda}{2}\\lVert\\theta\\rVert_2^2\\\\\n", "&=\\nabla L(\\theta, S_n)+\\lambda \\theta\n", "\\end{aligned}$$\n", "\n", "Rappelons nous que le pas de descente de gradient est donn\u00e9 par :\n", "\n", "$$\\theta^{t+1}=\\theta^t-\\eta\\nabla \\tilde{L}(\\theta^t)$$\n", "\n", "o\u00f9 $\\eta>0$ est le pas d'apprentissage. On obtient donc dans le cas d'une p\u00e9nalit\u00e9 $\\ell_2$ le pas d'optimisation suivant :\n", "\n", "$$\\theta^{t+1}=(1-\\eta\\lambda)\\theta^t-\\eta\\nabla L(\\theta^t, S_n), $$\n", "\n", "o\u00f9 on observe qu'avant m\u00eame de se d\u00e9placer dans la direction de la plus forte pente ($-\\nabla$), notre vecteur de param\u00e8tre se voit contract\u00e9 par la p\u00e9nalit\u00e9 $\\lambda$.\n", "\n", "*La p\u00e9nalit\u00e9 $\\ell_2$ \u00e9tant standard \u00e0 beaucoup de mod\u00e8les, n'h\u00e9sitez pas \u00e0 vous r\u00e9f\u00e9rer aux autres s\u00e9quences afin d'approfondir le sujet*"]}, {"cell_type": "markdown", "id": "occupational-chess", "metadata": {}, "source": ["Consid\u00e9rons le jeu de donn\u00e9es suivant et construisons un r\u00e9seau de neurones."]}, {"cell_type": "code", "execution_count": null, "id": "surface-haiti", "metadata": {}, "outputs": [], "source": ["from torchvision import datasets\n", "from torch.utils.data import DataLoader, Dataset\n", "from torchvision import transforms\n", "import torch\n", "import torch.optim as optim\n", "import torch.nn.functional as F\n", "import torch.nn as nn\n", "import numpy as np\n", "from PIL import Image\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "code", "execution_count": null, "id": "subsequent-surge", "metadata": {}, "outputs": [], "source": ["# label names\n", "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n", "\n", "transform = transforms.Compose(\n", "  [\n", "      transforms.ToTensor(),\n", "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n", "  ]\n", ")\n", "\n", "batch_size = 128\n", "\n", "#root_directory where images are.\n", "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n", "\n", "trainloader = DataLoader(\n", "  trainset, batch_size=batch_size\n", ")\n", "\n", "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n", "testloader = DataLoader(\n", "  testset, batch_size=batch_size, shuffle=True\n", ")\n", "\n", "print('Nb test batchs:', len(testloader))"]}, {"cell_type": "code", "execution_count": null, "id": "unexpected-barrier", "metadata": {}, "outputs": [], "source": ["def imshow(images, labels, predicted=None):\n", "    plt.figure(figsize=(15, 10))\n", "    for idx in range(8):\n", "        plt.subplot(2, 4, idx+1)\n", "        plt.axis('off')\n", "        img = (images[idx] * 0.224 + 0.456)#/ 2 + 0.5  # unnormalize\n", "        npimg = img.numpy()\n", "        plt.axis('off')\n", "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n", "        title = str(classes[labels[idx]]) + \\\n", "        ('' if predicted is None else ' - ' + str(classes[predicted[idx]]))\n", "        plt.title(title)\n", "        \n", "    plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "scheduled-choice", "metadata": {}, "outputs": [], "source": ["dataiter = iter(testloader)\n", "images, labels = dataiter.next()\n", "\n", "# show images\n", "imshow(images[:8], labels[:8])"]}, {"cell_type": "markdown", "id": "actual-monthly", "metadata": {}, "source": ["Et maintenant, construisons un r\u00e9seau de neurones :"]}, {"cell_type": "code", "execution_count": null, "id": "reflected-importance", "metadata": {}, "outputs": [], "source": ["class Net(nn.Module):\n", "    def __init__(self):\n", "        super(Net, self).__init__()\n", "        self.conv1 = nn.Conv2d(3, 16, 5)\n", "        self.pool = nn.MaxPool2d(2, 2)\n", "        self.conv2 = nn.Conv2d(16, 32, 5)\n", "        self.fc1 = nn.Linear(32 * 5 * 5, 120)\n", "        self.fc2 = nn.Linear(120, 84)\n", "        self.fc = nn.Linear(84, 10)\n", "\n", "    def forward(self, x):\n", "        x = self.pool(F.relu(self.conv1(x)))\n", "        x = self.pool(F.relu(self.conv2(x)))\n", "        x = x.view(-1, 32 * 5 * 5)\n", "        x = F.relu(self.fc1(x))\n", "        x = F.relu(self.fc2(x))\n", "        x = self.fc(x)\n", "        return x"]}, {"cell_type": "markdown", "id": "competitive-recall", "metadata": {}, "source": ["Et configurons notre optimisation."]}, {"cell_type": "code", "execution_count": null, "id": "vulnerable-championship", "metadata": {}, "outputs": [], "source": ["criterion = nn.CrossEntropyLoss()"]}, {"cell_type": "code", "execution_count": null, "id": "available-woman", "metadata": {}, "outputs": [], "source": ["def train(model, criterion, optimizer, n_epoch=2, eval_frequency=2):\n", "    loss_history = []\n", "    valid_loss_history = []\n", "\n", "    acc_history = []\n", "    val_acc_history = []\n", "\n", "    for epoch in range(n_epoch):  # loop over the dataset multiple times\n", "        print(\"[EPOCH %d, LR: %s]\"%(epoch + 1, str(0.01)))\n", "\n", "        #iterate over the training batches until all all samples have been considered\n", "        model.train()\n", "        running_loss = 0.0\n", "        for i, data in enumerate(trainloader, 0):\n", "            if i % 5 == 4:\n", "                pass\n", "                print('\\r[Batch id: %d/%d]' % (i+1, len(trainloader)), end='')\n", "            # get the inputs; data is a list of [inputs, labels]\n", "            inputs, labels = data\n", "\n", "            # Convert to cuda is device is gpu\n", "            # inputs = inputs.cuda()\n", "            # labels = labels.cuda()\n", "\n", "            # Zero the parameter gradients\n", "            optimizer.zero_grad()\n", "\n", "            # Forward : compute outputs for all layers \n", "            outputs = model(inputs)\n", "            loss = criterion(outputs, labels)\n", "            running_loss += loss.item()\n", "\n", "            # Backward: Compute gradient wrt parameters of all layers\n", "            loss.backward()\n", "\n", "\n", "            optimizer.step()\n", "\n", "        print(' train loss: %.3f' % (running_loss/len(trainloader)))\n", "\n", "\n", "        if (epoch + 1) % eval_frequency == 0:\n", "            #Compute training statistics\n", "            accuracy_train = test(model, trainloader, 'train')\n", "            accuracy_test = test(model, testloader, 'test')\n", "            loss_train = loss_eval(model, trainloader, 'train')\n", "            loss_test = loss_eval(model, testloader, 'test')\n", "            loss_history.append(loss_train)\n", "            acc_history.append(accuracy_train)\n", "            valid_loss_history.append(loss_test)\n", "            val_acc_history.append(accuracy_test)\n", "\n", "        print(\"\\n\")\n", "\n", "    print('**** Finished Training ****')\n", "    return loss_history, valid_loss_history, acc_history, val_acc_history"]}, {"cell_type": "code", "execution_count": null, "id": "sacred-efficiency", "metadata": {}, "outputs": [], "source": ["def test(model, loader, title):\n", "    model.eval()  # on passe le modele en mode evaluation\n", "    correct = 0\n", "    total = 0\n", "    with torch.no_grad():\n", "        for data in loader:\n", "            images, labels = data\n", "            # images = images.cuda()\n", "            # labels = labels.cuda()\n", "            outputs = model(images)\n", "            predicted = torch.nn.functional.softmax(outputs, dim=1).max(dim=1)\n", "            \n", "            total += labels.size(0)\n", "            correct += (predicted[1] == labels).sum().item()\n", "            \n", "    model.train()  # on remet le modele en mode apprentissage\n", "    print('Accuracy du modele sur le jeu de ', title, ': %.2f' % (correct / total))\n", "    return correct / total\n", "  \n", "def loss_eval(model, loader, title):\n", "    model.eval()  # on passe le modele en mode evaluation\n", "    running_loss = 0.\n", "    total = 0.\n", "    with torch.no_grad():\n", "        for data in loader:\n", "            images, labels = data\n", "            # images = images.cuda()\n", "            # labels = labels.cuda()\n", "            outputs = model(images)\n", "\n", "            total += 1.\n", "            running_loss += criterion(outputs, labels).item()\n", "\n", "    model.train()  # on remet le modele en mode apprentissage\n", "    print('Loss du modele sur le jeu de ', title, ': %.2f' % (running_loss / total))\n", "    return (running_loss / total)\n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "nearby-syndication", "metadata": {}, "outputs": [], "source": ["def plot_loss(loss_history, valid_loss_history, acc_history, val_acc_history):\n", "    plt.figure()\n", "    plt.plot([i*eval_frequency for i in range(1, len(loss_history)+1)], loss_history, \n", "             label='Train loss')\n", "    plt.plot([i*eval_frequency for i in range(1, len(loss_history)+1)], valid_loss_history, \n", "             label='Validation loss')\n", "    plt.legend()\n", "    plt.show()\n", "\n", "    plt.figure()\n", "    plt.plot([i*eval_frequency for i in range(1, len(acc_history)+1)], acc_history, \n", "             label='Train Accuracy')\n", "    plt.plot([i*eval_frequency for i in range(1, len(acc_history)+1)], val_acc_history, \n", "             label='Validation Accuracy')\n", "    plt.legend()\n", "    plt.show()"]}, {"cell_type": "markdown", "id": "signed-basic", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**La p\u00e9nalit\u00e9 $\\ell_2$ s'appelle *weight decay* en *deep learning* et est configurable directement au niveau de l'optimiseur. Testez plusieurs valeurs afin d'en mesurer les effets.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "execution_count": null, "id": "regulation-repeat", "metadata": {}, "outputs": [], "source": ["model = Net()\n", "# model = model.cuda()\n", "\n", "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n", "\n", "eval_frequency=2\n", "\n", "loss_history, \\\n", "valid_loss_history, \\\n", "acc_history, \\\n", "val_acc_history = train(model, criterion, optimizer, n_epoch=1)"]}, {"cell_type": "code", "execution_count": null, "id": "characteristic-vocabulary", "metadata": {}, "outputs": [], "source": ["plot_loss(loss_history, valid_loss_history, acc_history, val_acc_history)"]}, {"cell_type": "markdown", "id": "liquid-reservation", "metadata": {}, "source": ["### B. La r\u00e9gularisation $\\ell_1$ ou *Lasso*"]}, {"cell_type": "markdown", "id": "28f49119", "metadata": {}, "source": ["La loss $\\ell_1$ offre une complexit\u00e9 particuli\u00e8re en cassant la diff\u00e9rentiabilit\u00e9 de notre fonction en certains points. De plus, ces points en \u00e9tant des solutions possibles de notre probl\u00e8me ne sont pas \u00e0 ignorer. Une strat\u00e9gie possible est l'algorithme proximal que nous avons (peut-\u00eatre) d\u00e9j\u00e0 vu. Nous n'entrerons pas plus en d\u00e9tail ici quant \u00e0 l'optimisation de la r\u00e9gularisation $\\ell_1$. Il est important de se rappeler que pour une p\u00e9nalit\u00e9 $\\lambda$ suffisamment grande, notre solution sera *sparse*."]}, {"cell_type": "markdown", "id": "earlier-analysis", "metadata": {}, "source": ["## II. *Early stopping*"]}, {"cell_type": "markdown", "id": "e43f6c44", "metadata": {}, "source": ["On remarque qu'apr\u00e8s un certain nombre d'epoch l'accuracy du mod\u00e8le en validation redescend et sa valeur de loss augmente \u00e0 nouveau. Attention, le changement de palier entre *loss* et *accuracy* ne se situe pas \u00e0 la m\u00eame *epoch*. L'id\u00e9e de *l'early stopping* est de ne garder que le mod\u00e8le qui a obtenu la meilleure validation relativement au score qui nous int\u00e9resse.\n", "\n", "**<span style='color:blue'> Exercice</span>** ", "\n", "**Adaptez le code suivant afin de ne r\u00e9cup\u00e9rer que le mod\u00e8le ayant obtenu la meilleure validation en *accuracy*.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "id": "f613ae87", "metadata": {}, "source": ["def train(model, criterion, optimizer, n_epoch=2, eval_frequency=2):\n", "    ####### Complete this part ######## or die ####################\n", "    loss_history = []\n", "    valid_loss_history = []\n", "\n", "    acc_history = []\n", "    val_acc_history = []\n", "\n", "    for epoch in range(n_epoch):  # loop over the dataset multiple times\n", "        print(\"[EPOCH %d, LR: %s]\"%(epoch + 1, str(0.01)))\n", "\n", "        #iterate over the training batches until all all samples have been considered\n", "        model.train()\n", "        running_loss = 0.0\n", "        for i, data in enumerate(trainloader, 0):\n", "            if i % 5 == 4:\n", "                print('\\r[Batch id: %d/%d]' % (i+1, len(trainloader)), end='')\n", "            # get the inputs; data is a list of [inputs, labels]\n", "            inputs, labels = data\n", "\n", "            # Convert to cuda is device is gpu\n", "            # inputs = inputs.cuda()\n", "            # labels = labels.cuda()\n", "\n", "            # Zero the parameter gradients\n", "            optimizer.zero_grad()\n", "\n", "            # Forward : compute outputs for all layers \n", "            outputs = model(inputs)\n", "            loss = criterion(outputs, labels)\n", "            running_loss += loss.item()\n", "\n", "            # Backward: Compute gradient wrt parameters of all layers\n", "            loss.backward()\n", "\n", "\n", "            optimizer.step()\n", "\n", "        print(' train loss: %.3f' % (running_loss/len(trainloader)))\n", "\n", "\n", "        if (epoch + 1) % eval_frequency == 0:\n", "            #Compute training statistics\n", "            accuracy_train = test(model, trainloader, 'train')\n", "            accuracy_test = test(model, testloader, 'test')\n", "            loss_train = loss_eval(model, trainloader, 'train')\n", "            loss_test = loss_eval(model, testloader, 'test')\n", "            loss_history.append(loss_train)\n", "            acc_history.append(accuracy_train)\n", "            valid_loss_history.append(loss_test)\n", "            val_acc_history.append(accuracy_test)\n", "\n", "            print(\"\\n\")\n", "    model.load_state_dict(best_model.state_dict())\n", "    \n", "    print('**** Finished Training ****')\n", "    ###############################################################\n", "    return loss_history, valid_loss_history, acc_history, val_acc_history\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "id": "a6e8bb4c", "metadata": {}, "source": ["model = Net()\n", "# model = model.cuda()\n", "\n", "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n", "\n", "eval_frequency=1\n", "\n", "loss_history, \\\n", "valid_loss_history, \\\n", "acc_history, \\\n", "val_acc_history = train(model, criterion, optimizer, n_epoch=1, \n", "                        eval_frequency=eval_frequency)\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "id": "7fcfafe7", "metadata": {}, "source": ["plot_loss(loss_history, valid_loss_history, acc_history, val_acc_history)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "specific-prayer", "metadata": {}, "source": ["## III. La *data augmentation*"]}, {"cell_type": "markdown", "id": "acknowledged-biography", "metadata": {}, "source": ["Nous avons d\u00e9j\u00e0 vu lors de s\u00e9quences pr\u00e9c\u00e9dentes que lorsque la quantit\u00e9 de donn\u00e9es \u00e9tait trop limit\u00e9e nous pouvions r\u00e9gulariser notre mod\u00e8le. Une strat\u00e9gie de r\u00e9gularisation devient donc de cr\u00e9er des images de mani\u00e8re bien choisie afin d'augmenter virtuellement la taille de notre jeu de donn\u00e9es et de stabiliser notre optimisation tout en choisissant les param\u00e9trisations les plus parcimonieuses. La *data augmentation* est une des strat\u00e9gies les plus courantes et standards utilis\u00e9es en *deep learning*.\n", "\n", "Nous allons nous concentrer ici sur le cas o\u00f9 les donn\u00e9es \u00e0 perturber sont des images. Intuitivement, si je fais une petite rotation d'une image de chat, celle-ci sera toujours une image de chat et les pixels de la nouvelle image seront significativement diff\u00e9rents des pixels de l'ancienne image : nous avons donc bien une nouvelle image. L'id\u00e9e va \u00eatre de chercher \u00e0 construire des perturbations qui conservent la s\u00e9mantique de l'image (i.e. l'objet qu'elle repr\u00e9sente)."]}, {"cell_type": "code", "execution_count": null, "id": "respective-edmonton", "metadata": {}, "outputs": [], "source": ["from PIL import Image\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "markdown", "id": "massive-album", "metadata": {}, "source": ["L'image est t\u00e9l\u00e9chargeable \u00e0 l'adresse suivante : \n", "[lemon.jpeg](https://raw.githubusercontent.com/maximiliense/lmiprp/main/Travaux%20Pratiques/Machine%20Learning/Introduction/data/Adversarial/lemon.jpeg)."]}, {"cell_type": "code", "execution_count": null, "id": "simplified-antenna", "metadata": {}, "outputs": [], "source": ["image = Image.open('lemon.jpeg')\n", "\n", "plt.figure(figsize=(8, 8))\n", "plt.imshow(image)\n", "plt.axis('off')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "urban-tender", "metadata": {}, "source": ["La librairie $\\text{pytorch}$ offre d\u00e9j\u00e0 un certain nombre de transformations possibles de nos donn\u00e9es. En voici quelques exemples : "]}, {"cell_type": "code", "execution_count": null, "id": "abroad-flesh", "metadata": {}, "outputs": [], "source": ["from torchvision import transforms"]}, {"cell_type": "code", "execution_count": null, "id": "practical-local", "metadata": {}, "outputs": [], "source": ["crops = [transforms.RandomCrop(224)(image) for _ in range(4)]\n", "\n", "plt.figure(figsize=(12, 12))\n", "for i in range(4):\n", "    plt.subplot(2, 2, i+1)\n", "    plt.imshow(crops[i])\n", "    plt.axis('off')\n", "\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "intense-scholar", "metadata": {}, "outputs": [], "source": ["crops = [transforms.RandomResizedCrop(224)(image) for _ in range(4)]\n", "\n", "plt.figure(figsize=(12, 12))\n", "for i in range(4):\n", "    plt.subplot(2, 2, i+1)\n", "    plt.imshow(crops[i])\n", "    plt.axis('off')\n", "\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "welsh-summit", "metadata": {}, "outputs": [], "source": ["rotations = [transforms.RandomRotation(180)(image) for _ in range(4)]\n", "\n", "plt.figure(figsize=(12, 12))\n", "for i in range(4):\n", "    plt.subplot(2, 2, i+1)\n", "    plt.imshow(rotations[i])\n", "    plt.axis('off')\n", "\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "confused-girlfriend", "metadata": {}, "outputs": [], "source": ["flips = [transforms.transforms.RandomHorizontalFlip()(image) for _ in range(4)]\n", "\n", "plt.figure(figsize=(12, 10))\n", "for i in range(4):\n", "    plt.subplot(2, 2, i+1)\n", "    plt.imshow(flips[i])\n", "    plt.axis('off')\n", "\n", "plt.show()"]}, {"cell_type": "markdown", "id": "legitimate-charles", "metadata": {}, "source": ["Nous pouvons aussi combiner ces transformations :"]}, {"cell_type": "code", "execution_count": null, "id": "comparable-visit", "metadata": {}, "outputs": [], "source": ["augmentation = transforms.Compose([\n", "    transforms.RandomRotation(90),\n", "    transforms.RandomResizedCrop(224),\n", "    transforms.transforms.RandomHorizontalFlip()\n", "])\n", "aug = [augmentation(image) for _ in range(4)]\n", "\n", "plt.figure(figsize=(12, 10))\n", "for i in range(4):\n", "    plt.subplot(2, 2, i+1)\n", "    plt.imshow(aug[i])\n", "    plt.axis('off')\n", "\n", "plt.show()"]}, {"cell_type": "markdown", "id": "eleven-spotlight", "metadata": {}, "source": ["Nous avons r\u00e9ussi \u00e0 construire une infinit\u00e9 d'images \u00e0 partir d'un unique exemple ! Observons maintenant les effets sur l'apprentissage que donne cette m\u00e9thode."]}, {"cell_type": "markdown", "id": "convinced-productivity", "metadata": {}, "source": ["### a. Un premier mod\u00e8le sans data augmentation"]}, {"cell_type": "markdown", "id": "unknown-bruce", "metadata": {}, "source": ["Nous allons dans un premier temps reprendre le mod\u00e8le du TP 5 (le code est fourni ci-dessous). Ce mod\u00e8le nous permettra \"d'\u00e9talonner\" les performances d'apprentissage sans aucune r\u00e9gularisation. Nous r\u00e9utiliserons \u00e9galement ces r\u00e9sultats dans les sections *dropout* ou encore *batchnorm*."]}, {"cell_type": "code", "execution_count": null, "id": "discrete-anxiety", "metadata": {}, "outputs": [], "source": ["from torchvision import datasets\n", "from torch.utils.data import DataLoader\n", "import torch.optim as optim\n", "import torch.nn.functional as F\n", "import torch.nn as nn\n", "import numpy as np"]}, {"cell_type": "code", "execution_count": null, "id": "illegal-fence", "metadata": {}, "outputs": [], "source": ["# label names\n", "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n", "\n", "transform = transforms.Compose(\n", "  [\n", "      transforms.ToTensor(),\n", "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n", "  ]\n", ")\n", "\n", "batch_size = 128\n", "\n", "#root_directory where images are.\n", "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n", "\n", "trainloader = DataLoader(\n", "  trainset, batch_size=batch_size\n", ")\n", "\n", "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n", "testloader = DataLoader(\n", "  testset, batch_size=batch_size, shuffle=True\n", ")\n", "\n", "print('Nb test batchs:', len(testloader))"]}, {"cell_type": "code", "execution_count": null, "id": "yellow-albania", "metadata": {}, "outputs": [], "source": ["def imshow(images, labels, predicted=None):\n", "    plt.figure(figsize=(15, 10))\n", "    for idx in range(8):\n", "        plt.subplot(2, 4, idx+1)\n", "        plt.axis('off')\n", "        img = (images[idx] * 0.224 + 0.456)#/ 2 + 0.5  # unnormalize\n", "        npimg = img.numpy()\n", "        plt.axis('off')\n", "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n", "        title = str(classes[labels[idx]]) + \\\n", "        ('' if predicted is None else ' - ' + str(classes[predicted[idx]]))\n", "        plt.title(title)\n", "        \n", "    plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "destroyed-moral", "metadata": {}, "outputs": [], "source": ["dataiter = iter(testloader)\n", "images, labels = dataiter.next()\n", "\n", "# show images\n", "imshow(images[:8], labels[:8])"]}, {"cell_type": "code", "execution_count": null, "id": "married-relation", "metadata": {}, "outputs": [], "source": ["class Net(nn.Module):\n", "    def __init__(self):\n", "        super(Net, self).__init__()\n", "        self.conv1 = nn.Conv2d(3, 16, 5)\n", "        self.pool = nn.MaxPool2d(2, 2)\n", "        self.conv2 = nn.Conv2d(16, 32, 5)\n", "        self.fc1 = nn.Linear(32 * 5 * 5, 120)\n", "        self.fc2 = nn.Linear(120, 84)\n", "        self.fc = nn.Linear(84, 10)\n", "\n", "    def forward(self, x):\n", "        x = self.pool(F.relu(self.conv1(x)))\n", "        x = self.pool(F.relu(self.conv2(x)))\n", "        x = x.view(-1, 32 * 5 * 5)\n", "        x = F.relu(self.fc1(x))\n", "        x = F.relu(self.fc2(x))\n", "        x = self.fc(x)\n", "        return x"]}, {"cell_type": "code", "execution_count": null, "id": "classical-provider", "metadata": {}, "outputs": [], "source": ["criterion = nn.CrossEntropyLoss()"]}, {"cell_type": "code", "execution_count": null, "id": "caroline-reliance", "metadata": {}, "outputs": [], "source": ["def test(model, loader, title):\n", "    model.eval()  # on passe le modele en mode evaluation\n", "    correct = 0\n", "    total = 0\n", "    with torch.no_grad():\n", "        for data in loader:\n", "            images, labels = data\n", "            # images = images.cuda()\n", "            # labels = labels.cuda()\n", "            outputs = model(images)\n", "            predicted = torch.nn.functional.softmax(outputs, dim=1).max(dim=1)\n", "            \n", "            total += labels.size(0)\n", "            correct += (predicted[1] == labels).sum().item()\n", "            \n", "    model.train()  # on remet le modele en mode apprentissage\n", "    print('Accuracy du modele sur le jeu de ', title, ': %.2f' % (correct / total))\n", "    return correct / total\n", "  \n", "def loss_eval(model, loader, title):\n", "    model.eval()  # on passe le modele en mode evaluation\n", "    running_loss = 0.\n", "    total = 0.\n", "    with torch.no_grad():\n", "        for data in loader:\n", "            images, labels = data\n", "            # images = images.cuda()\n", "            # labels = labels.cuda()\n", "            outputs = model(images)\n", "\n", "            total += 1.\n", "            running_loss += criterion(outputs, labels).item()\n", "\n", "    model.train()  # on remet le modele en mode apprentissage\n", "    print('Loss du modele sur le jeu de ', title, ': %.2f' % (running_loss / total))\n", "    return (running_loss / total)\n"]}, {"cell_type": "code", "id": "dc91a4d2", "metadata": {}, "source": ["model = Net()\n", "# model = model.cuda()\n", "\n", "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n", "\n", "eval_frequency=2\n", "\n", "loss_history, \\\n", "valid_loss_history, \\\n", "acc_history, \\\n", "val_acc_history = train(model, criterion, optimizer, n_epoch=30)\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "execution_count": null, "id": "physical-fabric", "metadata": {}, "outputs": [], "source": ["def plot_loss(loss_history, valid_loss_history, acc_history, val_acc_history):\n", "    plt.figure()\n", "    plt.plot([i*eval_frequency for i in range(1, len(loss_history)+1)], loss_history, \n", "             label='Train loss')\n", "    plt.plot([i*eval_frequency for i in range(1, len(loss_history)+1)], valid_loss_history, \n", "             label='Validation loss')\n", "    plt.legend()\n", "    plt.show()\n", "\n", "    plt.figure()\n", "    plt.plot([i*eval_frequency for i in range(1, len(acc_history)+1)], acc_history, \n", "             label='Train Accuracy')\n", "    plt.plot([i*eval_frequency for i in range(1, len(acc_history)+1)], val_acc_history, \n", "             label='Validation Accuracy')\n", "    plt.legend()\n", "    plt.show()"]}, {"cell_type": "code", "id": "b830a66f", "metadata": {}, "source": ["plot_loss(loss_history, valid_loss_history, acc_history, val_acc_history)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "consolidated-bermuda", "metadata": {}, "source": ["### b. Un mod\u00e8le avec data augmentation"]}, {"cell_type": "markdown", "id": "opening-sequence", "metadata": {}, "source": ["Attention, les effets de ces strat\u00e9gies deviennent d'autant plus visibles qu'on utilise des mod\u00e8les v\u00e9ritablement complexes. Les effets ici resteront limit\u00e9s car le mod\u00e8le est pens\u00e9 pour que l'apprentissage soit \"rapide\"."]}, {"cell_type": "markdown", "id": "accompanied-strand", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Construisez un $\\texttt{trainloader}$ qui r\u00e9alise des *flips* horizontaux ainsi que des rotation al\u00e9atoire d'au plus $45^\\circ$. Attention, n'oubliez pas que les donn\u00e9es doivent \u00eatre transform\u00e9es en tenseur et normalis\u00e9es comme au-dessus.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "id": "e7b07ba7", "metadata": {}, "source": ["####### Complete this part ######## or die ####################\n", "transform = transforms.Compose(\n", "      [\n", "          transforms.ToTensor(),\n", "          ...\n", "          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n", "      ]\n", "  )\n", "\n", "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n", "\n", "trainloader = DataLoader(\n", "  trainset, batch_size=batch_size\n", ")\n", "###############################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "id": "4245f32e", "metadata": {}, "source": ["model = Net()\n", "# model = model.cuda()\n", "\n", "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n", "\n", "eval_frequency=2\n", "\n", "loss_history_a, \\\n", "valid_loss_history_a, \\\n", "acc_history_a, \\\n", "val_acc_history_a = train(model, criterion, optimizer, n_epoch=30)\n", "\n", "plot_loss(loss_history_a, valid_loss_history_a, acc_history_a, val_acc_history_a)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "anticipated-internet", "metadata": {}, "source": ["## IV. Le *dropout*"]}, {"cell_type": "markdown", "id": "loose-parking", "metadata": {}, "source": ["L'id\u00e9e du *dropout* est de perturber l'apprentissage en mettant al\u00e9atoirement certains neurones de notre r\u00e9seau \u00e0 $0$. Ainsi, le mod\u00e8le, afin de minimiser l'erreur, est oblig\u00e9 d'exploiter de l'information provenant de plusieurs neurones : cette redondance limite l'effet de surapprentissage tr\u00e8s li\u00e9 \u00e0 la capacit\u00e9 du mod\u00e8le \u00e0 exploiter le bruit des donn\u00e9es, bruit qui par d\u00e9finition n'offre pas de redondance.\n", "\n", "Notons $\\sigma$ une non-lin\u00e9arit\u00e9 et $w_{ij}, b_{ij}$ les param\u00e8tres de la couche $i$ et du neurone $j$ de notre r\u00e9seau de neurones. Nous avons donc pour la couche $i$ et son neurone $j$ :\n", "\n", "$$\\sigma(\\langle w_{ij}, z\\rangle +b_{ij}),$$\n", "\n", "o\u00f9 $z$ est la sortie de la couche pr\u00e9c\u00e9dente. L'id\u00e9e du *Dropout* est de consid\u00e9rer un vecteur $\\delta$ tel que $\\delta_i\\sim \\text{Bernoulli}(1-p)$ o\u00f9 $p$ est le param\u00e8tre de *dropout* si $p=0$ alors le *dropout* est d\u00e9sactiv\u00e9, et si $p$ est proche de $1$ alors aucune information ne passe. Une couche de dropout est :\n", "\n", "$$\\text{do}(z)=\\frac{\\delta \\odot z}{1-p}, $$\n", "\n", "o\u00f9 $\\odot$ est la multiplication \u00e9l\u00e9ment par \u00e9l\u00e9ment. Le d\u00e9nominateur permet de garantir que \"l'amplitude\" du vecteur sortant du *dropout* est la m\u00eame avec ou sans *dropout*. Plus rigoureusement, nous avons : \n", "\n", "$$\\mathbb{E}\\Big[\\frac{\\delta_i}{1-p}\\Big]=1.$$\n", "\n", "Le dropout est d\u00e9sactiv\u00e9 lorsqu'on utilise le mod\u00e8le pour faire des pr\u00e9dictions (et sa normalisation \u00e9galement). En reprenant notre couche $i$, tous ses neurones $j$ ont la forme suivante :\n", "\n", "$$\\sigma(\\langle w_{ij}, \\text{do}(z)\\rangle +b_{ij}),$$"]}, {"cell_type": "code", "execution_count": null, "id": "faced-marshall", "metadata": {}, "outputs": [], "source": ["# on reprend le dataset sans data augmentation\n", "transform = transforms.Compose(\n", "  [\n", "      transforms.ToTensor(),\n", "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n", "  ]\n", ")\n", "\n", "batch_size = 128\n", "\n", "#root_directory where images are.\n", "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n", "\n", "trainloader = DataLoader(\n", "  trainset, batch_size=batch_size\n", ")"]}, {"cell_type": "markdown", "id": "uniform-elimination", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Modifiez le mod\u00e8le propos\u00e9 ci-dessous en int\u00e9grant une couche de *dropout* de param\u00e8tre $p=0.2$.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "id": "ee61d5e3", "metadata": {}, "source": ["####### Complete this part ######## or die ####################\n", "class Net(nn.Module):\n", "    def __init__(self):\n", "        super(Net, self).__init__()\n", "        self.conv1 = nn.Conv2d(3, 16, 5)\n", "        self.pool = nn.MaxPool2d(2, 2)\n", "        self.conv2 = nn.Conv2d(16, 32, 5)\n", "        self.fc1 = nn.Linear(32 * 5 * 5, 120)\n", "        self.fc2 = nn.Linear(120, 84)\n", "        self.fc = nn.Linear(84, 10)\n", "\n", "    def forward(self, x):\n", "        x = self.pool(F.relu(self.conv1(x)))\n", "        x = self.pool(F.relu(self.conv2(x)))\n", "        x = x.view(-1, 32 * 5 * 5)\n", "        x = F.relu(self.fc1(x))\n", "        x = F.relu(self.fc2(x))\n", "        x = self.fc(x)\n", "        return x\n", "###############################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "id": "62e5f547", "metadata": {}, "source": ["model = Net()\n", "model = model.cuda()\n", "\n", "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n", "\n", "eval_frequency=2\n", "\n", "loss_history_do, \\\n", "valid_loss_history_do, \\\n", "acc_history_do, \\\n", "val_acc_history_do = train(model, criterion, optimizer, n_epoch=30)\n", "\n", "plot_loss(loss_history_do, valid_loss_history_do, acc_history_do, val_acc_history_do)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "helpful-street", "metadata": {}, "source": ["## V. La *batch normalization*"]}, {"cell_type": "markdown", "id": "satisfied-today", "metadata": {}, "source": ["La batch normalization est une autre technique de r\u00e9gularisation qui n'a cependant, \u00e0 premi\u00e8re vue, rien \u00e0 voir avec la r\u00e9gularisation. Ses effets sont encore mal compris et sont probablement li\u00e9s \u00e0 la r\u00e9gularisation implicite qui d\u00e9pend de l'optimiseur lui-m\u00eame (comme nous allons le voir plus bas). La batch normalization ne fonctionne que si les donn\u00e9es arrivent par batch de taille sup\u00e9rieure \u00e0 1. Nous allons voir une mani\u00e8re standard de faire de la *batch normalization* mais il est \u00e0 noter que la couche est g\u00e9n\u00e9ralement ultra-configurable et beaucoup de ses composantes peuvent \u00eatre d\u00e9sactiv\u00e9es. Une couche est d\u00e9crite par l'\u00e9quation suivante : \n", "\n", "$$\\text{bn}(z)=\\frac{z-\\mathbb{E}\\big[z\\big]}{\\sqrt{\\text{Var}(z)+\\epsilon}}\\odot \\gamma+\\beta,$$\n", "o\u00f9 $z$ est un batch de donn\u00e9es, $\\gamma$ et $\\beta$ sont des vecteurs dont la dimension est celle d'une donn\u00e9e de $z$ et dont les op\u00e9rations sont appliqu\u00e9es \u00e9l\u00e9ment par \u00e9l\u00e9ment dans $z$. L'esp\u00e9rance et la variance sont calcul\u00e9es \u00e0 partir du batch mais peuvent conserver une inertie des batchs pr\u00e9c\u00e9dents. Lorsque notre mod\u00e8le passe en mode pr\u00e9diction, on r\u00e9utilise les statistiques de moyenne et de variance obtenues lors de l'apprentissage.\n", "\n", "**Attention,** certains objets $\\texttt{pytorch}$ demande si leur entr\u00e9e sera un vecteur (1d), une image (2d), etc. C'est le cas de la *batch normalization* avec notamment *BatchNorm1d* ou *BatchNorm2d*, etc."]}, {"cell_type": "markdown", "id": "adjusted-printing", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Proposez ci-dessous une modification permettant d'int\u00e9grer une couche de *batch normalization*.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "id": "715a04ef", "metadata": {}, "source": ["####### Complete this part ######## or die ####################\n", "class Net(nn.Module):\n", "    def __init__(self):\n", "        super(Net, self).__init__()\n", "        self.conv1 = nn.Conv2d(3, 16, 5)\n", "        self.pool = nn.MaxPool2d(2, 2)\n", "        self.conv2 = nn.Conv2d(16, 32, 5)\n", "        self.fc1 = nn.Linear(32 * 5 * 5, 120)\n", "        self.fc2 = nn.Linear(120, 84)\n", "        self.fc = nn.Linear(84, 10)\n", "\n", "    def forward(self, x):\n", "        x = self.pool(F.relu(self.conv1(x)))\n", "        x = self.pool(F.relu(self.conv2(x)))\n", "        x = x.view(-1, 32 * 5 * 5)\n", "        x = F.relu(self.fc1(x))\n", "        x = F.relu(self.fc2(x))\n", "        x = self.fc(x)\n", "        return x\n", "###############################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "id": "faa4570c", "metadata": {}, "source": ["model = Net()\n", "model = model.cuda()\n", "\n", "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n", "\n", "eval_frequency=2\n", "\n", "loss_history_do, \\\n", "valid_loss_history_do, \\\n", "acc_history_do, \\\n", "val_acc_history_do = train(model, criterion, optimizer, n_epoch=30)\n", "\n", "plot_loss(loss_history_do, valid_loss_history_do, acc_history_do, val_acc_history_do)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "stainless-ranking", "metadata": {}, "source": ["## VI. Le biais implicite de SGD"]}, {"cell_type": "markdown", "id": "remarkable-profile", "metadata": {}, "source": ["Il se trouve qu'une des raisons pouvant expliquer le succ\u00e8s des r\u00e9seaux de neurones est li\u00e9es \u00e0 la mani\u00e8re dont on les optimise. On parle soucent de \"biais implicite\" li\u00e9 \u00e0 l'optimiseur qui entra\u00eenerait une r\u00e9gularisation par lui-m\u00eame. Nous allons ici observer cet effet sur un jeu de donn\u00e9es synth\u00e9tique."]}, {"cell_type": "code", "execution_count": null, "id": "novel-lighting", "metadata": {}, "outputs": [], "source": ["def make_dec_func(model):\n", "    def dec_func(X):\n", "        data = torch.from_numpy(X).float()\n", "        Z = model(data)\n", "        return Z\n", "    return dec_func\n", "\n", "def make_pred_func(model):\n", "    dec_func = make_dec_func(model)\n", "\n", "    def predict(X):\n", "        output = dec_func(X)\n", "        return output.argmax(dim=-1)\n", "\n", "    return predict\n", "\n", "def plot(X, y, clf=None):\n", "    plt.figure(figsize=(14, 8))\n", "    # plt.xticks(())\n", "    # plt.yticks(())\n", "    if clf is not None:\n", "        clf = make_pred_func(clf)\n", "        XX, YY = np.mgrid[-1:1:1000j, -1:1:1000j]\n", "        Z = clf(np.c_[XX.ravel(), YY.ravel()])\n", "        plt.title('Les donn\u00e9es et la fronti\u00e8re de d\u00e9cision')\n", "        # Put the result into a color plot\n", "        Z = Z.reshape(XX.shape)\n", "        plt.pcolormesh(XX, YY, Z, cmap=plt.cm.Paired, shading='auto')\n", "    else:\n", "        plt.title('Les donn\u00e9es')\n", "\n", "    plt.xlim(-1, 1)\n", "    plt.ylim(-1, 1)\n", "\n", "    plt.scatter(X[:, 0], X[:, 1], c=y)\n", "\n", "    plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "expressed-seattle", "metadata": {}, "outputs": [], "source": ["def sample_data(size=200):\n", "    X = np.random.uniform(-1, 1, size=(size, 2))\n", "    y = X[:, 0]**3 < X[:, 1]\n", "    return X, y\n", "X, y = sample_data()\n", "\n", "plot(X, y)"]}, {"cell_type": "code", "execution_count": null, "id": "preceding-mouth", "metadata": {}, "outputs": [], "source": ["class SyntheticDataset(Dataset):\n", "    def __init__(self, X, y):\n", "        self.dataset, self.label = X, y\n", "\n", "    def __len__(self):\n", "        return len(self.label)\n", "\n", "    def __getitem__(self, idx):\n", "        return torch.from_numpy(self.dataset[idx]).float(), int(self.label[idx])\n", "    \n", "dataset = SyntheticDataset(X, y)"]}, {"cell_type": "code", "execution_count": null, "id": "sticky-skill", "metadata": {}, "outputs": [], "source": ["def fullyconnected_layer(in_f, out_f):\n", "    \"\"\"\n", "    \" this function returns a fully connected layer with batchnorm\n", "    \"\"\"\n", "    return nn.Sequential(\n", "        nn.Linear(in_f, out_f),\n", "        nn.BatchNorm1d(out_f),\n", "        nn.ReLU()\n", "    )\n", "\n", "class FullyConnectedNetwork(nn.Module):\n", "    def __init__(self, n_labels=2, n_input=2, architecture=(2,)):\n", "        \"\"\"\n", "        \" n_labels is the dimension of the output\n", "        \" n_input is the dimension of the input\n", "        \" architecture describes the series of hidden layers\n", "        \"\"\"\n", "        super(FullyConnectedNetwork, self).__init__()\n", "\n", "        layer_size = [n_input] + [i for i in architecture]\n", "\n", "        layers = [\n", "            fullyconnected_layer(in_f, out_f) for in_f, out_f in zip(layer_size, layer_size[1:])\n", "        ]\n", "        self.layers = nn.Sequential(*layers)\n", "        self.fc = nn.Linear(architecture[-1], n_labels)\n", "\n", "    def forward(self, x):\n", "        x = self.layers(x)\n", "        x = self.fc(x)\n", "        return x"]}, {"cell_type": "code", "execution_count": 1, "id": "textile-jimmy", "metadata": {}, "outputs": [], "source": ["def train_and_plot(lr=0.005, batch_size=200, epochs=5000, logs=10):\n", "    model = FullyConnectedNetwork(architecture=tuple(10 for _ in range(10)))\n", "    # model.cuda()\n", "    \n", "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.)\n", "    criterion = nn.CrossEntropyLoss()\n", "    \n", "    train_loader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=0)\n", "    \n", "    loss_history = []\n", "    running_loss = 0.0\n", "    for e in range(epochs):\n", "        for idx, data in enumerate(train_loader):\n", "            inputs, labels = data\n", "            # labels = labels.cuda()\n", "            optimizer.zero_grad()\n", "            outputs = model(inputs)\n", "\n", "            loss = criterion(outputs, labels)\n", "            running_loss += loss.item()\n", "            if idx % logs == logs - 1:  # print every 2000 mini-batches\n", "                print('\\r[%d, %5d] loss: %.3f' % (e + 1, idx + 1, running_loss / logs), end=\"\")\n", "                loss_history.append(running_loss / logs)\n", "                running_loss = 0.0\n", "\n", "            loss.backward() # on calcule le gradient\n", "            optimizer.step() # on fait un pas d'optimisation\n", "    print('\\r************* Training done! *************')\n", "    plt.plot(loss_history)\n", "    plt.title('Loss')\n", "    plt.show()\n", "    plot(X, y, model)"]}, {"cell_type": "markdown", "id": "ac8cdf10", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Testez diff\u00e9rentes valeurs du batch size et du learning rate:**\n", "\n", "1.  **lr: 0.001, 0.1,**\n", "2.  **bs: 50, 20, 5.**\n", "\n", "**Attention, si le batch size est plus grand, il faut faire plus de pas d'optimisation pour commencer la quantit\u00e9 moindre de pas \u00e0 chaque *epoch*.**\n", "\n", "\n\n ----"]}, {"cell_type": "markdown", "id": "regulated-logistics", "metadata": {}, "source": ["Tester batch size : 5 (et epochs 1000).... BS 5+ LR 0.1"]}, {"cell_type": "code", "execution_count": null, "id": "combined-marathon", "metadata": {}, "outputs": [], "source": ["train_and_plot(lr=0.001, batch_size=20, epochs=1000, logs=1)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.2"}}, "nbformat": 4, "nbformat_minor": 5}
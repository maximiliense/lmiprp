
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Calibration des probabilit√©s et quelques notions ‚òïÔ∏è &#8212; Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="R√©gularisation en deep learning ‚òïÔ∏è‚òïÔ∏è" href="4_regularization_deep.html" />
    <link rel="prev" title="Filtres et espace de repr√©sentation des r√©seaux de neurones ‚òïÔ∏è‚òïÔ∏è" href="2_filters_representation.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-72WWYCKNK6"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-72WWYCKNK6');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../0_requisite/rappels_python.html">
   Rappels de Python et Numpy
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../1_what_is_ml/0_propos_liminaire.html">
   <em>
    Machine Learning
   </em>
   , initiation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/1_introduction_ml.html">
     <em>
      Machine learning
     </em>
     et mal√©diction de la dimension
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/2_regression_and_classification_trees.html">
     Les arbres de r√©gression et de classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../2_regression/0_propos_liminaire.html">
   La
   <em>
    r√©gression
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/1_linear_regression.html">
     La r√©gression lin√©aire ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/2_optimization.html">
     L‚Äôoptimisation ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/3_interpolation.html">
     Interpolation ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/4_algo_proximal_lasso.html">
     Sous-diff√©rentiel et le cas du Lasso ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/5_least_square_qr.html">
     Les moindres carr√©s via une d√©composition QR (et plus)‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/6_ridge.html">
     Une analyse de la r√©gularisation Ridge ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../3_classification/0_propos_liminaire.html">
   La
   <em>
    classification
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/1_logistic_regression.html">
     La r√©gression logistique ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/2_fonctions_proxy.html">
     Les fonctions de perte (loss function) ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/3_bayes_classifier.html">
     Le classifieur de Bayes ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/4_VC_theory.html">
     Un mod√®le formel de l‚Äôapprentissage ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è (üíÜ‚Äç‚ôÇÔ∏è)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../4_kernel_methods/0_propos_liminaire.html">
   Les m√©thodes √† noyaux
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../4_kernel_methods/1_svm.html">
     Le SVM ou l‚Äôhypoth√®se max-margin ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5_ensembles/0_propos_liminaire.html">
   Les m√©thodes
   <em>
    ensemblistes
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/1_ensembles.html">
     M√©thodes ensemblistes ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/2_bayesian_linear_regression.html">
     Bayesian linear regression ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="0_propos_liminaire.html">
   <em>
    deep learning
   </em>
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="1_autodiff.html">
     La diff√©rentiation automatique et un d√©but de
     <em>
      deep learning
     </em>
     ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="2_filters_representation.html">
     Filtres et espace de repr√©sentation des r√©seaux de neurones ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Calibration des probabilit√©s et quelques notions ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="4_regularization_deep.html">
     R√©gularisation en
     <em>
      deep learning
     </em>
     ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5_transfer_multitask.html">
     <em>
      Transfer learning
     </em>
     et apprentissage multi-t√¢ches ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="6_adversarial.html">
     Les attaques adversaires ‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../7_unsupervised/0_propos_liminaire.html">
   L‚Äôapprentissage non-supervis√©
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_unsupervised/1_principal_component_analysis.html">
     L‚ÄôAnalyse en Composantes Principales ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_unsupervised/2_em_gaussin_mixture_model.html">
     Mod√®le de M√©lange Gaussien et algorithme
     <em>
      Expectation-Maximization
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../8_set_prediction/0_propos_liminaire.html">
   Pr√©diction d‚Äôensembles
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/1_well_defined.html">
     Jeu d‚Äôapprentissage
     <em>
      set-valued
     </em>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/2_ill_defined.html">
     Jeu d‚Äôapprentissage uniquement multi-classes ‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../biblio.html">
   Bibliographie
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/6_deeplearning/3_probabilities_calibration.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/maximiliense/lmiprp"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/maximiliense/lmiprp/issues/new?title=Issue%20on%20page%20%2F6_deeplearning/3_probabilities_calibration.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/maximiliense/lmiprp/blob/master/Travaux Pratiques/Machine Learning/Book/6_deeplearning/3_probabilities_calibration.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#i-introduction">
   I. Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ii-estimation-de-la-probabilite-conditionnelle">
   II. Estimation de la probabilit√© conditionnelle
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-proper-loss-et-cross-entropy">
     A.
     <em>
      Proper loss
     </em>
     et
     <em>
      cross-entropy
     </em>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-consistence-de-la-cross-entropy">
     B. Consistence de la
     <em>
      cross-entropy
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iii-calibration-de-la-probabilite">
   III. Calibration de la probabilit√©
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="calibration-des-probabilites-et-quelques-notions">
<h1>Calibration des probabilit√©s et quelques notions ‚òïÔ∏è<a class="headerlink" href="#calibration-des-probabilites-et-quelques-notions" title="Permalink to this headline">¬∂</a></h1>
<div class="admonition-objectifs-de-la-sequence admonition">
<p class="admonition-title">Objectifs de la s√©quence</p>
<ul class="simple">
<li><p>√ätre sensibilis√©¬†:</p>
<ul>
<li><p>aux difficult√©s √† estimer la loi conditionnelle <span class="math notranslate nohighlight">\(\eta_k(x)=\mathbb{P}(Y=k|X=x)\)</span>.</p></li>
</ul>
</li>
<li><p>√ätre capable de¬†:</p>
<ul>
<li><p>de calibrer les probabilit√©s d‚Äôun r√©seau de neurones pour am√©liorer le fit de <span class="math notranslate nohighlight">\(\eta\)</span>.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="i-introduction">
<h2>I. Introduction<a class="headerlink" href="#i-introduction" title="Permalink to this headline">¬∂</a></h2>
<p>Notez que par soucis de simplicit√©, nous consid√©rons dans nos formules le cas de la classification binaire. Le propos se g√©n√©ralise bien s√ªr !</p>
<p>Consid√©rons un probl√®me de classification o√π <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> repr√©sente notre espace d‚Äôentr√©e et <span class="math notranslate nohighlight">\(\mathcal{Y}=\{0, 1\}\)</span> l‚Äôensemble de nos classes (ici deux classes). Notons <span class="math notranslate nohighlight">\(X,Y\)</span> deux variables al√©atoires sur <span class="math notranslate nohighlight">\(\mathcal{X}\times\mathcal{Y}\)</span> et notons <span class="math notranslate nohighlight">\(\mu\)</span> la mesure de <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(\eta(x)=\mathbb{P}(Y=1|X=x)\)</span> la ‚Äúprobabilit√© <em>a posteriori</em>‚Äù. Notre objectif est assez traditionnellement de trouver une application <span class="math notranslate nohighlight">\(h:\mathcal{X}\rightarrow\mathcal{Y}\)</span> telle que le risque suivant est minimis√©¬†:</p>
<div class="math notranslate nohighlight">
\[R(h)=\mathbb{E}\big[\textbf{1}\{h(X)\neq Y\}\big].\]</div>
<p>Ne connaissant ni <span class="math notranslate nohighlight">\(\mu\)</span> ni <span class="math notranslate nohighlight">\(\eta\)</span>, nous ne pouvons estimer ce risque et devons l‚Äôestimer empiriquement en collectant un jeu de donn√©es <span class="math notranslate nohighlight">\(S_n=\{(X_i, Y_i)\}_{i\leq n}\)</span>. Utilisant ce jeu de donn√©es, nous pouvons construire la notion de risque empirique¬†:</p>
<div class="math notranslate nohighlight">
\[R_n(h)=\frac{1}{n}\sum_{i=1}^n \textbf{1}\{h(X_i)\neq Y_i\}.\]</div>
<p>La s√©quence sur les fonctions proxy nous a montr√© que minimiser ce risque est g√©n√©ralement difficile. Nous ne pouvons en particulier pas utiliser la descente de gradient puisque ce dernier est presque partout nul. Nous avons vu que nous pouvions cependant construire une strat√©gie o√π notre application ne retourne pas un label mais un score sur <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> (qu‚Äôon appelle logit en <em>deep learning</em> ou en r√©gression logistique), score dont le signe nous permet de d√©terminer la classe. Une <em>loss</em> poss√©dant un certain nombre de propri√©t√©s (e.g. convexe) √©tait ensuite optimis√©e. Les propri√©t√©s de cette derni√®re nous permettait de conclure que le r√©sultat ne sera pas trop mauvais. Un exemple de loss est la <em>logistic loss</em> d√©finie comme¬†:</p>
<div class="math notranslate nohighlight">
\[\text{log}\big(1+e^{-z}\big),\]</div>
<p>o√π <span class="math notranslate nohighlight">\(z\)</span> est justement le score retourn√© par notre mod√®le. Nous allons dans cette s√©quence une approche parall√®le. Cette fois-ci, notre mod√®le ne donnera pas un score sur <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> mais sur <span class="math notranslate nohighlight">\([0, 1]\)</span> (la probabilit√© de la classe <span class="math notranslate nohighlight">\(1\)</span> dans le cas √† deux classes). Nous avons vu dans la s√©quence sur les fonctions proxy que consid√©rer un mod√®le qui retourne un score sur <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> suivi de la <em>logistic loss</em> √©tait √©quivalent au m√™me mod√®le dont on donne le score √† une fonction de lien sigmoid (qui retourne une probabilit√©) et qu‚Äôon optimise avec l‚Äôinverse de la log-vraisemblance¬†: la vision statistique ou <em>machine learning</em> sont en r√©alit√© les deux faces d‚Äôune m√™me pi√®ce.</p>
<p>L‚Äôapproche <em>machine learning</em> nous permet de r√©fl√©chir avec une vision purement pr√©dictive et performances de pr√©diction alors que l‚Äôapproche statistiques nous donne cette information sur les probabilit√©s. Cette derni√®re peut √™tre importante par exemple si on veut seuiller, e.g. je ne retourne la classe <span class="math notranslate nohighlight">\(1\)</span> que si sa probabilit√© est sup√©rieure √† <span class="math notranslate nohighlight">\(99\%\)</span>. On verra que cette id√©e est cl√© lorsqu‚Äôon cherche √† pr√©dire des ensembles (cf. la s√©quence d√©di√©e).</p>
<p>Nous allons ici voir comment estimer ses probabilit√©s sur un jeu d‚Äôapprentissage puis comment corriger/calibrer ces derni√®res pour qu‚Äôelles soient le plus r√©alistes possibles.</p>
</div>
<div class="section" id="ii-estimation-de-la-probabilite-conditionnelle">
<h2>II. Estimation de la probabilit√© conditionnelle<a class="headerlink" href="#ii-estimation-de-la-probabilite-conditionnelle" title="Permalink to this headline">¬∂</a></h2>
<div class="section" id="a-proper-loss-et-cross-entropy">
<h3>A. <em>Proper loss</em> et <em>cross-entropy</em><a class="headerlink" href="#a-proper-loss-et-cross-entropy" title="Permalink to this headline">¬∂</a></h3>
<p>Nous allons ici prendre la perspective statistique, c‚Äôest-√†-dire celle o√π on cherche √† estimer la probabilit√© conditionnelle. Nous voulons construire un estimateur¬†:</p>
<div class="math notranslate nohighlight">
\[\hat{\eta}(x)=\hat{\mathbb{P}}\big(Y=1|X=x\big),\]</div>
<p>o√π <span class="math notranslate nohighlight">\(\hat{\eta}\in\mathcal{F}\)</span> et <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> est notre ensemble de ‚Äúprobabilit√©s conditionnelles‚Äù (e.g. les param√©trisations d‚Äôune r√©gression logistique ou d‚Äôun r√©seau de neurones). Il existe de nombreuses strat√©gies afin d‚Äôatteindre cet objectif. L‚Äôune consiste √† trouver une loss qui nous permettra de faire le meilleur choix de probabilit√© conditionnelle. La strat√©gie est souvent de maximiser la vraisemblance, ou, de mani√®re totalement √©quivalente, de minimiser l‚Äôoppos√© de la log-vraisemblance¬†:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\hat{\eta})=-\sum_{i=1}^n Y_i\text{log}(\hat{\eta}(X_i))+(1-Y_i)\text{log}(1-\hat{\eta}(X_i)).\]</div>
<p>En esp√©rance, nous notons¬†:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_\eta(\hat{\eta})=-\mathbb{E}_X\big[\eta(X)\text{log}(\hat{\eta}(X))+(1-\eta(X))\text{log}(1-\hat{\eta}(X))\big].\]</div>
<p>Est-ce une bonne strat√©gie. Finalement la premi√®re chose que notre <em>loss</em> doit satisfaire est la suivante. Si on lui donnait la vraie probabilit√© conditionnelle <span class="math notranslate nohighlight">\(\eta\)</span>, serait-elle minimis√©e ? Serait-ce l‚Äôunique minimiseur ? Les d√©finitions suivantes formalisent ces id√©es.</p>
<hr class="docutils" />
<p><strong>D√©finition 1 (<em>Proper loss</em>)</strong></p>
<p>Une loss <span class="math notranslate nohighlight">\(\mathcal{L}_\eta\)</span> est dite <em>proper</em> si <span class="math notranslate nohighlight">\(\eta\)</span> est un minimiseur de cette derni√®re¬†</p>
<div class="math notranslate nohighlight">
\[\forall \hat{\eta},\ \mathcal{L}_\eta(\hat{\eta})\geq \mathcal{L}_\eta(\eta).\]</div>
<hr class="docutils" />
<p><strong>D√©finition 2 (<em>Strictly proper loss</em>)</strong>
Une loss <span class="math notranslate nohighlight">\(\mathcal{L}_\eta\)</span> est dite <em>strictly proper</em> si <span class="math notranslate nohighlight">\(\eta\)</span> est l‚Äôunique minimiseur de cette derni√®re¬†</p>
<div class="math notranslate nohighlight">
\[\forall \hat{\eta},\ \hat{\eta}\neq \eta,\ \mathcal{L}_\eta(\hat{\eta})&gt; \mathcal{L}_\eta(\eta).\]</div>
<hr class="docutils" />
<p>Il se trouve que la <em>cross-entropy</em> ou log-vraisemblance n√©gative est <em>strictly proper</em>.</p>
</div>
<div class="section" id="b-consistence-de-la-cross-entropy">
<h3>B. Consistence de la <em>cross-entropy</em><a class="headerlink" href="#b-consistence-de-la-cross-entropy" title="Permalink to this headline">¬∂</a></h3>
<p><strong>D√©finition 3 (Plugin rule)</strong>
√âtant donn√©e un estimateur des probabilit√©s conditionnelles <span class="math notranslate nohighlight">\(\hat{\eta}\)</span>, la plugin rule est celle qui retourne la classe associ√©e √† la plus forte probabilit√©¬†:</p>
<div class="math notranslate nohighlight">
\[\begin{split}g_{\hat{\eta}}(x)=\begin{cases}1\text{ si }\hat{\eta}(x)\geq 0.5\\ 0\text{ sinon.}\end{cases}\end{split}\]</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p>Soit <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> un ensemble d‚Äôestimateurs du vrai <span class="math notranslate nohighlight">\(\eta\)</span> (e.g. r√©gression logistique) et <span class="math notranslate nohighlight">\(\mathcal{H}_{\mathcal{F}}\)</span> les classifieurs (i.e. plugin rule) associ√©s. Le choix du bon estimateur <span class="math notranslate nohighlight">\(\hat{\eta}\in\mathcal{F}\)</span> se fait via la <em>cross entropy</em> qui est, rappelons le, <em>strictly proper</em>. Le classifieur associ√© est not√© <span class="math notranslate nohighlight">\(g_{\hat{\eta}}\in\mathcal{H}_{\mathcal{F}}\)</span>. Notons <span class="math notranslate nohighlight">\(g^\star\in\mathcal{H}_{\mathcal{F}}\)</span> le minimiseur du risque (ou le minimiseur du risque empirique apr√®s avoir vu une infinit√© de donn√©es). Le choix de <span class="math notranslate nohighlight">\(\hat{\eta}\)</span> en minimisant la <em>cross entropy</em> sur une infinit√© de donn√©es garantit-il que le classifieur associ√© converge vers <span class="math notranslate nohighlight">\(g^\star\in\mathcal{H}_{\mathcal{F}}\)</span> ?</p>
</div>
<div class="hint dropdown admonition">
<p class="admonition-title">Indices</p>
<p>Consid√©rer le sc√©nario suivant. <span class="math notranslate nohighlight">\(\mathcal{X}=\{0, 1\}\)</span>, <span class="math notranslate nohighlight">\(\mu(0)=0.5\)</span>, <span class="math notranslate nohighlight">\(\mu(1)=0.5\)</span>, <span class="math notranslate nohighlight">\(\eta(0)=0.55\)</span> et <span class="math notranslate nohighlight">\(\eta(1)=0.45\)</span>. Consid√©rez maintenant la classe d‚Äôestimateurs suivante <span class="math notranslate nohighlight">\(\mathcal{F}=\{\eta_1,\eta_2\}\)</span> o√π <span class="math notranslate nohighlight">\(\eta_1(0)=0.99\)</span> et <span class="math notranslate nohighlight">\(\eta_1(1)=0.01\)</span> ainsi que <span class="math notranslate nohighlight">\(\eta_2(0)=0.49\)</span> et <span class="math notranslate nohighlight">\(\eta_1(1)=0.51\)</span>. Calculez la <em>cross-entropy</em> et d√©duisez-en le meilleur estimateur de la probabilit√© conditionnelle. Ensuite calculez le minimiseur du risque (l‚Äôestimateur dont la plugin rule fera le moins d‚Äôerreurs).</p>
</div>
<hr class="docutils" />
<p>En r√©alit√©, la cross-entropy garantit que l‚Äôon obtienne la meilleure plugin-rule √† partir du moment o√π on accepte certaines hypoth√®ses. Ainsi, si le vrai <span class="math notranslate nohighlight">\(\eta\)</span> fait partie des param√©trisations possibles alors, √ßa sera le cas.</p>
</div>
</div>
<div class="section" id="iii-calibration-de-la-probabilite">
<h2>III. Calibration de la probabilit√©<a class="headerlink" href="#iii-calibration-de-la-probabilite" title="Permalink to this headline">¬∂</a></h2>
<p>Dans de nombreuses applications, il est n√©cessaire d‚Äôavoir une estimation ‚Äúrelativement correcte‚Äù de la probabilit√© conditionnelle <span class="math notranslate nohighlight">\(\eta\)</span>. Consid√©rons le jeu de donn√©es synth√©tique suivant. Nous allons d√©lib√©r√©ment consid√©rer un mod√®le avec une forte capacit√© de sur-apprentissage afin de mettre ces effets en avant dans des temps d‚Äôapprentissage raisonables.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">expit</span> <span class="k">as</span> <span class="n">sigmoid</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">15</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">3.</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">)])</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">logits</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Entra√Ænons notre premier mod√®le sur ces donn√©es via un r√©seau de neurones sur <span class="math notranslate nohighlight">\(\texttt{pytorch}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;torch._C.Generator at 0x11bf31e90&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fullyconnected_layer</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">out_f</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    &quot; this function returns a fully connected layer with batchnorm</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">out_f</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">out_f</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
    <span class="p">)</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_labels</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_input</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">architecture</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,)):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        &quot; n_labels is the dimension of the output</span>
<span class="sd">        &quot; n_input is the dimension of the input</span>
<span class="sd">        &quot; architecture describes the series of hidden layers</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="n">layer_size</span> <span class="o">=</span> <span class="p">[</span><span class="n">n_input</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">architecture</span><span class="p">]</span>

        <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">fullyconnected_layer</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">out_f</span><span class="p">)</span> <span class="k">for</span> <span class="n">in_f</span><span class="p">,</span> <span class="n">out_f</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">layer_size</span><span class="p">,</span> <span class="n">layer_size</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
        <span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">architecture</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">n_labels</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SyntheticDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">label</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">label</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
    
<span class="n">dataset</span> <span class="o">=</span> <span class="n">SyntheticDataset</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># TODO D√©commenter les prints de loss, etc.</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">train_and_plot</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">logs</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">architecture</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="mi">10</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)))</span>
    <span class="c1"># model.cuda()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
    
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="n">loss_history</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
            <span class="c1"># labels = labels.cuda()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
            <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">idx</span> <span class="o">%</span> <span class="n">logs</span> <span class="o">==</span> <span class="n">logs</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># print every 2000 mini-batches</span>
                <span class="c1"># TODO uncomment print(&#39;\r[%d, %5d] loss: %.3f&#39; % (e + 1, idx + 1, running_loss / logs), end=&quot;&quot;)</span>
                <span class="n">loss_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">running_loss</span> <span class="o">/</span> <span class="n">logs</span><span class="p">)</span>
                <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>

            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># on calcule le gradient</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span> <span class="c1"># on fait un pas d&#39;optimisation</span>
        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\r</span><span class="s1">************* Training done! *************&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss_history</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">train_and_plot</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">logs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>************* Training done! *************
</pre></div>
</div>
<img alt="../_images/3_probabilities_calibration_10_1.png" src="../_images/3_probabilities_calibration_10_1.png" />
</div>
</div>
<p>L‚Äôoptimisation se passe bien. Notre <em>loss</em> d√©cro√Æt et se rapproche asymptotiquement de <span class="math notranslate nohighlight">\(0\)</span>. On observe √©galement quelques fluctuations li√©es au fait qu‚Äôon optimise notre r√©seau par <em>batch</em> (i.e. SGD). Consid√©rons les notations suivantes. Soit <span class="math notranslate nohighlight">\(h:\mathcal{X}\rightarrow\mathbb{R}\)</span> notre r√©seau de neurones dont la sortie sont appel√©s <em>logit</em>. Notons <span class="math notranslate nohighlight">\(\sigma(z)=(1+e^{-z})^{-1}\)</span> la fonction sigmoid. C‚Äôest la fonction de lien qui permet de transformer nos <em>logits</em> en probabilit√©. Nous avons donc un estimateur de la vraie probabilit√© conditionnelle¬†:</p>
<div class="math notranslate nohighlight">
\[\hat{\eta}(x)=\sigma(h(x)).\]</div>
<p>Notre r√©seau est optimis√© via la <em>cross-entropy</em> d√©crite plus haut¬†:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(h)=-\frac{1}{n}\sum_{i=1}^n y_i\log\big(\sigma(h(x_i))\big)+(1-y_i)\log\big(1-\sigma(h(x_i))\big).\]</div>
<div class="admonition-question admonition">
<p class="admonition-title">Question</p>
<p>Notre <em>loss</em> atteint quasiment <span class="math notranslate nohighlight">\(0\)</span>. Supposons qu‚Äôelle soit aussi proche de <span class="math notranslate nohighlight">\(0\)</span> que l‚Äôon veut. Que pouvons-nous dire sur l‚Äôerreur de classification <span class="math notranslate nohighlight">\(0/1\)</span>¬†:</p>
<div class="math notranslate nohighlight">
\[Re(h)=\frac{1}{n}\sum_{i=1}^n\textbf{1}\{h(x_i)\neq y_i\},\]</div>
<p>relativement √† la <em>plugin-rule</em> associ√©e √† <span class="math notranslate nohighlight">\(\sigma(h(x))\)</span>¬†?</p>
</div>
<p>Si notre <em>loss</em> est proche de <span class="math notranslate nohighlight">\(0\)</span>, alors les probabilit√©s retourn√©es par notre mod√®le sont soit tr√®s proches de <span class="math notranslate nohighlight">\(0\)</span> pour les points de notre jeu de donn√©es de la classe <span class="math notranslate nohighlight">\(0\)</span>, soit tr√®s proches de <span class="math notranslate nohighlight">\(1\)</span> pour les points associ√©s √† la classe <span class="math notranslate nohighlight">\(1\)</span>. Notre mod√®le indique donc une ‚Äúforte confiance‚Äù dans ses pr√©dictions tout en ne faisant que peu d‚Äôerreurs sur notre jeu d‚Äôapprentissage. Il existe une strat√©gie permettant de quantifier la qualit√© de la ‚Äúconfiance‚Äù d‚Äôun mod√®le¬†: la courbe de calibration. Cette derni√®re va confronter les scores de pr√©diction de notre mod√®le √† leur valeur empirique sur nos donn√©es (i.e. si notre mod√®le pr√©dit <span class="math notranslate nohighlight">\(70\%\)</span>, on s‚Äôattend que <span class="math notranslate nohighlight">\(70\%\)</span> des points associ√©s √† ce score soient de la classe <span class="math notranslate nohighlight">\(1\)</span>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trainloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">loss_value</span><span class="p">(</span><span class="n">loader</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="n">running_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">true_labels</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">loader</span><span class="p">):</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
            <span class="n">true_labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">temperature</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span> <span class="o">*</span> <span class="n">temperature</span>
            <span class="n">pred</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
            <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">true_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">true_labels</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">running_loss</span>

<span class="n">true_train</span><span class="p">,</span> <span class="n">pred_train</span><span class="p">,</span> <span class="n">loss_train</span> <span class="o">=</span> <span class="n">loss_value</span><span class="p">(</span><span class="n">trainloader</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Cross-entropy sur le train: </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">loss_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cross-entropy sur le train: 0.014
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.calibration</span> <span class="kn">import</span> <span class="n">calibration_curve</span>
<span class="k">def</span> <span class="nf">calibration</span><span class="p">(</span><span class="n">true_value</span><span class="p">,</span> <span class="n">pred_value</span><span class="p">):</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;k:&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Perfectly calibrated&quot;</span><span class="p">)</span>

    <span class="n">fraction_of_positives</span><span class="p">,</span> <span class="n">mean_predicted_value</span> <span class="o">=</span> \
        <span class="n">calibration_curve</span><span class="p">(</span><span class="n">true_value</span><span class="p">,</span> <span class="n">pred_value</span><span class="p">,</span> <span class="n">n_bins</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mean_predicted_value</span><span class="p">,</span> <span class="n">fraction_of_positives</span><span class="p">,</span> <span class="s2">&quot;s-&quot;</span><span class="p">,</span>
             <span class="n">label</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="s1">&#39;Deep net&#39;</span><span class="p">,</span> <span class="p">))</span>

    <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Fraction of positives&quot;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Mean predicted value&quot;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">])</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower right&quot;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Calibration plots&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    
<span class="n">calibration</span><span class="p">(</span><span class="n">true_train</span><span class="p">,</span> <span class="n">pred_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/3_probabilities_calibration_14_0.png" src="../_images/3_probabilities_calibration_14_0.png" />
</div>
</div>
<p>Ce r√©sultat √©tait attendu. Notre mod√®le est en surapprentissage : il ne se trompe pas sur le jeu d‚Äôapprentissage et pr√©dit des scores indiquant une forte confiance. Nous devons √©valuer la calibration <strong>et</strong> la <em>cross-entropy</em> sur un jeu de test afin d‚Äôavoir une id√©e de la qualit√© des pr√©dictions de notre mod√®le.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="n">testset</span> <span class="o">=</span> <span class="n">SyntheticDataset</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="n">testloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">testset</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">true_test</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">,</span> <span class="n">loss_test</span> <span class="o">=</span> <span class="n">loss_value</span><span class="p">(</span><span class="n">testloader</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Cross-entropy sur le train: </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">loss_test</span><span class="p">)</span>
<span class="n">calibration</span><span class="p">(</span><span class="n">true_test</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cross-entropy sur le train: 0.753
</pre></div>
</div>
<img alt="../_images/3_probabilities_calibration_17_1.png" src="../_images/3_probabilities_calibration_17_1.png" />
</div>
</div>
<p>Notons tout d‚Äôabord qu‚Äôun mod√®le qui pr√©dirait <span class="math notranslate nohighlight">\(50\%\)</span> (une sorte de mod√®le al√©atoire) pour chacun des points de notre jeu d‚Äôapprentissage aurait une <em>cross-entropy</em> de <span class="math notranslate nohighlight">\(0.7\)</span>. Notre mod√®le est donc pire, d‚Äôun point de vue des probabilit√©s, qu‚Äôun mod√®le al√©atoire.</p>
<div class="admonition-question admonition">
<p class="admonition-title">Question</p>
<p>Une forte <em>cross-entropy</em> en validation indique-t-elle un mauvais classifieur du point de vue de l‚Äôerreur <span class="math notranslate nohighlight">\(0/1\)</span>¬†?</p>
</div>
<p>Concernant notre courbe de calibration, le meilleur mod√®le (du point de vue de la calibration) est celui qui serait situ√© totalement sur la diagonal. C‚Äôest le mod√®le dont les probabilit√©s estim√©es correspond aux erreurs de pr√©dictions constat√©s empiriquement. On observe que les probabilit√©s estim√©es ne correspondent pas du tout √† ce qui est observ√© en pratique. Ainsi, lorsqu‚Äôon regarde les points pr√©dit √† <span class="math notranslate nohighlight">\(45\%\)</span> comme appartenant √† la classe <span class="math notranslate nohighlight">\(1\)</span>, ils sont en r√©alit√© √† <span class="math notranslate nohighlight">\(0\%\)</span> du c√¥t√© de la classe <span class="math notranslate nohighlight">\(1\)</span>. √Ä l‚Äôinverse, si je prends ceux qui sont √† <span class="math notranslate nohighlight">\(20\%\)</span> associ√©s √† la classe <span class="math notranslate nohighlight">\(1\)</span>, ils le sont r√©ellement <span class="math notranslate nohighlight">\(65\%\)</span> du temps. Nos probabilit√©s sont absolument impossibles √† interpr√©ter.</p>
<div class="admonition-question admonition">
<p class="admonition-title">Question</p>
<p>Un mod√®le situ√© parfaitement sur la diagonale de calibration est-il n√©cessairement un bon mod√®le pr√©dictif de classification¬†?</p>
</div>
<p>Notre strat√©gie va √™tre de trouver une ‚Äúadaptation‚Äù de notre mod√®le qui ne change en rien son pouvoir pr√©dictif mais qui calibre mieux les probabilit√©s. Observons tout d‚Äôabord notre fonction de lien.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/3_probabilities_calibration_19_0.png" src="../_images/3_probabilities_calibration_19_0.png" />
</div>
</div>
<p>Lorsque les valeurs des <em>logits</em> (i.e. <span class="math notranslate nohighlight">\(h(x)\)</span>) s‚Äô√©cartent de <span class="math notranslate nohighlight">\(0\)</span>, la probabilit√© estim√©e se rapproche des fortement de <span class="math notranslate nohighlight">\(0\)</span> ou de <span class="math notranslate nohighlight">\(1\)</span>. Ainsi, en r√©duisant l‚Äôamplitude des <em>logits</em>, nous rapprochons nos probabilit√©s estim√©es de <span class="math notranslate nohighlight">\(50\%\)</span>. Cela nous permet de r√©duire l‚Äôexc√®s de confiance de notre mod√®le. On constate de plus que cela ne change en rien le pouvoir pr√©dictif de notre mod√®le. Illustrons ce dernier point par un exemple.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span>
    <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
    <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="o">/</span><span class="mi">50</span><span class="p">)</span> <span class="c1"># on divise nos logits par 50</span>
<span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">&gt;</span><span class="mf">0.5</span><span class="p">)</span> <span class="c1"># on a la valeur True lorque c&#39;est la classe $1$ qui est pr√©dite</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[3.96615899e-11 3.82485275e-01]
 [9.99786832e-01 5.42165693e-01]
 [1.23220231e-16 3.24613217e-01]
 [1.59447060e-04 4.56392875e-01]
 [2.14210123e-14 3.47626623e-01]
 [1.42318632e-02 4.78822987e-01]
 [1.19047669e-07 4.20949984e-01]
 [1.49836228e-10 3.88783271e-01]
 [8.16885093e-08 4.19115100e-01]
 [1.00000000e+00 6.54963400e-01]]
[[False False]
 [ True  True]
 [False False]
 [False False]
 [False False]
 [False False]
 [False False]
 [False False]
 [False False]
 [ True  True]]
</pre></div>
</div>
</div>
</div>
<p>On remarque qu‚Äôind√©pendamment de l‚Äôamplitude de nos logits, les pr√©dictions restent les m√™mes. Cela se g√©n√©ralise bien s√ªr au cas multiclasse.</p>
<p>Nous devons donc estimer un facteur multiplicatif permettant d‚Äôobtenir une meilleure calibration (i.e. on veut √™tre sur la diagonale de calibration). Ce n‚Äôest en r√©alit√© pas suffisant car il suffit que le facteur multiplicatif soit aussi proche de <span class="math notranslate nohighlight">\(0\)</span> qu‚Äôon le souhtaite afin d‚Äô√™tre aussi proche de la diagonale que possible. La strat√©gie va √™tre d‚Äôoptimiser ce score multiplicatif afin que notre mod√®le soit bon sur un ensemble de validation. Ainsi, on ne change pas la forme de notre mod√®le pr√©dictif (la fronti√®re de d√©cision reste fixe), mais on alt√®re l‚Äôestimation des probabilit√©s afin d‚Äôobtenir un bon score sur un jeu de validation. Notre mod√®le devient donc¬†:</p>
<div class="math notranslate nohighlight">
\[\sigma(t h(x)),\]</div>
<p>o√π <span class="math notranslate nohighlight">\(t\in\mathbb{R}^+\)</span> est notre scalaire qu‚Äôon appelle ‚Äútemp√©rature‚Äù. L‚Äôid√©e va ensuite d‚Äôoptimiser notre mod√®le dont l‚Äôunique param√®tre est <span class="math notranslate nohighlight">\(t\)</span> pour <span class="math notranslate nohighlight">\(h\)</span> fix√© sur un jeu d‚Äôapprentissage avec la <em>cross-entropy</em>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="n">valset</span> <span class="o">=</span> <span class="n">SyntheticDataset</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)</span>
<span class="n">valloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valset</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p>Compl√©tez le code ci-dessous afin d‚Äôestimer le scalaire de temp√©rature.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">temperature</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="c1">####### Complete this part ######## or die ####################</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="c1">###############################################################</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Temperature:&#39;</span><span class="p">,</span> <span class="n">temperature</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">true_test</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">,</span> <span class="n">loss_test</span> <span class="o">=</span> <span class="n">loss_value</span><span class="p">(</span><span class="n">testloader</span><span class="p">,</span> <span class="n">temperature</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Cross-entropy sur le test: </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">loss_test</span><span class="p">)</span>
</pre></div>
</div>
<p>Notre mod√®le est donc meilleur qu‚Äôun mod√®le al√©atoire en terme d‚Äôestimation des probabilit√©s sur un jeu de test.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">calibration</span><span class="p">(</span><span class="n">true_test</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">)</span>
</pre></div>
</div>
<p>On observe √©galement que la calibration est bien meilleure !</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./6_deeplearning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="2_filters_representation.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Filtres et espace de repr√©sentation des r√©seaux de neurones ‚òïÔ∏è‚òïÔ∏è</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="4_regularization_deep.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">R√©gularisation en <em>deep learning</em> ‚òïÔ∏è‚òïÔ∏è</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Servajean, Leveau & Chailan<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Calibration des probabilités et quelques notions ☕️ &#8212; Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Régularisation en deep learning ☕️☕️" href="4_regularization_deep.html" />
    <link rel="prev" title="Filtres et espace de représentation des réseaux de neurones ☕️☕️" href="2_filters_representation.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-72WWYCKNK6"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-72WWYCKNK6');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../0_requisite/rappels_python.html">
   Rappels de Python et Numpy
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../1_what_is_ml/0_propos_liminaire.html">
   <em>
    Machine Learning
   </em>
   , initiation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/1_introduction_ml.html">
     <em>
      Machine learning
     </em>
     et malédiction de la dimension
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/2_regression_and_classification_trees.html">
     Les arbres de régression et de classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../2_regression/0_propos_liminaire.html">
   La
   <em>
    régression
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/1_linear_regression.html">
     La régression linéaire ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/2_optimization.html">
     L’optimisation ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/3_interpolation.html">
     Interpolation ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/4_algo_proximal_lasso.html">
     Sous-différentiel et le cas du Lasso ☕️☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/5_least_square_qr.html">
     Les moindres carrés via une décomposition QR (et plus)☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/6_ridge.html">
     Une analyse de la régularisation Ridge ☕️☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../3_classification/0_propos_liminaire.html">
   La
   <em>
    classification
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/1_logistic_regression.html">
     La régression logistique ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/2_fonctions_proxy.html">
     Les fonctions de perte (loss function) ☕️☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/3_bayes_classifier.html">
     Le classifieur de Bayes ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/4_VC_theory.html">
     Un modèle formel de l’apprentissage ☕️☕️☕️☕️ (💆‍♂️)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../4_kernel_methods/0_propos_liminaire.html">
   Les méthodes à noyaux
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../4_kernel_methods/1_svm.html">
     Le SVM ou l’hypothèse max-margin ☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5_ensembles/0_propos_liminaire.html">
   Les méthodes
   <em>
    ensemblistes
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/1_ensembles.html">
     Méthodes ensemblistes ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/2_bayesian_linear_regression.html">
     Bayesian linear regression ☕️☕️☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="0_propos_liminaire.html">
   <em>
    deep learning
   </em>
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="1_autodiff.html">
     La différentiation automatique et un début de
     <em>
      deep learning
     </em>
     ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="2_filters_representation.html">
     Filtres et espace de représentation des réseaux de neurones ☕️☕️
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Calibration des probabilités et quelques notions ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="4_regularization_deep.html">
     Régularisation en
     <em>
      deep learning
     </em>
     ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5_transfer_multitask.html">
     <em>
      Transfer learning
     </em>
     et apprentissage multi-tâches ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="6_adversarial.html">
     Les attaques adversaires ☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../7_unsupervised/0_propos_liminaire.html">
   L’apprentissage non-supervisé
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_unsupervised/1_principal_component_analysis.html">
     L’Analyse en Composantes Principales ☕️☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_unsupervised/2_em_gaussin_mixture_model.html">
     Modèle de Mélange Gaussien et algorithme
     <em>
      Expectation-Maximization
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../8_set_prediction/0_propos_liminaire.html">
   Prédiction d’ensembles
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/1_well_defined.html">
     Jeu d’apprentissage
     <em>
      set-valued
     </em>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/2_ill_defined.html">
     Jeu d’apprentissage uniquement multi-classes ☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../biblio.html">
   Bibliographie
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/6_deeplearning/3_probabilities_calibration.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/maximiliense/lmiprp"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/maximiliense/lmiprp/issues/new?title=Issue%20on%20page%20%2F6_deeplearning/3_probabilities_calibration.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/maximiliense/lmiprp/blob/master/Travaux Pratiques/Machine Learning/Book/6_deeplearning/3_probabilities_calibration.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#i-introduction">
   I. Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ii-estimation-de-la-probabilite-conditionnelle">
   II. Estimation de la probabilité conditionnelle
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-proper-loss-et-cross-entropy">
     A.
     <em>
      Proper loss
     </em>
     et
     <em>
      cross-entropy
     </em>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-consistence-de-la-cross-entropy">
     B. Consistence de la
     <em>
      cross-entropy
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iii-calibration-de-la-probabilite">
   III. Calibration de la probabilité
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="calibration-des-probabilites-et-quelques-notions">
<h1>Calibration des probabilités et quelques notions ☕️<a class="headerlink" href="#calibration-des-probabilites-et-quelques-notions" title="Permalink to this headline">¶</a></h1>
<div class="admonition-objectifs-de-la-sequence admonition">
<p class="admonition-title">Objectifs de la séquence</p>
<ul class="simple">
<li><p>Être sensibilisé :</p>
<ul>
<li><p>aux difficultés à estimer la loi conditionnelle <span class="math notranslate nohighlight">\(\eta_k(x)=\mathbb{P}(Y=k|X=x)\)</span>.</p></li>
</ul>
</li>
<li><p>Être capable de :</p>
<ul>
<li><p>de calibrer les probabilités d’un réseau de neurones pour améliorer le fit de <span class="math notranslate nohighlight">\(\eta\)</span>.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="i-introduction">
<h2>I. Introduction<a class="headerlink" href="#i-introduction" title="Permalink to this headline">¶</a></h2>
<p>Notez que par soucis de simplicité, nous considérons dans nos formules le cas de la classification binaire. Le propos se généralise bien sûr !</p>
<p>Considérons un problème de classification où <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> représente notre espace d’entrée et <span class="math notranslate nohighlight">\(\mathcal{Y}=\{0, 1\}\)</span> l’ensemble de nos classes (ici deux classes). Notons <span class="math notranslate nohighlight">\(X,Y\)</span> deux variables aléatoires sur <span class="math notranslate nohighlight">\(\mathcal{X}\times\mathcal{Y}\)</span> et notons <span class="math notranslate nohighlight">\(\mu\)</span> la mesure de <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(\eta(x)=\mathbb{P}(Y=1|X=x)\)</span> la “probabilité <em>a posteriori</em>”. Notre objectif est assez traditionnellement de trouver une application <span class="math notranslate nohighlight">\(h:\mathcal{X}\rightarrow\mathcal{Y}\)</span> telle que le risque suivant est minimisé :</p>
<div class="math notranslate nohighlight">
\[R(h)=\mathbb{E}\big[\textbf{1}\{h(X)\neq Y\}\big].\]</div>
<p>Ne connaissant ni <span class="math notranslate nohighlight">\(\mu\)</span> ni <span class="math notranslate nohighlight">\(\eta\)</span>, nous ne pouvons estimer ce risque et devons l’estimer empiriquement en collectant un jeu de données <span class="math notranslate nohighlight">\(S_n=\{(X_i, Y_i)\}_{i\leq n}\)</span>. Utilisant ce jeu de données, nous pouvons construire la notion de risque empirique :</p>
<div class="math notranslate nohighlight">
\[R_n(h)=\frac{1}{n}\sum_{i=1}^n \textbf{1}\{h(X_i)\neq Y_i\}.\]</div>
<p>La séquence sur les fonctions proxy nous a montré que minimiser ce risque est généralement difficile. Nous ne pouvons en particulier pas utiliser la descente de gradient puisque ce dernier est presque partout nul. Nous avons vu que nous pouvions cependant construire une stratégie où notre application ne retourne pas un label mais un score sur <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> (qu’on appelle logit en <em>deep learning</em> ou en régression logistique), score dont le signe nous permet de déterminer la classe. Une <em>loss</em> possédant un certain nombre de propriétés (e.g. convexe) était ensuite optimisée. Les propriétés de cette dernière nous permettait de conclure que le résultat ne sera pas trop mauvais. Un exemple de loss est la <em>logistic loss</em> définie comme :</p>
<div class="math notranslate nohighlight">
\[\text{log}\big(1+e^{-z}\big),\]</div>
<p>où <span class="math notranslate nohighlight">\(z\)</span> est justement le score retourné par notre modèle. Nous allons dans cette séquence une approche parallèle. Cette fois-ci, notre modèle ne donnera pas un score sur <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> mais sur <span class="math notranslate nohighlight">\([0, 1]\)</span> (la probabilité de la classe <span class="math notranslate nohighlight">\(1\)</span> dans le cas à deux classes). Nous avons vu dans la séquence sur les fonctions proxy que considérer un modèle qui retourne un score sur <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> suivi de la <em>logistic loss</em> était équivalent au même modèle dont on donne le score à une fonction de lien sigmoid (qui retourne une probabilité) et qu’on optimise avec l’inverse de la log-vraisemblance : la vision statistique ou <em>machine learning</em> sont en réalité les deux faces d’une même pièce.</p>
<p>L’approche <em>machine learning</em> nous permet de réfléchir avec une vision purement prédictive et performances de prédiction alors que l’approche statistiques nous donne cette information sur les probabilités. Cette dernière peut être importante par exemple si on veut seuiller, e.g. je ne retourne la classe <span class="math notranslate nohighlight">\(1\)</span> que si sa probabilité est supérieure à <span class="math notranslate nohighlight">\(99\%\)</span>. On verra que cette idée est clé lorsqu’on cherche à prédire des ensembles (cf. la séquence dédiée).</p>
<p>Nous allons ici voir comment estimer ses probabilités sur un jeu d’apprentissage puis comment corriger/calibrer ces dernières pour qu’elles soient le plus réalistes possibles.</p>
</div>
<div class="section" id="ii-estimation-de-la-probabilite-conditionnelle">
<h2>II. Estimation de la probabilité conditionnelle<a class="headerlink" href="#ii-estimation-de-la-probabilite-conditionnelle" title="Permalink to this headline">¶</a></h2>
<div class="section" id="a-proper-loss-et-cross-entropy">
<h3>A. <em>Proper loss</em> et <em>cross-entropy</em><a class="headerlink" href="#a-proper-loss-et-cross-entropy" title="Permalink to this headline">¶</a></h3>
<p>Nous allons ici prendre la perspective statistique, c’est-à-dire celle où on cherche à estimer la probabilité conditionnelle. Nous voulons construire un estimateur :</p>
<div class="math notranslate nohighlight">
\[\hat{\eta}(x)=\hat{\mathbb{P}}\big(Y=1|X=x\big),\]</div>
<p>où <span class="math notranslate nohighlight">\(\hat{\eta}\in\mathcal{F}\)</span> et <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> est notre ensemble de “probabilités conditionnelles” (e.g. les paramétrisations d’une régression logistique ou d’un réseau de neurones). Il existe de nombreuses stratégies afin d’atteindre cet objectif. L’une consiste à trouver une loss qui nous permettra de faire le meilleur choix de probabilité conditionnelle. La stratégie est souvent de maximiser la vraisemblance, ou, de manière totalement équivalente, de minimiser l’opposé de la log-vraisemblance :</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\hat{\eta})=-\sum_{i=1}^n Y_i\text{log}(\hat{\eta}(X_i))+(1-Y_i)\text{log}(1-\hat{\eta}(X_i)).\]</div>
<p>En espérance, nous notons :</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_\eta(\hat{\eta})=-\mathbb{E}_X\big[\eta(X)\text{log}(\hat{\eta}(X))+(1-\eta(X))\text{log}(1-\hat{\eta}(X))\big].\]</div>
<p>Est-ce une bonne stratégie. Finalement la première chose que notre <em>loss</em> doit satisfaire est la suivante. Si on lui donnait la vraie probabilité conditionnelle <span class="math notranslate nohighlight">\(\eta\)</span>, serait-elle minimisée ? Serait-ce l’unique minimiseur ? Les définitions suivantes formalisent ces idées.</p>
<hr class="docutils" />
<p><strong>Définition 1 (<em>Proper loss</em>)</strong></p>
<p>Une loss <span class="math notranslate nohighlight">\(\mathcal{L}_\eta\)</span> est dite <em>proper</em> si <span class="math notranslate nohighlight">\(\eta\)</span> est un minimiseur de cette dernière </p>
<div class="math notranslate nohighlight">
\[\forall \hat{\eta},\ \mathcal{L}_\eta(\hat{\eta})\geq \mathcal{L}_\eta(\eta).\]</div>
<hr class="docutils" />
<p><strong>Définition 2 (<em>Strictly proper loss</em>)</strong>
Une loss <span class="math notranslate nohighlight">\(\mathcal{L}_\eta\)</span> est dite <em>strictly proper</em> si <span class="math notranslate nohighlight">\(\eta\)</span> est l’unique minimiseur de cette dernière </p>
<div class="math notranslate nohighlight">
\[\forall \hat{\eta},\ \hat{\eta}\neq \eta,\ \mathcal{L}_\eta(\hat{\eta})&gt; \mathcal{L}_\eta(\eta).\]</div>
<hr class="docutils" />
<p>Il se trouve que la <em>cross-entropy</em> ou log-vraisemblance négative est <em>strictly proper</em>.</p>
</div>
<div class="section" id="b-consistence-de-la-cross-entropy">
<h3>B. Consistence de la <em>cross-entropy</em><a class="headerlink" href="#b-consistence-de-la-cross-entropy" title="Permalink to this headline">¶</a></h3>
<p><strong>Définition 3 (Plugin rule)</strong>
Étant donnée un estimateur des probabilités conditionnelles <span class="math notranslate nohighlight">\(\hat{\eta}\)</span>, la plugin rule est celle qui retourne la classe associée à la plus forte probabilité :</p>
<div class="math notranslate nohighlight">
\[\begin{split}g_{\hat{\eta}}(x)=\begin{cases}1\text{ si }\hat{\eta}(x)\geq 0.5\\ 0\text{ sinon.}\end{cases}\end{split}\]</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p>Soit <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> un ensemble d’estimateurs du vrai <span class="math notranslate nohighlight">\(\eta\)</span> (e.g. régression logistique) et <span class="math notranslate nohighlight">\(\mathcal{H}_{\mathcal{F}}\)</span> les classifieurs (i.e. plugin rule) associés. Le choix du bon estimateur <span class="math notranslate nohighlight">\(\hat{\eta}\in\mathcal{F}\)</span> se fait via la <em>cross entropy</em> qui est, rappelons le, <em>strictly proper</em>. Le classifieur associé est noté <span class="math notranslate nohighlight">\(g_{\hat{\eta}}\in\mathcal{H}_{\mathcal{F}}\)</span>. Notons <span class="math notranslate nohighlight">\(g^\star\in\mathcal{H}_{\mathcal{F}}\)</span> le minimiseur du risque (ou le minimiseur du risque empirique après avoir vu une infinité de données). Le choix de <span class="math notranslate nohighlight">\(\hat{\eta}\)</span> en minimisant la <em>cross entropy</em> sur une infinité de données garantit-il que le classifieur associé converge vers <span class="math notranslate nohighlight">\(g^\star\in\mathcal{H}_{\mathcal{F}}\)</span> ?</p>
</div>
<div class="hint dropdown admonition">
<p class="admonition-title">Indices</p>
<p>Considérer le scénario suivant. <span class="math notranslate nohighlight">\(\mathcal{X}=\{0, 1\}\)</span>, <span class="math notranslate nohighlight">\(\mu(0)=0.5\)</span>, <span class="math notranslate nohighlight">\(\mu(1)=0.5\)</span>, <span class="math notranslate nohighlight">\(\eta(0)=0.55\)</span> et <span class="math notranslate nohighlight">\(\eta(1)=0.45\)</span>. Considérez maintenant la classe d’estimateurs suivante <span class="math notranslate nohighlight">\(\mathcal{F}=\{\eta_1,\eta_2\}\)</span> où <span class="math notranslate nohighlight">\(\eta_1(0)=0.99\)</span> et <span class="math notranslate nohighlight">\(\eta_1(1)=0.01\)</span> ainsi que <span class="math notranslate nohighlight">\(\eta_2(0)=0.49\)</span> et <span class="math notranslate nohighlight">\(\eta_1(1)=0.51\)</span>. Calculez la <em>cross-entropy</em> et déduisez-en le meilleur estimateur de la probabilité conditionnelle. Ensuite calculez le minimiseur du risque (l’estimateur dont la plugin rule fera le moins d’erreurs).</p>
</div>
<hr class="docutils" />
<p>En réalité, la cross-entropy garantit que l’on obtienne la meilleure plugin-rule à partir du moment où on accepte certaines hypothèses. Ainsi, si le vrai <span class="math notranslate nohighlight">\(\eta\)</span> fait partie des paramétrisations possibles alors, ça sera le cas.</p>
</div>
</div>
<div class="section" id="iii-calibration-de-la-probabilite">
<h2>III. Calibration de la probabilité<a class="headerlink" href="#iii-calibration-de-la-probabilite" title="Permalink to this headline">¶</a></h2>
<p>Dans de nombreuses applications, il est nécessaire d’avoir une estimation “relativement correcte” de la probabilité conditionnelle <span class="math notranslate nohighlight">\(\eta\)</span>. Considérons le jeu de données synthétique suivant. Nous allons délibérément considérer un modèle avec une forte capacité de sur-apprentissage afin de mettre ces effets en avant dans des temps d’apprentissage raisonables.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">expit</span> <span class="k">as</span> <span class="n">sigmoid</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">15</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">3.</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">)])</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">logits</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Entraînons notre premier modèle sur ces données via un réseau de neurones sur <span class="math notranslate nohighlight">\(\texttt{pytorch}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;torch._C.Generator at 0x11bf31e90&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fullyconnected_layer</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">out_f</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    &quot; this function returns a fully connected layer with batchnorm</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">out_f</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">out_f</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
    <span class="p">)</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_labels</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_input</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">architecture</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,)):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        &quot; n_labels is the dimension of the output</span>
<span class="sd">        &quot; n_input is the dimension of the input</span>
<span class="sd">        &quot; architecture describes the series of hidden layers</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="n">layer_size</span> <span class="o">=</span> <span class="p">[</span><span class="n">n_input</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">architecture</span><span class="p">]</span>

        <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">fullyconnected_layer</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">out_f</span><span class="p">)</span> <span class="k">for</span> <span class="n">in_f</span><span class="p">,</span> <span class="n">out_f</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">layer_size</span><span class="p">,</span> <span class="n">layer_size</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
        <span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">architecture</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">n_labels</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SyntheticDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">label</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">label</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
    
<span class="n">dataset</span> <span class="o">=</span> <span class="n">SyntheticDataset</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># TODO Décommenter les prints de loss, etc.</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">train_and_plot</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">logs</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">architecture</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="mi">10</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)))</span>
    <span class="c1"># model.cuda()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
    
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="n">loss_history</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
            <span class="c1"># labels = labels.cuda()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
            <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">idx</span> <span class="o">%</span> <span class="n">logs</span> <span class="o">==</span> <span class="n">logs</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># print every 2000 mini-batches</span>
                <span class="c1"># TODO uncomment print(&#39;\r[%d, %5d] loss: %.3f&#39; % (e + 1, idx + 1, running_loss / logs), end=&quot;&quot;)</span>
                <span class="n">loss_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">running_loss</span> <span class="o">/</span> <span class="n">logs</span><span class="p">)</span>
                <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>

            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># on calcule le gradient</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span> <span class="c1"># on fait un pas d&#39;optimisation</span>
        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\r</span><span class="s1">************* Training done! *************&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss_history</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">train_and_plot</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">logs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>************* Training done! *************
</pre></div>
</div>
<img alt="../_images/3_probabilities_calibration_10_1.png" src="../_images/3_probabilities_calibration_10_1.png" />
</div>
</div>
<p>L’optimisation se passe bien. Notre <em>loss</em> décroît et se rapproche asymptotiquement de <span class="math notranslate nohighlight">\(0\)</span>. On observe également quelques fluctuations liées au fait qu’on optimise notre réseau par <em>batch</em> (i.e. SGD). Considérons les notations suivantes. Soit <span class="math notranslate nohighlight">\(h:\mathcal{X}\rightarrow\mathbb{R}\)</span> notre réseau de neurones dont la sortie sont appelés <em>logit</em>. Notons <span class="math notranslate nohighlight">\(\sigma(z)=(1+e^{-z})^{-1}\)</span> la fonction sigmoid. C’est la fonction de lien qui permet de transformer nos <em>logits</em> en probabilité. Nous avons donc un estimateur de la vraie probabilité conditionnelle :</p>
<div class="math notranslate nohighlight">
\[\hat{\eta}(x)=\sigma(h(x)).\]</div>
<p>Notre réseau est optimisé via la <em>cross-entropy</em> décrite plus haut :</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(h)=-\frac{1}{n}\sum_{i=1}^n y_i\log\big(\sigma(h(x_i))\big)+(1-y_i)\log\big(1-\sigma(h(x_i))\big).\]</div>
<div class="admonition-question admonition">
<p class="admonition-title">Question</p>
<p>Notre <em>loss</em> atteint quasiment <span class="math notranslate nohighlight">\(0\)</span>. Supposons qu’elle soit aussi proche de <span class="math notranslate nohighlight">\(0\)</span> que l’on veut. Que pouvons-nous dire sur l’erreur de classification <span class="math notranslate nohighlight">\(0/1\)</span> :</p>
<div class="math notranslate nohighlight">
\[Re(h)=\frac{1}{n}\sum_{i=1}^n\textbf{1}\{h(x_i)\neq y_i\},\]</div>
<p>relativement à la <em>plugin-rule</em> associée à <span class="math notranslate nohighlight">\(\sigma(h(x))\)</span> ?</p>
</div>
<p>Si notre <em>loss</em> est proche de <span class="math notranslate nohighlight">\(0\)</span>, alors les probabilités retournées par notre modèle sont soit très proches de <span class="math notranslate nohighlight">\(0\)</span> pour les points de notre jeu de données de la classe <span class="math notranslate nohighlight">\(0\)</span>, soit très proches de <span class="math notranslate nohighlight">\(1\)</span> pour les points associés à la classe <span class="math notranslate nohighlight">\(1\)</span>. Notre modèle indique donc une “forte confiance” dans ses prédictions tout en ne faisant que peu d’erreurs sur notre jeu d’apprentissage. Il existe une stratégie permettant de quantifier la qualité de la “confiance” d’un modèle : la courbe de calibration. Cette dernière va confronter les scores de prédiction de notre modèle à leur valeur empirique sur nos données (i.e. si notre modèle prédit <span class="math notranslate nohighlight">\(70\%\)</span>, on s’attend que <span class="math notranslate nohighlight">\(70\%\)</span> des points associés à ce score soient de la classe <span class="math notranslate nohighlight">\(1\)</span>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trainloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">loss_value</span><span class="p">(</span><span class="n">loader</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="n">running_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">true_labels</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">loader</span><span class="p">):</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
            <span class="n">true_labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">temperature</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span> <span class="o">*</span> <span class="n">temperature</span>
            <span class="n">pred</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
            <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">true_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">true_labels</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">running_loss</span>

<span class="n">true_train</span><span class="p">,</span> <span class="n">pred_train</span><span class="p">,</span> <span class="n">loss_train</span> <span class="o">=</span> <span class="n">loss_value</span><span class="p">(</span><span class="n">trainloader</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Cross-entropy sur le train: </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">loss_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cross-entropy sur le train: 0.014
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.calibration</span> <span class="kn">import</span> <span class="n">calibration_curve</span>
<span class="k">def</span> <span class="nf">calibration</span><span class="p">(</span><span class="n">true_value</span><span class="p">,</span> <span class="n">pred_value</span><span class="p">):</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;k:&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Perfectly calibrated&quot;</span><span class="p">)</span>

    <span class="n">fraction_of_positives</span><span class="p">,</span> <span class="n">mean_predicted_value</span> <span class="o">=</span> \
        <span class="n">calibration_curve</span><span class="p">(</span><span class="n">true_value</span><span class="p">,</span> <span class="n">pred_value</span><span class="p">,</span> <span class="n">n_bins</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mean_predicted_value</span><span class="p">,</span> <span class="n">fraction_of_positives</span><span class="p">,</span> <span class="s2">&quot;s-&quot;</span><span class="p">,</span>
             <span class="n">label</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="s1">&#39;Deep net&#39;</span><span class="p">,</span> <span class="p">))</span>

    <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Fraction of positives&quot;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Mean predicted value&quot;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">])</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower right&quot;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Calibration plots&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    
<span class="n">calibration</span><span class="p">(</span><span class="n">true_train</span><span class="p">,</span> <span class="n">pred_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/3_probabilities_calibration_14_0.png" src="../_images/3_probabilities_calibration_14_0.png" />
</div>
</div>
<p>Ce résultat était attendu. Notre modèle est en surapprentissage : il ne se trompe pas sur le jeu d’apprentissage et prédit des scores indiquant une forte confiance. Nous devons évaluer la calibration <strong>et</strong> la <em>cross-entropy</em> sur un jeu de test afin d’avoir une idée de la qualité des prédictions de notre modèle.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="n">testset</span> <span class="o">=</span> <span class="n">SyntheticDataset</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="n">testloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">testset</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">true_test</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">,</span> <span class="n">loss_test</span> <span class="o">=</span> <span class="n">loss_value</span><span class="p">(</span><span class="n">testloader</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Cross-entropy sur le train: </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">loss_test</span><span class="p">)</span>
<span class="n">calibration</span><span class="p">(</span><span class="n">true_test</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cross-entropy sur le train: 0.753
</pre></div>
</div>
<img alt="../_images/3_probabilities_calibration_17_1.png" src="../_images/3_probabilities_calibration_17_1.png" />
</div>
</div>
<p>Notons tout d’abord qu’un modèle qui prédirait <span class="math notranslate nohighlight">\(50\%\)</span> (une sorte de modèle aléatoire) pour chacun des points de notre jeu d’apprentissage aurait une <em>cross-entropy</em> de <span class="math notranslate nohighlight">\(0.7\)</span>. Notre modèle est donc pire, d’un point de vue des probabilités, qu’un modèle aléatoire.</p>
<div class="admonition-question admonition">
<p class="admonition-title">Question</p>
<p>Une forte <em>cross-entropy</em> en validation indique-t-elle un mauvais classifieur du point de vue de l’erreur <span class="math notranslate nohighlight">\(0/1\)</span> ?</p>
</div>
<p>Concernant notre courbe de calibration, le meilleur modèle (du point de vue de la calibration) est celui qui serait situé totalement sur la diagonal. C’est le modèle dont les probabilités estimées correspond aux erreurs de prédictions constatés empiriquement. On observe que les probabilités estimées ne correspondent pas du tout à ce qui est observé en pratique. Ainsi, lorsqu’on regarde les points prédit à <span class="math notranslate nohighlight">\(45\%\)</span> comme appartenant à la classe <span class="math notranslate nohighlight">\(1\)</span>, ils sont en réalité à <span class="math notranslate nohighlight">\(0\%\)</span> du côté de la classe <span class="math notranslate nohighlight">\(1\)</span>. À l’inverse, si je prends ceux qui sont à <span class="math notranslate nohighlight">\(20\%\)</span> associés à la classe <span class="math notranslate nohighlight">\(1\)</span>, ils le sont réellement <span class="math notranslate nohighlight">\(65\%\)</span> du temps. Nos probabilités sont absolument impossibles à interpréter.</p>
<div class="admonition-question admonition">
<p class="admonition-title">Question</p>
<p>Un modèle situé parfaitement sur la diagonale de calibration est-il nécessairement un bon modèle prédictif de classification ?</p>
</div>
<p>Notre stratégie va être de trouver une “adaptation” de notre modèle qui ne change en rien son pouvoir prédictif mais qui calibre mieux les probabilités. Observons tout d’abord notre fonction de lien.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/3_probabilities_calibration_19_0.png" src="../_images/3_probabilities_calibration_19_0.png" />
</div>
</div>
<p>Lorsque les valeurs des <em>logits</em> (i.e. <span class="math notranslate nohighlight">\(h(x)\)</span>) s’écartent de <span class="math notranslate nohighlight">\(0\)</span>, la probabilité estimée se rapproche des fortement de <span class="math notranslate nohighlight">\(0\)</span> ou de <span class="math notranslate nohighlight">\(1\)</span>. Ainsi, en réduisant l’amplitude des <em>logits</em>, nous rapprochons nos probabilités estimées de <span class="math notranslate nohighlight">\(50\%\)</span>. Cela nous permet de réduire l’excès de confiance de notre modèle. On constate de plus que cela ne change en rien le pouvoir prédictif de notre modèle. Illustrons ce dernier point par un exemple.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span>
    <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
    <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="o">/</span><span class="mi">50</span><span class="p">)</span> <span class="c1"># on divise nos logits par 50</span>
<span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">&gt;</span><span class="mf">0.5</span><span class="p">)</span> <span class="c1"># on a la valeur True lorque c&#39;est la classe $1$ qui est prédite</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[3.96615899e-11 3.82485275e-01]
 [9.99786832e-01 5.42165693e-01]
 [1.23220231e-16 3.24613217e-01]
 [1.59447060e-04 4.56392875e-01]
 [2.14210123e-14 3.47626623e-01]
 [1.42318632e-02 4.78822987e-01]
 [1.19047669e-07 4.20949984e-01]
 [1.49836228e-10 3.88783271e-01]
 [8.16885093e-08 4.19115100e-01]
 [1.00000000e+00 6.54963400e-01]]
[[False False]
 [ True  True]
 [False False]
 [False False]
 [False False]
 [False False]
 [False False]
 [False False]
 [False False]
 [ True  True]]
</pre></div>
</div>
</div>
</div>
<p>On remarque qu’indépendamment de l’amplitude de nos logits, les prédictions restent les mêmes. Cela se généralise bien sûr au cas multiclasse.</p>
<p>Nous devons donc estimer un facteur multiplicatif permettant d’obtenir une meilleure calibration (i.e. on veut être sur la diagonale de calibration). Ce n’est en réalité pas suffisant car il suffit que le facteur multiplicatif soit aussi proche de <span class="math notranslate nohighlight">\(0\)</span> qu’on le souhtaite afin d’être aussi proche de la diagonale que possible. La stratégie va être d’optimiser ce score multiplicatif afin que notre modèle soit bon sur un ensemble de validation. Ainsi, on ne change pas la forme de notre modèle prédictif (la frontière de décision reste fixe), mais on altère l’estimation des probabilités afin d’obtenir un bon score sur un jeu de validation. Notre modèle devient donc :</p>
<div class="math notranslate nohighlight">
\[\sigma(t h(x)),\]</div>
<p>où <span class="math notranslate nohighlight">\(t\in\mathbb{R}^+\)</span> est notre scalaire qu’on appelle “température”. L’idée va ensuite d’optimiser notre modèle dont l’unique paramètre est <span class="math notranslate nohighlight">\(t\)</span> pour <span class="math notranslate nohighlight">\(h\)</span> fixé sur un jeu d’apprentissage avec la <em>cross-entropy</em>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="n">valset</span> <span class="o">=</span> <span class="n">SyntheticDataset</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)</span>
<span class="n">valloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valset</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p>Complétez le code ci-dessous afin d’estimer le scalaire de température.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">temperature</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="c1">####### Complete this part ######## or die ####################</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="c1">###############################################################</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Temperature:&#39;</span><span class="p">,</span> <span class="n">temperature</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">true_test</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">,</span> <span class="n">loss_test</span> <span class="o">=</span> <span class="n">loss_value</span><span class="p">(</span><span class="n">testloader</span><span class="p">,</span> <span class="n">temperature</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Cross-entropy sur le test: </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">loss_test</span><span class="p">)</span>
</pre></div>
</div>
<p>Notre modèle est donc meilleur qu’un modèle aléatoire en terme d’estimation des probabilités sur un jeu de test.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">calibration</span><span class="p">(</span><span class="n">true_test</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">)</span>
</pre></div>
</div>
<p>On observe également que la calibration est bien meilleure !</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./6_deeplearning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="2_filters_representation.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Filtres et espace de représentation des réseaux de neurones ☕️☕️</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="4_regularization_deep.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Régularisation en <em>deep learning</em> ☕️☕️</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Servajean, Leveau & Chailan<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>
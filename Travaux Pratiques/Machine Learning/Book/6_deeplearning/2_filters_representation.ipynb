{"cells": [{"cell_type": "markdown", "id": "liked-monaco", "metadata": {}, "source": ["# Filtres et espace de repr\u00e9sentation des r\u00e9seaux de neurones \u2615\ufe0f\u2615\ufe0f"]}, {"cell_type": "markdown", "id": "ongoing-section", "metadata": {}, "source": ["**<span style='color:blue'> Objectifs de la s\u00e9quence</span>** ", "\n", "* \u00catre sensibilis\u00e9&nbsp;:\n", "    * principe d'espace de repr\u00e9sentation en *deep learning*.\n", "* \u00catre capable de&nbsp;:\n", "    * de visualiser les convolutions d'un r\u00e9seau de neurones,\n", "    * de visualiser l'espace de repr\u00e9sentation d'un r\u00e9seau de neurones,\n", "    * de manipuler les librairies $\\texttt{pytorch}$, $\\texttt{plotly}$, $\\texttt{umap}$ et $\\texttt{bokeh}$.\n", "    \n", "    \n", "\n\n ----", "\n", "Les exercices de cette session seront r\u00e9alis\u00e9s deux fois. Une fois sur le jeu de donn\u00e9es CIFAR10 et une fois sur un jeu de donn\u00e9es repr\u00e9sentant les personnages des Simpsons."]}, {"cell_type": "markdown", "id": "swedish-longer", "metadata": {}, "source": ["## Imports "]}, {"cell_type": "code", "execution_count": null, "id": "recreational-basic", "metadata": {}, "outputs": [], "source": ["# Pour Google Colaboratory\n", "# D\u00e9commenter la ligne suivante\n", "# !pip install umap-learn\n", "# monter le jeu de donn\u00e9es simpsons en local"]}, {"cell_type": "code", "execution_count": null, "id": "annoying-meter", "metadata": {}, "outputs": [], "source": ["import torch\n", "import torchvision\n", "import torch.nn as nn\n", "import torch.optim as optim\n", "import torch.nn.functional as F\n", "from torch.optim.lr_scheduler import MultiStepLR\n", "from torchvision import datasets, models\n", "from torch.utils.data import DataLoader\n", "import torchvision.transforms as transforms\n", "from torch.utils.data.sampler import SubsetRandomSampler\n", "\n", "import umap\n", "import plotly.express as px\n", "\n", "import numpy as np\n", "\n", "import copy"]}, {"cell_type": "markdown", "id": "stuck-bubble", "metadata": {}, "source": ["## Configuration"]}, {"cell_type": "code", "execution_count": null, "id": "orange-oregon", "metadata": {}, "outputs": [], "source": ["import os\n", "# Each itereates over the dataloader define with batch size will get a batch of batch size samples\n", "# After the iterator has gone through every data sample (one epoch), then we shuffle the order and we go again.\n", "batch_size = 128\n", "num_workers = 2"]}, {"cell_type": "markdown", "id": "bright-afternoon", "metadata": {}, "source": ["## I. Construction du jeu de donn\u00e9es CIFAR10"]}, {"cell_type": "markdown", "id": "selected-season", "metadata": {}, "source": ["Le $\\texttt{dataset}$ est une sorte de tableau qui contient les \u00e9l\u00e9ments de notre jeu de donn\u00e9es. Le $\\texttt{dataloader}$ est l'objet qui nous permettra d'acc\u00e9der \u00e0 nos donn\u00e9es via des batchs al\u00e9atoires. Rappelons que le calcul du gradient se fait sur les donn\u00e9es. Cependant avec des fonctions aussi complexes qu'un r\u00e9seau de neurones et avec des jeu de donn\u00e9es aussi gros, il devient n\u00e9cessaire de n'estimer le gradient que sur une partie de ces donn\u00e9es.\n", "\n", "L'objet $\\texttt{transform}$ permettra de normaliser les donn\u00e9es qui seront donn\u00e9es \u00e0 notre mod\u00e8le. En $\\texttt{pytorch}$, les donn\u00e9es sont g\u00e9r\u00e9es par un *data loader*. En effet, on ne traite que tr\u00e8s rarement tout le jeu de donn\u00e9es d'un coup. On estime plut\u00f4t le gradient via un *batch* de donn\u00e9es. De meilleurs r\u00e9sultats sont g\u00e9n\u00e9ralement observ\u00e9s lorsque le jeu de donn\u00e9es est m\u00e9lang\u00e9 entre chaque it\u00e9ration d'optimisation. \n", "\n", "Les parties qui commencent par un **[\u2022] M\u00e9thode de....** sont celles qu'il faudra r\u00e9utiliser plus tard (plusieurs fois)."]}, {"cell_type": "markdown", "id": "specified-spouse", "metadata": {}, "source": ["### [\u2022] M\u00e9thode de split"]}, {"cell_type": "code", "execution_count": null, "id": "insured-contrast", "metadata": {}, "outputs": [], "source": ["def split_dataset(dataset, valid_size = 0.0, random_state=42):\n", "    dataset_size = len(dataset)\n", "    indices = list(range(dataset_size))\n", "    split = int(np.floor(valid_size * dataset_size))\n", "\n", "    np.random.seed(random_state)\n", "    np.random.shuffle(indices)\n", "\n", "    train_idx, valid_idx = indices[split:], indices[:split]\n", "    train_sampler = SubsetRandomSampler(train_idx)\n", "    valid_sampler = SubsetRandomSampler(valid_idx)\n", "\n", "    return dataset, copy.deepcopy(dataset), train_sampler, valid_sampler"]}, {"cell_type": "markdown", "id": "connected-relationship", "metadata": {}, "source": ["### Construction du jeu de donn\u00e9es"]}, {"cell_type": "code", "execution_count": null, "id": "premier-dragon", "metadata": {}, "outputs": [], "source": ["# label names\n", "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n", "\n", "transform = transforms.Compose(\n", "  [\n", "      transforms.ToTensor(),\n", "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n", "  ]\n", ")\n", "\n", "#root_directory where images are.\n", "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n", "\n", "trainset, validset, train_sampler, valid_sampler = split_dataset(trainset, valid_size = 0.2, random_state=None)\n", "\n", "trainloader = DataLoader(\n", "  trainset, batch_size=batch_size, sampler=train_sampler,\n", "  num_workers=num_workers,\n", ")\n", "\n", "validloader = DataLoader(\n", "  validset, batch_size=batch_size, sampler=valid_sampler,\n", "  num_workers=num_workers,\n", ")\n", "\n", "print(len(trainloader), len(validloader))\n", "\n", "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n", "testloader = DataLoader(\n", "  testset, batch_size=batch_size, shuffle=True,\n", "  num_workers=num_workers,\n", ")\n", "\n", "print('Nb test batchs:', len(testloader))"]}, {"cell_type": "markdown", "id": "electronic-pointer", "metadata": {}, "source": ["### Visualisation des donn\u00e9es"]}, {"cell_type": "markdown", "id": "diagnostic-motion", "metadata": {}, "source": ["####  [\u2022] M\u00e9thode de visualisation"]}, {"cell_type": "code", "execution_count": null, "id": "compact-situation", "metadata": {}, "outputs": [], "source": ["#### Visualisation d'images du jeu de donn\u00e9es\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "\n", "def imshow(images, labels, predicted=None):\n", "    plt.figure(figsize=(15, 10))\n", "    for idx in range(8):\n", "        plt.subplot(2, 4, idx+1)\n", "        plt.axis('off')\n", "        img = (images[idx] * 0.224 + 0.456)#/ 2 + 0.5  # unnormalize\n", "        npimg = img.numpy()\n", "        plt.axis('off')\n", "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n", "        title = str(classes[labels[idx]]) + \\\n", "        ('' if predicted is None else ' - ' + str(classes[predicted[idx]]))\n", "        plt.title(title)\n", "        \n", "    plt.show()"]}, {"cell_type": "markdown", "id": "handed-president", "metadata": {}, "source": ["#### Visualisation "]}, {"cell_type": "code", "execution_count": null, "id": "alien-annual", "metadata": {}, "outputs": [], "source": ["# get some random training images\n", "\n", "dataiter = iter(testloader)\n", "images, labels = dataiter.next()\n", "\n", "# show images\n", "imshow(images[:8], labels[:8])"]}, {"cell_type": "markdown", "id": "convertible-pierce", "metadata": {}, "source": ["## II. Construction du mod\u00e8le"]}, {"cell_type": "code", "execution_count": null, "id": "premier-colombia", "metadata": {}, "outputs": [], "source": ["class Net(nn.Module):\n", "    def __init__(self):\n", "        super(Net, self).__init__()\n", "        self.conv1 = nn.Conv2d(3, 16, 5)\n", "        self.pool = nn.MaxPool2d(2, 2)\n", "        self.conv2 = nn.Conv2d(16, 32, 5)\n", "        self.fc1 = nn.Linear(32 * 5 * 5, 120)\n", "        self.fc2 = nn.Linear(120, 84)\n", "        self.fc = nn.Linear(84, 10)\n", "\n", "    def forward(self, x):\n", "        x = self.pool(F.relu(self.conv1(x)))\n", "        x = self.pool(F.relu(self.conv2(x)))\n", "        x = x.view(-1, 32 * 5 * 5)\n", "        x = F.relu(self.fc1(x))\n", "        x = F.relu(self.fc2(x))\n", "        x = self.fc(x)\n", "        return x"]}, {"cell_type": "code", "execution_count": null, "id": "developing-calvin", "metadata": {}, "outputs": [], "source": ["model = Net()\n", "# model = model.cuda()"]}, {"cell_type": "markdown", "id": "loving-regression", "metadata": {}, "source": ["## III. Visualisation des filtres/param\u00e8tres du mod\u00e8le \u00e0 l'initialisation"]}, {"cell_type": "markdown", "id": "committed-pacific", "metadata": {}, "source": ["### [\u2022] M\u00e9thode de visualisation des filtres"]}, {"cell_type": "code", "execution_count": null, "id": "aware-elizabeth", "metadata": {}, "outputs": [], "source": ["def visualize_filters(tensor, ch=0, allkernels=False, nrow=8, padding=1): \n", "    n,c,w,h = tensor.shape\n", "\n", "    if allkernels: tensor = tensor.view(n*c, -1, w, h)\n", "    elif c != 3: tensor = tensor[:,ch,:,:].unsqueeze(dim=1)\n", "\n", "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n", "    grid = torchvision.utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n", "    plt.figure( figsize=(nrow,rows) )\n", "    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n", "    plt.axis('off')\n", "    plt.ioff()\n", "    plt.show()"]}, {"cell_type": "markdown", "id": "thermal-support", "metadata": {}, "source": ["### Visualisation des filtres"]}, {"cell_type": "code", "execution_count": null, "id": "operating-gross", "metadata": {}, "outputs": [], "source": ["filters = model.conv1.weight.data.clone().cpu()\n", "visualize_filters(filters, ch=0, allkernels=False)"]}, {"cell_type": "markdown", "id": "attended-alert", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Que pouvez-vous dire de ces filtres ?**\n", "\n", "\n\n ----"]}, {"cell_type": "markdown", "id": "raised-sharp", "metadata": {}, "source": ["## IV. L'apprentissage"]}, {"cell_type": "markdown", "id": "brown-matrix", "metadata": {}, "source": ["### Fonction objectif, scheduler et optimizer"]}, {"cell_type": "code", "execution_count": null, "id": "tight-living", "metadata": {}, "outputs": [], "source": ["#Choose the loss function\n", "criterion = nn.CrossEntropyLoss()\n", "\n", "#Optimizer\n", "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n", "scheduler = MultiStepLR(optimizer, milestones=[25, 50], gamma=0.1)"]}, {"cell_type": "markdown", "id": "rental-wings", "metadata": {}, "source": ["### [\u2022] M\u00e9thodes d'\u00e9valuation"]}, {"cell_type": "code", "execution_count": null, "id": "electoral-sustainability", "metadata": {}, "outputs": [], "source": ["import cv2\n", "\n", "# Let us code a generic prediction function\n", "def predict(model, loader, criterion=nn.CrossEntropyLoss(), feature_extract=False, max_size=0, resize=128):\n", "    with torch.no_grad():\n", "        if not feature_extract:\n", "            model.eval()\n", "\n", "        y_preds = []\n", "        y_labels = []\n", "        inputs_ = []\n", "\n", "        running_loss = 0.0\n", "        size = 0.0\n", "        for idx, data in enumerate(loader):\n", "            inputs, labels = data\n", "            # inputs = inputs.cuda()\n", "            # labels = labels.cuda()\n", "\n", "            # wrap them in Variable\n", "            outputs = model(inputs)\n", "            loss = criterion(outputs, labels)\n", "            running_loss += loss.item()\n", "\n", "            y_preds.extend(outputs.data.tolist())\n", "            y_labels.extend(labels.data.tolist())\n", "            if size <= max_size and feature_extract:\n", "                images = [\n", "                    cv2.resize(\n", "                        (\n", "                            (i*0.224+0.456)*255).astype('uint8').transpose((1, 2, 0)), dsize=(resize, resize)\n", "                    ) for i in inputs.data.cpu().numpy()\n", "                ]\n", "                inputs_.extend(images)\n", "                size = len(inputs_)\n", "\n", "        predictions, labels, inputs = np.asarray(y_preds), np.asarray(y_labels), np.asarray(inputs_)\n", "\n", "    if not feature_extract:\n", "        return predictions, labels, running_loss/len(loader)\n", "\n", "    return predictions, labels, inputs\n", "\n", "\n", "def accuracy_topk(predictions, labels, top_k=1):\n", "        res = 0\n", "        for i, pred in enumerate(predictions):\n", "            answer = np.argsort(-pred)[0:top_k]\n", "            if labels[i] in answer:\n", "                res += 1\n", "        acc = float(res) / float(labels.shape[0])\n", "        return acc\n", "\n", "    \n", "def evaluate(loader, model, top_k = 1, criterion = nn.CrossEntropyLoss()):\n", "    predictions, labels, loss = predict(model, loader, criterion)\n", "    return accuracy_topk(predictions, labels, top_k = top_k), loss"]}, {"cell_type": "markdown", "id": "different-moore", "metadata": {}, "source": ["### [\u2022] M\u00e9thode d'apprentissage (i.e. d'optimisation)"]}, {"cell_type": "markdown", "id": "described-likelihood", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Proposez le code en utilisant une fonction (pour pouvoir r\u00e9utiliser le code plus tard) permettant d'optimiser votre r\u00e9seau pendant deux *epochs*.**\n", "\n", "**Attention, votre code doit renvoyer 4 tableaux : l'historique de la loss de train, l'historique la loss de validation, l'historique de l'accuracy de train et de l'accuracy de test.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "id": "d1085dd7", "metadata": {}, "source": ["####### Complete this part ######## or die ####################\n", "def train(model, criterion, optimizer, scheduler, n_epoch=2):\n", "    loss_history = []\n", "    valid_loss_history = []\n", "\n", "    acc_history = []\n", "    val_acc_history = []\n", "\n", "    for epoch in range(n_epoch):  # loop over the dataset multiple times\n", "        ...\n", "        ...\n", "\n", "    print('**** Finished Training ****')\n", "    return loss_history, valid_loss_history, acc_history, val_acc_history\n", "###############################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "decent-ensemble", "metadata": {}, "source": ["### L'entra\u00eenement"]}, {"cell_type": "code", "id": "f9e40f8e", "metadata": {}, "source": ["eval_frequency=1\n", "\n", "loss_history, \\\n", "valid_loss_history, \\\n", "acc_history, \\\n", "val_acc_history = train(model, criterion, optimizer, scheduler, n_epoch=2)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "pressed-territory", "metadata": {}, "source": ["### Affichage des courbes de loss et de pr\u00e9cision"]}, {"cell_type": "markdown", "id": "initial-precipitation", "metadata": {}, "source": ["#### [\u2022] M\u00e9thode d'affichage des courbes"]}, {"cell_type": "code", "execution_count": null, "id": "stopped-retro", "metadata": {}, "outputs": [], "source": ["def plot_loss(loss_history, valid_loss_history, acc_history, val_acc_history):\n", "    plt.figure()\n", "    plt.plot([i*eval_frequency for i in range(1, len(loss_history)+1)], loss_history, \n", "             label='Train loss')\n", "    plt.plot([i*eval_frequency for i in range(1, len(loss_history)+1)], valid_loss_history, \n", "             label='Validation loss')\n", "    plt.legend()\n", "    plt.show()\n", "\n", "    plt.figure()\n", "    plt.plot([i*eval_frequency for i in range(1, len(acc_history)+1)], acc_history, \n", "             label='Train Accuracy')\n", "    plt.plot([i*eval_frequency for i in range(1, len(acc_history)+1)], val_acc_history, \n", "             label='Validation Accuracy')\n", "    plt.legend()\n", "    plt.show()"]}, {"cell_type": "markdown", "id": "czech-science", "metadata": {}, "source": ["#### Affichage des courbes"]}, {"cell_type": "code", "id": "519b5a52", "metadata": {}, "source": ["plot_loss(loss_history, valid_loss_history, acc_history, val_acc_history)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "supported-functionality", "metadata": {}, "source": ["**<span style='color:blue'> Question</span>** ", "\n", "**Que pouvez-vous conclure en regardant la loss et l'accuracy ?**\n", "\n", "\n\n ----"]}, {"cell_type": "markdown", "id": "documented-theme", "metadata": {}, "source": ["### Sauvegarde et chargement du mod\u00e8le"]}, {"cell_type": "markdown", "id": "fewer-unknown", "metadata": {}, "source": ["En *deep learning* l'apprentissage d'un mod\u00e8le peut prendre \u00e9norm\u00e9ment de temps. Pensez \u00e0 toujours sauvegarder votre mod\u00e8le r\u00e9guli\u00e8rement afin de ne pas le perdre. (Attention, il faut parfois aussi sauvegarder les variables li\u00e9es \u00e0 l'optimiseur lui-m\u00eame...)"]}, {"cell_type": "markdown", "id": "suitable-taxation", "metadata": {}, "source": ["#### Sauvegarde"]}, {"cell_type": "code", "execution_count": null, "id": "scientific-damage", "metadata": {}, "outputs": [], "source": ["torch.save(model.state_dict(), 'my_model.torch')"]}, {"cell_type": "markdown", "id": "empirical-youth", "metadata": {}, "source": ["#### Chargement"]}, {"cell_type": "code", "execution_count": null, "id": "theoretical-kennedy", "metadata": {}, "outputs": [], "source": ["model = Net()\n", "model.load_state_dict(torch.load('my_model.torch'))\n", "# model = model.cuda()"]}, {"cell_type": "markdown", "id": "normal-identification", "metadata": {}, "source": ["## V. Visualisation des filtres/param\u00e8tres appris"]}, {"cell_type": "code", "execution_count": null, "id": "union-expense", "metadata": {}, "outputs": [], "source": ["filters = model.conv1.weight.data.clone().cpu()\n", "visualize_filters(filters, ch=0, allkernels=False)"]}, {"cell_type": "markdown", "id": "alive-prison", "metadata": {}, "source": ["**<span style='color:blue'> Question</span>** ", "\n", "**Que conclure en regardant ces filtres relativement aux filtres avant l'entra\u00eenement du mod\u00e8le ?**\n", "\n", "\n\n ----"]}, {"cell_type": "markdown", "id": "available-machine", "metadata": {}, "source": ["## VI. \u00c9valuasion du mod\u00e8le sur l'ensemble de test"]}, {"cell_type": "code", "execution_count": null, "id": "headed-minority", "metadata": {}, "outputs": [], "source": ["accuracy, _ = evaluate(testloader, model)\n", "print('Test accuracy: %.3f' % (accuracy), end='\\n')"]}, {"cell_type": "markdown", "id": "unusual-region", "metadata": {}, "source": ["**<span style='color:blue'> Question</span>** ", "\n", "**Que dire de l'accuracy ? Quel est le score attendu d'un mod\u00e8le al\u00e9atoire ?**\n", "\n", "\n\n ----"]}, {"cell_type": "markdown", "id": "experienced-cigarette", "metadata": {}, "source": ["## VII. Test des pr\u00e9dictions sur quelques images"]}, {"cell_type": "code", "execution_count": null, "id": "rough-uncle", "metadata": {}, "outputs": [], "source": ["#Test prediction on some images\n", "dataiter = iter(testloader)\n", "images, labels = dataiter.next()\n", "outputs = model(images[:8])#  .to(device))  # we use the loaded model\n", "_, predicted = torch.max(outputs, 1)\n", "\n", "imshow(images[:8], labels[:8], predicted[:8])"]}, {"cell_type": "markdown", "id": "impressed-berlin", "metadata": {}, "source": ["Tester son mod\u00e8le sur quelques images peut \u00eatre int\u00e9ressant lorsqu'il s'agit de comprendre le type d'erreurs qui sont faites. \u00c7a ne peut JAMAIS \u00eatre un argument suffisant pour dire que le mod\u00e8le \"marche\" !"]}, {"cell_type": "markdown", "id": "quick-romantic", "metadata": {}, "source": ["## VIII. Extraction des features et Dataviz"]}, {"cell_type": "markdown", "id": "false-chambers", "metadata": {}, "source": ["Rappelons nous qu'une r\u00e9seau de neurones est la composition d'une premi\u00e8re fonction $\\phi:\\mathbb{R}^p\\mapsto\\mathbb{R}^f$ qui apprend un *feature space* et d'un classifieur lin\u00e9aire $\\psi:\\mathbb{R}^f\\mapsto\\mathbb{R}^C$ qui retourne un score pour chacune des classes de notre probl\u00e8mes \u00e0 $C$ classes. Il est int\u00e9ressant d'\u00e9tudier la mani\u00e8re dont la fonction $\\phi$ a d\u00e9form\u00e9 l'espace d'entr\u00e9e en regroupant certaines images entre elles, etc. Notons que la fonction $\\phi$ est elle-m\u00eame une composition et qu'il est possible d'\u00e9tudier les sorties des diff\u00e9rentes couches.\n", "\n", "Visualiser la sortie de la fonction $\\phi$ n'est pas directement possible puisqe l'espace poss\u00e8de $f$ dimensions et que $f$ est g\u00e9n\u00e9ralement tr\u00e8s loin devant $2$ et $3$. Il convient donc d'utiliser un algorithme de r\u00e9duction de dimension. Ces algorithmes fonctionnent tr\u00e8s bien sur la sortie de la fonction $\\phi$ car la \"dimension effective\" de nos donn\u00e9es s'y retrouvent tr\u00e8s r\u00e9duites : les images similaires se retrouvent tr\u00e8s proches les une des autres et tr\u00e8s diff\u00e9rentes des autres images, etc."]}, {"cell_type": "code", "execution_count": null, "id": "cellular-travel", "metadata": {}, "outputs": [], "source": ["print(model)"]}, {"cell_type": "markdown", "id": "occupied-reception", "metadata": {}, "source": ["Dans un mod\u00e8le $\\texttt{Pytorch}$, la coutume est d'appeler $\\texttt{fc}$ la fonction $\\psi$. Si nous souhaitons r\u00e9cup\u00e9rer la sortie de la fonction $\\phi$ il suffit de remplacer $\\texttt{fc}$ par la fonction identit\u00e9. C'est ce que nous faisons maintenant."]}, {"cell_type": "code", "execution_count": null, "id": "intended-orientation", "metadata": {}, "outputs": [], "source": ["# Replacing the classification lyaer by an identify function forward the feature space to the end\n", "# We now may forward and get the features as output of the model\n", "model.fc = nn.Identity()\n", "print(model)"]}, {"cell_type": "markdown", "id": "young-karma", "metadata": {}, "source": ["### Reduction de dimensionalit\u00e9: PCA, t-SNE, UMAP, etc ....\n", "L'id\u00e9e est ici d'apprendre \u00e0 partir de l'espace de caract\u00e9ristique (i.e. de repr\u00e9sentation) du mod\u00e8le, un projecteur qui va faire passer d'un espace de dimension $f$ \u00e0 un espace de dimension $2$ qu'on va pouvoir visualiser sur un graphe. Vous connaissez d\u00e9j\u00e0 un certains nombre d'algorithmes de ce type (PCA, t-SNE, etc.). Nous utiliserons ici UMAP qui on pour objectif d'apprendre une fonction $map : x \\rightarrow map(x) :  \\mathbb{R}^k \\rightarrow \\mathbb{R}^2$ de sorte \u00e0 ce que les vecteurs voisins au sens d'une norme (e.g. distance euclidienne $L_2$) soient voisin au sens de la norme euclidienne dans l'espace de basse dimension. "]}, {"cell_type": "code", "execution_count": null, "id": "electoral-confidence", "metadata": {}, "outputs": [], "source": ["#Extract feature vectors:\n", "features, labels, images = predict(\n", "    model, \n", "    trainloader, \n", "    feature_extract=True, \n", "    max_size=len(trainset)\n", ")\n", "\n", "print(features.shape, labels.shape, images.shape)\n", "\n", "labels = [classes[labels[j]] for j in range(labels.shape[0])]"]}, {"cell_type": "code", "execution_count": null, "id": "painted-incidence", "metadata": {}, "outputs": [], "source": ["umap_2d = umap.umap_.UMAP(n_components=2, random_state=0)\n", "umap_2d.fit(features)"]}, {"cell_type": "code", "execution_count": null, "id": "persistent-mainstream", "metadata": {}, "outputs": [], "source": ["projections_umap = umap_2d.transform(features)"]}, {"cell_type": "markdown", "id": "correct-there", "metadata": {}, "source": ["###  Visualisation avec plotly"]}, {"cell_type": "code", "id": "c5dab035", "metadata": {}, "source": ["    fig = px.scatter(\n", "        projections_umap, x=0, y=1,\n", "        color=labels\n", "    )\n", "    fig.show()\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "found-badge", "metadata": {}, "source": ["N'h\u00e9sitez pas \u00e0 d\u00e9selectionner en cliquant sur le label associ\u00e9 ou \u00e0 ne s\u00e9lectionner qu'une seule cat\u00e9gorie en double cliquant !"]}, {"cell_type": "markdown", "id": "opposed-costs", "metadata": {}, "source": ["**<span style='color:blue'> Question</span>** ", "\n", "**Que dire de ce feature space. Permet-il d'expliquer les performances de votre mod\u00e8le ? Pourquoi ?**\n", "\n", "\n\n ----"]}, {"cell_type": "markdown", "id": "desperate-uruguay", "metadata": {}, "source": ["### Visualisation interactive avec Bokeh"]}, {"cell_type": "markdown", "id": "polished-three", "metadata": {}, "source": ["En r\u00e9alit\u00e9, chaque point de notre feature space est l'image d'un $x\\in\\mathbb{R}^d$ par la fonction $\\phi$. Il est particuli\u00e8rement int\u00e9ressant d'essayer de visualiser les $x$ qui ont permis de produire chacun des points. Cela nous permettra de constater les proximit\u00e9s et/ou diff\u00e9rences entre les points en fonction de leur proximit\u00e9/distance."]}, {"cell_type": "markdown", "id": "reported-acoustic", "metadata": {}, "source": ["#### [\u2022] M\u00e9thode de visualisation du feature space"]}, {"cell_type": "code", "execution_count": null, "id": "downtown-carry", "metadata": {}, "outputs": [], "source": ["from io import BytesIO\n", "from PIL import Image\n", "import base64\n", "\n", "import pandas as pd\n", "\n", "from bokeh.plotting import figure, show, output_notebook\n", "from bokeh.models import HoverTool, ColumnDataSource, CategoricalColorMapper\n", "from bokeh.palettes import Spectral10, Viridis256, Category20, Turbo256, mpl\n", "import itertools\n", "\n", "def plot_feature_space_with_images(classes, images, labels):\n", "    output_notebook()\n", "    colors = itertools.cycle(Category20[20])    \n", "    pal = [color for m, color in zip(range(len(classes)), colors)]\n", "    np.random.shuffle(pal)\n", "\n", "    def embeddable_image(data):\n", "        image = Image.fromarray(data, mode='RGB')\n", "        buffer = BytesIO()\n", "        image.save(buffer, format='jpeg')\n", "        for_encoding = buffer.getvalue()\n", "        return 'data:image/jpeg;base64,' + base64.b64encode(for_encoding).decode()\n", "    max_size = 2000\n", "    data_df = pd.DataFrame(projections_umap[:max_size], columns=('x', 'y'))\n", "    data_df['class'] = [x for x in labels][:max_size]\n", "    data_df['image'] = list(map(embeddable_image, images[:max_size]))\n", "\n", "    datasource = ColumnDataSource(data_df)\n", "    color_mapping = CategoricalColorMapper(factors=classes,\n", "                                           palette=pal)\n", "    plot_figure = figure(\n", "        title='UMAP projection of the dataset',\n", "        plot_width=900,\n", "        plot_height=600,\n", "        tools=('pan, wheel_zoom, reset')\n", "    )\n", "    plot_figure.add_tools(HoverTool(tooltips=\"\"\"\n", "    <div>\n", "        <div>\n", "            <img src='@image' style='float: left; margin: 5px 5px 5px 5px'/>\n", "        </div>\n", "        <div>\n", "            <span style='font-size: 16px; color: #224499'>Classe:</span>\n", "            <span style='font-size: 18px'>@class</span>\n", "        </div>\n", "    </div>\n", "    \"\"\"))\n", "\n", "    plot_figure.circle(\n", "        'x',\n", "        'y',\n", "        source=datasource,\n", "        color=dict(field='class', transform=color_mapping),\n", "        line_alpha=0.6,\n", "        fill_alpha=0.6,\n", "        size=10,\n", "        legend_field=\"class\",\n", "    )\n", "    plot_figure.legend.location = \"top_left\"\n", "    #plot_figure.legend.click_policy=\"mute\"\n", "    plot_figure.legend.label_text_font_size = \"8px\"\n", "    show(plot_figure)"]}, {"cell_type": "markdown", "id": "touched-harbor", "metadata": {}, "source": ["#### Visualisation"]}, {"cell_type": "code", "execution_count": null, "id": "established-egyptian", "metadata": {}, "outputs": [], "source": ["plot_feature_space_with_images(classes, images, labels)"]}, {"cell_type": "markdown", "id": "stock-permission", "metadata": {}, "source": ["## IX. On recommence avec les Simpsons !"]}, {"cell_type": "markdown", "id": "impressive-indication", "metadata": {}, "source": ["Attention, afin de ne pas tout recoder, pensez \u00e0 ex\u00e9cuter les cellules des sections dont le titre est au format **[\u2022] M\u00e9thode de ...** qui contiennent du code r\u00e9utilisable !\n"]}, {"cell_type": "markdown", "id": "caroline-framework", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**R\u00e9pondez \u00e0 toutes les questions pr\u00e9c\u00e9dentes dans le cadre de ce nouveau jeu de donn\u00e9es et de ce nouveau mod\u00e8le !**\n", "\n", "\n\n ----"]}, {"cell_type": "markdown", "id": "russian-paint", "metadata": {}, "source": ["### A. Construction du jeu de donn\u00e9es"]}, {"cell_type": "code", "execution_count": null, "id": "twenty-twins", "metadata": {}, "outputs": [], "source": ["# Pour Google Colaboratory\n", "# D\u00e9commenter les lignes suivantes\n", "# import os\n", "# from google.colab import drive\n", "\n", "# drive.mount('/content/drive')\n", "\n", "# dataset_path = '/content/drive/MyDrive/DeepTP/archive/simpsons_dataset'\n", "# dataset_path_test = '/content/drive/MyDrive/DeepTP/archive/kaggle_simpson_testset'"]}, {"cell_type": "code", "id": "3e1f5f4a", "metadata": {}, "source": ["# Chemin local vers le dataset\n", "dataset_path = './data/Simpsons/simpsons_dataset'\n", "dataset_path_test ='./data/Simpsons/kaggle_simpson_testset'\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "id": "7ed65464", "metadata": {}, "source": ["data_transform = transforms.Compose([\n", "    transforms.Resize((224, 224)), \n", "    #  RandomSizedCrop(224),\n", "    transforms.RandomHorizontalFlip(),\n", "    transforms.ToTensor(),\n", "    transforms.Normalize(\n", "        mean=[0.485, 0.456, 0.406],\n", "        std=[0.229, 0.224, 0.225]\n", "    )\n", "])\n", "\n", "dataset_train = datasets.ImageFolder(root=dataset_path, transform=data_transform)\n", "\n", "trainset, validset, train_sampler, valid_sampler = split_dataset(\n", "    dataset_train, valid_size = 0.2, random_state=None\n", ")\n", "\n", "trainloader = DataLoader(\n", "  trainset, batch_size=batch_size, sampler=train_sampler,\n", "  num_workers=num_workers,\n", ")\n", "\n", "validloader = DataLoader(\n", "  validset, batch_size=batch_size, sampler=valid_sampler,\n", "  num_workers=num_workers,\n", ")\n", "print('Number of batches in train/val:', len(trainloader), len(validloader))\n", "\n", "#  Get the test data from the test directory\n", "dataset_test = datasets.ImageFolder(root=dataset_path_test,\n", "                                    transform=data_transform)\n", "\n", "# We don't need to split train val, all test data are in one folder\n", "testloader = DataLoader(\n", "  dataset_test, batch_size=batch_size, shuffle=True,\n", "  num_workers=num_workers,\n", ")\n", "print('Number of batches in test:', len(testloader))\n", "\n", "# We list all the directories in alphabetical order to have the label classes.\n", "classes = [c for c in sorted(os.listdir(dataset_path))]\n", "print('Classes :\\n\\t- ' + '\\n\\t- '.join(classes))\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "rough-franchise", "metadata": {}, "source": ["### B. Visualisation de quelques images"]}, {"cell_type": "code", "id": "0fc5d10b", "metadata": {}, "source": ["dataiter = iter(testloader)\n", "images, labels = dataiter.next()\n", "\n", "imshow(images[:8], labels[:8])\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "golden-panama", "metadata": {}, "source": ["### C. Construction du mod\u00e8le"]}, {"cell_type": "markdown", "id": "welsh-subscription", "metadata": {}, "source": ["Dans certains cas, nous ne voulons apprendre que le classifieur final en esp\u00e9rant que l'espace de repr\u00e9sentation appris nous permettra de r\u00e9soudre notre t\u00e2che."]}, {"cell_type": "code", "id": "67d60656", "metadata": {}, "source": ["print(\"Loading existing architecture and init parameters of model pretrained on ImageNet...\")\n", "model = models.resnet18(pretrained=True)\n", "\n", "finetuning = True\n", "if finetuning:\n", "    for p in model.parameters():\n", "        p.requires_grad = False\n", "model.fc = nn.Linear(model.fc.in_features, len(classes))\n", "# model = model.cuda()\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "digital-canvas", "metadata": {}, "source": ["### D. Visualisation des filtres"]}, {"cell_type": "code", "id": "c1187f83", "metadata": {}, "source": ["filters = model.conv1.weight.data.clone().cpu()\n", "visualize_filters(filters, ch=0, allkernels=False)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "related-giving", "metadata": {}, "source": ["### E. L'apprentissage"]}, {"cell_type": "markdown", "id": "executive-privacy", "metadata": {}, "source": ["#### Fonction objectif, scheduler et optimizer"]}, {"cell_type": "code", "id": "85c77921", "metadata": {}, "source": ["#Choose the loss function\n", "criterion = nn.CrossEntropyLoss()\n", "\n", "#Optimizer\n", "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n", "scheduler = MultiStepLR(optimizer, milestones=[25, 50], gamma=0.1)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "different-march", "metadata": {}, "source": ["#### L'entra\u00eenement"]}, {"cell_type": "code", "id": "c2909636", "metadata": {}, "source": ["eval_frequency=1\n", "\n", "loss_history, \\\n", "valid_loss_history, \\\n", "acc_history, \\\n", "val_acc_history = train(model, criterion, optimizer, scheduler, n_epoch=2)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "threatened-attention", "metadata": {}, "source": ["#### Visualisation des courbes de loss et de pr\u00e9cision"]}, {"cell_type": "code", "id": "b9eef7ee", "metadata": {}, "source": ["plot_loss(\n", "    loss_history, valid_loss_history, acc_history, val_acc_history\n", ")\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "micro-pillow", "metadata": {}, "source": ["#### Sauvegarde et chargement du mod\u00e8le"]}, {"cell_type": "code", "id": "a6582215", "metadata": {}, "source": ["torch.save(model.state_dict(), 'my_model.torch')\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "id": "0bbdd34c", "metadata": {}, "source": ["model = Net()\n", "model.load_state_dict(torch.load('my_model.torch'))\n", "# model = model.cuda()\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "limiting-deployment", "metadata": {}, "source": ["### F. Visualisation des filtres/param\u00e8tres appris"]}, {"cell_type": "markdown", "id": "existing-glance", "metadata": {}, "source": ["Attention, si le mod\u00e8le a \u00e9t\u00e9 finetun\u00e9, les filtres n'ont pas \u00e9t\u00e9 modifi\u00e9s et sont donc les m\u00eames qu'au d\u00e9part."]}, {"cell_type": "code", "id": "839a7f46", "metadata": {}, "source": ["filters = model.conv1.weight.data.clone().cpu()\n", "visualize_filters(filters, ch=0, allkernels=False)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "enhanced-burning", "metadata": {}, "source": ["### G. \u00c9valuation du mod\u00e8le et test de quelques pr\u00e9dictions"]}, {"cell_type": "code", "id": "77171fd9", "metadata": {}, "source": ["accuracy, _ = evaluate(testloader, model)\n", "\n", "print('Test accuracy: %.3f' % (accuracy), end='\\n')\n", "\n", "#Test prediction on some images\n", "dataiter = iter(testloader)\n", "images, labels = dataiter.next()\n", "outputs = model(images[:8])#  .to(device))  # we use the loaded model\n", "_, predicted = torch.max(outputs, 1)\n", "\n", "imshow(images[:8], labels[:8], predicted[:8])\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "active-chain", "metadata": {}, "source": ["### H. Extraction de features et Dataviz"]}, {"cell_type": "code", "id": "c889aa09", "metadata": {}, "source": ["#Extract feature vectors:\n", "features, labels, images = predict(\n", "    model, \n", "    trainloader, \n", "    feature_extract=True, \n", "    max_size=len(trainset)\n", ")\n", "\n", "print(features.shape, labels.shape, images.shape)\n", "\n", "labels = [classes[labels[j]] for j in range(labels.shape[0])]\n", "\n", "umap_2d = umap.umap_.UMAP(n_components=2, random_state=0)\n", "umap_2d.fit(features)\n", "\n", "projections_umap = umap_2d.transform(features)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "placed-rendering", "metadata": {}, "source": ["####  Visualisation avec plotly"]}, {"cell_type": "code", "id": "365d08b8", "metadata": {}, "source": ["fig = px.scatter(#_3d(\n", "    projections_umap, x=0, y=1, # z=2,\n", "    color=labels\n", ")\n", "fig.show()\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "literary-victory", "metadata": {}, "source": ["#### Visualisation interactive avec Bokeh"]}, {"cell_type": "code", "id": "dc6a434f", "metadata": {}, "source": ["plot_feature_space_with_images(classes, images, labels)\n"], "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.2"}}, "nbformat": 4, "nbformat_minor": 5}
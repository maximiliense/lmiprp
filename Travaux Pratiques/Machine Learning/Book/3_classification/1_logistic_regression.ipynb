{"cells": [{"cell_type": "markdown", "metadata": {"id": "KoFZYuLKaBB8"}, "source": ["# La r\u00e9gression logistique \u2615\ufe0f\n", "\n", "**<span style='color:blue'> Objectifs de la s\u00e9quence</span>** ", "\n", "* Concevoir&nbsp;:\n", "    * la r\u00e9gression logistique d'un point de vue pr\u00e9dictif,\n", "    * la r\u00e9gression logistique au travers d'un probl\u00e8me d'optimisation.\n", "* \u00catre capable&nbsp;:\n", "    * d'impl\u00e9menter un algorithme de descente de gradient,\n", "    * de transformer les variables d'entr\u00e9e pour rendre le mod\u00e8le non lin\u00e9aire,\n", "    * d'utiliser la librairie $\\texttt{sklearn}$.\n", "* De s'initier \u00e0 la notion de r\u00e9gularisation et de s\u00e9lection de variables.\n", "\n", "\n\n ----"]}, {"cell_type": "markdown", "metadata": {"id": "S6XPOcE_aBCA"}, "source": ["## I. Introduction"]}, {"cell_type": "markdown", "metadata": {"id": "UsjqxNxAaBCA"}, "source": ["Dans cette partie, nous allons impl\u00e9menter un algorithme de classification supervis\u00e9e. Contrairement \u00e0 la r\u00e9gression lin\u00e9aire qui consiste \u00e0 pr\u00e9dire une valeur scalaire, la r\u00e9gression logistique a pour but d'estimer la probabilit\u00e9 d'une variable cat\u00e9gorielle. Une variable cat\u00e9gorielle correspond \u00e0 un nombre entier compris entre $1$ et $K$ pour un probl\u00e8me \u00e0 $K$ classes o\u00f9 la notion de proximit\u00e9 (1 est plus proche de 2 que de 3) est oubli\u00e9e. Nous considererons dans un premier temps un cas simple \u00e0 deux classes. Puis nous mettrons en place un classificateur de chiffre manuscrit compris entre 0 et 9."]}, {"cell_type": "markdown", "metadata": {"id": "mJ1490zpaBCA"}, "source": ["**La r\u00e9gression logistique** cherche \u00e0 estimer la probabilit\u00e9 $\\mathbb{P}(Y=1|X=\\boldsymbol{x})$ o\u00f9 $y\\in\\{0,1\\}$. On obtient la probabilit\u00e9 inverse de la mani\u00e8re suivante : $\\mathbb{P}(Y=0|X=\\boldsymbol{x})$=1-$\\mathbb{P}(Y=1|X=\\boldsymbol{x})$. De la m\u00eame mani\u00e8re que pour la s\u00e9quence traitant de la r\u00e9gression lin\u00e9aire et par un abus de langage fort, nous utiliserons de mani\u00e8re interchangeable la notation $\\boldsymbol{x}, y$ pour faire r\u00e9f\u00e9rence aux variables al\u00e9atoires et \u00e0 leur r\u00e9alisation. Dans le cas de la r\u00e9gression logistique, on suppose que le param\u00e8tre naturel $\\eta$ de notre loi est estimable \u00e0 partir d'une combinaison lin\u00e9aire des variables explicatives :\n", "\n", "$$\\exists\\boldsymbol{\\beta}\\in\\mathbb{R}^d,\\ \\eta(\\boldsymbol{x}) = \\langle\\boldsymbol{\\beta}, \\boldsymbol{x}\\rangle$$\n", "\n", "Attention, la notation $\\eta$ est aussi utilis\u00e9e en machine learning afin de faire directement r\u00e9f\u00e9rence \u00e0 :\n", "\n", "$$\\eta(x)=\\mathbb{P}(Y=1|X=\\boldsymbol{x}).$$\n", "\n", "La fonction de lien $\\sigma$ est la fonction qui permet du passer du param\u00e8tre naturel \u00e0 notre probabilit\u00e9. Dans le cas d'une loi de Bernoulli (loi d'une variable binaire), la fonction de lien est la sigmoid :\n", "\n", "$$\\begin{aligned}\n", "\\sigma:\\mathbb{R}&\\rightarrow \\big[0, 1\\big]\\\\\n", "z&\\mapsto (1+\\text{exp}(-z))^{-1}\n", "\\end{aligned}$$\n", "\n", "\n", "La fonction sigmoid est illustr\u00e9e par la figure suivante."]}, {"cell_type": "code", "execution_count": 1, "metadata": {"id": "ib8Z2rgZaBCB", "outputId": "1af53520-224f-4169-e8f9-5d27a9974b99", "tags": ["hide-input"]}, "outputs": [{"data": {"image/png": "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\n", "text/plain": ["<Figure size 864x576 with 1 Axes>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["import matplotlib\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "from scipy.special import expit as sigmoid\n", "\n", "# configuration generale de matplotlib\n", "%matplotlib inline\n", "matplotlib.rcParams['figure.figsize'] = (12.0, 8.0)\n", "plt.style.use('ggplot')\n", "\n", "x = np.linspace(-5, 5, 100)\n", "y = sigmoid(x)  # (1+np.exp(-x))**(-1) + numerical stability.. Question ?\n", "plt.plot(x, y, label=\"fonction sigmoid\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"id": "sowVRoo_aBCC"}, "source": ["Si le param\u00e8tre naturel $\\eta$ est n\u00e9gatif, la probabilit\u00e9 estim\u00e9e sera inf\u00e9rieure \u00e0 $0.5$ et notre \u00e9chantillon appartiendra plus probablement \u00e0 la classse $0$ ($y=0$). \u00c0 l'inverse, si $\\eta$ est positif, on dira que notre \u00e9chantillon appartient \u00e0 la classe positive.\n", "\n", "il est ainsi possible d'obtenir notre probabilit\u00e9 de la mani\u00e8re suivante :\n", "\n", "\n", "$$\\mathbb{P}(y_{\\text{new}}=1|\\boldsymbol{x_{\\text{new}}}, \\boldsymbol{\\beta})=\\sigma(\\boldsymbol{\\beta}^T\\boldsymbol{x_{\\text{new}}} ).$$\n", "\n", "De la m\u00eame mani\u00e8re que pour la r\u00e9gression lin\u00e9aire, on supposera que le vecteur $\\boldsymbol{x}$ poss\u00e8de une dimension $0$ avec la valeur $1$ faisant office de biais.\n", "\n", "---\n", "**Un probl\u00e8me de classification avec une s\u00e9paration par hyperplan.**\n", "\n", "On dit que deux vecteurs $u$ et $v$ sont orthogonaux si leur produit scalaire est nul :\n", "\n", "$$\\langle u, v \\rangle = 0$$\n", "\n", "\n", "La fronti\u00e8re de d\u00e9cision est l'ensemble de tous les points qu'il n'est pas possible de classer $1$ ou $0$. C'est l'ensemble des points tels que $\\mathbb{P}(y=1|\\boldsymbol{x}, \\boldsymbol{\\beta})=0.5$. Dit encore autrement, et en nous r\u00e9f\u00e9rant \u00e0 la figure ci-dessus, il s'agit de l'ensemble des points tels que le param\u00e8tre naturel estim\u00e9 $\\eta(\\boldsymbol{x})=\\boldsymbol{\\beta}^T\\boldsymbol{x}=0$. Dit encore autrement, il s'agit de l'ensemble des points orthogonaux au vecteur de param\u00e8tres $\\boldsymbol{\\beta}$. Le vecteur $\\boldsymbol{\\beta}$ est appel\u00e9 vecteur normal \u00e0 l'hyperplan s\u00e9parateur.  \n", "\n", "---\n", "\n", "La r\u00e9gression logistique nous donne la probabilit\u00e9 $\\mathbb{P}(y_{\\text{new}}=1|\\boldsymbol{x_{\\text{new}}}, \\boldsymbol{\\beta})$. Il est trivial d'obtenir la classe \u00e0 partir de ce score. On dira que l'\u00e9chantillon appartient \u00e0 la classe $1$ si la probabilit\u00e9 est sup\u00e9rieure \u00e0 $0.5$ et \u00e0 la classe $0$ dans le cas contraire.\n", "\n", "On peut parfois vouloir bouger ce seuil. Ainsi, si on pr\u00e9dit qu'un patient a $45\\%$ de chance d'avoir un cancer, on voudra refaire des tests plut\u00f4t que lui dire que tout est bon."]}, {"cell_type": "markdown", "metadata": {"id": "oaQY3BmRaBCC"}, "source": ["## II. Construction d'un jeu de donn\u00e9es"]}, {"cell_type": "markdown", "metadata": {"id": "iaT-KDxVaBCC"}, "source": ["Consid\u00e9rons le mod\u00e8le g\u00e9n\u00e9ratif suivant :\n", "\n", "\n", "$$\\boldsymbol{\\beta} \\sim \\mathcal{N}(0, 1)^3 \\in \\mathbb{R}^3$$\n", "\n", "Nos \u00e9chantillons sont simul\u00e9s via une loi normale de moyenne centr\u00e9e sur la fronti\u00e8re de d\u00e9cision. Notons $\\boldsymbol{\\beta^\\prime}=\\begin{bmatrix}\\beta_1\\\\ \\beta_2\\end{bmatrix}$. On fixera cette moyenne de la mani\u00e8re suivante :\n", "\n", "$$\\boldsymbol{\\mu}=\\boldsymbol{\\beta^\\prime}\\Bigg(-\\frac{\\beta_0}{\\lVert \\boldsymbol{\\beta^\\prime}\\rVert^2}\n", "\\Bigg).$$\n", "\n", "\n", "**<span style='color:blue'> Exercice</span>** ", "\n", "**V\u00e9rifier qu'on obtient bien :**\n", "\n", "$$\\langle \\boldsymbol{\\beta^\\prime}, \\boldsymbol{\\mu}\\rangle + \\beta_0=0$$\n", "\n", "**Dit autrement, il s'agit de v\u00e9rifier que $\\boldsymbol{\\mu}$ est bien sur la fronti\u00e8re.**\n", "\n", "\n\n ----", "\n", "\n", "La moyenne de notre loi normale \u00e9tant maintenant fix\u00e9e, nous pouvons simuler nos donn\u00e9es :\n", "\n", "$$\\boldsymbol{x}\\sim\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{1})\\in\\mathbb{R}^2$$\n", "\n", "\n", "La classe d'un \u00e9chantillon est donn\u00e9e par :\n", "\n", "$$\\begin{aligned}\n", "y_i=\\begin{cases}\n", "1\\text{ si }\\langle\\boldsymbol{\\beta^\\prime},\\boldsymbol{x_i}\\rangle +\\beta_0>0\\\\\n", "0\\text{ sinon.}\n", "\\end{cases}\\end{aligned}$$\n", "\n", "Notre probl\u00e8me est donc par construction totalement lin\u00e9airement s\u00e9parable dans le sens o\u00f9 la fronti\u00e8re de d\u00e9cision d\u00e9finie par le vecteur normal $\\boldsymbol{\\beta^\\prime}$ et par le biais $\\beta_0$ s\u00e9pare totalement et sans erreur notre jeu de donn\u00e9es.\n", "\n", "Le code ci dessous affiche le jeux de donn\u00e9es ainsi que la repr\u00e9sentation graphique de la fronti\u00e8re de decision $f(x)=-\\frac{\\beta_1}{\\beta_2}x_1-\\frac{\\beta_0}{\\beta_2}$. On v\u00e9rifie facilement que le vecteur construit tel que $\\beta_2=f(x)$ pour $\\beta_1$ et $\\beta_0$ quelconques (\u00e0 part les cas particuliers) sont bien sur la fronti\u00e8re."]}, {"cell_type": "code", "execution_count": 2, "metadata": {"id": "q72vZKIpaBCD"}, "outputs": [], "source": ["import numpy as np\n", "\n", "real_beta = np.random.normal(0, 1, size=3)\n", "\n", "def sample_data(n, beta):\n", "    # constructing mean \n", "    mu = beta[1:3]*(-beta[0]/(np.linalg.norm(beta[1:3])**2))\n", "    # covariance is the same for each class\n", "    cov  = np.diag(np.ones(2))\n", "    \n", "    # sampling x and adding the bias\n", "    X = np.insert(np.random.multivariate_normal(mu, cov, size=n), 0, 1, axis=1)\n", "    \n", "    # the label is deterministic\n", "    y = (np.dot(X, beta)>0)*1\n", "    \n", "    return X, y\n", "    \n", "X, y = sample_data(100, real_beta)"]}, {"cell_type": "code", "execution_count": 3, "metadata": {"id": "yw8QfHMraBCD", "outputId": "2ded8c01-c6ad-4aec-f854-78edf4c8861a", "scrolled": false}, "outputs": [{"data": {"image/png": "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\n", "text/plain": ["<Figure size 864x576 with 1 Axes>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["import matplotlib\n", "import matplotlib.pyplot as plt\n", "\n", "%matplotlib inline\n", "\n", "matplotlib.rcParams['figure.figsize'] = (12.0, 8.0)\n", "plt.style.use('ggplot')\n", "\n", "def plot(X, y, beta=None, predictor=None, title=None):\n", "    ymin_ = X[:,2].min()\n", "    ymax_ = X[:,2].max()\n", "    min_ = X[:,1].min()\n", "    max_ = X[:,1].max()\n", "    \n", "    if predictor is not None:\n", "        h = 0.02\n", "        xx, yy = np.meshgrid(np.arange(min_, max_, h), np.arange(ymin_, ymax_, h))\n", "        Z = predictor.predict(np.insert(np.c_[xx.ravel(), yy.ravel()], 0, 1, axis=1))\n", "        Z = Z.reshape(xx.shape)\n", "        plt.pcolormesh(xx, yy, Z,shading='auto', alpha=0.01)\n", "    \n", "    plt.scatter(X[:,1], X[:,2], c=y)\n", "    \n", "    if beta is not None:\n", "        x_ = np.linspace(min_, max_, 500)\n", "        y_  = -beta[0]/beta[2] - x_ * beta[1] / beta[2]\n", "        plt.plot(x_, y_)\n", "    \n", "    if title is not None:\n", "        plt.title(title)\n", "    plt.xlim(min_, max_)\n", "    plt.ylim(ymin_, ymax_)\n", "    plt.show()\n", "plot(X, y, real_beta)"]}, {"cell_type": "markdown", "metadata": {"id": "TwpfZNZRaBCD"}, "source": ["## III. Fonction objectif et gradient"]}, {"cell_type": "markdown", "metadata": {"id": "0hbb8ejMaBCE"}, "source": ["De la m\u00eame mani\u00e8re que pour la r\u00e9gression lin\u00e9aire, nous pouvons obtenir notre fonction objectif \u00e0 partir de la formulation de la vraisemblance de notre probl\u00e8me :\n", "\n", "$$\\mathcal{L}_{\\boldsymbol{\\beta}}(\\mathcal{S})=\\prod_{(\\boldsymbol{x}, y)\\in\\mathcal{S}}\\mathbb{P}(y=1|\\boldsymbol{x},\\boldsymbol{\\beta})^y\\mathbb{P}(y=0|\\boldsymbol{x},\\boldsymbol{\\beta})^{1-y}$$\n", "\n", "\n", "Le param\u00e8tre maximisant la vraisemblance est aussi celui minimisant la log vraisemblance n\u00e9gative :\n", "\n", "$$J(\\beta)=-\\text{log}\\big(\\mathcal{L}_{\\boldsymbol{\\beta}}(\\mathcal{S})\\big)=-\\sum_{(\\boldsymbol{x}, y)\\in\\mathcal{S}}y\\text{log}(p)+(1-y)\\text{log}(1-p)$$\n", "\n", "\n", "o\u00f9 $p=\\mathbb{P}(y=1|\\boldsymbol{x},\\boldsymbol{\\beta})=\\sigma(\\boldsymbol{\\beta}^T\\boldsymbol{x})$. Cette fonction objectif, ou *loss* s'appelle la *cross entropy* ou entropie crois\u00e9e. On obtient donc :\n", "\n", "$$\\hat{\\boldsymbol{\\beta}}=\\text{argmin}_{\\boldsymbol{\\beta}}\\Big[-\\sum_{(\\boldsymbol{x}, y)\\in\\mathcal{S}}y\\text{log}(\\sigma(\\boldsymbol{\\beta}^T\\boldsymbol{x}))+(1-y)\\text{log}(1-\\sigma(\\boldsymbol{\\beta}^T\\boldsymbol{x}))\\Big]$$\n", "\n", "La fonction $J$ est ainsi ce qu'on souhaite minimiser."]}, {"cell_type": "markdown", "metadata": {"id": "ABTO8Hf5aBCE"}, "source": ["**<span style='color:blue'> Question 1</span>** ", "\n", "**Compl\u00e9ter la m\u00e9thode $\\texttt{val}$ de l'objet $\\texttt{CrossEntropy}$ ci-dessous.**\n", "\n", "\n\n ----", "\n", "\n", "**<span style='color:blue'> Question 2</span>** ", "\n", "**Calculez les d\u00e9riv\u00e9es partielles $\\partial J(\\beta)/\\partial \\beta_0$, $\\partial J(\\beta)/\\partial \\beta_1$ et  $\\partial J(\\beta)/\\partial \\beta_2$ de la fonction de co\u00fbt de notre mod\u00e8le de r\u00e9gr\u00e9ssion logistique. Compl\u00e9tez la m\u00e9thode $\\texttt{grad}$ de l'objet $\\texttt{CrossEntropy}$ ci dessous.**\n", "\n", "\n\n ----", "\n", "**<span style='color:green'> Indice</span>** ", "\n", "Rappellez-vous que la d\u00e9riv\u00e9e d'une composition de fonction s'\u00e9crit $(g \\circ f)^\\prime (x) = f^\\prime(x) g^\\prime(f(x))$ et que la fonction de co\u00fbt de notre mod\u00e8le s'\u00e9crit:\n", "\n", "\n", "$$J(\\boldsymbol{\\beta}) = \\frac{1}{n}\\sum_j^n g_1(f_{\\boldsymbol{\\beta}}(x_j)) + g_2(f_{\\boldsymbol{\\beta}}(x_j))$$\n", "\n", "avec $g$, $f$, etc. choisis intelligemment.\n", "\n", "\n\n ----", "\n", "\n", "**<span style='color:blue'> Question 2${}^\\star$</span>** ", "\n", "**Calculez le gradient de la fonction $J(\\boldsymbol{\\beta})$ en utilisant les d\u00e9riv\u00e9es matricielles et modifiez la m\u00e9thode $\\texttt{grad}$ ci-dessous en cons\u00e9quence.**\n", "\n", "\n\n ----", "\n", "\n", "\n", "**<span style='color:blue'> Question 3</span>** ", "\n", "**Compl\u00e9tez la m\u00e9thode $\\texttt{predict}$ de l'objet ci-dessous**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "metadata": {}, "source": ["class CrossEntropy(object):\n", "    def __init__(self, X, y):\n", "        self.X = X\n", "        self.y = y\n", "        self.idx = np.array([i for i in range(self.X.shape[0])])\n", "        self._pos = 0\n", "        \n", "    def _format_ndarray(arr):\n", "        arr = np.array(arr) if type(arr) is not np.ndarray else arr\n", "        return arr.reshape((arr.shape[0], 1)) if len(arr.shape) == 1 else arr\n", "    \n", "    def predict(self, X):\n", "    \n", "        ####### Complete this part ######## or die ####################\n", "        ...\n", "        ###############################################################\n", "\n", "        return y_pred\n", "    \n", "    def _sigmoid(X, beta):\n", "        return (1+np.exp(-np.dot(X, beta)))**(-1)\n", "    \n", "    def val(self, beta):\n", "        beta = CrossEntropy._format_ndarray(beta)\n", "\n", "        ####### Complete this part ######## or die ####################\n", "        ...\n", "        ...\n", "        return ...\n", "        ###############################################################\n", "    \n", "    def _shuffle(self):\n", "        np.random.shuffle(self.idx)\n", "    \n", "    def grad(self, beta, batch_size=-1):\n", "        batch_size = self.X.shape[0] if batch_size == -1 else batch_size\n", "        idx = self.idx[self._pos:self._pos+batch_size]\n", "\n", "        self._pos = (self._pos+batch_size) % self.X.shape[0]\n", "        if self._pos == 0:\n", "            self._shuffle()\n", "            \n", "        X, y = self.X[idx], self.y[idx]\n", "        y = CrossEntropy._format_ndarray(y)\n", "\n", "        \n", "        beta = CrossEntropy._format_ndarray(beta)\n", "        \n", "        ####### Complete this part ######## or die ####################\n", "        ...\n", "        return ...\n", "        ###############################################################\n", "    \n", "\n", "l = CrossEntropy(X, y)\n", "print('La valeur de la loss pour beta est', l.val(real_beta))\n", "\n", "print('Le gradient pour beta est\\n', l.grad(real_beta))\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "h7V8AarsaBCF"}, "source": ["## IV. Optimisation et dynamique de la descente de gradient dans le cas s\u00e9parable"]}, {"cell_type": "markdown", "metadata": {"id": "xx9ZVwqqaBCF"}, "source": ["R\u00e9cup\u00e9rons l'algorithme de descente de gradient d\u00e9velopp\u00e9 lors du TP sur la r\u00e9gression lin\u00e9aire. Il s'agit exactement du m\u00eame code \u00e0 la diff\u00e9rence pr\u00e8s qu'on optimise la fonction $\\texttt{CrossEntropy}$ et non $\\texttt{LeastSquare}$."]}, {"cell_type": "code", "metadata": {}, "source": ["class GradientDescent(object):\n", "    init = np.random.uniform(-4, 4, size=3).reshape((3, 1))\n", "    def __init__(self, X, y, loss=CrossEntropy):\n", "        self.loss = loss(X, y)\n", "        \n", "    def optimize(self, learning_rate = 0.5, nb_iterations=10, beta=init, batch_size=-1):\n", "        param_trace = [beta.T[0]]\n", "        loss_trace = [self.loss.val(beta)]\n", "        for i in range(nb_iterations):\n", "            beta = beta - learning_rate * self.loss.grad(beta, batch_size=batch_size)\n", "            param_trace.append(beta.T[0])\n", "            loss_trace.append(self.loss.val(beta))\n", "            \n", "        return param_trace, loss_trace\n", "\n", "\n", "gd = GradientDescent(X, y)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "MA90hvQtaBCG"}, "source": ["La dimension de l'espace des param\u00e8tres est $3$ et il n'est plus possible de visualiser ce dernier pour voir si notre algorithme d'optimisation fonctionne. \n", "\n", "**<span style='color:blue'> Exercice</span>** ", "\n", "**Proposez une strat\u00e9gie permettant d'\u00e9valuer la convergence de notre algorithme. Jouez sur le *learning rate* et sur le nombre d'it\u00e9rations afin d'am\u00e9liorer la qualit\u00e9 de notre estimateur.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "metadata": {}, "source": ["####### Complete this part ######## or die ####################\n", "...\n", "...\n", "###############################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "ZKQTMcaJaBCG"}, "source": ["---\n", "\n", "Analysons maintenant l'\u00e9volution de notre param\u00e8tres en terme de distance par rapport au \"vrai\" param\u00e8tre ainsi que l'\u00e9volution de la *loss* pour plusieurs configurations d'apprentissage."]}, {"cell_type": "code", "metadata": {}, "source": ["distance = []\n", "loss_evolution = []\n", "for it in range(10, 1000, 10):\n", "    params, loss_trace = gd.optimize(nb_iterations=it, learning_rate=1.)\n", "    distance.append(np.linalg.norm(params[-1]-real_beta))\n", "    loss_evolution.append(loss_trace[-1])\n", "plt.figure()\n", "plt.plot(list(range(10, 1000, 10)),distance)\n", "plt.title(\"Distance Euclidienne entre les parametres estimes et la vraie solution en fonction du\"+\n", "          \" nombre d'it\u00e9rations\")\n", "plt.show()\n", "plt.figure()\n", "plt.title(\"\u00c9volution de la loss en fonction du nombre d'it\u00e9rations\")\n", "plt.plot(list(range(10, 1000, 10)), loss_evolution)\n", "plt.show()\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "mct2qLf7aBCH"}, "source": ["On remarque que notre vecteur de param\u00e8tres se rapproche dans un premier temps de la vraie solution (voire m\u00eame pas) puis s'en \u00e9carte in\u00e9xorablement. Il est l\u00e9gitime de se poser la question du bug dans l'algorithme. Cependant, l'affichage de la *loss* nous montre que plus on s'\u00e9carte du vrai param\u00e8tre, plus notre mod\u00e8le *fit* correctement les donn\u00e9es dans le sens o\u00f9 il minimise bien la fonction objectif. De plus un affichage de la fronti\u00e8re de d\u00e9cision ainsi calcul\u00e9e montre que notre fronti\u00e8re de d\u00e9cision semble visuellement assez proche de la vraie solution."]}, {"cell_type": "code", "metadata": {}, "source": ["plot(X, y, params[-1], title='Solution estimee')\n", "plot(X, y, real_beta, title='Vraie solution')\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "VykU0n8kaBCH"}, "source": ["Il se produit donc un ph\u00e9nom\u00e8ne qu'il convient de comprendre. Ce ph\u00e9nom\u00e8ne est en r\u00e9alit\u00e9 le pendant du r\u00e9gime interpolatoire (i.e. 0 erreur) de la r\u00e9gression lin\u00e9aire pour la r\u00e9gression logistique.\n", "\n", "\u00c9tudions cela plus en d\u00e9tails."]}, {"cell_type": "markdown", "metadata": {"id": "oJhOKlrzaBCH"}, "source": ["**<span style='color:blue'> Exercice 1</span>** ", "\n", "**Montrer que si $\\beta$ est le vecteur normal d'un hyperplan qui s\u00e9pare correctement (i.e. aucune erreur) les deux classes de notre jeu de donn\u00e9es, alors $k\\beta,\\ k\\in\\mathbb{R}^{\\star+}$ est aussi un vecteur normal s\u00e9parateur pour notre jeu de donn\u00e9es.**\n", "\n", "\n\n ----", "\n", "**<span style='color:blue'> Exercice 2</span>** ", "\n", "**Montrer que si $\\beta$ est le vecteur normal d'un hyperplan qui s\u00e9pare correctement (i.e. aucune erreur) les deux classes de notre jeu de donn\u00e9es, alors $J(\\beta)>J(k\\beta)$ si $k>1$.**\n", "\n", "\n\n ----"]}, {"cell_type": "markdown", "metadata": {"id": "fdFoWTIfaBCH"}, "source": ["Autrement dit, s'il existe un vecteur $\\beta$ qui d\u00e9finit une bonne fronti\u00e8re de d\u00e9cision avec aucune erreur, alors tout vecteur $\\gamma=k\\beta,\\ k>0$ d\u00e9finira le m\u00eame hyperplan. De plus, plus $k$ sera grand, plus notre loss sera petite. Cela nous indique qu'en r\u00e9alit\u00e9, la fonction $J(\\beta)$ n'admet **aucun** minimum ou, d'un point de vue statistique, que le maximum de vraisemblance n'existe pas. On se rapproche du minimum de la fonction $J$ lorsque $\\beta$ diverge vers l'infini.\n", "\n", "D'un point de vue purement pr\u00e9dictif/classification, cela n'est pas g\u00eanant car toutes ces solutions d\u00e9finissent le m\u00eame hyperplan qui n'est d\u00e9crit que par la direction du vecteur $\\beta$. D'un point de vue statistique, cela est plus g\u00eanant car les \"probabilit\u00e9s\" retourn\u00e9es par notre mod\u00e8le convergent toutes soit vers $1$ soit vers $0$ et ne sont plus interpr\u00e9tables.\n", "\n", "La figure suivante montre que bien que notre vecteur de param\u00e8tres diverge, son cosinus avec le vrai vecteur de param\u00e8tres tend vers $1$ : ils sont donc bien colin\u00e9aires."]}, {"cell_type": "code", "metadata": {}, "source": ["cos = []\n", "for it in range(10, 1000, 10):\n", "    params, loss_trace = gd.optimize(nb_iterations=it, learning_rate=1.)\n", "    cos.append(np.dot(params[-1], real_beta)/(np.linalg.norm(params[-1])*np.linalg.norm(real_beta)))\n", "plt.figure()\n", "plt.title(\"Cosinus entre les parametres estimes et la vraie solution\")\n", "plt.plot(list(range(10, 1000, 10)), cos)\n", "plt.show()\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Ce n'est bien s\u00fbr pas exactement $1$ puisque notre vecteur est estim\u00e9 sur un jeu de donn\u00e9es empirique de taille finie."]}, {"cell_type": "markdown", "metadata": {"id": "Z2AogJLOaBCI"}, "source": ["## V. Optimisation et dynamique de la descente de gradient dans le cas non-s\u00e9parable"]}, {"cell_type": "markdown", "metadata": {"id": "6WbOfAxYaBCI"}, "source": ["\u00c0 l'inverse, si le probl\u00e8me n'\u00e9tait pas s\u00e9parable, une solution optimale existerait et notre algorithme s'en serait approch\u00e9. Cette solution serait notre maximum de vraisemblance statistique."]}, {"cell_type": "markdown", "metadata": {"id": "PSb9C_lyaBCI"}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Soit $\\beta^\\star$ le minimum de notre fonction $J$ tel qu'il existe un unique \u00e9chantillon $(\\boldsymbol{x}, y)$ mal class\u00e9. Montrer que $J(k\\beta^\\star)$ diverge lorsque $k$ tend vers l'infini.**\n", "\n", "\n\n ----"]}, {"cell_type": "markdown", "metadata": {"id": "Ig-7LXeWaBCI"}, "source": ["### Construction d'un jeu de donn\u00e9es non-s\u00e9parable"]}, {"cell_type": "markdown", "metadata": {"id": "K19URTPdaBCI"}, "source": ["Consid\u00e9rons tout d'abord le mod\u00e8le g\u00e9n\u00e9ratif suivant:\n", "\n", "\n", "$$\\boldsymbol{x^+} \\sim \\mathcal{N}(\\mu^+, 1)^2 \\in \\mathbb{R}^2, \\boldsymbol{x^-} \\sim \\mathcal{N}(\\mu^-, 1)^2 \\in \\mathbb{R}^2$$\n", "\n", "Les \u00e9chantillons $\\boldsymbol{x^+}$ sont associ\u00e9s \u00e0 $y=1$ et $\\boldsymbol{x^-}$ \u00e0 $y=0$. La variable $y$ est notre variable binaire \u00e0 expliquer. Le centre de chaque *cluster* est d\u00e9fini de la mani\u00e8re suivante :\n", "\n", "$$\\boldsymbol{\\mu^{+/-}}=\\boldsymbol{\\beta^\\prime}\\Bigg(-\\frac{\\beta_0}{\\lVert \\boldsymbol{\\beta^\\prime}\\rVert^2}\\pm\\rho\n", "\\Bigg),$$\n", "\n", "\n", "o\u00f9 $\\rho$ nous permet de contr\u00f4ler l'\u00e9cart du centre de chaque cluster avec la fronti\u00e8re de d\u00e9cision. De la m\u00eame mani\u00e8re que pr\u00e9c\u00e9demment, nous choissons une r\u00e8gle arbitraire pour g\u00e9n\u00e9rer al\u00e9atoirement les param\u00e8tres du \"vrai\" mod\u00e8le en incluant un biais:\n", "\n", "\n", "$$\\boldsymbol{\\beta} \\sim \\mathcal{N}(0, 1)^3 \\in \\mathbb{R}^3$$\n", "\n", "\n", "Ainsi $\\beta_1$ et $\\beta_2$ correspondent aux param\u00e8tres associ\u00e9s \u00e0 nos variables explicatives et $\\beta_0$ est le biais."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "We6HSWp2aBCI"}, "outputs": [], "source": ["real_beta = np.random.normal(0, 1, size=3)\n", "\n", "def sample_data(n, beta, rho=1):\n", "    # constructing mean of each class\n", "    mu_1 = beta[1:3]*((-beta[0]/(np.linalg.norm(beta[1:3])**2))-rho)\n", "    mu_0 = beta[1:3]*((-beta[0]/(np.linalg.norm(beta[1:3])**2))+rho)\n", "    # covariance is the same for each class\n", "    cov  = np.diag(np.ones(2))\n", "    \n", "    # the two classes have the same number of samples\n", "    X = np.concatenate([\n", "        np.random.multivariate_normal(mu_1, cov, size=int(n/2)),\n", "        np.random.multivariate_normal(mu_0, cov, size=int(n/2))\n", "    ], axis=0)\n", "    \n", "    # the label is deterministic\n", "    y = np.array([0 if i < int(n/2) else 1 for i in range(2*int(n/2))])\n", "    \n", "    # we shuffle the samples\n", "    idx = [i for i in range(X.shape[0])]\n", "    np.random.shuffle(idx)\n", "    \n", "    # we insert a bias\n", "    return np.insert(X[idx], 0, 1, axis=1), y[idx]\n", "    \n", "X, y = sample_data(1000, real_beta, rho=2)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "2jTvsBqSaBCJ", "outputId": "b66451a3-e691-4450-d7a9-5c18e39bb4cc"}, "outputs": [], "source": ["plot(X, y, real_beta)"]}, {"cell_type": "markdown", "metadata": {"id": "dKKIY7aGaBCJ"}, "source": ["on retrace les m\u00eames courbes que nous avions r\u00e9alis\u00e9es pr\u00e9c\u00e9demment et on constate la diff\u00e9rence."]}, {"cell_type": "code", "metadata": {"id": "Q-YdDJyKaBCJ"}, "source": ["gd = GradientDescent(X, y)\n", "\n", "distance = []\n", "loss_evolution = []\n", "cos = []\n", "for it in range(10, 1000, 10):\n", "    params, loss_trace = gd.optimize(nb_iterations=it, learning_rate=1.)\n", "    distance.append(np.linalg.norm(params[-1]-real_beta))\n", "    loss_evolution.append(loss_trace[-1])\n", "    cos.append(np.dot(params[-1], real_beta)/(np.linalg.norm(params[-1])*np.linalg.norm(real_beta)))\n", "plt.figure()\n", "plt.plot(list(range(10, 1000, 10)),distance)\n", "plt.title(\"Distance Euclidienne entre les parametres estimes et la vraie solution en fonction du\"+\n", "          \" nombre d'it\u00e9rations\")\n", "plt.show()\n", "plt.figure()\n", "plt.title(\"\u00c9volution de la loss en fonction du nombre d'it\u00e9rations\")\n", "plt.plot(list(range(10, 1000, 10)), loss_evolution)\n", "plt.show()\n", "\n", "plt.figure()\n", "plt.title(\"Cosinus entre les parametres estimes et la vraie solution\")\n", "plt.plot(list(range(10, 1000, 10)), cos)\n", "plt.show()\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "am0ujz06aBCJ"}, "source": ["On remarque qu'on ne converge pas vers le vecteur que nous avions utilis\u00e9 lors de la construction du jeu de donn\u00e9es. Cela est du au fait que nous ne nous sommes servi de ce vecteur que pour positionner l'hyperplan. Ainsi, nous avons utilis\u00e9 sa direction et non sa norme. Rajoutons que maintenant le probl\u00e8me n'est plus s\u00e9parable et notre vecteur estim\u00e9 converge vers une valeur. On peut \u00e9galement confirmer que le probl\u00e8me n'est plus s\u00e9parable car notre *loss* ne tend plus vers 0."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**<span style='color:blue'> Question</span>** ", "\n", "**Quel processus g\u00e9n\u00e9ratif aurait pu \u00eatre utilis\u00e9 afin que le vecteur $\\beta$ r\u00e9el soit bien celui vers lequel on retombe lorsqu'on optimise notre probl\u00e8me ?**\n", "\n", "\n\n ----", "\n", "\n", "**<span style='color:blue'> Exercice</span>** ", "\n", "**Testez votre mod\u00e8le g\u00e9n\u00e9ratif et comparez les r\u00e9sultats.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "metadata": {}, "source": ["real_beta = np.random.normal(0, 0.1, size=3)\n", "real_beta[1:3] *= 10\n", "\n", "def perfect_data_sampler(n, beta):\n", "    ####### Complete this part ######## or die ####################\n", "    ...\n", "    ...\n", "    ...\n", "    ...\n", "    ...\n", "    ...\n", "    ...\n", "    ###############################################################\n", "    return X, y\n", "    \n", "X, y = perfect_data_sampler(1000, real_beta)\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["plot(X, y, real_beta)\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["gd = GradientDescent(X, y)\n", "\n", "distance = []\n", "loss_evolution = []\n", "cos = []\n", "for it in range(10, 1000, 10):\n", "    params, loss_trace = gd.optimize(nb_iterations=it, learning_rate=1.)\n", "    distance.append(np.linalg.norm(params[-1]-real_beta))\n", "    loss_evolution.append(loss_trace[-1])\n", "    cos.append(np.dot(params[-1], real_beta)/(np.linalg.norm(params[-1])*np.linalg.norm(real_beta)))\n", "plt.figure()\n", "plt.plot(list(range(10, 1000, 10)),distance)\n", "plt.title(\"Distance Euclidienne entre les parametres estimes et la vraie solution en fonction du\"+\n", "          \" nombre d'it\u00e9rations\")\n", "plt.show()\n", "plt.figure()\n", "plt.title(\"\u00c9volution de la loss en fonction du nombre d'it\u00e9rations\")\n", "plt.plot(list(range(10, 1000, 10)), loss_evolution)\n", "plt.show()\n", "\n", "plt.figure()\n", "plt.title(\"Cosinus entre les parametres estimes et la vraie solution\")\n", "plt.plot(list(range(10, 1000, 10)), cos)\n", "plt.show()\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Succ\u00e8s !"]}, {"cell_type": "markdown", "metadata": {"id": "ng-yVRHQaBCJ"}, "source": ["## VI. Transformation des variables explicatives"]}, {"cell_type": "markdown", "metadata": {"id": "TvKKnediaBCJ"}, "source": ["Consid\u00e9rons maintenant un jeu de donn\u00e9es tel qu'il n'est pas possible de s\u00e9parer nos deux classes lin\u00e9airement."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "ijrQHfgGaBCK", "outputId": "d1979b0f-4141-4501-acb4-103151668b6b"}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "def sample_data(n, sigma1=0.1, sigma2=0.3, r=2):\n", "    class_1 = np.random.uniform(0, 2*np.pi, size=(int(n/2), 1))\n", "    class_1 = np.concatenate(\n", "        [r*np.cos(class_1), r*np.sin(class_1)], axis=1) + np.random.normal(0, \n", "                                                                       sigma1, \n", "                                                                       size=(int(n/2), 2))\n", "    class_2 = np.random.normal(0, sigma2, size=(int(n/2), 2))\n", "    X = np.insert(np.concatenate([class_1, class_2]), 0, 1, axis=1)\n", "    y = np.array([1 if i < int(n/2) else 0 for i in range(2*int(n/2))])\n", "    \n", "    return X, y\n", "    \n", "X, y = sample_data(100)\n", "plot(X, y)"]}, {"cell_type": "markdown", "metadata": {"id": "XA930y2UaBCK"}, "source": ["On remarque assez na\u00efvement et rapidement qu'une simple r\u00e9gression logistique n'est plus une solution acceptable."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "zlUCxfpdaBCK", "outputId": "45ce8d59-0b77-48ef-cff4-75854526331e"}, "outputs": [], "source": ["from sklearn.linear_model import LogisticRegression\n", "\n", "model = LogisticRegression(fit_intercept=False)\n", "model.fit(X, y)\n", "plot(X, y, beta=model.coef_[0])"]}, {"cell_type": "markdown", "metadata": {"id": "6D7D66t8aBCK"}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Proposez une transformation polynomiale de vos variables explicatives permettant de r\u00e9soudre correctement ce probl\u00e8me.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "metadata": {}, "source": ["from sklearn.preprocessing import PolynomialFeatures\n", "from sklearn.pipeline import make_pipeline\n", "\n", "####### Complete this part ######## or die ####################\n", "deg = ...\n", "\n", "model = ...\n", "...\n", "###############################################################\n", "\n", "plot(X, y, predictor=model)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "MdID39YxaBCK"}, "source": ["## VII. Classificateur de chiffres manuscrits : Le dataset MNIST\n", "Le jeu de donn\u00e9es MNIST n'est plus un probl\u00e8me de classification binaire mais multi-classes (i.e. 10). Le passage de la r\u00e9gression logistique \u00e0 une t\u00e2che multi-classes n'est pas directe. Il y a deux strat\u00e9gies&nbsp;:\n", "\n", "1. Remplacer notre vecteur de param\u00e8tre par une matrice dont la sortie est un vecteur dont la dimension est le nombre de classes et remplacer la sigmo\u00efd par la fonction softmax,\n", "2. Entra\u00eener autant de mod\u00e8les qu'il n'y a de classes via une strat\u00e9gie \"one versus rest\" (OVR).\n", "\n", "\n", "La librairie $\\texttt{scikit-learn}$ s'en charge bien s\u00fbr pour vous."]}, {"cell_type": "markdown", "metadata": {"id": "nuPs72t6aBCL"}, "source": ["### A. Chargement du dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "8W10_EMNaBCL"}, "outputs": [], "source": ["from sklearn.datasets import fetch_openml\n", "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n", "X = X.to_numpy()\n", "y = y.astype(int)"]}, {"cell_type": "markdown", "metadata": {"id": "hXRlAAcNaBCL"}, "source": ["### B. Visualisation d'un exemple representatif du jeu de donn\u00e9es"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "VaVzGczYaBCL", "outputId": "d0a68c50-d11f-4e2d-bdb9-eaa8b4a5bb46"}, "outputs": [], "source": ["X_img = np.reshape(X,(X.shape[0],28*28))\n", "\n", "#Recuperation du nombre d'exemples d'apprentissage ainsi que la dimension des vecteurs\n", "n_samples = X.shape[0]\n", "print(\"Nombre d'exemples d'apprentissage n_samples = %d \" % n_samples)\n", "\n", "def plotImg(X):\n", "    plt.figure(figsize=(7.195, 3.841), dpi=100)\n", "    for i in range(200):\n", "        plt.subplot(10,20,i+1)\n", "        plt.imshow(X[i,:].reshape([28,28]), cmap='gray')\n", "        plt.axis('off')\n", "    plt.show()\n", "    plt.close()\n", "    \n", "plotImg(X_img)\n", "    \n", "n_classes = np.max(y) + 1\n", "print(\"Nombre de classes d'objets n_classes = %d \" % n_classes)"]}, {"cell_type": "markdown", "metadata": {"id": "aJZKH49paBCL"}, "source": ["### C. Construction d'un ensemble de test"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "10UvWFdxaBCL"}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=10000)"]}, {"cell_type": "markdown", "metadata": {"id": "5TeIBCvCaBCL"}, "source": ["### D. Model fit\n", "\n", "**<span style='color:blue'> Exercice</span>** ", "\n", "**Proposez un mod\u00e8le de classification permettant de classer correctement nos chiffres. N'h\u00e9sitez pas \u00e0 jouer avec de notions non abord\u00e9es dans ce TP (e.g. r\u00e9gularisation). Testez votre mod\u00e8le !**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "metadata": {}, "source": ["####### Complete this part ######## or die ####################\n", "from sklearn.pipeline import make_pipeline\n", "from sklearn.preprocessing import StandardScaler\n", "\n", "model = ...\n", "\n", "model.fit(...\n", "###############################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "CwY_Em-XaBCM"}, "source": ["### E. model test"]}, {"cell_type": "code", "metadata": {}, "source": ["####### Complete this part ######## or die ####################\n", "print(\"L'accuracy de notre modele est\", model.score(...\n", "###############################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["n_test_visu = 10\n", "\n", "print(\"On teste le modele sur\", n_test_visu, \"images de test selectionnees aleatoirement\")\n", "\n", "idx = [i for i in range(X_test.shape[0])]\n", "\n", "np.random.shuffle(idx)\n", "\n", "predicted = model.predict(X_test[idx][:n_test_visu])\n", "\n", "plt.figure(figsize=(12, 17.14), dpi=100)\n", "for i in range(n_test_visu):\n", "    plt.subplot(n_test_visu,1,i+1)\n", "    plt.title('Predicted:' + str(predicted[i]))\n", "    plt.imshow(X_test[idx][i,:].reshape([28,28]), cmap='gray')\n", "    plt.axis('off')\n", "    plt.subplots_adjust(hspace=1)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "rbONfxiCaBCM"}, "source": ["## VIII. Iterative Re-Weighted Least Square (IRWLS)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Contrairement au cas de la regression lin\u00e9aire, il n'est ici plus possible de r\u00e9soudre le probl\u00e8me analytiquement, comme nous l'avons vu (i.e. en annulant le gradient). Nous allons cependant voir une autre approche it\u00e9rative d'optimisation comme alternative \u00e0 la descente de gradient qui va nous permettre de faire des ponts avec la solution analytique de la regression lin\u00e9aire. Il s'agit ici d'utiliser une m\u00e9thode d'optimisation de type Newton que vous avez peut-\u00eatre vue dans les exercices d'approfondissement."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### A. M\u00e9thode de type Newton"]}, {"cell_type": "markdown", "metadata": {}, "source": ["La m\u00e9thode de Newton consiste \u00e0 considerer une fonction (qu'on suppose $C^{n}$) par son approximation au voisinage de $\\beta_0$ (notre point d'initialisation) par une expansion de Taylor:\n", "\n", "$$\n", "  f(\\beta) = f(\\beta_0)\n", "  + \\frac{f'(\\beta_0)}{1!}(\\beta - \\beta_0)\n", "  + \\frac{f^{(2)}(\\beta_0)}{2!}(\\beta - \\beta_0)^2\n", "  + \\cdots\n", "  + \\frac{f^{(n)}(a)}{n!}(\\beta - \\beta_0)^n\n", "  + R_n(\\beta)\n", "$$\n", "\n", "C'est \u00e0 dire, de mani\u00e8re g\u00e9n\u00e9rale:\n", "\n", "$$f(\\beta) = \\sum_{k=0}^n \\frac{f^{(k)}(\\beta_0)}{k!}(\\beta-\\beta_0)^k + R_n(\\beta)$$\n", "\n", "o\u00f9 $R_n(\\beta)$ est le r\u00e9sidu qui est n\u00e9gligeable par rapport \u00e0 $(\\beta-\\beta_0)^{n}$. C'est \u00e0 dire $R_n(\\beta)=o((\\beta-\\beta_0)^n) \\Leftrightarrow \\lim_{\\beta\\to \\beta_0\\atop \\beta\\ne \\beta_0}\\frac{R_n(\\beta)}{(\\beta-\\beta_0)^n}=0$. \n", "\n", "On a donc:\n", "\n", "$$f(\\beta) \\approx \\sum_{k=0}^n \\frac{f^{(k)}(\\beta_0)}{k!}(\\beta-\\beta_0)^k $$\n", "\n", "qu'on d\u00e9finira comme l'approximation de Taylor \u00e0 l'ordre $n$. La m\u00e9thode de Newton consiste \u00e0 approximer la fonction qu'on veut minimiser par son expression \u00e0 l'ordre 2:\n", "\n", "$$  f(\\beta) \\approx f(\\beta_0)\n", "  + f'(\\beta_0)(\\beta - \\beta_0)\n", "  + \\frac{f^{(2)}(\\beta_0)}{2!}(\\beta - \\beta_0)^2$$\n", "\n", "  Puis de r\u00e9soudre l'annulation du gradient de cette approximation locale. On trouve donc une valeur qui annule le gradient qu'on nomme $\\beta_1$:\n", "\n", "$$  f'(\\beta) = f'(\\beta_0) + f^{(2)}(\\beta_0)(\\beta_1 - \\beta_0) = 0 \\Leftrightarrow \\beta_1 = \\beta_0 -\\frac{f'(\\beta_0)}{f^{(2)}(\\beta_0)}$$\n", "\n", "  on exprime ensuite \u00e0 nouveau l'approximation \u00e0 l'ordre 2 au voisinage de $\\beta_1$ et on recommence. On construit ainsi un algorithme it\u00e9ratif pour produire une s\u00e9quences de solutions convergeants vers celle qui minimise localement la fonction de co\u00fbt (i.e. la fonction objectif). \n", "\n", "  Cette proc\u00e9dure se g\u00e9n\u00e9ralise tr\u00e8s bien pour des fonctions \u00e0 plusieurs variables ou l'approximation de Taylor \u00e0 l'odre 2 est donn\u00e9e par :\n", "\n", "$$ f(\\boldsymbol{\\beta}) \\approx f(\\boldsymbol{\\beta_0})\n", "  + (\\boldsymbol{\\beta} - \\boldsymbol{\\beta_0})^T\\nabla_{\\boldsymbol{\\beta_0}}f\n", "  + \\frac{1}{2}(\\boldsymbol{\\beta} - \\boldsymbol{\\beta_0})^T\\big[H_{\\boldsymbol{\\beta_0}}\\big](\\boldsymbol{\\beta} - \\boldsymbol{\\beta_0})$$\n", "\n", "  o\u00f9 $\\nabla_{\\boldsymbol{\\beta_0}}f$ et $H_{\\boldsymbol{\\beta_0}}$ sont respeictvement le gradient et la matrice Hessienne de f calcul\u00e9es en $\\boldsymbol{\\beta_0}$. Et on obtient une it\u00e9ration de la forme:\n", "\n", "\n", "$$  \\boldsymbol{\\beta_{t+1}} = \\boldsymbol{\\beta_t} -\\big[H_{\\boldsymbol{\\beta_0}}\\big]^{-1}\\nabla_{\\boldsymbol{\\beta_0}}f$$\n", "\n", "\n", "**<span style='color:blue'> Remarque</span>** ", "\n", "Cela ressemble aux it\u00e9ration de l'algorithme de descente de gradient ou l'inverse de la matrice hessienne jouerait le r\u00f4le du learning rate (si on remplace $\\big[H_{\\boldsymbol{\\beta_0}}\\big]^{-1}$ par $\\rho \\big[\\boldsymbol{I}\\big]$ on retrouve bien la m\u00eame chose). Intuitivement, les m\u00e9thode du second ordre sont en quelque sorte adaptative en fonction de la courbure local de la fonction de cout. Plus la fonction est courb\u00e9 localement, plus le pas de gradient sera petit, et vice versa. Ce genre d'approche permettant d'adapter le learning rate en fonction de la courbure a pour avantage de converger plus rapidement mais au prix couteux d'une inversion de la matrice Hessienne qui peut \u00eatre lourd mais aussi instable num\u00e9riquement. De nombreuses m\u00e9thodes on \u00e9t\u00e9 d\u00e9velopp\u00e9es pour approximer cette matrice hessienne par une matrice plus creuses et dont l'inversion est plus stable (m\u00e9thode de type approximation quasi-diagonale).\n", "\n", "\n\n ----"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### B. Application au cas de la regression logistique"]}, {"cell_type": "markdown", "metadata": {}, "source": ["On a d\u00e9j\u00e0 vu au dessus l'expression du gradient de la fonction de cout associ\u00e9 \u00e0 la regression logistique :\n", "\n", "$$\\nabla f(\\boldsymbol{\\beta})=\\frac{1}{n}X^T(\\sigma(X\\boldsymbol{\\beta})-\\boldsymbol{y})= \\frac{1}{n}X^T(\\hat{\\boldsymbol{y}}-\\boldsymbol{y})$$\n", "\n", "\n", "On peut montrer que la hessienne peut s'exprimer comme :\n", "\n", "$$\\boldsymbol{H} = \\frac{1}{n}\\sum_{i=0}^n \\boldsymbol{x_i}\\big(\\sigma(\\boldsymbol{x_i}^T\\boldsymbol{\\beta})(1-\\sigma(\\boldsymbol{x_i}^T\\boldsymbol{\\beta}))\\big)\\boldsymbol{x_i}^T = \\frac{1}{n} X^TWX$$\n", "\n", "\n", "O\u00f9 l'on note $W_t$ la matrice diagonale dont le $i$-eme terme sur la diagonale vaut $\\sigma(\\boldsymbol{x_i}^T\\boldsymbol{\\beta})(1-\\sigma(\\boldsymbol{x_i}^T\\boldsymbol{\\beta}) = \\hat{y_i}(1-\\hat{y_i})$. On a donc une expression pour l'it\u00e9ration:\n", "\n", "$$  \\boldsymbol{\\beta_{t+1}} = \\boldsymbol{\\beta_t} - [X^TW_tX]^{-1}X^T(\\hat{\\boldsymbol{y}}-\\boldsymbol{y})$$\n", "\n", "**<span style='color:blue'> Remarque</span>** ", "\n", "On trouve une expression tr\u00e8s analogue \u00e0 celle des \u00e9quations normales de la regression lin\u00e9aire si ce n'est qu'on doit appliquer cette op\u00e9ration de mani\u00e8re it\u00e9rative (en pratique peut d'it\u00e9rations suffisent comme c'est une m\u00e9thode d'ordre 2). C'est d'ailleurs pour cette raison que l'on utilise l'expression Iterative Reweighted Least Square pour cette m\u00e9thode d'optimisation dans le cadre de la regression logistique, car cela revient quasiment \u00e0 la m\u00eame chose en rempla\u00e7ant la cible y par l'erreur de prediction et en ajoutant une matrice de poids W. Cette matrice va clairement jouer sur la singularit\u00e9/le conditionnement de la matrice \u00e0 inverser. Le probl\u00e8me qu'on avait pr\u00e9c\u00e9demment pour la regression lin\u00e9aire sur le conditionnement d\u00e9pendait uniquement du fait que diff\u00e9rents exemples pouvait \u00eatre trop \"corr\u00e9l\u00e9s\", ici la matrice W peut venir soit am\u00e9liorer le conditionnement de $X^TX$ ou bien l'empirer. D'un cot\u00e9 les le termes $\\hat{y_i}(1 - y_i)$ font converger, mais comme ils aparaissent aussi dans $W_t$ qui est en inverse dans l'expression, si des points commencent \u00e0 \u00eatre bien pr\u00e9dit, ils peuvent provoquer l'explosion de certains coefficients de la solution du fait de la singularit\u00e9 de $[X^TW_tX]$. Une technique de r\u00e9gularisation (\u00e0 la ridge) peut donc vite \u00eatre n\u00e9cessaire.\n", "\n", "\n\n ----"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**<span style='color:blue'> Question</span>** ", "\n", "Proposez une m\u00e9thode $\\texttt{hessian}$ dans l'objet $\\texttt{CrossEntropy}$ permettant de calculer la hessienne et, \u00e0 l'instar de l'objet $\\texttt{GradientDescent}$ qui vous est adress\u00e9 en bas, construisez un objet $\\texttt{Newton}$ qui impl\u00e9mente la m\u00e9thode IRWLS vu dans cette section. Testez votre code.\n", "\n", "\n\n ----"]}, {"cell_type": "code", "metadata": {}, "source": ["from scipy.special import expit as sigmoid\n", "\n", "class CrossEntropyNewton(object):\n", "    def __init__(self, X, y):\n", "        self.X = X\n", "        self.y = y\n", "        self.idx = np.array([i for i in range(self.X.shape[0])])\n", "        self._pos = 0\n", "        \n", "    def _format_ndarray(arr):\n", "        arr = np.array(arr) if type(arr) is not np.ndarray else arr\n", "        return arr.reshape((arr.shape[0], 1)) if len(arr.shape) == 1 else arr\n", "    \n", "    def predict(self, X):\n", "        y_pred = (1+np.exp(-np.dot(X, self.beta)))**(-1)\n", "\n", "        return y_pred\n", "    \n", "    def val(self, beta):\n", "        beta = CrossEntropy._format_ndarray(beta)\n", "        p = sigmoid(np.dot(X, beta))\n", "        log_p = -np.concatenate([np.log(1-p), np.log(p)], axis=1)[np.arange(len(self.X)), y]\n", "        return log_p.sum()/len(self.X)\n", "    \n", "    \n", "    def grad(self, beta):\n", "        X, y = self.X, self.y\n", "        y = CrossEntropy._format_ndarray(y)\n", "\n", "        beta = CrossEntropy._format_ndarray(beta)\n", "\n", "        grad = np.dot(X.T, sigmoid(np.dot(X, beta))   - y)\n", "        return grad\n", "    \n", "    def hessian(self, beta):\n", "        ####### Complete this part ######## or die ####################\n", "        ...\n", "        ...\n", "        ...\n", "        ###############################################################\n", "        return XWX\n", "    \n", "X, y = perfect_data_sampler(1000, real_beta)\n", "\n", "\n", "loss = CrossEntropyNewton(X, y)\n", "beta = np.random.uniform(-4, 4, size=3).reshape((3, 1))\n", "print('Hessian:\\n', loss.hessian(beta))\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["class Newton(object):\n", "    init = np.random.uniform(-4, 4, size=3).reshape((3, 1))\n", "    def __init__(self, X, y, loss=CrossEntropyNewton):\n", "        self.loss = loss(X, y)\n", "        \n", "    def optimize(self, nb_iterations=10, beta=init):\n", "        param_trace = [beta.T[0]]\n", "        loss_trace = [self.loss.val(beta)]\n", "        for i in range(nb_iterations):\n", "            ####### Complete this part ######## or die ####################\n", "            beta = ...\n", "            param_trace.append(beta.T[0])\n", "            loss_trace.append(self.loss.val(beta))\n", "            ###############################################################\n", "            \n", "        return param_trace, loss_trace\n", "\n", "\n", "X, y = perfect_data_sampler(100, real_beta)\n", "init = np.zeros((3, 1))\n", "    \n", "gd = GradientDescent(X, y)\n", "newton = Newton(X, y)\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["loss_gd = []\n", "loss_newton = []\n", "it = 50\n", "params, loss_gd = gd.optimize(nb_iterations=it, learning_rate=2., beta=init)\n", "params, loss_newton = newton.optimize(nb_iterations=it, beta=init)\n", "plt.figure()\n", "plt.title(\"\u00c9volution de la loss en fonction du nombre d'it\u00e9rations\")\n", "plt.plot([i+1 for i in range(len(loss_gd))], loss_gd, label='GD')\n", "plt.plot([i+1 for i in range(len(loss_gd))], loss_newton, label='Newton')\n", "plt.legend()\n", "plt.show()\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["On peut jouer avec le learning rate pour se rendre compte de l'efficacit\u00e9 de la m\u00e9thode \"Newton\". Un learning rate trop fort pour la descente de gradient la rendra instable."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## IX. Lien entre r\u00e9gression et classification\n", "\n", "Lorsque nous sommes face \u00e0 un probl\u00e8me de classification, notre objectif est de trouver une application de $\\mathcal{X}\\subseteq\\mathbb{R}^d$ dans $\\mathcal{Y}=\\{1,\\ldots, C\\}$, notre ensemble de classes. Nous ne cherchons ce pendant pas directement cette fonction mais plut\u00f4t un estimateur $\\eta_n$ (l'indice $n$ indique la d\u00e9pendance dans la taille du jeu de donn\u00e9es) de la probabilit\u00e9 conditionnelle $\\eta_k(x)=\\mathbb{P}(Y=k|X=x)$. Une fois cet estimateur obtenu, nous pouvons construire ce qu'on appelle la *plugin rule*&nbsp;:\n", "\n", "$$\\begin{aligned}\\hat{g}_n=\\begin{cases}1\\text{ si }\\hat{\\eta}_n(x)\\geq 0.5\\\\0\\text{ sinon.}\\end{cases}\\end{aligned}$$\n", "\n", "Est-ce une bonne strat\u00e9gie ? La r\u00e9ponse est oui et nous allons montrer un th\u00e9or\u00e8me allant dans ce sens. Consid\u00e9rons dans un premier temps la figure suivante."]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["remove-input"]}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "from scipy.special import expit as sigmoid\n", "\n", "steps = 501\n", "\n", "x = np.linspace(-1, 1, steps)\n", "y = sigmoid(5*x)\n", "y_hat = sigmoid(2*(x-0.15)-2.5*x**2+8*x**3+2*(x+0.3)**4)\n", "g = y>=0.5\n", "g_hat = y_hat <= 0.5\n", "\n", "ambiguous_region = g*g_hat\n", "# print(x[ambiguous_region])\n", "\n", "limit = [0.5 for _ in range(steps)]\n", "\n", "plt.figure(figsize=(12, 8))\n", "plt.plot(x, y, label='$\\eta(x)$')\n", "plt.plot(x, y_hat, label='$\\hat{\\eta}(x)$')\n", "plt.plot(x, limit, label='$0.5$')\n", "plt.fill_between(\n", "    x, y, y_hat, where=ambiguous_region,\n", "    facecolor='red', interpolate=False, alpha=0.3,\n", "    label='Erreur de classification'\n", ")\n", "\n", "plt.xlabel('$\\mathcal{X}\\subseteq\\mathbb{R}$')\n", "\n", "#plt.vlines(x=0, ymin=0, ymax=1)\n", "#plt.vlines(x=0.12, ymin=0, ymax=1)\n", "\n", "plt.title('Erreur de r\u00e9gression versus erreur de classification')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["On observe que bien que notre estimateur de la probabilit\u00e9 conditionnelle $\\hat{\\eta}$ soit imparfait partout, seule la r\u00e9gion o\u00f9 il se situe du mauvais c\u00f4t\u00e9 de $y=0.5$ entra\u00eene une erreur de classification vis-\u00e0-vis du classifieur de Bayes (i.e. la *plugin rule* associ\u00e9e \u00e0 la vraie probabilit\u00e9 conditionnelle). On sent donc l'id\u00e9e que notre erreur de classificaiton pourra se r\u00e9duire m\u00eame si nous avons du mal \u00e0 bien estimer notre r\u00e9gression sur la probabilit\u00e9 conditionnelle. C'est ce que montre le th\u00e9or\u00e8me suivant.\n", "\n", "**<span style='color:blue'> Th\u00e9or\u00e8me (La classification est plus facile que la r\u00e9gression)</span>** ", "Soit $\\hat{\\eta}_n$ notre estimateur de la vraie probabilit\u00e9 conditionnelle $\\eta$ o\u00f9 l'indice $n$ indique la d\u00e9pendance \u00e0 un jeu de donn\u00e9es de taille $n$. Supposons $\\hat{\\eta}_n$ construit de mani\u00e8re \u00e0 ce que nous ayons une convergence faible au sens suivant&nbsp;:\n", "\n", "$$\\lim_{n\\rightarrow\\infty}\\big[(\\hat{\\eta}_n(X)-\\eta(X))^2\\big]=0.$$\n", "\n", "Construisons les *plugin rules* suivantes&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "\\hat{g}_n(x)=\\begin{cases}1\\text{ si }\\hat{\\eta}_n(x)\\geq 0.5\\\\0\\text{ sinon}\\end{cases}\\text{ et }g(x)=\\begin{cases}1\\text{ si }\\eta(x)\\geq 0.5\\\\0\\text{ sinon.}\\end{cases}\n", "\\end{aligned}$$\n", "\n", "$\\hat{g}_n$ est la r\u00e8gle de classification que nous avons construite et $g$ le classifieur de Bayes. Notons&nbsp;:\n", "\n", "$$L(g)=\\mathbb{E}\\big[\\textbf{1}\\{g(X)\\neq Y\\}\\big].$$\n", "\n", "Nous avons alors&nbsp;:\n", "\n", "$$\\lim_{n\\rightarrow\\infty}\\frac{\\mathbb{E}\\big[L(\\hat{g}_n)\\big]-L(g)}{\\sqrt{\\mathbb{E}\\big[(\\hat{\\eta}_n(X)-\\eta(X))^2\\big]}}=0,$$\n", "\n", "o\u00f9 l'esp\u00e9rance est prise sur le jeu de donn\u00e9es.\n", "\n", "\n\n ----", "\n", "**<span style='color:orange'> Preuve</span>** ", "\n", "Supposons $\\mathbb{E}\\big[L(\\hat{g}_n)\\big]-L(g)=2\\mathbb{E}\\big[|\\eta(X)-1/2|\\mathbf{1}\\{\\hat{g}_n(X)\\neq g(X)\\}\\}\\big]$. Cette \u00e9galit\u00e9 est d\u00e9montr\u00e9e dans la s\u00e9quence sur le [classifieur de Bayes](./3_bayes_classifier.ipynb).\n", "\n", "Soit $\\epsilon>0$. Nous avons&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "\\mathbb{E}\\big[&|\\eta(X)-1/2|\\mathbf{1}\\{\\hat{g}_n(X)\\neq g(X)\\}\\}\\big]\\leq \\mathbb{E}\\big[\\mathbf{1}\\{\\eta(X)\\neq 1/2\\}|\\eta(X)-\\hat{\\eta}_n(X)|\\mathbf{1}\\{\\hat{g}_n(X)\\neq g(X)\\}\\}\\big]\\\\\n", "&=\\mathbb{E}\\big[\\mathbf{1}\\{\\eta(X)\\neq 1/2\\}\\mathbf{1}\\{|\\eta(X)-1/2|\\leq \\epsilon\\}|\\eta(X)-\\hat{\\eta}_n(X)|\\mathbf{1}\\{\\hat{g}_n(X)\\neq g(X)\\}\\}\\big]\\\\\n", "&\\ \\ \\ \\ +\\mathbb{E}\\big[\\mathbf{1}\\{|\\eta(X)-1/2|> \\epsilon\\}\\mathbf{1}\\{\\eta(X)\\neq 1/2\\}|\\eta(X)-\\hat{\\eta}_n(X)|\\mathbf{1}\\{\\hat{g}_n(X)\\neq g(X)\\}\\}\\big]=(\\star)\n", "\\end{aligned}$$\n", "\n", "Appliquons l'in\u00e9galit\u00e9 de [Cauchy-Scharz](https://fr.wikipedia.org/wiki/In\u00e9galit\u00e9_de_Cauchy-Schwarz) et majorons $\\mathbf{1}\\{\\hat{g}_n(X)\\neq g(X)\\}$ par $1$ sur le premier terme&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "(\\star)\\leq &\\sqrt{\\mathbb{E}\\big[(\\hat{\\eta}_n(X)-\\eta(X))^2\\big]}\\Bigg(\\sqrt{\\mathbb{P}\\big(|\\eta(X)-1/2|\\leq \\epsilon, \\eta(X)\\neq 1/2\\big)}\n", "\\\\&\\ \\ \\ + \\sqrt{\\mathbb{P}\\big(\\hat{g}_n(X)\\neq g(X), |\\eta(X)-1/2|>\\epsilon\\big)}\\Bigg)\n", "\\end{aligned}$$\n", "\n", "Remarquons que si $\\hat{g}_n(X)\\neq g^\\star(X)$ et que $|\\eta(X)-1/2|>\\epsilon$, alors $\\hat{\\eta}_n$ est de \"l'autre c\u00f4t\u00e9\" de $1/2$ et on a n\u00e9cessairement $|\\eta_n(X)-\\eta(X)|>\\epsilon$. Cependant, $\\hat{\\eta}_n$ est consistent. Cela implique que la probabilit\u00e9 de l'\u00e9v\u00e8nement $|\\eta_n(X)-\\eta(X)|>\\epsilon$ tend vers $0$ pour $\\epsilon$ fix\u00e9 et&nbsp;:\n", "\n", "$$\\lim_{n\\rightarrow\\infty}\\mathbb{P}\\big(\\hat{g}_n(X)\\neq g(X), |\\eta(X)-1/2|>\\epsilon\\big)=0$$\n", "\n", "D'un autre c\u00f4t\u00e9, \n", "\n", "$$\\lim_{\\epsilon\\rightarrow 0}. \\mathbb{P}\\big(|\\eta(X)-1/2|\\leq \\epsilon, \\eta(X)\\neq 1/2\\big)=0$$\n", "\n", "Il suffit de reprendre le tout et diviser notre in\u00e9galit\u00e9 par $\\sqrt{\\mathbb{E}\\big[(\\hat{\\eta}_n(X)-\\eta(X))^2\\big]}$ pour constater qu'on majore notre fraction par un terme qui converge vers $0$.\n", "\n", "\n\n ----", "\n", "Dit autrement, l'erreur de classification d\u00e9cro\u00eet plus vite que l'erreur $\\ell_2$ sur la r\u00e9gression. Il est beaucoup plus facile de construire un classifieur que d'avoir une \"id\u00e9e\" pr\u00e9cise de la probabilit\u00e9 conditionnelle."]}], "metadata": {"colab": {"collapsed_sections": [], "name": "TP 2 - Logistic regression.ipynb", "provenance": [], "toc_visible": true}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.2"}}, "nbformat": 4, "nbformat_minor": 1}
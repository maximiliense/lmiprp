
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Un modÃ¨le formel de lâ€™apprentissage â˜•ï¸â˜•ï¸â˜•ï¸â˜•ï¸ (ğŸ’†â€â™‚ï¸) &#8212; Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Les mÃ©thodes Ã  noyaux" href="../4_kernel_methods/0_propos_liminaire.html" />
    <link rel="prev" title="Le classifieur de Bayes â˜•ï¸" href="3_bayes_classifier.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-72WWYCKNK6"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-72WWYCKNK6');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../0_requisite/rappels_python.html">
   Rappels de Python et Numpy
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../1_what_is_ml/0_propos_liminaire.html">
   <em>
    Machine Learning
   </em>
   , initiation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/1_introduction_ml.html">
     <em>
      Machine learning
     </em>
     et malÃ©diction de la dimension
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/2_regression_and_classification_trees.html">
     Les arbres de rÃ©gression et de classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../2_regression/0_propos_liminaire.html">
   La
   <em>
    rÃ©gression
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/1_linear_regression.html">
     La rÃ©gression linÃ©aire â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/2_optimization.html">
     Lâ€™optimisation â˜•ï¸â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/3_interpolation.html">
     Interpolation â˜•ï¸â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/4_algo_proximal_lasso.html">
     Sous-diffÃ©rentiel et le cas du Lasso â˜•ï¸â˜•ï¸â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/5_least_square_qr.html">
     Les moindres carrÃ©s via une dÃ©composition QR (et plus)â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/6_ridge.html">
     Une analyse de la rÃ©gularisation Ridge â˜•ï¸â˜•ï¸â˜•ï¸
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="0_propos_liminaire.html">
   La
   <em>
    classification
   </em>
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="1_logistic_regression.html">
     La rÃ©gression logistique â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="2_fonctions_proxy.html">
     Les fonctions de perte (loss function) â˜•ï¸â˜•ï¸â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3_bayes_classifier.html">
     Le classifieur de Bayes â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Un modÃ¨le formel de lâ€™apprentissage â˜•ï¸â˜•ï¸â˜•ï¸â˜•ï¸ (ğŸ’†â€â™‚ï¸)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../4_kernel_methods/0_propos_liminaire.html">
   Les mÃ©thodes Ã  noyaux
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../4_kernel_methods/1_svm.html">
     Le SVM ou lâ€™hypothÃ¨se max-margin â˜•ï¸â˜•ï¸
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5_ensembles/0_propos_liminaire.html">
   Les mÃ©thodes
   <em>
    ensemblistes
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/1_ensembles.html">
     MÃ©thodes ensemblistes â˜•ï¸â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/2_bayesian_linear_regression.html">
     Bayesian linear regression â˜•ï¸â˜•ï¸â˜•ï¸â˜•ï¸
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../6_deeplearning/0_propos_liminaire.html">
   <em>
    deep learning
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/1_autodiff.html">
     La diffÃ©rentiation automatique et un dÃ©but de
     <em>
      deep learning
     </em>
     â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/2_filters_representation.html">
     Filtres et espace de reprÃ©sentation des rÃ©seaux de neurones â˜•ï¸â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/3_probabilities_calibration.html">
     Calibration des probabilitÃ©s et quelques notions â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/4_regularization_deep.html">
     RÃ©gularisation en
     <em>
      deep learning
     </em>
     â˜•ï¸â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/5_transfer_multitask.html">
     <em>
      Transfer learning
     </em>
     et apprentissage multi-tÃ¢ches â˜•ï¸â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/6_adversarial.html">
     Les attaques adversaires â˜•ï¸
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../7_unsupervised/0_propos_liminaire.html">
   Lâ€™apprentissage non-supervisÃ©
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_unsupervised/1_principal_component_analysis.html">
     Lâ€™Analyse en Composantes Principales â˜•ï¸â˜•ï¸â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_unsupervised/2_em_gaussin_mixture_model.html">
     ModÃ¨le de MÃ©lange Gaussien et algorithme
     <em>
      Expectation-Maximization
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../8_set_prediction/0_propos_liminaire.html">
   PrÃ©diction dâ€™ensembles
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/1_well_defined.html">
     Jeu dâ€™apprentissage
     <em>
      set-valued
     </em>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/2_ill_defined.html">
     Jeu dâ€™apprentissage uniquement multi-classes â˜•ï¸
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../biblio.html">
   Bibliographie
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/3_classification/4_VC_theory.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/maximiliense/lmiprp"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/maximiliense/lmiprp/issues/new?title=Issue%20on%20page%20%2F3_classification/4_VC_theory.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/maximiliense/lmiprp/blob/master/Travaux Pratiques/Machine Learning/Book/3_classification/4_VC_theory.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#i-inegalites-de-concentration-et-union-bound">
   I. InÃ©galitÃ©s de concentration et
   <em>
    union bound
   </em>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-inegalites-de-concentration">
     A. InÃ©galitÃ©s de concentration
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-union-bound">
     B.
     <em>
      Union bound
     </em>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#c-reverse-union-bound">
     C.
     <em>
      Reverse union bound
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ii-en-supposant-mathcal-h-infty">
   II. En supposant
   <span class="math notranslate nohighlight">
    \(|\mathcal{H}|&lt;\infty\)
   </span>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-aucune-hypothese-sur-eta">
     A. Aucune hypothÃ¨se sur
     <span class="math notranslate nohighlight">
      \(\eta\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-en-supposant-eta-in-0-1">
     B. En supposant
     <span class="math notranslate nohighlight">
      \(\eta\in\{0, 1\}\)
     </span>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iii-le-cas-general-mathcal-h-infty">
   III. Le cas gÃ©nÃ©ral :
   <span class="math notranslate nohighlight">
    \(|\mathcal{H}|=\infty\)
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iv-la-dimension-vc">
   IV. La dimension VC
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#v-une-formule-generale">
   V. Une formule gÃ©nÃ©rale
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vi-un-resultat-avec-une-marge-et-mathcal-h-infty">
   VI. Un rÃ©sultat avec une marge (et
   <span class="math notranslate nohighlight">
    \(|\mathcal{H}|&lt;\infty\)
   </span>
   )
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vii-la-sample-complexity">
   VII. La
   <em>
    sample complexity
   </em>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#viii-pac-learnability-et-theoreme-fondamental-du-machine-learning">
   VIII. PAC
   <em>
    learnability
   </em>
   et thÃ©orÃ¨me fondamental du
   <em>
    machine learning
   </em>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ix-decidabilite-de-l-apprentissage">
   IX. DÃ©cidabilitÃ© de lâ€™apprentissage
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#x-l-approche-train-test-ou-validation-croisee">
   X. Lâ€™approche train-test (ou validation croisÃ©e)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#xi-en-conclusion">
   XI. En conclusion
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-difficulte-epistemique-incertitude-du-modele">
     A. DifficultÃ© Ã©pistÃ©mique (incertitude du modÃ¨le)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-difficulte-aleatorique-incertitude-des-donnees">
     B. DifficultÃ© alÃ©atorique (incertitude des donnÃ©es)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#c-autres-mesures-de-complexite">
     C. Autres mesures de complexitÃ©
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="un-modele-formel-de-l-apprentissage">
<h1>Un modÃ¨le formel de lâ€™apprentissage â˜•ï¸â˜•ï¸â˜•ï¸â˜•ï¸ (ğŸ’†â€â™‚ï¸)<a class="headerlink" href="#un-modele-formel-de-l-apprentissage" title="Permalink to this headline">Â¶</a></h1>
<div class="admonition-objectifs-de-la-sequence admonition">
<p class="admonition-title">Objectifs de la sÃ©quence</p>
<ul class="simple">
<li><p>ComprendreÂ :</p>
<ul>
<li><p>les principes liÃ©s Ã  la minimisation du risque empirique,</p></li>
<li><p>les quantitÃ©s qui entrent en jeu dans lâ€™apprentissage et entraÃ®nent le sur-apprentissage,</p></li>
<li><p>les raisons qui peuvent rendrent lâ€™apprentissage difficile.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">Â¶</a></h2>
<p>La thÃ©orie de Vapnik et Chervonenkis, ou thÃ©orie VC, cherche Ã  expliquer via des approches statistiques pourquoi lâ€™apprentissage automatique fonctionne dans certains cas. Cette section du cours de <em>machine learning</em>, plus thÃ©orique, nous permettra de dÃ©montrer plusieurs rÃ©sultats fondamentaux dans le cas dâ€™un problÃ¨me de classification binaire. Ces rÃ©sultats se gÃ©nÃ©ralisent bien entendu Ã  dâ€™autres problÃ¨mes de <em>machine learning</em>.</p>
<p>Notons <span class="math notranslate nohighlight">\(\mathcal{X}\subseteq\mathbb{R}^d\)</span> notre espace dâ€™entrÃ©e de dimension <span class="math notranslate nohighlight">\(d\)</span> et <span class="math notranslate nohighlight">\(\mathcal{Y}=\{0, 1\}\)</span> lâ€™espace de nos labels que nous restreindrons dans cette section au cas binaire. Notons <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> lâ€™ensemble des fonctions de <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> dans <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> parmi lesquelles nous voulons rÃ©aliser notre apprentissage. Comme toujours, notre objectif est de trouver une fonction <span class="math notranslate nohighlight">\(h\in\mathcal{H}\)</span> qui fait peu dâ€™erreur relativement au risque suivantÂ :</p>
<div class="math notranslate nohighlight">
\[L(h)=\mathbb{E}\big[\textbf{1}\{h(X)\neq Y\}\big]=\mathbb{P}\big(h(X)\neq Y\big).\]</div>
<p>Câ€™est tout simplement la probabilitÃ© que notre fonction <span class="math notranslate nohighlight">\(h\in\mathcal{H}\)</span> prÃ©dise le mauvais label pour un <span class="math notranslate nohighlight">\(X\)</span> observÃ©. Bien sÃ»r, nous ne connaissons pas le processus gÃ©nÃ©rateurs de nos donnÃ©es ni le modÃ¨le probabiliste le dÃ©crivant. Pour cela, nous devons estimer ce risque en Ã©chantillonnant. Notons <span class="math notranslate nohighlight">\(S_n=\{(X_i, Y_i)\}_{i\leq n}\sim\mathbb{P}^n\)</span> un jeu de donnÃ©es de taille <span class="math notranslate nohighlight">\(n\)</span> oÃ¹ les couples sont iid. Nous pouvons maintenant utiliser ce jeu de donnÃ©es afin de dÃ©terminer empiriquement le risque de chaque fonction <span class="math notranslate nohighlight">\(h\in\mathcal{H}\)</span> de la maniÃ¨re suivante :</p>
<div class="math notranslate nohighlight">
\[L_n(h)=\frac{1}{n}\sum_i \textbf{1}\{h(X_i)\neq Y_i\}\text{ oÃ¹ }\mathbb{E}\big[L_n(h)\big]=L(h)\]</div>
<p>Câ€™est juste le nombre moyen dâ€™erreurs commises par la fonction <span class="math notranslate nohighlight">\(h\)</span> sur le jeu de donnÃ©es <span class="math notranslate nohighlight">\(S_n\)</span>. Notre objectif devient donc :</p>
<div class="math notranslate nohighlight">
\[h_n=\text{argmin}_{h\in\mathcal{H}}L_n(h),\]</div>
<p>oÃ¹ on appellera <span class="math notranslate nohighlight">\(h_n\)</span> le minimiseur du risque empirique. Câ€™est la fonction dans <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> qui fait le moins dâ€™erreurs en moyenne sur notre jeu de donnÃ©es. On espÃ¨re quâ€™elle ne fera pas non plus beaucoup dâ€™erreurs sur le vrai risque <span class="math notranslate nohighlight">\(L\)</span>. Nous pouvons quantifier ce â€œmauvais choixâ€ via la formule suivante :</p>
<div class="math notranslate nohighlight">
\[L(h_n)-\inf_{h\in\mathcal{H}}L(h).\]</div>
<p>Câ€™est lâ€™Ã©cart entre le vrai risque de notre minimiseur du risque empirique et le vrai risque de la meilleure fonction dans <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>.</p>
<p>Lâ€™idÃ©e de la thÃ©orie VC est que finalement, si nous sommes capable dâ€™estimer prÃ©cisÃ©ment le risque de chacune des fonctions en utilisant notre jeu de donnÃ©es, <em>a priori</em>, nous ne devrions pas faire un mauvais choix en choisissant le minimiseur du risque empirique. Le lemme suivant valide cette idÃ©e :</p>
<div class="admonition-lemme-1 admonition">
<p class="admonition-title">Lemme 1</p>
<div class="math notranslate nohighlight">
\[L(h_n)-\inf_{h\in\mathcal{H}}L(h)\leq 2\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|\]</div>
</div>
<div class="caution dropdown admonition">
<p class="admonition-title">Preuve</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
L(h_n)-\inf_{h\in\mathcal{H}}L(h)&amp;=L(h_n)-L_n(h_n)+L_n(h_n)-\inf_{h\in\mathcal{H}}L(h)\\
&amp;\leq L(h_n)-L_n(h_n)+\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|\\
&amp;\leq 2\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|
\end{aligned}\end{split}\]</div>
</div>
<p>Dit autrement, <span class="math notranslate nohighlight">\(\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|\)</span> est la plus grosse erreur dâ€™estimation du risque sur notre jeu de donnÃ©es <span class="math notranslate nohighlight">\(S_n\)</span> dans notre classe de fonctions <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>. Cette derniÃ¨re permet de majorer le choix du minimiseur du risque empirique. NaÃ¯vement, si <span class="math notranslate nohighlight">\(\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|=0\)</span> (on estime parfaitement nos fonctions), alors <span class="math notranslate nohighlight">\(L(h_n)-\inf_{h\in\mathcal{H}}L(h)=0\)</span> et on choisit la meilleure fonction par rapport au vrai risque.</p>
<p>Lâ€™objectif de la thÃ©orie VC va Ãªtre de trouver des stratÃ©gies permettant de quantifier lâ€™Ã©volution de <span class="math notranslate nohighlight">\(\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|\)</span>. La calculer exactement Ã©tant impossible, nous allons donc la majorer par des quantitÃ©s plus simples Ã  estimer. Et si nos quantitÃ©s convergent vers <span class="math notranslate nohighlight">\(0\)</span>, alors <span class="math notranslate nohighlight">\(\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|\)</span>, en Ã©tant majorÃ© par quelque chose qui converge vers <span class="math notranslate nohighlight">\(0\)</span> et en restant positif devra nÃ©cessairement converger vers <span class="math notranslate nohighlight">\(0\)</span> Ã©galement.</p>
</div>
<div class="section" id="i-inegalites-de-concentration-et-union-bound">
<h2>I. InÃ©galitÃ©s de concentration et <em>union bound</em><a class="headerlink" href="#i-inegalites-de-concentration-et-union-bound" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="a-inegalites-de-concentration">
<h3>A. InÃ©galitÃ©s de concentration<a class="headerlink" href="#a-inegalites-de-concentration" title="Permalink to this headline">Â¶</a></h3>
<p>La thÃ©orie VC fait souvent appel Ã  ce quâ€™on appelle une inÃ©galitÃ© de concentration. Lâ€™idÃ©e dâ€™une inÃ©galitÃ© de concentration est de constater que nos variables alÃ©atoires ne peuvent finalement pas trop sâ€™Ã©carter de certaines valeurs oÃ¹ elles se retrouvent â€œconcentrÃ©esâ€. La plus simple dâ€™entre elle est <a class="reference external" href="https://fr.wikipedia.org/wiki/In%C3%A9galit%C3%A9_de_Markov">lâ€™inÃ©galitÃ© de Markov</a>:</p>
<div class="admonition-l-inegalite-de-markov admonition">
<p class="admonition-title">Lâ€™inÃ©galitÃ© de Markov </p>
<p>Soit <span class="math notranslate nohighlight">\(X\)</span> une variable alÃ©atoire rÃ©elle <strong>positive</strong>, nous avons :</p>
<div class="math notranslate nohighlight">
\[\forall a\in\mathbb{R}^+,\ \mathbb{P}\big(X\geq a\big)\leq \frac{\mathbb{E}\big[X\big]}{a}.\]</div>
</div>
<div class="caution dropdown admonition">
<p class="admonition-title">Preuve</p>
<p>Soit <span class="math notranslate nohighlight">\(a\in\mathbb{R}^+\)</span> et <span class="math notranslate nohighlight">\(\textbf{1}\{X\geq a\}\)</span> la fonction qui vaut <span class="math notranslate nohighlight">\(1\)</span> si <span class="math notranslate nohighlight">\(X\)</span> est supÃ©rieur ou Ã©gal Ã  <span class="math notranslate nohighlight">\(a\)</span> et <span class="math notranslate nohighlight">\(0\)</span> sinon. Nous avons nÃ©cessairement :</p>
<div class="math notranslate nohighlight">
\[a \textbf{1}\{X\geq a\}\leq X\textbf{1}\{X\geq a\}\leq X,\]</div>
<p>quelque soit la rÃ©alisation de <span class="math notranslate nohighlight">\(X\)</span>. En effet si <span class="math notranslate nohighlight">\(X&lt;a\)</span> alors <span class="math notranslate nohighlight">\(\textbf{1}\{X\geq a\}\)</span> vaut <span class="math notranslate nohighlight">\(0\)</span>. Lâ€™espÃ©rance Ã©tant croissante, nous avons :</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\Big[a\textbf{1}\{X\geq a\}\Big]\leq\mathbb{E}\Big[X\Big].\]</div>
<p>Par dÃ©finition de lâ€™espÃ©rance, nous avons :</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\Big[a\textbf{1}\{X\geq a\}\Big]=a \mathbb{P}\big(X\geq a\big) + 0\mathbb{P}\big(X&lt; a\big)=a \mathbb{P}\big(X\geq a\big).\]</div>
<p>Ainsi, en combinant le tout, nous avons :</p>
<div class="math notranslate nohighlight">
\[a \mathbb{P}\big(X\geq a\big)\leq \mathbb{E}\Big[X\Big]\Leftrightarrow \mathbb{P}\big(X\geq a\big)\leq \frac{\mathbb{E}\big[X\big]}{a}.\]</div>
</div>
<p>En dâ€™autres termes, pour une variable alÃ©atoire <span class="math notranslate nohighlight">\(X\)</span> de loi fixÃ©e sa masse ne se situe pas Ã  lâ€™infinie et plus <span class="math notranslate nohighlight">\(a\)</span> sera grand, plus il sera improbable que <span class="math notranslate nohighlight">\(X\)</span> soit au-delÃ .</p>
<p>Convainquons-nous du rÃ©sultat prÃ©cÃ©dent via une simulation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># on simule une variable uniforme entre 0 et 100</span>
<span class="k">def</span> <span class="nf">simulate_random_variable</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">nb_times</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">nb_times</span><span class="p">)</span>
    <span class="n">empirical_expectation</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">empirical_probability</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="o">&gt;</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="n">nb_times</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;P(X&gt;=a)&lt;= E[X]/a :&#39;</span><span class="p">,</span> <span class="n">empirical_probability</span><span class="p">,</span> <span class="s1">&#39;&lt;=&#39;</span><span class="p">,</span> <span class="n">empirical_expectation</span><span class="o">/</span><span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">simulate_random_variable</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">75</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>P(X&gt;=a)&lt;= E[X]/a : 0.242 &lt;= 0.6807126390404562
</pre></div>
</div>
</div>
</div>
<p>Et câ€™est lÃ  tout lâ€™intÃ©rÃªt ! Supposons que nous ne sachions pas calculer <span class="math notranslate nohighlight">\(\mathbb{P}\big(X\geq a)\)</span> mais que nous sachions calculer <span class="math notranslate nohighlight">\(\mathbb{E}\big[X\big]\)</span>. Nous pouvons alors garantir un â€œpire scÃ©narioâ€ de la vitesse Ã  la quelle la probabilitÃ© converge vers <span class="math notranslate nohighlight">\(0\)</span> lorsque <span class="math notranslate nohighlight">\(a\)</span> augmente.</p>
<p>ConsidÃ©rons une variable uniforme entre <span class="math notranslate nohighlight">\(0\)</span> et <span class="math notranslate nohighlight">\(100\)</span>. Son espÃ©rance et <span class="math notranslate nohighlight">\(\frac{100-0}{2}=50\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">999</span><span class="p">)</span>

<span class="n">probability</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">a</span><span class="o">/</span><span class="mi">100</span>
<span class="n">expectation</span> <span class="o">=</span> <span class="mi">50</span><span class="o">/</span><span class="n">a</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">probability</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$\mathbb</span><span class="si">{P}</span><span class="s1">(X\geq a)$ (inconnue)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">expectation</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\mathbb</span><span class="si">{E}</span><span class="s1">[X]/a$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4_VC_theory_6_0.png" src="../_images/4_VC_theory_6_0.png" />
</div>
</div>
<p>La quantitÃ© <span class="math notranslate nohighlight">\(\mathbb{E}\big[X\big]/a\)</span> est potentiellement supÃ©rieure Ã  <span class="math notranslate nohighlight">\(1\)</span> et nous avons contraint matplotlib Ã  nâ€™afficher que les valeurs entre <span class="math notranslate nohighlight">\(0\)</span> et <span class="math notranslate nohighlight">\(1\)</span>. Pour cette exemple, nous aurions pu trÃ¨s naÃ¯vement calculer la probabilitÃ© nous-mÃªme et nous nâ€™avions aucun intÃ©rÃªt Ã  passer par une inÃ©galitÃ© de concentration. Cependant, dans certains problÃ¨mes, câ€™est inÃ©vitable !</p>
</div>
<div class="section" id="b-union-bound">
<h3>B. <em>Union bound</em><a class="headerlink" href="#b-union-bound" title="Permalink to this headline">Â¶</a></h3>
<p>Nous avons tous vu au lycÃ©e la formule suivante :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\big(A\cup B\big)=\mathbb{P}\big(A\big)+\mathbb{P}\big(B\big)-\mathbb{P}\big(A\cap B\big).\]</div>
<p>Si on Ã©limine la probabilitÃ© liÃ©e Ã  lâ€™intersection, nous avons :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\big(A\cup B\big)\leq\mathbb{P}\big(A\big)+\mathbb{P}\big(B\big)\]</div>
<p>De maniÃ¨re plus gÃ©nÃ©rale, imaginons une famille dâ€™Ã©vÃ¨nements <span class="math notranslate nohighlight">\(A_i\)</span>, <span class="math notranslate nohighlight">\(i\leq N\)</span>, tels que <span class="math notranslate nohighlight">\(\forall i\leq N\)</span>, nous avons <span class="math notranslate nohighlight">\(\mathbb{P}(A_i)\leq K\)</span> (la probabilitÃ© est majorÃ©e par une quantitÃ© <span class="math notranslate nohighlight">\(K\)</span>). Nous avons alors lâ€™inÃ©galitÃ© suivante :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\big(\cup_i A_i\big)\leq \sum_i \mathbb{P}\big(A_i\big)\leq \sum_i K\leq KN.\]</div>
<p>Câ€™est ce quâ€™on appelle un <em>union bound</em> : on majore grÃ¢ce Ã  une inÃ©galitÃ© liÃ©e Ã  lâ€™union.</p>
</div>
<div class="section" id="c-reverse-union-bound">
<h3>C. <em>Reverse union bound</em><a class="headerlink" href="#c-reverse-union-bound" title="Permalink to this headline">Â¶</a></h3>
<p>Soient <span class="math notranslate nohighlight">\(A\)</span> et <span class="math notranslate nohighlight">\(B\)</span> deux Ã©vÃ¨nements. De maniÃ¨re Ã©tudiante, nous avonsÂ :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\big(\bar{A}\big)=1-\mathbb{P}\big(A\big)\]</div>
<p>Le <em>union bound</em> permettait de majorer lâ€™union dâ€™Ã©vÃ¨nements. Le <em>reverse union bound</em> permet de minorer la probabilitÃ© de lâ€™intersection dâ€™Ã©vÃ¨nements. Nous retrouvons cette formule de la maniÃ¨re suivanteÂ :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{P}\big(A\cap B\big)&amp;=1-\mathbb{P}\big(\overline{A\cap B}\big)\\
&amp;=1-\mathbb{P}\big(\overline{A}\cup\overline{B}\big)\\
&amp;\geq 1-\Big(\mathbb{P}\big(\overline{A}\big)+\mathbb{P}\big(\overline{B}\big)\Big)
\end{aligned}\end{split}\]</div>
<p>De maniÃ¨re plus gÃ©nÃ©rale, imaginons une famille dâ€™Ã©vÃ¨nements <span class="math notranslate nohighlight">\(A_i\)</span>, <span class="math notranslate nohighlight">\(i\leq N\)</span>, tels que <span class="math notranslate nohighlight">\(\forall i\leq N\)</span>, nous avons <span class="math notranslate nohighlight">\(\mathbb{P}(\overline{A_i})\leq K\)</span> (la probabilitÃ© est majorÃ©e par une quantitÃ© <span class="math notranslate nohighlight">\(K\)</span>). Nous avons alors lâ€™inÃ©galitÃ© suivante :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\big(\cap_i A_i\big)\geq 1- \Big(\sum_i \mathbb{P}(\overline{A_i})\Big)\geq 1-\sum_i K\geq 1-KN\]</div>
</div>
</div>
<div class="section" id="ii-en-supposant-mathcal-h-infty">
<h2>II. En supposant <span class="math notranslate nohighlight">\(|\mathcal{H}|&lt;\infty\)</span><a class="headerlink" href="#ii-en-supposant-mathcal-h-infty" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="a-aucune-hypothese-sur-eta">
<h3>A. Aucune hypothÃ¨se sur <span class="math notranslate nohighlight">\(\eta\)</span><a class="headerlink" href="#a-aucune-hypothese-sur-eta" title="Permalink to this headline">Â¶</a></h3>
<p>CommenÃ§ons par un commentaire sur une autre inÃ©galitÃ© trÃ¨s souvent utilisÃ©e.</p>
<p>Soit une famille de fonctions <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> de <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> dans <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> telle que <span class="math notranslate nohighlight">\(|\mathcal{H}|&lt;\infty\)</span>. Et soit <span class="math notranslate nohighlight">\(h\in\mathcal{H}\)</span>. Nous avons dÃ©fini le risque et le risque empirique notÃ©s respectivement <span class="math notranslate nohighlight">\(L\)</span> et <span class="math notranslate nohighlight">\(L_n\)</span>. La probabilitÃ© que nous souhaiterions voir la plus petite possible est <span class="math notranslate nohighlight">\(\mathbb{P}\big(|L_n(h)-L(h)|&gt;\epsilon)\)</span>. Nous pouvons la majorer grÃ¢ce Ã  lâ€™<a class="reference external" href="https://fr.wikipedia.org/wiki/In%C3%A9galit%C3%A9_de_Hoeffding">inÃ©galitÃ© dâ€™Hoeffding</a>Â :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\big(|L_n(h)-L(h)|&gt;\epsilon) \leq 2 e^{-2\epsilon^2n}.\]</div>
<p>Cela nous dit que la probabitÃ© que notre estimateur empirique sâ€™Ã©carte de plus de <span class="math notranslate nohighlight">\(\epsilon\)</span> de son espÃ©rance dÃ©croÃ®t exponentiellement vite lorsque la taille du jeu de donnÃ©es augmente !</p>
<p>Cependant, pour pouvoir garantir que lâ€™apprentissage se fera bien (i.e. quâ€™on choisira un bon minimiseur du risque empirique), nous devons majorer la probabilitÃ© que TOUTES les fonctions soient bien estimÃ©es.</p>
<div class="admonition-theoreme admonition">
<p class="admonition-title">ThÃ©orÃ¨me</p>
<p>Soit <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> une classe de fonctions de <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> dans <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> et <span class="math notranslate nohighlight">\(n\)</span> la taille de notre jeu de donnÃ©es. Nous avonsÂ :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\big(\sup_{h\in\mathcal{H}} |L_n(h)-L(h)|&gt;\epsilon\big)\leq |\mathcal{H}|2 e^{-2\epsilon^2n}\]</div>
</div>
<div class="caution admonition">
<p class="admonition-title">Preuve</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{P}\big(\sup_{h\in\mathcal{H}} |L_n(h)-L(h)|&gt;\epsilon\big)&amp;=\mathbb{P}\big(\cup_{h\in\mathcal{H}} |L_n(h)-L(h)|&gt;\epsilon\big)\\
&amp;\leq \sum_{h\in\mathcal{H}}\mathbb{P}\big(|L_n(h)-L(h)|&gt;\epsilon\big)\\
&amp;\leq |\mathcal{H}|2 e^{-2\epsilon^2n}.
\end{aligned}\end{split}\]</div>
</div>
<p>Câ€™est un premier rÃ©sultat fondamental qui nous permet de dire que si <span class="math notranslate nohighlight">\(|\mathcal{H}|&lt;\infty\)</span>, alors on peut rÃ©duire uniformÃ©ment (i.e. pour toutes fonctions de <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>) les dÃ©viations entre le risque empirique et son espÃ©rance et donc que le choix du minimiseur empirique ne sera pas mauvais (pour peu que le jeu de donnÃ©es soit assez grand).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">scale</span><span class="p">):</span>
    <span class="n">H_size</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.04</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2000</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">H_size</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">epsilon</span><span class="p">):</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="o">*</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">e</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">n</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\epsilon=$</span><span class="si">{}</span><span class="s1">, $|H|=$</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">k</span><span class="p">)))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="kc">None</span> <span class="k">if</span> <span class="n">scale</span> <span class="o">==</span> <span class="s1">&#39;log&#39;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot</span><span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4_VC_theory_11_0.png" src="../_images/4_VC_theory_11_0.png" />
<img alt="../_images/4_VC_theory_11_1.png" src="../_images/4_VC_theory_11_1.png" />
</div>
</div>
<p>Nous avons ainsi fixÃ© lâ€™erreur <span class="math notranslate nohighlight">\(\epsilon\)</span> et avons observÃ© que la probabilitÃ© que la dÃ©viation â€œsupâ€ entre le risque empirique et son espÃ©rance soit plus grande que <span class="math notranslate nohighlight">\(\epsilon\)</span> dÃ©croissait exponentiellement vite. Nous pouvons Ã©galement prendre une perspective diffÃ©rente en fixant la probabilitÃ© et en regardant lâ€™Ã©volution de lâ€™erreur Ã  probabilitÃ© fixeÂ :</p>
<div class="admonition-proposition admonition">
<p class="admonition-title">Proposition</p>
<p>Soit <span class="math notranslate nohighlight">\(\delta&gt;0\)</span> et <span class="math notranslate nohighlight">\(|\mathcal{H}|\)</span>, alors, avec probabilitÃ© au moins <span class="math notranslate nohighlight">\(1-\delta\)</span>, on aÂ :</p>
<div class="math notranslate nohighlight">
\[\sup_{h\in\mathcal{H}} |L_n(h)-L(h)| \leq \sqrt{\frac{\log|\mathcal{H}|+\log\frac{2}{\delta}}{2n}}.\]</div>
</div>
<div class="caution dropdown admonition">
<p class="admonition-title">Preuve</p>
<p>Nous avionsÂ :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\big(\sup_{h\in\mathcal{H}} |L_n(h)-L(h)|&gt;\epsilon\big)\leq |\mathcal{H}|2 e^{-2\epsilon^2n}.\]</div>
<p>Ã‰galons la probabitÃ© Ã  droite (i.e. de dÃ©viation) avec <span class="math notranslate nohighlight">\(\delta\)</span> et rÃ©solvons lâ€™inÃ©galitÃ© pour <span class="math notranslate nohighlight">\(\epsilon\)</span>Â :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
|\mathcal{H}|2e^{-2\epsilon^2n}&amp;= \delta\\
\Leftrightarrow\ e^{-2\epsilon^2n}&amp;= \frac{\delta}{|\mathcal{H}|2}\\
\Leftrightarrow \epsilon^2&amp;= \frac{\log|\mathcal{H}|+\log\frac{2}{\delta}}{2n}\\
\Leftrightarrow \epsilon&amp;= \sqrt{\frac{\log|\mathcal{H}|+\log\frac{2}{\delta}}{2n}}
\end{aligned}\end{split}\]</div>
<p>Puisque <span class="math notranslate nohighlight">\(\delta\)</span> majore la probabilitÃ© dâ€™une dÃ©viation plus grande que <span class="math notranslate nohighlight">\(\epsilon\)</span>, <span class="math notranslate nohighlight">\(1-\delta\)</span> minore la probabilitÃ© que la dÃ©viation soit au plus <span class="math notranslate nohighlight">\(\epsilon\)</span>. On obtient doncÂ :</p>
<div class="math notranslate nohighlight">
\[\sup_{h\in\mathcal{H}} |L_n(h)-L(h)| \leq \sqrt{\frac{\log|\mathcal{H}|+\log\frac{2}{\delta}}{2n}}.\]</div>
</div>
<p>En rÃ©utilisant le lemme 1, nous en dÃ©duisons le corollaire suivantÂ :</p>
<div class="admonition-corollaire admonition">
<p class="admonition-title">Corollaire</p>
<p>Soit <span class="math notranslate nohighlight">\(\delta&gt;0\)</span> et une classe de fonctions de <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> dans <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> notÃ©e <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>. Notons <span class="math notranslate nohighlight">\(h_n\)</span> le minimiseur du risque empirique. Nous avons avec probabilitÃ© <span class="math notranslate nohighlight">\(1-\delta\)</span>Â :</p>
<div class="math notranslate nohighlight">
\[L(h_n)-\inf_{h\in\mathcal{H}}L(h)\leq 2\sqrt{\frac{\log|\mathcal{H}|+\log\frac{2}{\delta}}{2n}}\]</div>
</div>
<p>Ainsi, le minimiseur du risque empirique nâ€™est pas un mauvais choix si le jeu de donnÃ©es est assez grand et <span class="math notranslate nohighlight">\(|\mathcal{H}|&lt;\infty\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">plot</span><span class="p">():</span>
    <span class="n">H_size</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1511</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">H_size</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">delta</span><span class="p">):</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">k</span><span class="p">)</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">e</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">n</span><span class="p">)),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\delta=$</span><span class="si">{:.2f}</span><span class="s1">, $|H|=$</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">k</span><span class="p">)))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Majoration du gap de gÃ©nÃ©ralisation&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4_VC_theory_14_0.png" src="../_images/4_VC_theory_14_0.png" />
</div>
</div>
</div>
<div class="section" id="b-en-supposant-eta-in-0-1">
<h3>B. En supposant <span class="math notranslate nohighlight">\(\eta\in\{0, 1\}\)</span><a class="headerlink" href="#b-en-supposant-eta-in-0-1" title="Permalink to this headline">Â¶</a></h3>
<p>Jusque lÃ , nous nâ€™avions pas dâ€™hypothÃ¨se sur <span class="math notranslate nohighlight">\(\eta\)</span>. Les valeurs que peut prendre <span class="math notranslate nohighlight">\(\eta\)</span> influencent grandement la vitesse (en termes de taille de jeu de donnÃ©es) avec laquelle nous allons converger vers la meilleure fonction de notre classe de fonctions. En effet, imaginons que <span class="math notranslate nohighlight">\(\eta(x)\approx 0.5\)</span>, et notons <span class="math notranslate nohighlight">\(Y\sim\mathcal{B}(\eta(x))\)</span> (i.e. une Bernoulli). Nous avons environ une chance sur deux que <span class="math notranslate nohighlight">\(Y\)</span> soit la valeur quâ€™il faille retourner pour minimiser lâ€™erreur et une chance sur deux que ce ne soit pas le cas. Ainsi, nous avons une chance sur deux de favoriser une â€œmauvaiseâ€ fonction. ÃŠtre autour de <span class="math notranslate nohighlight">\(0.5\)</span> est bien sÃ»r un cas extrÃªme, mais cela ralentit la convergence. Ã€ lâ€™inverse, si <span class="math notranslate nohighlight">\(\eta(x)\in\{0, 1\}\)</span> alors soit notre fonction retourne la bonne valeur et ne fait aucune erreur tout le temps, soit elle retourne une mauvaise valeur et est pÃ©nalisÃ©e tout le temps. On imagine bien ici que la convergence sera plus rapide.</p>
<div class="admonition-proposition admonition">
<p class="admonition-title">Proposition</p>
<p>Soit <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> notre classe de fonctions et <span class="math notranslate nohighlight">\(\delta&gt;0\)</span>. Supposons <span class="math notranslate nohighlight">\(g^\star\in\mathcal{H}\)</span> et <span class="math notranslate nohighlight">\(\eta\in\{0, 1\}\)</span>. Alors, nous avons avec probabilitÃ© <span class="math notranslate nohighlight">\(1-\delta\)</span>Â :</p>
<div class="math notranslate nohighlight">
\[L(h_n)-\inf_{h\in\mathcal{H}}L(h)\leq  \frac{\log |\mathcal{H}|+\log\frac{1}{\delta}}{n}\]</div>
</div>
<div class="caution dropdown admonition">
<p class="admonition-title">Preuve</p>
<p>Tout dâ€™abord, remarquons que puisque <span class="math notranslate nohighlight">\(\eta\in\{0, 1\}\)</span> ainsi que <span class="math notranslate nohighlight">\(g^\star\in\mathcal{H}\)</span> nous avons nÃ©cessairementÂ :</p>
<div class="math notranslate nohighlight">
\[\inf_{h\in\mathcal{H}}L(h)=0\]</div>
<p>mais aussi</p>
<div class="math notranslate nohighlight">
\[L_n(h_n)=0.\]</div>
<p>Si la meilleure fonction fait toujours <span class="math notranslate nohighlight">\(0\)</span> erreur, alors celle quâ€™on choisira fera <span class="math notranslate nohighlight">\(0\)</span> erreur. NotonsÂ :</p>
<div class="math notranslate nohighlight">
\[\mathcal{H}_b=\{h\in\mathcal{H}:L(h)&gt;\epsilon\}\]</div>
<p>Lâ€™ensemble des mauvaises (bad) fonctions de notre classe de fonctions. Ainsi,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{P}\left(L(h_n)-\inf_{h\in\mathcal{H}}L(h)&gt;\epsilon\right)&amp;=\mathbb{P}\left(L(h_n)&gt;\epsilon\right)\\
&amp;\leq\mathbb{P}\left(\cup_{h\in\mathcal{H}_b} L_n(h)=0\right)
\end{aligned}\end{split}\]</div>
<p>Il y a une inÃ©galitÃ© pour la raison suivante. La probabilitÃ© que le minimiseur du risque empirique fasse plus de <span class="math notranslate nohighlight">\(\epsilon\)</span> erreurs dÃ©pend de la probabilitÃ© des mauvaises fonctions Ã  faire <span class="math notranslate nohighlight">\(0\)</span> erreur et que ce soit <strong>lâ€™une de celles-lÃ  quâ€™on choisisse</strong>.</p>
<p>Appliquons maintenant le <em>union bound</em>Â :</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\mathbb{P}\left(\cup_{h\in\mathcal{H}_b} L_n(h)=0\right)\leq \sum_{h\in\mathcal{H}_b}\mathbb{P}\left(L_n(h)=0\right)
\end{aligned}\]</div>
<p>Calculons maintenantÂ :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{P}\left(L_n(h)=0\right)&amp;=(1-L(h))^n\\
&amp;\leq (1-\epsilon)^n\text{ (car }h\in\mathcal{H}_b\text{)}\\
&amp;\leq (e^{-\epsilon})^n=e^{-\epsilon n}
\end{aligned}\end{split}\]</div>
<p>Reprenons toutÂ :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{P}\left(L(h_n)-\inf_{h\in\mathcal{H}}L(h)&gt;\epsilon\right)&amp;\leq \sum_{h\in\mathcal{H}_b}\mathbb{P}\left(L_n(h)=0\right)\\
&amp;\leq \sum_{h\in\mathcal{H}_b}e^{-\epsilon n}\\
&amp;\leq \sum_{h\in\mathcal{H}}e^{-\epsilon n}\\
&amp;\leq |\mathcal{H}|e^{-\epsilon n}
\end{aligned}\end{split}\]</div>
<p>Soit <span class="math notranslate nohighlight">\(\delta&gt;0\)</span>, nous avonsÂ :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
|\mathcal{H}|e^{-\epsilon n}&amp;=\delta\\
\Leftrightarrow\ e^{-\epsilon n}&amp;=\frac{\delta}{|\mathcal{H}|}\\
\Leftrightarrow\ \epsilon &amp;= \frac{\log |\mathcal{H}|+\log\frac{1}{\delta}}{n}
\end{aligned}\end{split}\]</div>
<p>La probabilitÃ© de dÃ©viation est dâ€™au plus <span class="math notranslate nohighlight">\(\delta\)</span>. Donc la probabilitÃ© de ne pas avoir de dÃ©viation est dâ€™au moins <span class="math notranslate nohighlight">\(1-\delta\)</span> et nous avonsÂ :</p>
<div class="math notranslate nohighlight">
\[L(h_n)-\inf_{h\in\mathcal{H}}L(h)\leq  \frac{\log |\mathcal{H}|+\log\frac{1}{\delta}}{n}.\]</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">plot</span><span class="p">():</span>
    <span class="n">H_size</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1511</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">H_size</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">delta</span><span class="p">):</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">k</span><span class="p">)</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">e</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">n</span><span class="p">)),</span> 
                     <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\delta=$</span><span class="si">{:.2f}</span><span class="s1">, $|H|=$</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">k</span><span class="p">)))</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">H_size</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">delta</span><span class="p">):</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">k</span><span class="p">)</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">e</span><span class="p">))</span><span class="o">/</span><span class="n">n</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span>
                     <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\eta\in${{0, 1}}, $g^\star\in\mathcal{{H}}$, $\delta=$</span><span class="si">{:.2f}</span><span class="s1">, $|H|=$</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">k</span><span class="p">)))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Majoration du gap de gÃ©nÃ©ralisation&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4_VC_theory_17_0.png" src="../_images/4_VC_theory_17_0.png" />
</div>
</div>
<p>Dans le premier cas, nous nâ€™avions pas dâ€™hypothÃ¨se sur la difficultÃ© de la tÃ¢che (i.e. <span class="math notranslate nohighlight">\(\eta\)</span>). Dans le second, nous avons supposÃ© que les labels Ã©taient calculÃ©s de maniÃ¨re dÃ©terministe Ã  partir de <span class="math notranslate nohighlight">\(x\)</span>. Nous avons Ã©galement supposÃ© que nous pouvions calculer le classifieur de Bayes (i.e. <span class="math notranslate nohighlight">\(g^\star\in\mathcal{H}\)</span>).</p>
</div>
</div>
<div class="section" id="iii-le-cas-general-mathcal-h-infty">
<h2>III. Le cas gÃ©nÃ©ral : <span class="math notranslate nohighlight">\(|\mathcal{H}|=\infty\)</span><a class="headerlink" href="#iii-le-cas-general-mathcal-h-infty" title="Permalink to this headline">Â¶</a></h2>
<p>Le cas gÃ©nÃ©ral permet Ã©galement de conclure pour des cas oÃ¹ le cardinal de <span class="math notranslate nohighlight">\(|\mathcal{H}|\)</span> serait fini.</p>
<p>Lâ€™intuition derriÃ¨re le cas gÃ©nÃ©ral est quâ€™au lieu de considÃ©rer toutes les fonctions de <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>, nous nâ€™en considÃ©rerons que quelques une. Il sâ€™agit de discrÃ©tiser la variable alÃ©atoire <span class="math notranslate nohighlight">\(\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|\)</span> de maniÃ¨re Ã  ne considÃ©rer que les â€œvaleurs effectivesâ€ que notre classe de fonctions peut prendre sur notre jeu de donnÃ©e.</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<ol class="simple">
<li><p><strong>CommenÃ§ons par un exercice afin dâ€™illustrer le point prÃ©cÃ©dent. Soit le jeu de donnÃ©es <span class="math notranslate nohighlight">\(S_1=\{(X_1, Y_1)\}\)</span> (toujours dans le cadre dâ€™un problÃ¨me de classification binaire, i.e. <span class="math notranslate nohighlight">\(Y_i\in\{0, 1\}\)</span>). Quel est le plus grand nombre de maniÃ¨res de labelliser ce jeu de donnÃ©es indÃ©pendamment de <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>Â ?</strong></p></li>
<li><p><strong>Soit un jeu de donnÃ©es de taille <span class="math notranslate nohighlight">\(n\)</span>, notÃ© <span class="math notranslate nohighlight">\(S_n\)</span>. Quel est le plus grand nombre de maniÃ¨res de labelliser ce jeu de donnÃ©esÂ ?</strong></p></li>
</ol>
</div>
<p>Intuitivement, on se rend dÃ©jÃ  compte (Ã  quelques astuces prÃ¨s) que nous nâ€™avons pas Ã  considÃ©rer lâ€™infini. Cependant, <span class="math notranslate nohighlight">\(2^n\)</span> reste une quantitÃ© trop grosse commme nous le verrons.</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Soit <span class="math notranslate nohighlight">\(\mathcal{X}=\mathbb{R}\)</span> et <span class="math notranslate nohighlight">\(S_2=\{(X_1, Y_1), (X_2, Y_2)\}\)</span>. Sans perte de gÃ©nÃ©ralitÃ© supposons <span class="math notranslate nohighlight">\(X_1&lt;X_2\)</span>. SoitÂ :</strong></p>
<div class="math notranslate nohighlight">
\[\mathcal{H}=\{h(x)=\mathbf{1}\{x&gt;\tau\},\ \tau\in\mathbb{R}\}\]</div>
<p><strong>Câ€™est lâ€™ensemble des fonctions seuil qui valent <span class="math notranslate nohighlight">\(1\)</span> si <span class="math notranslate nohighlight">\(x\)</span> est plus grand quâ€™un paramÃ¨tre <span class="math notranslate nohighlight">\(\tau\)</span> et <span class="math notranslate nohighlight">\(0\)</span> sinon. On a dÃ©jÃ  vu que le nombre maximum de maniÃ¨res de labelliser un jeu de donnÃ©es de taille <span class="math notranslate nohighlight">\(2\)</span> Ã©tait <span class="math notranslate nohighlight">\(4\)</span>. Quel est-il <em>via</em> la classe de fonctions <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> prÃ©cÃ©denteÂ ?</strong></p>
</div>
<p>Nous avons vu que de toute maniÃ¨re nous ne pouvions pas labelliser un jeu de donnÃ©es de taille <span class="math notranslate nohighlight">\(n\)</span> de plus de <span class="math notranslate nohighlight">\(2^n\)</span> maniÃ¨res diffÃ©rentes et nous avons que que certaines classes de fonctions nâ€™atteignent pas <span class="math notranslate nohighlight">\(2^n\)</span>. <em>A priori</em>, le nombre de valeurs que peut prendre <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> sur un jeu de donnÃ©es <span class="math notranslate nohighlight">\(S_n\)</span> dÃ©pendra du tirage du jeu de donnÃ©es. Pour cela, nous pouvons majorer cette quantitÃ© en ne considÃ©rant que le â€œpire des casâ€. Câ€™est lâ€™objet de la dÃ©finition suivanteÂ :</p>
<div class="admonition-definition-la-fonction-de-croissance admonition">
<p class="admonition-title">DÃ©finition (La fonction de croissance)</p>
<div class="math notranslate nohighlight">
\[\tau_\mathcal{H}(n)=\max_{\{X_1, ..., X_n\}}|\{h(X_1), ..., h(X_n):\ h\in\mathcal{H}\}|.\]</div>
</div>
<p>Câ€™est le plus grand nombre de maniÃ¨res diffÃ©rentes quâ€™une classe de fonctions pourrait labÃ©liser un jeu de donnÃ©es de taille <span class="math notranslate nohighlight">\(n\)</span>. Câ€™est le plus grand nombre configuration de taille <span class="math notranslate nohighlight">\(n\)</span> atteignables par notre classe de fonctions.</p>
<p>Maintenant que nous avons donnÃ© les Ã©lÃ©ments de base, nous pouvons rÃ©diger le thÃ©orÃ¨me principal.</p>
<div class="admonition-theoreme admonition">
<p class="admonition-title">ThÃ©orÃ¨me</p>
<p>Soit <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> une classe de fonctions, un jeu de donnÃ©es de taille <span class="math notranslate nohighlight">\(n\)</span>, <span class="math notranslate nohighlight">\(L_n\)</span> et <span class="math notranslate nohighlight">\(L\)</span> respectivement le risque empirique et le risque et <span class="math notranslate nohighlight">\(\epsilon&gt;0\)</span>. Nous avons lâ€™inÃ©galitÃ© suivanteÂ :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\Big(\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|&gt;\epsilon\Big)\leq 8 \tau_{\mathcal{H}}(n)e^{-n\epsilon^2/32}\]</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Preuve (structure)</p>
<p>On remarque tout dâ€™abord que si <span class="math notranslate nohighlight">\(n\epsilon^2&lt;2\)</span>, alors la partie droite de lâ€™inÃ©galitÃ© est supÃ©rieure Ã  <span class="math notranslate nohighlight">\(1\)</span> et lâ€™inÃ©galitÃ© est trivialement vraie (i.e. une probabilitÃ© vaut au plus <span class="math notranslate nohighlight">\(1\)</span>). Supposons <span class="math notranslate nohighlight">\(n\epsilon^2\geq 2\)</span>.</p>
<p>Le reste de la preuve se struture en <span class="math notranslate nohighlight">\(3\)</span> Ã©tapesÂ :</p>
<ol class="simple">
<li><p>La premiÃ¨re Ã©tape consiste Ã  remplacer le risque par un risque empirique sur un jeu de test â€œfantÃ´meâ€. Comparer les performances avec un jeu de test est du mÃªme ordre de grandeur quâ€™avec le vrai risque. Notez que des coefficients multiplicatifs supplÃ©mentaires apparaÃ®tront suite Ã  cette manipulation,</p></li>
<li><p>On va symmÃ©triser avec des signes alÃ©atoire. Lâ€™idÃ©e est de dire que si un point est dans le test ou le train, il a la mÃªme loi de probabilitÃ©. Une fois cela dit, on se rend compte que finalement, on peut Ã©liminer le test et ne regarder que la â€œdynamiqueâ€ de notre train,</p></li>
<li><p>On conditionne sur un jeu de donnÃ© fixÃ© et on regarde la dynamique de nos â€œsignes alÃ©atoiresâ€. On constate quâ€™on peut majorer cela avec lâ€™inÃ©galitÃ© de Hoeffding. Enfin, on se rend compte quâ€™on peut faire un <em>union-bound</em> sur nos <span class="math notranslate nohighlight">\(\tau_\mathcal{H}(n)\)</span> fonctions.</p></li>
</ol>
</div>
<div class="caution dropdown admonition">
<p class="admonition-title">Ã‰tape 1 : SymÃ©trisation par un Ã©chantillon fantÃ´me (Ã©chantillon de test).</p>
<p>Construisons un jeu de donnÃ©es de test virtuel <span class="math notranslate nohighlight">\(S_n^\prime=\{(X_i^\prime, Y_i^\prime)\}_{i\leq n}\)</span> tel que <span class="math notranslate nohighlight">\(S_n\sim S_n^\prime\)</span> (iid). Notons <span class="math notranslate nohighlight">\(L_n^\prime\)</span> le risque empirique associÃ© Ã  cet Ã©chantillon. Supposons <span class="math notranslate nohighlight">\(n\epsilon^2\geq 2\)</span>, alors nous avons :</p>
<div class="admonition-un-jeu-de-test admonition">
<p class="admonition-title">Un jeu de test</p>
<p>Intuitivement, comparer notre <em>loss</em> de <em>train</em> avec sa valeur en espÃ©rance revient Ã  peu prÃ¨s Ã  la comparer avec la <em>loss</em> de <em>test</em>.</p>
</div>
<div class="math notranslate nohighlight">
\[\mathbb{P}\Big(\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|&gt;\epsilon\Big)\leq 2 \mathbb{P}\Big(\sup_{h\in\mathcal{H}}|L_n(h)-L_n^\prime(h)|&gt;\epsilon/2\Big)\]</div>
<p>Pour voir cela, notons <span class="math notranslate nohighlight">\(h^\star\)</span> une fonction telle que <span class="math notranslate nohighlight">\(|L_n(h^\star)-L(h^\star)|&gt;\epsilon\)</span> si une telle fonction existe, sinon une fonction fixÃ©e au hasard. Nous avons alors :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{P}\Big(\sup_{h\in\mathcal{H}}&amp;|L_n(h)-L_n^\prime(h)|&gt;\epsilon/2\Big)\geq \mathbb{P}\Big(|L_n(h^\star)-L_n^\prime(h^\star)|&gt;\epsilon/2\Big)\\
&amp;\geq\mathbb{P}\Big(|L_n(h^\star)-L(h^\star)|&gt;\epsilon, |L_n^\prime(h^\star)-L(h^\star)|&lt;\epsilon/2\Big)\\
&amp;=\mathbb{E}\Big[\textbf{1}\{|L_n(h^\star)-L(h^\star)|&gt;\epsilon\}\mathbb{P}\Big(|L_n^\prime(h^\star)-L(h^\star)|&lt;\epsilon/2|Z_1,\ldots, Z_n\Big)\Big]
\end{aligned}\end{split}\]</div>
<p>oÃ¹ <span class="math notranslate nohighlight">\(Z_i=(X_i, Y_i)\)</span>. En utilisant lâ€™<a class="reference external" href="https://fr.wikipedia.org/wiki/In%C3%A9galit%C3%A9_de_Bienaym%C3%A9-Tchebychev">InÃ©galitÃ© de BienaymÃ©-Tchebychev</a>, nous avons :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{P}\Big(|L_n^\prime(h^\star)-L(h^\star)|&lt;\epsilon/2|Z_1,\ldots, Z_n\Big)&amp;\geq 1-\frac{L(h^\star)(1-L(h^\star))/n}{(\epsilon/2)^2}\\
&amp;\geq 1-\frac{1/4}{n\epsilon^2/4}\geq \frac{1}{2}.
\end{aligned}\end{split}\]</div>
<p>(car <span class="math notranslate nohighlight">\(n\epsilon^2&gt;2\)</span>).
Ainsi :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}\mathbb{E}\Big[\textbf{1}\{|L_n(h^\star)-L(h^\star)|&gt;\epsilon\}\mathbb{P}&amp;\Big(|L_n^\prime(h^\star)-L(h^\star)|&lt;\epsilon/2|Z_1,\ldots, Z_n\Big)\Big]\\
&amp;\geq \mathbb{E}\Big[\textbf{1}\{|L_n(h^\star)-L(h^\star)|&gt;\epsilon\}\Big]\frac{1}{2}\\&amp;=\mathbb{P}\Big(|L_n(h^\star)-L(h^\star)|&gt;\epsilon\Big)\frac{1}{2},\end{aligned}\end{split}\]</div>
<p>et nous obtenons le rÃ©sultat voulu.</p>
<p>Cette premiÃ¨re Ã©tape nous dit que comparer le score empirique de notre risque par rapport Ã  son espÃ©rance est Ã  peu prÃ¨s la mÃªme chose que le comparer avec un jeu de test. Testons cela au travers dâ€™une petite expÃ©rience avec des tirages binomiaux normalisÃ©s.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">H</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">]</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">epsilon_list</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>
<span class="c1"># on rÃ©pÃ¨te l&#39;expÃ©rience pour calculer empiriquement la probabilitÃ©</span>
<span class="n">size</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="k">for</span> <span class="n">epsilon</span> <span class="ow">in</span> <span class="n">epsilon_list</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;*&#39;</span> <span class="o">*</span> <span class="mi">20</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Epsilon=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epsilon</span><span class="p">))</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">H</span><span class="p">:</span>
        <span class="n">Ln</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span><span class="o">/</span><span class="n">n</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">Ln</span><span class="o">-</span><span class="n">h</span><span class="p">)</span><span class="o">&gt;</span><span class="n">epsilon</span><span class="p">)</span>
    <span class="n">empirical_probability_1</span> <span class="o">=</span> <span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span> <span class="o">/</span> <span class="n">size</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;* P(sup |L_n-L|&gt;epsilon)=&#39;</span><span class="p">,</span> <span class="n">empirical_probability_1</span><span class="p">)</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">H</span><span class="p">:</span>
        <span class="n">binom</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span><span class="o">/</span><span class="n">n</span>
        <span class="n">binom_ghost</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span><span class="o">/</span><span class="n">n</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">binom</span><span class="o">-</span><span class="n">binom_ghost</span><span class="p">)</span><span class="o">&gt;</span><span class="n">epsilon</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">empirical_probability_2</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">size</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;* P(sup |L_n-L_n</span><span class="se">\&#39;</span><span class="s1">|&gt;epsilon)=&#39;</span><span class="p">,</span> <span class="n">empirical_probability_2</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;* 2xP(sup |L_n-L_n</span><span class="se">\&#39;</span><span class="s1">|&gt;epsilon)=&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">empirical_probability_2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>********************
Epsilon=0.01
* P(sup |L_n-L|&gt;epsilon)= 1.0
* P(sup |L_n-L_n&#39;|&gt;epsilon)= 1.0
* 2xP(sup |L_n-L_n&#39;|&gt;epsilon)= 2.0
********************
Epsilon=0.1
* P(sup |L_n-L|&gt;epsilon)= 0.294
* P(sup |L_n-L_n&#39;|&gt;epsilon)= 0.924
* 2xP(sup |L_n-L_n&#39;|&gt;epsilon)= 1.848
********************
Epsilon=0.2
* P(sup |L_n-L|&gt;epsilon)= 0.003
* P(sup |L_n-L_n&#39;|&gt;epsilon)= 0.58
* 2xP(sup |L_n-L_n&#39;|&gt;epsilon)= 1.16
</pre></div>
</div>
</div>
</div>
<p>La seconde Ã©tape va nous permettre dâ€™Ã©liminer ce jeu fantÃ´me auquel nous nâ€™avons pas accÃ¨s. AussitÃ´t mis, aussitÃ´t retirÃ©.</p>
<div class="caution dropdown admonition">
<p class="admonition-title">Ã‰tape 2 : SymÃ©trisation avec des signes alÃ©atoires.</p>
<p>Nous avons donc la variable suivante :</p>
<div class="math notranslate nohighlight">
\[\sup_{h\in\mathcal{H}}|L_n(h)-L_n^\prime(h)|=\sup_{h\in\mathcal{H}}\frac{1}{n}|\sum_i\textbf{1}\{h(X_i)\neq Y_i\}-\textbf{1}\{h(X_i^\prime)\neq Y_i^\prime\}|\]</div>
<p>Puisque nos variables <span class="math notranslate nohighlight">\(\textbf{1}\{h(X_i)\neq Y_i\}\)</span> et <span class="math notranslate nohighlight">\(\textbf{1}\{h(X_i^\prime)\neq Y_i^\prime\}\)</span> sont iid, cela revient exactement Ã </p>
<div class="math notranslate nohighlight">
\[\sup_{h\in\mathcal{H}}\frac{1}{n}|\sum_i\sigma_i(\textbf{1}\{h(X_i)\neq Y_i\}-\textbf{1}\{h(X_i^\prime)\neq Y_i^\prime\})|,\]</div>
<p>oÃ¹ <span class="math notranslate nohighlight">\(\sigma_i\)</span> est une variable de Rademacher (i.e. <span class="math notranslate nohighlight">\(\mathbb{P}\big(\sigma_i=-1\big)=\mathbb{P}\big(\sigma_i=+1\big)=0.5\)</span>).</p>
<p>Nous avons donc :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\Big(\sup_{h\in\mathcal{H}}\frac{1}{n}|\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}-\sigma_i\textbf{1}\{h(X_i^\prime\neq Y_i^\prime\})|&gt;\epsilon/2\Big)\]</div>
<p>Et au moins lâ€™un deux deux termes <span class="math notranslate nohighlight">\(|\frac{1}{n}\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}|\)</span> doit Ãªtre supÃ©rieur Ã  <span class="math notranslate nohighlight">\(\epsilon/4\)</span> pour que la somme soit supÃ©rieure Ã  <span class="math notranslate nohighlight">\(\epsilon/2\)</span> (vÃ©rifier par contradiction). Ainsi, en appliquant le <em>union bound</em>, nous avons :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{P}\Big(\sup_{h\in\mathcal{H}}&amp;\frac{1}{n}|\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}-\sigma_i\textbf{1}\{h(X_i^\prime\neq Y_i^\prime\})|&gt;\epsilon/2\Big) \\
&amp;\leq\mathbb{P}\Big(\sup_{h\in\mathcal{H}}\frac{1}{n}|\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}|&gt;\epsilon/4\Big)+\mathbb{P}\Big(\sup_{h\in\mathcal{H}}\frac{1}{n}|\sigma_i\textbf{1}\{h(X_i^\prime\neq Y_i^\prime\}|&gt;\epsilon/4\Big)\\
&amp;=2\mathbb{P}\Big(\sup_{h\in\mathcal{H}}\frac{1}{n}|\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}|&gt;\epsilon/4\Big)
\end{aligned}\end{split}\]</div>
</div>
<div class="caution dropdown admonition">
<p class="admonition-title">Ã‰tape 3 : Conditionnement</p>
<p>Jusquâ€™ici, nous considÃ©rions le jeu de donnÃ©es comme une variable alÃ©atoire. Fixons le et Ã©tudions un cas particulier. Notons <span class="math notranslate nohighlight">\(z_1, \ldots, z_n=(x_1, y_1),\ldots, (x_n, y_n)\in\mathcal{X}\times\mathcal{Y}\)</span> cette rÃ©alisation.</p>
<p>Nous avons donc :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\Big(\sup_{h\in\mathcal{H}}\frac{1}{n}|\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}|&gt;\epsilon/4|Z_1=z_1, \ldots, Z_n=z_n\Big).\]</div>
<p>Le nombre de configurations Ã  tester est justement la fonction de croissance qui nous indique toutes les valeurs que peut prendre notre classe de fonctions <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> sur un jeu de donnÃ©es fixÃ© de taille <span class="math notranslate nohighlight">\(n\)</span>. En appliquant le <em>union bound</em> Ã  nouveau, nous obtenons doncÂ :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{P}\Big(\sup_{h\in\mathcal{H}}&amp;\frac{1}{n}|\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}|&gt;\epsilon/4|Z_1, \ldots, Z_n\Big)\\&amp;\leq\tau_\mathcal{H}(n)\sup_{h\in\mathcal{H}}\mathbb{P}\Big(\frac{1}{n}|\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}|&gt;\epsilon/4|Z_1, \ldots, Z_n\Big).
\end{aligned}\end{split}\]</div>
<p>De plus, en appliquant <a class="reference external" href="https://fr.wikipedia.org/wiki/In%C3%A9galit%C3%A9_de_Hoeffding">lâ€™inÃ©galitÃ© dâ€™Hoeffding</a> , nous pouvons majorer la probabilitÃ© de droite quelque soit la fonction :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\Big(\frac{1}{n}|\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}|&gt;\epsilon/4|Z_1, \ldots, Z_n\Big)\leq 2e^{-n\epsilon^2/32}\]</div>
<p>Cette variable ne dÃ©pend pas du conditionnement et nous pouvons donc prendre lâ€™espÃ©rance :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{P}\Big(\frac{1}{n}|\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}|&gt;\epsilon/4\Big)&amp;=\mathbb{E}\Big[\mathbb{P}\Big(\frac{1}{n}|\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}|&gt;\epsilon/4|Z_1, \ldots, Z_n\Big)\Big] \\
&amp;\leq 2e^{-n\epsilon^2/32}
\end{aligned}\end{split}\]</div>
<p><strong>Conclusion.</strong></p>
<p>En combinait les 3 Ã©tapes prÃ©cÃ©dentes, nous obtenons ainsi :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\Big(\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|&gt;\epsilon\Big)\leq 8 \tau_{\mathcal{H}}(n)e^{-n\epsilon^2/32}\]</div>
</div>
<p>Des rÃ©sultats beaucoup plus stricts existent (avec une dÃ©croissance plus rapide du point de vue des constantes comme <span class="math notranslate nohighlight">\(32\)</span> ici).</p>
<div class="admonition-exercice-star admonition">
<p class="admonition-title">Exercice (<span class="math notranslate nohighlight">\(\star\)</span>)</p>
<p><strong>La formule prÃ©cÃ©dente permet de majorer lâ€™Ã©volution de la probabilitÃ© dâ€™une dÃ©viation dâ€™au moins <span class="math notranslate nohighlight">\(\epsilon\)</span> en fonction de <span class="math notranslate nohighlight">\(\tau_\mathcal{H}(n)\)</span> ainsi que <span class="math notranslate nohighlight">\(n\)</span>. DÃ©montrez la variante suivante. Soit <span class="math notranslate nohighlight">\(\delta\in[0, 1]\)</span>, <span class="math notranslate nohighlight">\(\exists K&gt;0\)</span> tel quâ€™avec probabilitÃ© au moins <span class="math notranslate nohighlight">\(1-\delta\)</span>, nous avons :</strong></p>
<div class="math notranslate nohighlight">
\[\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|&lt; K \sqrt{\frac{\text{ln}\big(\tau_{\mathcal{H}}(n)\big)-\text{ln}\big(\delta\big)}{n}}\]</div>
</div>
</div>
<div class="section" id="iv-la-dimension-vc">
<h2>IV. La dimension VC<a class="headerlink" href="#iv-la-dimension-vc" title="Permalink to this headline">Â¶</a></h2>
<p>Le thÃ©orÃ¨me prÃ©cÃ©dent est trÃ¨s intÃ©ressant, mais sans hypothÃ¨se sur notre classe de fonctions <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>, nous pourrions trÃ¨s bien avoir, comme nous lâ€™avons vuÂ :</p>
<div class="math notranslate nohighlight">
\[\tau_\mathcal{H}(n)=2^n,\]</div>
<p>et cela nous donneraitÂ :</p>
<div class="math notranslate nohighlight">
\[\lim_{n\rightarrow\infty}8 \cdot 2^ne^{-n\epsilon^2/32}=\infty,\]</div>
<p>ce qui nous empÃªcherait de conclure !</p>
<p>Il se trouve que pour certaines classes de fonctions (et nous verrons des exemples), quand bien mÃªme <span class="math notranslate nohighlight">\(|\mathcal{H}|=\infty\)</span>, nous nâ€™aurions pas <span class="math notranslate nohighlight">\(\tau_\mathcal{H}(n)=2^n\)</span>.</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Soit <span class="math notranslate nohighlight">\(\mathcal{X}=\mathbb{R}\)</span>, <span class="math notranslate nohighlight">\(\mathcal{Y}=\{0, 1\}\)</span> et <span class="math notranslate nohighlight">\(\mathcal{H}=\{h_s(x)=\textbf{1}\{x&gt;s\}:\ s\in\mathbb{R}\}\)</span>, lâ€™ensemble des fonctions seuil (on retourne <span class="math notranslate nohighlight">\(1\)</span> si <span class="math notranslate nohighlight">\(x&gt;s\)</span> et <span class="math notranslate nohighlight">\(0\)</span> sinon). Montrer que <span class="math notranslate nohighlight">\(\tau_\mathcal{H}(2)=3\)</span> et non <span class="math notranslate nohighlight">\(2^2=4\)</span>.</strong></p>
</div>
<div class="admonition-definition-la-dimension-vc admonition">
<p class="admonition-title">DÃ©finition (La dimension VC)</p>
<p>Nous appelons la dimension VC ou VCdim dâ€™une classe de fonctions <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> le plus grand jeu de donnÃ©es <span class="math notranslate nohighlight">\(S_n\)</span> tel que <span class="math notranslate nohighlight">\(\tau_\mathcal{H}(n)=2^n\)</span>. Plus formellement, nous avons :</p>
<div class="math notranslate nohighlight">
\[\text{VCdim}(\mathcal{H})=\max_{\tau_{\mathcal{H}}(n)=2^n}n\]</div>
</div>
<div class="margin sidebar">
<p class="sidebar-title"><em>Understanding machine learning</em></p>
<p>Une citation du livre <em>Understanding machine learning</em> dÃ©crit trÃ¨s bien lâ€™idÃ©e derriÃ¨re la dimension VCÂ :</p>
<p><em>If someone can explain every phenomenon, his explanations are worthless.</em></p>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Soit <span class="math notranslate nohighlight">\(\mathcal{X}=\mathbb{R}\)</span>, <span class="math notranslate nohighlight">\(\mathcal{Y}=\{0, 1\}\)</span> et <span class="math notranslate nohighlight">\(\mathcal{H}=\{h_s(x)=\textbf{1}\{x&gt;s\}:\ s\in\mathbb{R}\}\)</span>, lâ€™ensemble des fonctions seuil (on retourne <span class="math notranslate nohighlight">\(1\)</span> si <span class="math notranslate nohighlight">\(x&gt;s\)</span> et <span class="math notranslate nohighlight">\(0\)</span> sinon). Quelle est la dimension VCÂ ?</strong></p>
</div>
<p>Lâ€™intÃ©rÃªt clÃ© de cette propriÃ©tÃ© nous vient du lemme suivantÂ :</p>
<div class="admonition-lemme-de-sauer admonition">
<p class="admonition-title">Lemme de Sauer</p>
<p>Soit <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> notre classe de fonctions telle que <span class="math notranslate nohighlight">\(\text{VCdim}(\mathcal{H})\leq d&lt;\infty\)</span>. Nous avons alors <span class="math notranslate nohighlight">\(\forall n&gt;0\)</span> :</p>
<div class="math notranslate nohighlight">
\[\tau_\mathcal{H}(n)\leq \sum_{i=0}^d{n\choose i}.\]</div>
<p>De plus, si <span class="math notranslate nohighlight">\(n&gt;d+1\)</span>, alors :</p>
<div class="math notranslate nohighlight">
\[\tau_\mathcal{H}(n)\leq\Bigg(\frac{en}{d}\Bigg)^d.\]</div>
</div>
<div class="caution dropdown admonition">
<p class="admonition-title">Preuve</p>
<p>Tout dâ€™abord, nous dirons quâ€™une classe de fonctions <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> <strong>Ã©clate</strong> (<em>shatters</em> en anglais) un ensemble de points <span class="math notranslate nohighlight">\(C_n=\{(X_1)\}_{i\leq n}\)</span> si toutes les labellisation des points de <span class="math notranslate nohighlight">\(C_n\)</span> sont possibles par <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>.</p>
<p><strong>PremiÃ¨re partie du lemme</strong></p>
<p>ConsidÃ©rons lâ€™ensemble <span class="math notranslate nohighlight">\(\{B\subseteq C_n: \mathcal{H}\text{ Ã©clate } B\}\)</span>. Câ€™est lâ€™ensemble des parties de <span class="math notranslate nohighlight">\(C_n\)</span> qui sont Ã©clatÃ©es par <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>. Par dÃ©finition, puisquâ€™il nâ€™y a pas de jeu de donnÃ©es plus grand que <span class="math notranslate nohighlight">\(d\)</span> qui puissent Ãªtre Ã©clatÃ©s par <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> (cf. dimension VC), alors on a nÃ©cessairementÂ :</p>
<div class="math notranslate nohighlight">
\[|\{B\subseteq C_n: \mathcal{H}\text{ Ã©clate } B\}|\leq \sum_{i=0}^d{n\choose i}\]</div>
<p>Notons <span class="math notranslate nohighlight">\(\mathcal{H}_C=\{h(X_1), ..., h(X_n):\ h\in\mathcal{H}\}\)</span>. Nous allons prouverÂ :</p>
<div class="math notranslate nohighlight">
\[|\mathcal{H}_C|\leq |\{B\subseteq C_n: \mathcal{H}\text{ Ã©clate } B\}|\]</div>
<p>ConsidÃ©rons que <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> Ã©clate toujours lâ€™ensemble vide <span class="math notranslate nohighlight">\(\emptyset\)</span>.</p>
<p>Lâ€™argument est par rÃ©curence.</p>
<p>Si <span class="math notranslate nohighlight">\(n=1\)</span>, si <span class="math notranslate nohighlight">\(|\mathcal{H}_C|=1\)</span>, alors aucune fonction nâ€™Ã©clate <span class="math notranslate nohighlight">\(C\)</span>. Alors <span class="math notranslate nohighlight">\(|\{B\subseteq C_n: \mathcal{H}\text{ Ã©clate } B\}|=1\)</span> grÃ¢ce Ã  <span class="math notranslate nohighlight">\(\emptyset\)</span>. Si <span class="math notranslate nohighlight">\(|\mathcal{H}_C|=2\)</span>, alors le point est Ã©clatÃ© par <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> et <span class="math notranslate nohighlight">\(|\{B\subseteq C_n: \mathcal{H}\text{ Ã©clate } B\}|=2\)</span> grÃ¢ce Ã  <span class="math notranslate nohighlight">\(\emptyset\)</span> et au point de <span class="math notranslate nohighlight">\(C\)</span>. Lâ€™inÃ©galitÃ© est vÃ©rifiÃ©e pour <span class="math notranslate nohighlight">\(n=1\)</span>.</p>
<p>Supposons lâ€™inÃ©galitÃ© vÃ©rifiÃ©e <span class="math notranslate nohighlight">\(\forall k&lt;n\)</span> et montrons quâ€™elle lâ€™est alors pour <span class="math notranslate nohighlight">\(k=n\)</span>. Pour rappel, <span class="math notranslate nohighlight">\(C=\{x_1,\ldots, x_n\}\)</span> et notons <span class="math notranslate nohighlight">\(C^\prime=\{x_2, \ldots, x_n\}\)</span>. Construisons les deux ensembles suivantsÂ :</p>
<div class="math notranslate nohighlight">
\[\mathbb{Y}_0=\{(y_2, \ldots, y_n): (0, y_2, \ldots, y_n)\in\mathcal{H}_C\lor(1, y_2, \ldots, y_n)\in\mathcal{H}_C\}\text{ ($\lor$ = or)}\]</div>
<p>Il sâ€™agit de toutes les labelisations de <span class="math notranslate nohighlight">\(C^\prime\)</span> peu importe le label de <span class="math notranslate nohighlight">\(x_1\)</span> .</p>
<div class="math notranslate nohighlight">
\[\mathbb{Y}_1=\{(y_2, \ldots, y_n): (0, y_2, \ldots, y_n)\in\mathcal{H}_C\land(1, y_2, \ldots, y_n)\in\mathcal{H}_C\}\text{ ($\land$ = and)}\]</div>
<p>Il sâ€™agit de toutes les labelisations de <span class="math notranslate nohighlight">\(C^\prime\)</span> telles que <span class="math notranslate nohighlight">\(x_1\)</span> puisse toujours Ãªtre labellisÃ© <span class="math notranslate nohighlight">\(1\)</span> et <span class="math notranslate nohighlight">\(0\)</span>. On a doncÂ :</p>
<div class="math notranslate nohighlight">
\[|\mathbb{Y}_1|+|\mathbb{Y}_0|=|\mathcal{H}_C|\]</div>
<p>car <span class="math notranslate nohighlight">\(\mathbb{Y}_0\)</span> contient toutes les labellisations de <span class="math notranslate nohighlight">\(C^\prime\)</span> et <span class="math notranslate nohighlight">\(\mathbb{Y}_1\)</span> double les cas oÃ¹ <span class="math notranslate nohighlight">\(x_1\)</span> est double (<span class="math notranslate nohighlight">\(1\)</span> et <span class="math notranslate nohighlight">\(0\)</span>). De plus, on peut remarquer queÂ :</p>
<div class="math notranslate nohighlight">
\[\mathbb{Y}_0=\mathcal{H}_{C^\prime},\]</div>
<p>Et puisque lâ€™inÃ©galitÃ© est vraie jusquâ€™Ã  <span class="math notranslate nohighlight">\(n-1\)</span>, nous avonsÂ :</p>
<div class="math notranslate nohighlight">
\[|\mathbb{Y}_0|=|\mathcal{H}_{C^\prime}|\leq |\{B\subseteq C^\prime: \mathcal{H}\text{ Ã©clate } B\}|=|\{B\subseteq C: x_1\not\in B\land\mathcal{H}\text{ Ã©clate } B\}|\]</div>
<p>SoitÂ :</p>
<div class="math notranslate nohighlight">
\[\mathcal{H}^\prime=\{h\in\mathcal{H}:\ \exists h^\prime\in\mathcal{H}\ s.t.\ (1-h(x_1), h(x_2),\ldots, h(x_n))=(h^\prime(x_1), h^\prime(x_2), \ldots, h^\prime(x_n))\}\]</div>
<p>Câ€™est lâ€™ensemble des fonctions telles quâ€™il existe une autre fonction en accourd sur tous les points de <span class="math notranslate nohighlight">\(C\)</span> sauf le premier. Ã‰videmment, si <span class="math notranslate nohighlight">\(\mathcal{H}^\prime\)</span> Ã©clate <span class="math notranslate nohighlight">\(B\subseteq C^\prime\)</span>, alors il Ã©clate aussi <span class="math notranslate nohighlight">\(B\cup x_1\)</span> (par construction de <span class="math notranslate nohighlight">\(\mathcal{H}^\prime\)</span>. Nous avons de fait <span class="math notranslate nohighlight">\(\mathcal{H}^\prime_{C^\prime}=\mathbb{Y}_1\)</span>.</p>
<p>Par rÃ©currence, nous avons Ã  noveauÂ :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}|\mathbb{Y}_1|=|\mathcal{H}^\prime_{C^\prime}|&amp;\leq  |\{B\subseteq C^\prime: \mathcal{H}^\prime\text{ Ã©clate } B\}|\\
&amp;=|\{B\subseteq C^\prime: \mathcal{H}^\prime\text{ Ã©clate } B\cup \{x_1\}\}|\\
&amp;=|\{B\subseteq C: x_1\in B\land\mathcal{H}^\prime\text{ Ã©clate } B\}|\\
&amp;\leq |\{B\subseteq C: x_1\in B\land\mathcal{H}\text{ Ã©clate } B\}|
\end{aligned}\end{split}\]</div>
<p>En rÃ©sumÃ©, nous avonsÂ :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
|\mathcal{H}_C|=|\mathbb{Y}_0|+|\mathbb{Y}_1|&amp;\leq|\{B\subseteq C: x_1\not\in B\land\mathcal{H}\text{ Ã©clate } B\}|+|\{B\subseteq C: x_1\in B\land\mathcal{H}\text{ Ã©clate } B\}|\\
&amp;=||\{B\subseteq C: \mathcal{H}\text{ Ã©clate } B\}|
\end{aligned}\end{split}\]</div>
<p>Et doncÂ :</p>
<div class="math notranslate nohighlight">
\[\tau_\mathcal{H}(n)\leq \sum_{i=0}^d{n\choose i}.\]</div>
<p><strong>Seconde partie du lemme</strong></p>
<p>Nous allons montrer lâ€™inÃ©galitÃ© suivante. Soit <span class="math notranslate nohighlight">\(n\)</span> et <span class="math notranslate nohighlight">\(d\)</span> deux entiers strictement positifs tels que <span class="math notranslate nohighlight">\(n&gt;d+1\)</span>, alorsÂ :</p>
<div class="math notranslate nohighlight">
\[\sum_{i=0}^d{n\choose i}\leq \Bigg(\frac{en}{d}\Bigg)^d.\]</div>
<p>Nous allons Ã  nouveau fonctionner par rÃ©currence. Soit <span class="math notranslate nohighlight">\(d=1\)</span>. On a doncÂ :</p>
<div class="math notranslate nohighlight">
\[\sum_{i=0}^d{n\choose i}={n\choose 0}+{n\choose 1}=1+n\]</div>
<p>etÂ :</p>
<div class="math notranslate nohighlight">
\[\Bigg(\frac{en}{d}\Bigg)^d=en\]</div>
<p>et on a bien <span class="math notranslate nohighlight">\(1+n\leq en\)</span>. Supposons que lâ€™inÃ©galitÃ© tienne jusquâ€™Ã  <span class="math notranslate nohighlight">\(d\)</span> et montrons <span class="math notranslate nohighlight">\(d+1\)</span>Â :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\sum_{i=0}^{d+1}{n\choose i}&amp;\leq \Bigg(\frac{en}{d}\Bigg)^d+{n\choose d+1}\\
&amp;= \Bigg(\frac{en}{d}\Bigg)^d\left(1+ \Bigg(\frac{d}{en}\Bigg)^d\frac{n!}{(d+1)!(n-d-1)!}\right)\\
&amp;= \Bigg(\frac{en}{d}\Bigg)^d\left(1+ \Bigg(\frac{d}{en}\Bigg)^d\frac{(n-d)}{(d+1)d!}\right)\\
&amp;= \Bigg(\frac{en}{d}\Bigg)^d\left(1+ \Bigg(\frac{d}{e}\Bigg)^d\frac{n(n-1)\ldots(n-d+1)}{n^d}\frac{(n-d)}{(d+1)d!}\right)\\
&amp;\leq \Bigg(\frac{en}{d}\Bigg)^d\left(1+ \Bigg(\frac{d}{e}\Bigg)^d\frac{(n-d)}{(d+1)d!}\right)\\
\end{aligned}\end{split}\]</div>
<hr class="docutils" />
<p>La <a class="reference external" href="https://fr.wikipedia.org/wiki/Formule_de_Stirling">formule de Stirling</a> donne lâ€™approximation suivanteÂ :</p>
<div class="math notranslate nohighlight">
\[n!\approx\sqrt{2\pi n}\left(\frac{n}{e}\right)^n.\]</div>
<p>En rÃ©alitÃ©, nous avons mÃªme lâ€™encadrement suivantÂ :</p>
<div class="math notranslate nohighlight">
\[\sqrt{2\pi n}\left(\frac{n}{e}\right)^n\leq n! \leq \sqrt{2\pi n}\left(\frac{n}{e}\right)^ne^{\frac{1}{4n}}\]</div>
<p>Lâ€™approximation initiale est donc un minorant de <span class="math notranslate nohighlight">\(n!\)</span>.</p>
<hr class="docutils" />
<p>Nous avons donc en appliquant la formule de Stirling Ã  <span class="math notranslate nohighlight">\(n!\)</span>Â :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\Bigg(\frac{en}{d}\Bigg)^d\left(1+ \Bigg(\frac{d}{e}\Bigg)^d\frac{(n-d)}{(d+1)d!}\right)&amp;\leq \Bigg(\frac{en}{d}\Bigg)^d\left(1+ \Bigg(\frac{d}{e}\Bigg)^d\frac{(n-d)}{(d+1)\sqrt{2\pi d}\left(\frac{d}{e}\right)^d}\right)\\
&amp;=\Bigg(\frac{en}{d}\Bigg)^d\left(1+\frac{(n-d)}{(d+1)\sqrt{2\pi d}}\right)\\
&amp;=\Bigg(\frac{en}{d}\Bigg)^d\frac{d+1+(n-d)/\sqrt{2\pi d}}{(d+1)}\\
&amp;\leq \Bigg(\frac{en}{d}\Bigg)^d\frac{d+1+(n-d)/2}{(d+1)}\\
&amp;= \Bigg(\frac{en}{d}\Bigg)^d\frac{d/2+1+n/2}{(d+1)}\\
\end{aligned}\end{split}\]</div>
<p>De plus, par hypothÃ¨se, nous avonsÂ : <span class="math notranslate nohighlight">\(n&gt;d+1\Leftrightarrow n-2\geq d\)</span>. Cela nous donneÂ :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\Bigg(\frac{en}{d}\Bigg)^d\frac{d/2+1+n/2}{(d+1)}&amp;\leq \Bigg(\frac{en}{d}\Bigg)^d\frac{(n-2)/2+1+n/2}{(d+1)}\\
&amp;=\Bigg(\frac{en}{d}\Bigg)^d\frac{n}{(d+1)}\\
&amp;\leq \Bigg(\frac{en}{d}\Bigg)^d\frac{n}{(d+1)}\frac{1}{e}\\
&amp;= \Bigg(\frac{en}{d}\Bigg)^d\frac{en}{(d+1)}\frac{1}{e}\\
&amp;\leq \Bigg(\frac{en}{d}\Bigg)^d\frac{en}{(d+1)}\frac{1}{(1+1/d)^d}\text{ (rappelez-vous $\lim_{x\rightarrow\infty}(1+1/x)^x=e$ et $(1+1/x)^x$ est croissante)}\\
&amp;=\Bigg(\frac{en}{d}\Bigg)^d\frac{en}{(d+1)}\Bigg(\frac{d}{1+d}\Bigg)^d=\Bigg(\frac{en}{d+1}\Bigg)^{d+1}
\end{aligned}\end{split}\]</div>
</div>
<p>Ainsi, la fonction de croissance ne croÃ®t plus exponentiellement vite mais polynomialement dÃ¨s quâ€™on dÃ©passe la dimension VC de notre classe de fonctions. Cela implique que notre majorant de gÃ©nÃ©ralisation converge vers <span class="math notranslate nohighlight">\(0\)</span>, quâ€™on puisse estimer lâ€™erreur de nos fonctions correctement et donc que le choix du minimiseur du risque empirique est un bon choix : ce rÃ©sultat (qui est lâ€™agrÃ©gation de tous les Ã©lÃ©ments prÃ©cÃ©dents) est ce quâ€™on appelle le <em>thÃ©orÃ¨me fondamental du machine learning</em>. Ce dernier est dÃ©taillÃ© plus bas.</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>En reprenant le rÃ©sultat de lâ€™exercice <em>(<span class="math notranslate nohighlight">\(\star\)</span>)</em>, remplacez la fonction de croissance par son majorant issu du Lemme de Sauer lorsque <span class="math notranslate nohighlight">\(n&gt;d+1\)</span>.</strong></p>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Soit <span class="math notranslate nohighlight">\(\mathcal{X}=\mathbb{R}^2\)</span>, <span class="math notranslate nohighlight">\(\mathcal{Y}=\{0, 1\}\)</span> et <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> lâ€™ensemble des classifieurs linÃ©aires de <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>. DÃ©montrer que la dimension VC de <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> est au moins <span class="math notranslate nohighlight">\(3\)</span>.</strong></p>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Soit <span class="math notranslate nohighlight">\(\mathcal{H}_{=k}\)</span> lâ€™ensemble des classifieurs qui ne peuvent associer le label <span class="math notranslate nohighlight">\(1\)</span> quâ€™Ã  <em>exactement</em> <span class="math notranslate nohighlight">\(k\)</span> Ã©lÃ©ments de <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>. Trouver la dimension VC. On supposera que <span class="math notranslate nohighlight">\(|\mathcal{X}|\geq k\)</span>.</strong></p>
</div>
</div>
<div class="section" id="v-une-formule-generale">
<h2>V. Une formule gÃ©nÃ©rale<a class="headerlink" href="#v-une-formule-generale" title="Permalink to this headline">Â¶</a></h2>
<p>Nous avons vu dans le cas oÃ¹ <span class="math notranslate nohighlight">\(|\mathcal{H}|&lt;\infty\)</span> que si <span class="math notranslate nohighlight">\(\eta\in\{0, 1\}\)</span> et <span class="math notranslate nohighlight">\(g^\star\in\mathcal{H}\)</span>, alors la convergence Ã©tait plus rapide car nous avions moins de bruit dans notre jeu de donnÃ©es. Câ€™est bien sÃ»r la mÃªme chose lorsque <span class="math notranslate nohighlight">\(|\mathcal{H}|=\infty\)</span>. Il est dâ€™ailleurs possible de fournir une formule gÃ©nÃ©rale qui interpole les deux scÃ©narios.</p>
<div class="admonition-theoreme admonition">
<p class="admonition-title">ThÃ©orÃ¨me</p>
<p>Soit <span class="math notranslate nohighlight">\(\delta&gt;0\)</span>, <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> notre classe de fonctions, <span class="math notranslate nohighlight">\(h_n\)</span> le minimiseur du risque empirique, <span class="math notranslate nohighlight">\(\delta&gt;0\)</span>. Notons <span class="math notranslate nohighlight">\(d\)</span> la dimension VC de <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>. Alors, <span class="math notranslate nohighlight">\(\exists K\)</span> telle quâ€™avec probabilitÃ© <span class="math notranslate nohighlight">\(1-\delta\)</span>, nous avonsÂ :</p>
<div class="math notranslate nohighlight">
\[L(h_n)-\inf_{h\in\mathcal{H}}L(h)\leq K\left(\sqrt{\inf_{h\in\mathcal{H}}L(h)\frac{d\log n+\log\frac{1}{\delta}}{n}}+\frac{d\log n+\log\frac{1}{\delta}}{n}\right)\]</div>
</div>
<p>On retrouve le terme en racine qui implique une convergence lente. Ce terme tend vers <span class="math notranslate nohighlight">\(0\)</span> lorsque lâ€™erreur minimale dans <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> tend vers <span class="math notranslate nohighlight">\(0\)</span>.</p>
</div>
<div class="section" id="vi-un-resultat-avec-une-marge-et-mathcal-h-infty">
<h2>VI. Un rÃ©sultat avec une marge (et <span class="math notranslate nohighlight">\(|\mathcal{H}|&lt;\infty\)</span>)<a class="headerlink" href="#vi-un-resultat-avec-une-marge-et-mathcal-h-infty" title="Permalink to this headline">Â¶</a></h2>
<p>Nous avons vu que le problÃ¨me de convergence lente apparaÃ®t lorsque <span class="math notranslate nohighlight">\(\eta(x)\)</span> est proche de <span class="math notranslate nohighlight">\(0.5\)</span>. Nous avons propsÃ© une formule interpolatrice qui allait dâ€™une convergence lente Ã  une convergence rapide lorsque la meilleure fonction de notre classe de fonctions se rapprochait de <span class="math notranslate nohighlight">\(0\)</span> erreur. Il existe encore une autre maniÃ¨re dâ€™obtenir une convergence rapide.</p>
<div class="admonition-definition-marge-de-massart admonition">
<p class="admonition-title">DÃ©finition (Marge de Massart)</p>
<p>Soit <span class="math notranslate nohighlight">\(\nu(x)=\left|\eta(x)-\frac{1}{2}\right|\)</span>. On parle de marge de Massart de paramÃ¨tre <span class="math notranslate nohighlight">\(\gamma\)</span> lorsque <span class="math notranslate nohighlight">\(\nu(x)\geq \gamma\)</span> preque sÃ»rement.</p>
</div>
<p>Si nous avons une marge de Massart de paramÃ¨tre <span class="math notranslate nohighlight">\(\gamma\)</span> alors soit <span class="math notranslate nohighlight">\(\eta(x)\geq \frac{1}{2}+\gamma\)</span>, soit <span class="math notranslate nohighlight">\(\eta(x)\leq \frac{1}{2}-\gamma\)</span> avec probabilitÃ© <span class="math notranslate nohighlight">\(1\)</span>. On ne peut plus se rapprocher de <span class="math notranslate nohighlight">\(0.5\)</span>.</p>
<div class="admonition-theoreme-convergence-rapide-avec-une-marge admonition">
<p class="admonition-title">ThÃ©orÃ¨me (Convergence rapide avec une marge)</p>
<p>Soit <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> notre classe de fonctions, <span class="math notranslate nohighlight">\(\gamma\)</span> le paramÃ¨tre de notre marge, <span class="math notranslate nohighlight">\(h_n\)</span> le minimiseur de notre risque empirique et <span class="math notranslate nohighlight">\(\delta&gt;0\)</span>. Supposons <span class="math notranslate nohighlight">\(g^\star\in\mathcal{H}\)</span>. Alors nous avons avec probabilitÃ© <span class="math notranslate nohighlight">\(1-\delta\)</span>Â :</p>
<div class="math notranslate nohighlight">
\[L(h_n)-\inf_{h\in\mathcal{H}}L(h)\leq 2\frac{\log |\mathcal{H}|+\log\frac{1}{\delta}}{\gamma n}\]</div>
</div>
<div class="caution dropdown admonition">
<p class="admonition-title">Preuve</p>
<p>Soit <span class="math notranslate nohighlight">\(h\in\mathcal{H}\)</span>. DÃ©finissons la variable alÃ©atoire suivanteÂ :</p>
<div class="math notranslate nohighlight">
\[Z_i(h)=\mathbf{1}\{g^\star(X_i)\neq Y_i\}-\mathbf{1}\{h(X_i)\neq Y_i\}.\]</div>
<p>Nous avons par ailleursÂ :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
L(h_n)-\inf_{h\in\mathcal{H}}L(h)&amp;=L(h_n)-L(g^\star)\text{ (par hypothÃ¨se)}\\
&amp;=\underbrace{L_n(h_n)-L_n(g^\star)}_{\leq 0}+L_n(g^\star)-L_n(h_n)-(L(g^\star)-L(h_n))\\
&amp;\leq \frac{1}{n}\sum_{i=1}^n\left(Z_i(h_n)-\mathbb{E}\left[Z_i(h_n)\right]\right)
\end{aligned}\end{split}\]</div>
<p>et nous retombons sur un problÃ¨me oÃ¹ nous souhaitons majorer la dÃ©viation entre une variable alÃ©atoire et son espÃ©rance. Lâ€™astuce est dâ€™utiliser une inÃ©galitÃ© de concentration qui sâ€™appuie sur la variance afin dâ€™obtenir un rÃ©sultat plus fin que lâ€™inÃ©galitÃ© dâ€™HoeffdingÂ : lâ€™<a class="reference external" href="https://en.wikipedia.org/wiki/Bernstein_inequalities_(probability_theory)">inÃ©galitÃ© de Bernstein</a>.</p>
<p>Comme indiquÃ©, celle-ci sâ€™appuie sur la variance de notre variable alÃ©atoire. Nous avonsÂ :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\text{Var}[Z_i(h)]&amp;\leq \mathbb{E}[Z_i(h)^2]\\
&amp;=\mathbb{E}[\mathbf{1}\{g^\star(X_i)\neq Y_i\}-2\mathbf{1}\{g^\star(X_i)\neq Y_i\}\mathbf{1}\{h(X_i)\neq Y_i\}+\mathbf{1}\{h(X_i)\neq Y_i\}]\\
&amp;=\mathbb{P}(h(X_i)\neq g^\star(X_i))
\end{aligned}\end{split}\]</div>
<p>Nous avons donc <span class="math notranslate nohighlight">\(\forall h\in\mathcal{H}\)</span>Â :</p>
<div class="math notranslate nohighlight">
\[\frac{1}{n}\sum_{i=1}^n \text{Var}[Z_i(h)]\leq \mathbb{P}(h(X)\neq g^\star(X))\]</div>
<p>et notonsÂ :</p>
<div class="math notranslate nohighlight">
\[\sigma_h^2:=\mathbb{P}(h(X)\neq g^\star(X)).\]</div>
<p>Nous avons via lâ€™inÃ©galitÃ© de Bernstein pour <span class="math notranslate nohighlight">\(h\in\mathcal{H}\)</span>Â :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\left(\frac{1}{n}\sum_{i=1}^n Z_i(h)-\mathbb{E}[Z_i(h)]&gt;\epsilon\right)\leq \text{exp}\left(-\frac{n\epsilon^2}{2\sigma_h^2+\frac{2}{3}\epsilon}\right)\]</div>
<p>Calculons maintenantÂ :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\text{exp}\left(-\frac{n\epsilon^2}{2\sigma_h^2+\frac{2}{3}\epsilon}\right)&amp;=\frac{\delta}{|\mathcal{H}|}\\
\Leftrightarrow\ \frac{n\epsilon^2}{2\sigma_h^2+\frac{2}{3}\epsilon}&amp;=\log\frac{|\mathcal{H}|}{\delta}\\
\Leftrightarrow\ \epsilon^2&amp;=\frac{2\sigma_h^2\log\frac{|\mathcal{H}|}{\delta}}{n}+\frac{2\epsilon\log\frac{|\mathcal{H}|}{\delta}}{3n}
\end{aligned}\end{split}\]</div>
<p>Remarquons que <span class="math notranslate nohighlight">\(a+b\leq 2\max(a;b)\)</span>. Nous avons doncÂ :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\epsilon^2&amp;\leq 2\max\left(\frac{2\sigma_h^2\log\frac{|\mathcal{H}|}{\delta}}{n};\frac{2\epsilon\log\frac{|\mathcal{H}|}{\delta}}{3n}\right)\\
\Leftrightarrow\ \epsilon&amp;\leq 2\max\left(\sqrt{\frac{2\sigma_h^2\log\frac{|\mathcal{H}|}{\delta}}{n}};\frac{2\log\frac{|\mathcal{H}|}{\delta}}{3n}\right)
\end{aligned}\end{split}\]</div>
<p>Ainsi, avec probabilitÃ© <span class="math notranslate nohighlight">\(1-\frac{\delta}{|\mathcal{H}|}\)</span> pour <span class="math notranslate nohighlight">\(h\in\mathcal{H}\)</span> quelconque, nous avonsÂ :</p>
<div class="math notranslate nohighlight">
\[\frac{1}{n}\sum_{i=1}^n Z_i(h)-\mathbb{E}[Z_i(h)]\leq 2\max\left(\sqrt{\frac{2\sigma_h^2\log\frac{|\mathcal{H}|}{\delta}}{n}};\frac{2\log\frac{|\mathcal{H}|}{\delta}}{3n}\right)\]</div>
<p>En appliquant un <em>union bound</em>, nous avons donc quâ€™avec probabilitÃ© <span class="math notranslate nohighlight">\(1-\sum_h\frac{\delta}{|\mathcal{H}|}=1-\delta\)</span>, <span class="math notranslate nohighlight">\(\forall h\in\mathcal{H}\)</span> lâ€™inÃ©galitÃ© est vÃ©rifiÃ©e. Câ€™est en particulier vrai pour le minimiseur du risque empiriqueÂ :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
L(h_n)-\inf_{h\in\mathcal{H}}L(h)&amp;\leq \frac{1}{n}\sum_{i=1}^nZ_i(h_n)-\mathbb{E}\left[Z_i(h_n)\right]\\
&amp;\leq 2\max\left(\sqrt{\frac{2\sigma_{h_n}^2\log\frac{|\mathcal{H}|}{\delta}}{n}};\frac{2\log\frac{|\mathcal{H}|}{\delta}}{3n}\right)
\end{aligned}\end{split}\]</div>
<p>Nous avons de plusÂ :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
L(h_n)-\inf_{h\in\mathcal{H}}L(h)&amp;=\mathbb{E}[|2\eta(X)-1|\mathbf{1}\{h_n(X)\neq g^\star(X)\}]\text{ (car $g^\star\in\mathcal{H}$)}\\
&amp;\geq 2\gamma \mathbb{P}(h_n(X)\neq g^\star(X)=2\gamma\sigma_{h_n}^2\\
\Leftrightarrow\ \sigma^2_{h_n}&amp;\leq \frac{L(h_n)-\inf_{h\in\mathcal{H}}L(h)}{2\gamma}
\end{aligned}\end{split}\]</div>
<p>(Pour la premiÃ¨re Ã©galitÃ©, voir le cours sur le classifieur de Bayes)</p>
<p>Nous en dÃ©duisons donc lâ€™inÃ©galitÃ© suivanteÂ :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
L(h_n)-\inf_{h\in\mathcal{H}}L(h)&amp;\leq2\max\left(\sqrt{\frac{2\sigma_{h_n}^2\log\frac{|\mathcal{H}|}{\delta}}{n}};\frac{2\log\frac{|\mathcal{H}|}{\delta}}{3n}\right)\\
&amp;\leq 2\max\left(\sqrt{\frac{(L(h_n)-\inf_{h\in\mathcal{H}}L(h))\log\frac{|\mathcal{H}|}{\delta}}{\gamma n}};\frac{2\log\frac{|\mathcal{H}|}{\delta}}{3n}\right)\\
\Leftrightarrow\ L(h_n)-\inf_{h\in\mathcal{H}}L(h)&amp;\leq 2\max\left(\frac{\log\frac{|\mathcal{H}|}{\delta}}{\gamma n};\frac{2\log\frac{|\mathcal{H}|}{\delta}}{3n}\right)=\frac{2\log\frac{|\mathcal{H}|}{\delta}}{\gamma n}
\end{aligned}\end{split}\]</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">plot</span><span class="p">():</span>
    <span class="n">H_size</span> <span class="o">=</span> <span class="mi">1000</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4511</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">delta</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">H_size</span><span class="p">)</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">e</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">n</span><span class="p">)),</span> 
                 <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\delta=$</span><span class="si">{:.2f}</span><span class="s1">, $|H|=$</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">H_size</span><span class="p">)))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">delta</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">gamma</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">H_size</span><span class="p">)</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">e</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">g</span><span class="p">),</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span> <span class="k">if</span> <span class="n">g</span><span class="o">==</span><span class="mf">0.1</span> <span class="k">else</span> <span class="s1">&#39;dotted&#39;</span><span class="p">,</span>
                     <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\gamma=</span><span class="si">{:.1f}</span><span class="s1">$, $g^\star\in\mathcal{{H}}$, $\delta=$</span><span class="si">{:.2f}</span><span class="s1">, $|H|=$</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                         <span class="n">g</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">H_size</span><span class="p">)))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Majoration du gap de gÃ©nÃ©ralisation avec hypothÃ¨se de marge&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4_VC_theory_32_0.png" src="../_images/4_VC_theory_32_0.png" />
</div>
</div>
<p>On observe que ce majorant est plus rapide que lorsquâ€™on nâ€™a aucune hypothÃ¨se, mais si la marge est petite, on part de plus haut et on met un peu plus de temps Ã  â€œdevenir meilleurâ€. Cependant, il existe toujours un moment oÃ¹ une convergence rapide est nÃ©cessairement meilleure quâ€™une convergence lente.</p>
<p>Ces stratÃ©gies sont des majorants. On observe que sous lâ€™hypohtÃ¨se de marge, le majorant est moins bon que sans hypothÃ¨se lors <span class="math notranslate nohighlight">\(n\)</span> est petit. En pratique, le majorant sans hypothÃ¨se reste valable et nous pourrions avoir une courbe ayant la forme suivante (i.e. on choisit le majorant le plus strict).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">plot</span><span class="p">():</span>
    <span class="n">H_size</span> <span class="o">=</span> <span class="mi">1000</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="mf">0.05</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.2</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4511</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">maj_1</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">H_size</span><span class="p">)</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">delta</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">n</span><span class="p">))</span>
    <span class="n">maj_2</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">H_size</span><span class="p">)</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">delta</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">gamma</span><span class="p">)</span>
    
    <span class="n">maj</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">maj_1</span><span class="p">,</span> <span class="n">maj_2</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">maj</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;Majorant combinÃ©&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Majoration du gap de gÃ©nÃ©ralisation combinÃ© lent+rapide&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4_VC_theory_34_0.png" src="../_images/4_VC_theory_34_0.png" />
</div>
</div>
</div>
<div class="section" id="vii-la-sample-complexity">
<h2>VII. La <em>sample complexity</em><a class="headerlink" href="#vii-la-sample-complexity" title="Permalink to this headline">Â¶</a></h2>
<p>Nous avons vu dans les sections prÃ©cÃ©dentes que pour une dÃ©viation <span class="math notranslate nohighlight">\(\epsilon\)</span> fixÃ©e nous pouvions majorer lâ€™Ã©volution de la probabilitÃ© que notre estimation empirique de lâ€™erreur dÃ©vie dâ€™au moins <span class="math notranslate nohighlight">\(\epsilon\)</span>. Cette probabilitÃ© diminue exponentiellement vite lorsque le jeu de donnÃ©es augmente. Nous avons Ã©galement vu que nous pouvions considÃ©rer une probabilitÃ© fixÃ©e <span class="math notranslate nohighlight">\(1-\delta\)</span> et observer les garanties que nous pouvions avoir sur la quantitÃ© de dÃ©viation relativement Ã  cette probabilitÃ©. Nous avons vu notamment le rÃ©sultat suivant.</p>
<hr class="docutils" />
<p>Soit <span class="math notranslate nohighlight">\(\delta\in[0, 1]\)</span>, <span class="math notranslate nohighlight">\(\exists K&gt;0\)</span> tel que avec probabilitÃ© au moins <span class="math notranslate nohighlight">\(1-\delta\)</span>, nous avons:</p>
<div class="math notranslate nohighlight">
\[\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|&lt;K\sqrt{\frac{d\text{ln}\Big(\frac{en}{d}\Big)-\text{ln}\big(\delta\big)}{n}},\]</div>
<p>oÃ¹ <span class="math notranslate nohighlight">\(d\)</span> est la dimension VC de notre classe de fonctions.</p>
<hr class="docutils" />
<p>Il existe en rÃ©alitÃ© une troisiÃ¨me alternative permettant de visualiser ces quantitÃ©s. Nous pouvons fixer <span class="math notranslate nohighlight">\(\epsilon\)</span> et <span class="math notranslate nohighlight">\(\delta\)</span> et Ã©tudier la taille du jeu de donnÃ©es quâ€™il faudrait afin de garantir que notre dÃ©viation du risque empirique soit infÃ©rieure Ã  <span class="math notranslate nohighlight">\(\epsilon\)</span> avec probabilitÃ© au moins <span class="math notranslate nohighlight">\(1-\delta\)</span>. La fonction dÃ©crivant la <em>sample complexity</em> est:</p>
<div class="math notranslate nohighlight">
\[m_\mathcal{H}:(0, 1)^2\rightarrow \mathbb{N}.\]</div>
<p>Dans le cadre gÃ©nÃ©ral, nous voulons donc trouver <span class="math notranslate nohighlight">\(n\)</span> tel que la partie Ã  droite de lâ€™inÃ©galitÃ© ci-dessus soit Ã©gale Ã  au plus <span class="math notranslate nohighlight">\(\epsilon\)</span>. Dit autrement, nous avons donc:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
K\sqrt{\frac{d\text{ln}\Big(\frac{en}{d}\Big)-\text{ln}\big(\delta\big)}{n}}&amp;\leq \epsilon\\
\Leftrightarrow K\frac{\sqrt{d\text{ln}\Big(\frac{en}{d}\Big)-\text{ln}\big(\delta\big)}}{\epsilon}&amp;\leq\sqrt{n}\\
\Leftrightarrow K^\prime \frac{d\text{ln}\Big(\frac{en}{d}\Big)-\text{ln}\big(\delta\big)}{\epsilon^2}&amp;\leq n
\end{aligned}\end{split}\]</div>
</div>
<div class="section" id="viii-pac-learnability-et-theoreme-fondamental-du-machine-learning">
<h2>VIII. PAC <em>learnability</em> et thÃ©orÃ¨me fondamental du <em>machine learning</em><a class="headerlink" href="#viii-pac-learnability-et-theoreme-fondamental-du-machine-learning" title="Permalink to this headline">Â¶</a></h2>
<p>Les thÃ©orÃ¨mes prÃ©cÃ©dents montrent des rÃ©sultats oÃ¹ avec forte probabilitÃ© (supÃ©rieure Ã  <span class="math notranslate nohighlight">\(1-\delta\)</span> pour <span class="math notranslate nohighlight">\(\delta\)</span> fixÃ©), nous pouvons garantir que le minimiseur du risque empirique sera proche de la plus petite erreur que nous puissions atteindre dans <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>. On parle alors de thÃ©orie PAC pour <em>Probably Approximately Correct</em>. Cela nous mÃ¨ne directement Ã  la dÃ©finition dâ€™Ãªtre apprenable au sens de PAC.</p>
<div class="admonition-definition-agnostic-pac-learnability admonition">
<p class="admonition-title">DÃ©finition (Agnostic PAC <em>learnability</em>)</p>
<p>Soit <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> une classe de fonctions de <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> dans <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>. On dit que <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> est â€œ<em>agnostic</em> PAC <em>learnable</em>â€ sâ€™il existe une fonction <span class="math notranslate nohighlight">\(m_\mathcal{H}:[0, 1]^2\rightarrow\mathbb{N}\)</span> indiquant le nombre de points nÃ©cessaires minimum dans notre jeu de donnÃ©es et un algorithme dâ€™apprentissage <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> tels que pour toute distribution <span class="math notranslate nohighlight">\(\mathbb{P}\)</span> sur <span class="math notranslate nohighlight">\(\mathcal{X}\times\mathcal{Y}\)</span> et <span class="math notranslate nohighlight">\(\forall \epsilon,\delta\)</span>, si <span class="math notranslate nohighlight">\(|S_n|\geq m_\mathcal{H}(\epsilon, \delta)\)</span> oÃ¹ <span class="math notranslate nohighlight">\(S_n\sim\mathbb{P}^n\)</span> on a avec probabilitÃ© au moins <span class="math notranslate nohighlight">\(1-\delta\)</span> (sur <span class="math notranslate nohighlight">\(S_n\)</span>)Â :</p>
<div class="math notranslate nohighlight">
\[L(\mathcal{A}(S_n))\leq \inf_{h^\prime\in\mathcal{H}}L(h^\prime)+\epsilon\]</div>
<p>oÃ¹ <span class="math notranslate nohighlight">\(\mathcal{A}(S_n)\)</span> est le rÃ©sultat de notre algoriothme dâ€™apprentissage sur <span class="math notranslate nohighlight">\(S_n\)</span>.</p>
</div>
<p>Nous utilisons le terme â€œagnostiqueâ€ pour souligner que nous nâ€™avons pas dâ€™hypothÃ¨se quant Ã  la capacitÃ© de <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> Ã  obtenir de bonnes performanceS. On parle de PAC <em>learnability</em> (sans â€œagnostiqueâ€) si <span class="math notranslate nohighlight">\(\inf_{h^\prime\in\mathcal{H}}L(h^\prime)=0\)</span>.</p>
<p>La question qui nous vient immÃ©diatement Ã  lâ€™esprit et de dÃ©terminer les cas oÃ¹ notre classe de fonctions est effectivement <em>agnostic PAC learnable</em>. Le thÃ©orÃ¨me suivante rÃ©pond Ã  cette question.</p>
<div class="admonition-theoreme-fondamental-de-l-apprentissage-statistique admonition">
<p class="admonition-title">ThÃ©orÃ¨me fondamental de lâ€™apprentissage statistique</p>
<p>Soit <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> une classe de fonctions de <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> dans <span class="math notranslate nohighlight">\(\mathcal{Y}=\{0, 1\}\)</span>. ConsidÃ©rons le risque classique <span class="math notranslate nohighlight">\(L(h)=\mathbb{P}(h(X)\neq Y)\)</span>. Les propositions suivantes sont Ã©quivalentesÂ :</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{H}\)</span> est <em>agnostic PAC learnable</em>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{H}\)</span> a une dimension VC finie.</p></li>
</ol>
</div>
<div class="tip admonition">
<p class="admonition-title">IdÃ©e de la preuve</p>
<p>Lâ€™intuition de la preuve va Ãªtre de trouver non seulement des majorant sur <span class="math notranslate nohighlight">\(m_\mathcal{H}\)</span> similaires Ã  ceux que nous avons pu montrer prÃ©cÃ©dement, mais Ã©galement des minorants (i.e. on ne peut pas faire mieux)Â :</p>
<div class="math notranslate nohighlight">
\[\exists C_1, C_2\text{ (universelles) et }\epsilon,\delta,\Â C_1\frac{d+\log\frac{1}{\delta}}{\epsilon^2}\leq m_\mathcal{H}(\epsilon, \delta)\leq C_2\frac{d+\log\frac{1}{\delta}}{\epsilon^2}\]</div>
</div>
<p>Ainsi, si la dimension VC est infinie, le minorant diverge et <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> nâ€™est pas PAC learnable. Je vous invite Ã  vous rÃ©fÃ©rer Ã </p>
<p><em>Shalev-Shwartz, Shai, et Shai Ben-David. Understanding Machine Learning: From Theory to Algorithms. Cambridge: Cambridge University Press, 2014.</em></p>
<p>pour la preuve dÃ©taillÃ©e.</p>
</div>
<div class="section" id="ix-decidabilite-de-l-apprentissage">
<h2>IX. DÃ©cidabilitÃ© de lâ€™apprentissage<a class="headerlink" href="#ix-decidabilite-de-l-apprentissage" title="Permalink to this headline">Â¶</a></h2>
<p>Nous venons de voir que la dimension VC dâ€™une classe de fonctions <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> permettait de caractÃ©riser lâ€™apprenabilitÃ© de cette derniÃ¨re. Il y a en effet une Ã©quivalence mathÃ©matique entre la finitude de la dimension VC (un critÃ¨re combinatoire) et la <em>PAC leanability</em>. Ainsi, si la dimension VC est infinie, minimiser le risque empirique ne nous permet pas de faire un bon choix.</p>
<p>De fait, dÃ©montrer la finitude de la dimension VC dâ€™une classe de fonctions <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> est Ã©quivalent Ã  dÃ©montrer lâ€™apprenabilitÃ© de cette classe. Malheureusement, cela peut Ãªtre indÃ©cidable.</p>
<div class="admonition-theoreme-indecidabilite-dans-pac admonition">
<p class="admonition-title">ThÃ©orÃ¨me (IndÃ©cidabilitÃ© dans PAC)</p>
<p>Soit <span class="math notranslate nohighlight">\(F\)</span> un <a class="reference external" href="https://fr.wikipedia.org/wiki/Syst%C3%A8me_formel">systÃ¨me formel</a> rÃ©cursif (calculable). Il existe des classes de fonctions de <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> dans <span class="math notranslate nohighlight">\(\mathcal{Y}=\{0, 1\}\)</span> notÃ©es <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> telles queÂ :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{H}\)</span> est <em>PAC learnable</em> sans que cela puisse Ãªtre dÃ©montrÃ© dans <span class="math notranslate nohighlight">\(F\)</span> et <span class="math notranslate nohighlight">\(F\)</span> est consistent,</p></li>
<li><p><span class="math notranslate nohighlight">\(F\)</span> nâ€™est pas consistent et <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> nâ€™est pas <em>PAC learnable</em>.</p></li>
</ul>
</div>
<p>Pour plus dâ€™informations quant aux thÃ©orÃ¨mes dâ€™incomplÃ©tude de GÃ¶del, voir cette <a class="reference external" href="https://www.youtube.com/watch?v=SBwupYwDgHg">vidÃ©o</a>.</p>
</div>
<div class="section" id="x-l-approche-train-test-ou-validation-croisee">
<h2>X. Lâ€™approche train-test (ou validation croisÃ©e)<a class="headerlink" href="#x-l-approche-train-test-ou-validation-croisee" title="Permalink to this headline">Â¶</a></h2>
<p>Les rÃ©sultats prÃ©cÃ©dents montrent que plus le modÃ¨le (i.e. <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>) est complexe, plus le risque que notre minimiseur ne soit pas un bon minimieur est grand. Cela vient du fait que plus notre modÃ¨le est complexe, plus notre estimation des performances de notre minimiseur risque dâ€™Ãªtre mauvaiseâ€¦</p>
<p>Pour cela, il est important de garder un second jeu de donnÃ©es de <strong>test</strong> que nous noterons <span class="math notranslate nohighlight">\(T_m\)</span> afin dâ€™Ã©valuer les performances de notre modÃ¨le de maniÃ¨re plus stricte.</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p>Soit un jeu de donnÃ©es de test <span class="math notranslate nohighlight">\(T_m\)</span> de taille <span class="math notranslate nohighlight">\(m\)</span>, notre minimiseur empirique <span class="math notranslate nohighlight">\(h_n\)</span> et <span class="math notranslate nohighlight">\(\delta&gt;0\)</span>. Majorez lâ€™erreur du risque empirique de <span class="math notranslate nohighlight">\(h_n\)</span> sur <span class="math notranslate nohighlight">\(T_m\)</span> avec probabilitÃ© <span class="math notranslate nohighlight">\(1-\delta\)</span>. Servez-vous dâ€™un des rÃ©sultats prÃ©cÃ©dents.</p>
</div>
</div>
<div class="section" id="xi-en-conclusion">
<h2>XI. En conclusion<a class="headerlink" href="#xi-en-conclusion" title="Permalink to this headline">Â¶</a></h2>
<p>Il y a deux sources de difficultÃ©.</p>
<div class="section" id="a-difficulte-epistemique-incertitude-du-modele">
<h3>A. DifficultÃ© Ã©pistÃ©mique (incertitude du modÃ¨le)<a class="headerlink" href="#a-difficulte-epistemique-incertitude-du-modele" title="Permalink to this headline">Â¶</a></h3>
<p>La premiÃ¨re source de difficultÃ© est celle Ã  laquelle on pense le plus souventÂ :</p>
<ul class="simple">
<li><p>La frontiÃ¨re de dÃ©cision est vraiment complexe,</p></li>
<li><p>Les donnÃ©es sont en grande dimension,</p></li>
<li><p>Il y a beaucoup de variables et peu de signal et nous ne savons repÃ©rer ce dernier,</p></li>
<li><p>etc.</p></li>
</ul>
<p>Dans ce cas de figure, Ã  moins dâ€™avoir une connaissance experte, nous devons considÃ©rer un grand nombre de fonctions <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> (e.g. un grand nombre de paramÃ¨tres). Ainsi, la dimension VC risque dâ€™Ãªtre importante et le risque de sur-apprentissage fort. Il nous faut donc un grand jeu de donnÃ©es pour compenser la taille de <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> et obtenir un bon modÃ¨le.</p>
</div>
<div class="section" id="b-difficulte-aleatorique-incertitude-des-donnees">
<h3>B. DifficultÃ© alÃ©atorique (incertitude des donnÃ©es)<a class="headerlink" href="#b-difficulte-aleatorique-incertitude-des-donnees" title="Permalink to this headline">Â¶</a></h3>
<p>La seconde difficultÃ© vient de lâ€™ambiguÃ¯tÃ© de la tÃ¢che. Dans le cadre de la classification binaire, en notant <span class="math notranslate nohighlight">\(\eta(x)=\mathbb{P}\big(Y=1|X=x\big)\)</span>, on parle dâ€™ambiguÃ¯tÃ© lorsque <span class="math notranslate nohighlight">\(\eta(x)\)</span> est proche de <span class="math notranslate nohighlight">\(0.5\)</span>. Dans ce cas de figure, il nous faut beaucoup plus de points pour converger vers la meilleure solution de notre classe de fonctions <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>.</p>
</div>
<div class="section" id="c-autres-mesures-de-complexite">
<h3>C. Autres mesures de complexitÃ©<a class="headerlink" href="#c-autres-mesures-de-complexite" title="Permalink to this headline">Â¶</a></h3>
<p>La dimension VC mesure la complexitÃ© dâ€™une classe de fonctions dans le cadre dâ€™un problÃ¨me de classification binaire. Cette mesure se gÃ©nÃ©ralise au cas multi-classes (voire top-k) avec la dimension de Natarajan et dâ€™autres. Les <em>fat-shattering coefficients</em> permettent de quantifier la mesure de complexitÃ© dâ€™une classe de fonctions de <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> dans <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>. Ces mesures de complexitÃ©s permettent de dÃ©finir une notion dâ€™apprenabilitÃ©. Il existe une Ã©quivalence entre leur finitude et la capacitÃ© Ã  choisir une bonne approximation pour notre problÃ¨me.</p>
<div class="admonition-theoreme-apprendre-peut-etre-indecidable admonition">
<p class="admonition-title">ThÃ©orÃ¨me (Apprendre peut Ãªtre indÃ©cidable)</p>
<p>Il existe des problÃ¨mes de <em>machine learning</em> oÃ¹ dÃ©terminer ce genre dâ€™Ã©quivalences nâ€™est pas possible Ã  moins que le systÃ¨me dans lequel la preuve est faite soit inconsistent (cf. incomplÃ©tude de GodÃ«l).</p>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./3_classification"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="3_bayes_classifier.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Le classifieur de Bayes â˜•ï¸</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../4_kernel_methods/0_propos_liminaire.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Les mÃ©thodes Ã  noyaux</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Servajean, Leveau & Chailan<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>
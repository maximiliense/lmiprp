{"cells": [{"cell_type": "markdown", "id": "resistant-diving", "metadata": {}, "source": ["# Le classifieur de Bayes \u2615\ufe0f\n", "\n", "**<span style='color:blue'> Objectifs de la s\u00e9quence</span>** ", "\n", "* Comprendre&nbsp;:\n", "    * le probl\u00e8me qu'on souhaite r\u00e9soudre en *machine learning*,\n", "* \u00catre sensibilis\u00e9 aux notions de&nbsp;:\n", "    * erreur de Bayes\n", "    * classifieur de Bayes.\n", "    \n", "\n\n ----"]}, {"cell_type": "markdown", "id": "bigger-contribution", "metadata": {}, "source": ["## I. Introduction\n", "\n", "Reformalisons notre probl\u00e8me de classification. Soit $\\mathcal{X}$ l'espace des donn\u00e9es d'entr\u00e9e et $\\mathcal{Y}$ celui des donn\u00e9es de sortie. On aurait par exemple $\\mathcal{X}$ l'ensemble de toutes les photos et $\\mathcal{Y}$ celui des labels \"chien\" et \"chat\". *A priori*, dans le cadre d'une application de d\u00e9tection \"chien/chat\", on ne s'attend pas \u00e0 trouver n'importe quelle image (e.g. on ne s'attend pas \u00e0 voir des photos de pizzas). L'id\u00e9e est d'interpr\u00e9ter cela de mani\u00e8re probabiliste en consid\u00e9rons le couple de variables al\u00e9atoires suivant : \n", "\n", "$$X, Y\\in\\mathcal{X}\\times\\mathcal{Y}.$$\n", "\n", "On appellera $\\mu$ la mesure de $X$ dans le sens o\u00f9 \u00e9tant donn\u00e9 $A\\subseteq\\mathcal{X}$, on a $\\mathbb{P}(X\\in A)=\\mu(A)$ et $\\eta(x)=\\mathbb{P}(Y=1|X=x)$ (dans le cas de la classification binaire) la \"probabilit\u00e9 a posteriori\". La fonction $\\eta$ rev\u00eate d'autres formes lorsqu'on travaille sur d'autres probl\u00e8mes (e.g. classification, top-k). Il est important de constater que $\\mu$ et $\\eta$ d\u00e9crivent totalement le couple $X, Y$. En effet, soit $A\\subseteq \\mathcal{X}\\times \\mathcal{Y}$, nous avons :\n", "\n", "$$\\mathbb{P}(X, Y\\in A)=\\int_{A\\cap \\mathcal{X}\\times \\{1\\}}\\eta(x)d\\mu+\\int_{A\\cap \\mathcal{X}\\times \\{0\\}}(1-\\eta(x))d\\mu.$$\n", "\n", "Notre objectif est de construire une application $h:\\mathcal{X}\\mapsto\\mathcal{Y}$ telle que les \"pr\u00e9dictions\" de $h$ soient bonnes dans le sens du risque suivant :\n", "\n", "$$L(h)=\\mathbb{P}(h(X)\\neq Y)=\\mathbb{E}\\big[\\textbf{1}\\{h(X)\\neq Y\\}\\big].$$\n", "\n", "On veut que la probabilit\u00e9 que $h$ fasse des erreurs soit la plus faible possible.\n", "\n", "Ne connaissant ni $\\mu$ ni $\\eta$, nous ne pouvons pas calculer $L$ et donc trouver le meilleur $h$. Habituellement, en *machine learning*, nous collectons des donn\u00e9es afin d'estimer $L$ et construire le classifieur appropri\u00e9. Cependant, dans ce *notebook*, nous supposerons que nous sommes un oracle et que nous connaissons le processus g\u00e9n\u00e9rateur.\n", "\n", "## II. Le classifieur de Bayes\n", "\n", "Supposons que nous connaissions $\\eta$ (nous sommes un oracle). Quel est le meilleur classifieur que nous puissions construire ?\n", "\n", "Il s'agit du classifieur suivant :\n", "\n", "$$g^\\star(x)=\\begin{cases}1&\\text{ si }\\eta(x)\\geq 0.5\\\\ 0&\\text{ sinon.}\\end{cases}$$\n", "\n", "C'est ce qu'on appelle le classifieur de Bayes. C'est lui qui fait le moins d'erreurs (notons qu'il peut exister plusieurs classifieurs qui font aussi peu d'erreurs). On peut quantifier le risque atteint par ce classifieur :\n", "\n", "$$L^\\star=\\mathbb{E}\\big[\\text{min}(\\eta(X), 1-\\eta(X))\\big].$$\n", "\n", "Si les labels sont d\u00e9terministes (i.e. $\\eta\\in\\{0, 1\\}$), alors $L^\\star=0$.\n", "\n", "**<span style='color:blue'> Proposition</span>** ", "\n", "$\\not\\exists g:\\mathcal{X}\\mapsto\\mathcal{Y}$ tel que $L(g)<L^\\star$. Dit autrement, on ne peut pas faire mieux que le classifieur de Bayes.\n", "\n", "\n\n ----", "\n", "**<span style='color:orange'> Preuve</span>** ", "\n", "Soit $x\\in\\mathcal{X}$ et $g:\\mathcal{X}\\mapsto\\mathcal{Y}$ un classifieur quelconque. On a :\n", "\n", "$$\\begin{aligned}\n", "\\mathbb{P}(g(X)\\neq Y|X=x)&=1-\\mathbb{P}(g(X)=Y|X=x)\\\\\n", "&= 1-(\\mathbb{P}(Y=1, g(X)=1|X=x)+\\mathbb{P}(Y=0, g(X)=0|X=x))\\\\\n", "&=1-(\\textbf{1}\\{g(x)=1\\}\\mathbb{P}(Y=1|X=x)+\\textbf{1}\\{g(x)=0\\}\\mathbb{P}(Y=0|X=x))\\\\\n", "&=1-(\\textbf{1}\\{g(x)=1\\}\\eta(x)+\\textbf{1}\\{g(x)=0\\}(1-\\eta(x))).\n", "\\end{aligned}$$\n", "\n", "Ainsi, nous avons :\n", "\n", "$$\\begin{aligned}\n", "\\mathbb{P}(g^\\star(X)\\neq Y|X=x)-\\mathbb{P}(g(X)\\neq Y|X=x)&=\\eta(x)(\\textbf{1}\\{g(x)=1\\}-\\textbf{1}\\{g^\\star(x)=1\\})\\\\&\\ \\ \\ +(1-\\eta(x))(\\textbf{1}\\{g^\\star(x)=0\\}-\\textbf{1}\\{g(x)=0\\})\\\\\n", "&=(2\\eta(x)-1)(\\textbf{1}\\{g^\\star(x)=1\\}-\\textbf{1}\\{g(x)=1\\})\\\\\n", "&\\geq 0\n", "\\end{aligned}$$\n", "\n", "Il suffit maintenant d'int\u00e9grer sur tous les \"$x$\" et le r\u00e9sultat est l\u00e0.\n", "\n", "\n\n ----", "\n", "Soit $h$ une fonction de $\\mathcal{X}$ dans $\\mathcal{Y}$. La question qu'on peut se poser est celle de l'exc\u00e8s de $h$ par rapport \u00e0 $g^\\star$. Comme on l'a vu dans la preuve pr\u00e9c\u00e9dente, nous avons&nbsp;:\n", "\n", "$$\n", "\\begin{aligned}\n", "\\mathbb{P}(h(X)\\neq Y)-L^\\star&=\\mathbb{E}\\big[|2\\eta(X)-1|\\textbf{1}\\{g^\\star(X)\\neq h(X)\\}\\big]\\\\\n", "\\end{aligned}$$"]}, {"cell_type": "markdown", "id": "twelve-perry", "metadata": {}, "source": ["## III. Un petit exercice"]}, {"cell_type": "markdown", "id": "compound-dress", "metadata": {}, "source": ["Construisons un jeu de donn\u00e9es de classification binaire."]}, {"cell_type": "code", "execution_count": null, "id": "wrapped-nurse", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from scipy.special import expit as sigmoid\n", "\n", "beta = np.random.uniform(-10, 10, size=(2, 1))\n", "\n", "def class_probability(x):\n", "    logit = np.dot(x, beta)\n", "    proba = sigmoid(logit)\n", "    \n", "    return proba\n", "\n", "def dataset(n):\n", "    X = np.random.uniform(-1, 1, size=(n, 2))\n", "    y = np.random.binomial(n=1, p=class_probability(X))\n", "    return X, y\n", "\n", "X, y = dataset(50)"]}, {"cell_type": "code", "execution_count": null, "id": "blind-potter", "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "\n", "plt.figure(figsize=(12, 8))\n", "plt.scatter(X[:, 0], X[:, 1], c=y)\n", "plt.show()"]}, {"cell_type": "markdown", "id": "mobile-composition", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**En vous appuyant sur le code pr\u00e9c\u00e9dent (en r\u00e9utilisant \u00e9ventuellement du code fourni), impl\u00e9mentez le classifieur de Bayes.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "id": "fallen-diary", "metadata": {}, "source": ["def bayes_classifier(x):\n", "    ####### Complete this part ######## or die ####################\n", "    ...\n", "    ###############################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "id": "ee18b7b0", "metadata": {}, "source": ["plt.figure(figsize=(12, 8))\n", "XX, YY = np.mgrid[-1:1:500j, -1:1:500j]\n", "predictions = bayes_classifier(np.stack([XX, YY], axis=2).reshape(500*500, 2))\n", "\n", "plt.pcolormesh(XX, YY, predictions.reshape(500, 500), cmap=plt.cm.Paired, shading='auto')\n", "plt.scatter(X[:, 0], X[:, 1], c=y)\n", "plt.xlim(-1, 1)\n", "plt.ylim(-1, 1)\n", "plt.title('Le classifieur de Bayes')\n", "plt.show()\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "e70d4a28", "metadata": {}, "source": ["## IV. Quelques exercices"]}, {"cell_type": "markdown", "id": "grateful-franklin", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Consid\u00e9rons cette fois-ci le probl\u00e8me de classification multi-classes. Ici, $\\mathcal{Y}=\\{1, \\ldots, C\\}$. Notons $\\eta_k(x)=\\mathbb{P}(Y=k|X=x)$.**\n", "\n", "* **Trouvez le classifieur de Bayes.**\n", "* **Trouvez l'expression de l'erreur de Bayes.**\n", "\n", "\n\n ----", "\n", "\n"]}, {"cell_type": "markdown", "id": "portable-paradise", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Consid\u00e9rons cette fois-ci le probl\u00e8me de classification multi-classes *Top-K*. \u00c0 chaque pr\u00e9diction exactement $K$ classes sont retourn\u00e9es. Cette fois-ci, nos pr\u00e9dictions se font dans l'espace $\\mathcal{P}(\\mathcal{Y})$ l'ensemble des parties de $\\mathcal{Y}$. Notons $\\eta_k(x)=\\mathbb{P}(Y=k|X=x)$.**\n", "\n", "* **Trouvez le classifieur de Bayes.**\n", "* **Trouvez l'expression de l'erreur de Bayes.**\n", "\n", "\n\n ----", "\n"]}, {"cell_type": "markdown", "id": "8ecdccc8", "metadata": {}, "source": ["**<span style='color:blue'> Exercice (partie I)</span>** ", "\n", "**Imaginons une situation o\u00f9 la variable al\u00e9atoire $X$ repr\u00e9sente le nombre d'heures pass\u00e9es \u00e0 revoir le cours chez soi par semaine. Soit la loi conditionnelle suivante&nbsp;:**\n", "\n", "$$\\eta(x)=\\frac{x}{c+x},\\ c>0,$$\n", "\n", "**o\u00f9 $c$ est une constante. Celle-ci indique la probabilit\u00e9 que l'\u00e9tudiant r\u00e9ussisse son ann\u00e9e. Dit autrement, plus un \u00e9tudiant travaille chez lui, plus la probabilit\u00e9 qu'il r\u00e9ussisse est grande. Construisez le classifieur de Bayes et d\u00e9duisez-en l'erreur de Bayes.**\n", "\n", "\n\n ----", "\n", "\n", "**<span style='color:blue'> Exercice (partie II)</span>** ", "\n", "**En compl\u00e9tant l'exercice pr\u00e9c\u00e9dent, supposez les cas suivants et calculez une valeur de $L^\\star$&nbsp;:**\n", "\n", "* $\\mu(\\{c\\})=\\mathbb{P}(X=c)=1$,\n", "* $X$ suit une loi uniforme sur $[0, 4c]$ (i.e. $\\mu([a;b])=\\int_a^b\\frac{1}{b-a}dx$).\n", "\n", "\n\n ----", "\n", "\n", "En pratique, nous aurions collect\u00e9 un jeu de donn\u00e9es $S_n=\\{(X_i,Y_i)\\}_{i\\leq n}$ et nous aurions d\u00fb choisir notre classifieur \u00e0 partir de ce dernier."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.2"}}, "nbformat": 4, "nbformat_minor": 5}
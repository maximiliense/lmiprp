La *classification*
=======================

Les problèmes de *classification* font références aux cas où notre espace cible $\mathcal{Y}$ est un ensemble de classes. Ce chapitre se construit de la manière suivante&nbsp;:

1. **Régression logistique** - Cette séquence est le miroir de la régression linéaire. On abordera le problème de manière approfondie au travers de son formalisme, de son optimisation, de la convergence de son optimisation ou encore de sa linéarité.
2. **Les fonctions proxy** - Nous montrerons pourquoi nous ne minimisons que rarement le nombre d’erreur de notre problème de classification. Nous montrerons les propriétés que doivent avoir les fonctions qu’on minimise en pratique.
3. **Le classifieur de Bayes** - Et si nous connaissions la loi de probabilité qui contrôle nos données ? Quel serait le meilleur modèle que nous puissions faire&nbsp;: c’est le classifieur de Bayes. Comment se construit-il ?
4. **La théorie de Vapnik et Chervonenkis** - C’est une séquence difficile. Nous essaierons en particulier de répondre à la question "pourquoi ça marche". Il n’est en effet ni logique ni trivial de constater que le *machine learning* "marche". 


<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Les fonctions de perte (loss function) â˜•ï¸â˜•ï¸â˜•ï¸ &#8212; Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Le classifieur de Bayes â˜•ï¸" href="3_bayes_classifier.html" />
    <link rel="prev" title="La rÃ©gression logistique â˜•ï¸" href="1_logistic_regression.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-72WWYCKNK6"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-72WWYCKNK6');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../0_requisite/rappels_python.html">
   Rappels de Python et Numpy
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../1_what_is_ml/0_propos_liminaire.html">
   <em>
    Machine Learning
   </em>
   , initiation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/1_introduction_ml.html">
     <em>
      Machine learning
     </em>
     et malÃ©diction de la dimension
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/2_regression_and_classification_trees.html">
     Les arbres de rÃ©gression et de classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../2_regression/0_propos_liminaire.html">
   La
   <em>
    rÃ©gression
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/1_linear_regression.html">
     La rÃ©gression linÃ©aire â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/2_optimization.html">
     Lâ€™optimisation â˜•ï¸â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/3_interpolation.html">
     Interpolation â˜•ï¸â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/4_algo_proximal_lasso.html">
     Sous-diffÃ©rentiel et le cas du Lasso â˜•ï¸â˜•ï¸â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/5_least_square_qr.html">
     Les moindres carrÃ©s via une dÃ©composition QR (et plus)â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/6_ridge.html">
     Une analyse de la rÃ©gularisation Ridge â˜•ï¸â˜•ï¸â˜•ï¸
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="0_propos_liminaire.html">
   La
   <em>
    classification
   </em>
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="1_logistic_regression.html">
     La rÃ©gression logistique â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Les fonctions de perte (loss function) â˜•ï¸â˜•ï¸â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3_bayes_classifier.html">
     Le classifieur de Bayes â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="4_VC_theory.html">
     Un modÃ¨le formel de lâ€™apprentissage â˜•ï¸â˜•ï¸â˜•ï¸â˜•ï¸ (ğŸ’†â€â™‚ï¸)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../4_kernel_methods/0_propos_liminaire.html">
   Les mÃ©thodes Ã  noyaux
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../4_kernel_methods/1_svm.html">
     Le SVM ou lâ€™hypothÃ¨se max-margin â˜•ï¸â˜•ï¸
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5_ensembles/0_propos_liminaire.html">
   Les mÃ©thodes
   <em>
    ensemblistes
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/1_ensembles.html">
     MÃ©thodes ensemblistes â˜•ï¸â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/2_bayesian_linear_regression.html">
     Bayesian linear regression â˜•ï¸â˜•ï¸â˜•ï¸â˜•ï¸
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../6_deeplearning/0_propos_liminaire.html">
   <em>
    deep learning
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/1_autodiff.html">
     La diffÃ©rentiation automatique et un dÃ©but de
     <em>
      deep learning
     </em>
     â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/2_filters_representation.html">
     Filtres et espace de reprÃ©sentation des rÃ©seaux de neurones â˜•ï¸â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/3_probabilities_calibration.html">
     Calibration des probabilitÃ©s et quelques notions â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/4_regularization_deep.html">
     RÃ©gularisation en
     <em>
      deep learning
     </em>
     â˜•ï¸â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/5_transfer_multitask.html">
     <em>
      Transfer learning
     </em>
     et apprentissage multi-tÃ¢ches â˜•ï¸â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/6_adversarial.html">
     Les attaques adversaires â˜•ï¸
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../7_unsupervised/0_propos_liminaire.html">
   Lâ€™apprentissage non-supervisÃ©
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_unsupervised/1_principal_component_analysis.html">
     Lâ€™Analyse en Composantes Principales â˜•ï¸â˜•ï¸â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_unsupervised/2_em_gaussin_mixture_model.html">
     ModÃ¨le de MÃ©lange Gaussien et algorithme
     <em>
      Expectation-Maximization
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../8_set_prediction/0_propos_liminaire.html">
   PrÃ©diction dâ€™ensembles
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/1_well_defined.html">
     Jeu dâ€™apprentissage
     <em>
      set-valued
     </em>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/2_ill_defined.html">
     Jeu dâ€™apprentissage uniquement multi-classes â˜•ï¸
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../biblio.html">
   Bibliographie
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/3_classification/2_fonctions_proxy.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/maximiliense/lmiprp"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/maximiliense/lmiprp/issues/new?title=Issue%20on%20page%20%2F3_classification/2_fonctions_proxy.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/maximiliense/lmiprp/blob/master/Travaux Pratiques/Machine Learning/Book/3_classification/2_fonctions_proxy.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#i-une-premiere-minimisation-du-risque-empirique">
   I. Une premiÃ¨re minimisation du risque empirique
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#construction-du-jeu-de-donnees">
     Construction du jeu de donnÃ©es
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#affichage-du-dataset">
     Affichage du dataset
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#notre-classe-de-fonctions-lineaires-mathcal-h">
     Notre classe de fonctions linÃ©aires
     <span class="math notranslate nohighlight">
      \(\mathcal{H}\)
     </span>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ii-construction-du-phi-risk">
   II. Construction du
   <span class="math notranslate nohighlight">
    \(\phi\)
   </span>
   -risk
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iii-les-proprietes-d-un-phi-risk">
   III. Les propriÃ©tÃ©s dâ€™un
   <span class="math notranslate nohighlight">
    \(\phi\)
   </span>
   -risk
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-majoration-du-risque-0-1">
     A. Majoration du risque 0/1
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-convexite">
     B. ConvexitÃ©
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#c-calibre-pour-la-classification">
     C. CalibrÃ© pour la classification
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#d-construction">
     D. Construction
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iv-logistic-loss-et-regression-logistique">
   IV.
   <em>
    Logistic loss
   </em>
   et rÃ©gression logistique
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#v-optimiser-votre-loss">
   V. Optimiser votre
   <em>
    loss
   </em>
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="les-fonctions-de-perte-loss-function">
<h1>Les fonctions de perte (loss function) â˜•ï¸â˜•ï¸â˜•ï¸<a class="headerlink" href="#les-fonctions-de-perte-loss-function" title="Permalink to this headline">Â¶</a></h1>
<div class="admonition-objectifs-de-la-sequence admonition">
<p class="admonition-title">Objectifs de la sÃ©quence</p>
<ul class="simple">
<li><p>ComprendreÂ :</p>
<ul>
<li><p>pourquoi on ne minimise pas en gÃ©nÃ©ral le nombre dâ€™erreurs,</p></li>
<li><p>les caractÃ©ristiques des fonctions (<em>loss</em>) quâ€™on minimise en pratique.</p></li>
</ul>
</li>
<li><p>ÃŠtre capable deÂ :</p>
<ul>
<li><p>proposer une <em>loss</em>,</p></li>
<li><p>et la minimiser.</p></li>
</ul>
</li>
</ul>
</div>
<p>Situons-nous dans le cadre dâ€™un problÃ¨me de classification supervisÃ© et notons <span class="math notranslate nohighlight">\(\mathcal{X}\subseteq\mathbb{R}^d\)</span> et <span class="math notranslate nohighlight">\(\mathcal{Y}=\{0, 1\}\)</span> ou <span class="math notranslate nohighlight">\(\mathcal{Y}=\{-1,+1\}\)</span> selon le contexte et afin de simplifier les notations. Notons Ã©galement <span class="math notranslate nohighlight">\(X,Y\in\mathcal{X}\times\mathcal{Y}\)</span> deux variables alÃ©atoires telles que <span class="math notranslate nohighlight">\(\mu\)</span> est la mesure de <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(\eta(x)=\mathbb{P}(Y=1|X=x)\)</span>. La connaissance de <span class="math notranslate nohighlight">\(\mu\)</span> et de <span class="math notranslate nohighlight">\(\eta\)</span> nous donne toutes les informations propres au processus gÃ©nÃ©rateur de notre problÃ¨me. Nous pouvons dâ€™ailleurs construire la mesure jointe. Soit <span class="math notranslate nohighlight">\(A\subseteq \mathcal{X}\times\mathcal{Y}\)</span>, nous avons:</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}(X, Y \in A)=\int_{A\cap \mathcal{X}\times \{1\}}\eta(x)d\mu+\int_{A\cap \mathcal{X}\times \{0\}}(1-\eta(x))d\mu.\]</div>
<p>Notre objectif est de trouver une application <span class="math notranslate nohighlight">\(h:\mathcal{X}\mapsto\mathcal{Y}\)</span> tel quâ€™un risque est minimisÃ©. <em>A priori</em> ce que nous souhaitons faire est de minimiser le nombre dâ€™erreurs. Pour cela, nous dÃ©finissons le risque dit <span class="math notranslate nohighlight">\(0/1\)</span>:</p>
<div class="math notranslate nohighlight">
\[L(h)=\mathbb{P}(h(X)\neq Y)=\mathbb{E}\big[1\{X\neq Y\}\big].\]</div>
<p>Nous ne connaissons malheureusement ni <span class="math notranslate nohighlight">\(\mu\)</span> ni <span class="math notranslate nohighlight">\(\eta\)</span> et pour cela, nous devons estimer le risque <span class="math notranslate nohighlight">\(L\)</span> sur un jeu de donnÃ©es:</p>
<div class="math notranslate nohighlight">
\[S_n=\{(X_i, Y_i)\}_{i\leq n}\sim \mathbb{P}^n,\]</div>
<p>quâ€™on supposera reprÃ©sentatif du problÃ¨me dans le sens oÃ¹ les couples <span class="math notranslate nohighlight">\((X, Y)\)</span> sont iid. Ã€ partir de lÃ , nous pouvons estimer notre risque empirique:</p>
<div class="math notranslate nohighlight">
\[L_n(h)=\frac{1}{n}\sum_i 1\{h(X_i)\neq Y_i\}.\]</div>
<p>Il sâ€™agit tout simplement de la moyenne des erreurs.</p>
<p>Notre objectif, en tant que <em>machine learner</em> est de minimiser cette erreur. Bien sÃ»r (#nofreelunchtheorem), nous ne pouvons pas considÃ©rer toutes les fonctions de <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> dans <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>. Soit <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> la classe de fonctions que nous souhaitons considÃ©rer. Le problÃ¨me Ã  rÃ©soudre est:</p>
<div class="math notranslate nohighlight">
\[h_n=\text{argmin}_{h\in\mathcal{H}}L_n(h).\]</div>
<p>Câ€™est ce quâ€™on appelle le minimiseur du risque empirique. Remarquez que ce nâ€™est pas exactement ce quâ€™on minimise en pratique. Nous allons voir pourquoi.</p>
<div class="section" id="i-une-premiere-minimisation-du-risque-empirique">
<h2>I. Une premiÃ¨re minimisation du risque empirique<a class="headerlink" href="#i-une-premiere-minimisation-du-risque-empirique" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="construction-du-jeu-de-donnees">
<h3>Construction du jeu de donnÃ©es<a class="headerlink" href="#construction-du-jeu-de-donnees" title="Permalink to this headline">Â¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">h_star</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">y</span><span class="p">[</span><span class="n">x</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">&gt;</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">y</span><span class="p">[</span><span class="n">x</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">&lt;=</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="k">return</span> <span class="n">y</span>

<span class="k">def</span> <span class="nf">construct_dataset</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">h_star</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">construct_dataset</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="affichage-du-dataset">
<h3>Affichage du dataset<a class="headerlink" href="#affichage-du-dataset" title="Permalink to this headline">Â¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_fonctions_proxy_9_0.png" src="../_images/2_fonctions_proxy_9_0.png" />
</div>
</div>
</div>
<div class="section" id="notre-classe-de-fonctions-lineaires-mathcal-h">
<h3>Notre classe de fonctions linÃ©aires <span class="math notranslate nohighlight">\(\mathcal{H}\)</span><a class="headerlink" href="#notre-classe-de-fonctions-lineaires-mathcal-h" title="Permalink to this headline">Â¶</a></h3>
<p>ConsidÃ©rons le cas trÃ¨s simple des modÃ¨les linÃ©aires. Ici, notre ensemble de fonctions <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> est lâ€™ensemble de cardinal infini dont les frontiÃ¨res de dÃ©cision sont les droites dans le planÂ :</p>
<div class="math notranslate nohighlight">
\[\mathcal{H}=\{x\mapsto\text{sign}(\langle \omega, x\rangle+b):\ \omega\in\mathbb{R}^2,\ b\in\mathbb{R}\},\]</div>
<p>oÃ¹Â :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{sign}(x)=\begin{cases}+1\text{ si }x\geq 0\\-1\text{ sinon.}\end{cases}\end{split}\]</div>
<p>oÃ¹, nous avons supposÃ© que <span class="math notranslate nohighlight">\(\mathcal{Y}=\{-1, 1\}\)</span> afin de simplifier les notations. Notre problÃ¨me de minimisation devient doncÂ :</p>
<div class="math notranslate nohighlight">
\[(\omega, b)=\text{argmin}_{\omega, b\in \mathbb{R}^2\times\mathbb{R}}\frac{1}{n}\sum_i \mathbf{1}\{\text{sign}(\langle \omega, x_i\rangle+b)\neq y_i\}.\]</div>
<p>Notez que nous avons la formulation Ã©quivalente suivanteÂ :</p>
<div class="math notranslate nohighlight">
\[(\omega, b)=\text{argmin}_{\omega, b\in \mathbb{R}^2\times\mathbb{R}}\frac{1}{n}\sum_i \mathbf{1}\{ y_i(\langle \omega, x_i\rangle+b)&lt; 0\}.\]</div>
<p>Comment faire cela ? La fonction Ã  optimiser est constante par morceau et les optimiseurs Ã  base dâ€™information du premier ordre (i.e. dÃ©rivÃ©e, gradient) ne peuvent pas nous aider. Une stratÃ©gie est de constater que dans le plan, une droite nâ€™a besoin que de deux points pour se positionner. Ã€ partir de deux points <span class="math notranslate nohighlight">\(x_1\)</span> et <span class="math notranslate nohighlight">\(x_2\)</span>, il est possible dâ€™obtenir une valeur de <span class="math notranslate nohighlight">\(\omega\)</span> et de <span class="math notranslate nohighlight">\(b\)</span> en rÃ©solvant le systÃ¨me dâ€™Ã©quations suivantÂ :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{cases}\langle\omega, x_1\rangle + b=0\\\langle\omega, x_2\rangle +b=0.\end{cases}\end{split}\]</div>
<p>Il y a Ã©videmment une infinitÃ© de solutions (deux Ã©quations et trois inconnues). On peut rÃ©soudre ce problÃ¨me en contraignant la norme de <span class="math notranslate nohighlight">\(\omega\)</span> Ã  valoir <span class="math notranslate nohighlight">\(1\)</span>.</p>
<p>Il suffit maintenant de tester toutes les valeurs possibles de <span class="math notranslate nohighlight">\(\omega\)</span> et de <span class="math notranslate nohighlight">\(b\)</span> sâ€™appuyant sur des points de notre jeu de donnÃ©es et de prendre celle qui fait le moins dâ€™erreursÂ !</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>ImplÃ©mentez notre modÃ¨le de classification <span class="math notranslate nohighlight">\(h\)</span> (i.e. la fonction qui fait une prÃ©diction quant Ã  la classe).</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">h</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">omega</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="c1">####### Complete this part ######## or die ####################</span>
    <span class="o">...</span>
    <span class="o">...</span>
    <span class="o">...</span>
    <span class="c1">###############################################################</span>
    <span class="k">return</span> <span class="n">predictions</span>
</pre></div>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>ImplÃ©mentez une methode qui prend en paramÃ¨tre deux points et retourne un vecteur <span class="math notranslate nohighlight">\(\omega\)</span> qui dÃ©crit lâ€™orientation de lâ€™hyperplan et le biais <span class="math notranslate nohighlight">\(b\)</span>.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parameterize</span><span class="p">(</span><span class="n">x_1</span><span class="p">,</span> <span class="n">x_2</span><span class="p">):</span>
    <span class="c1">####### Complete this part ######## or die ####################</span>
    <span class="o">...</span>
    <span class="o">...</span>
    <span class="o">...</span>
    <span class="o">...</span>
    <span class="c1">###############################################################</span>
    <span class="k">return</span> <span class="n">omega</span><span class="p">,</span> <span class="n">b</span>
</pre></div>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>ImplÃ©mentez une mÃ©thode qui calcule le risque empirique.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">empirical_risk</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">omega</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="c1">####### Complete this part ######## or die ####################</span>
    <span class="n">risk</span> <span class="o">=</span> <span class="o">...</span>
    <span class="c1">###############################################################</span>
    <span class="k">return</span> <span class="n">risk</span>
</pre></div>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>ImplÃ©mentez une mÃ©thode qui calcule le minimiseur du risque empirique.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fit_empirical_risk</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1">####### Complete this part ######## or die ####################</span>
    <span class="o">...</span>
    <span class="o">...</span>
    <span class="o">...</span>
    <span class="o">...</span>
    <span class="o">...</span>
    <span class="o">...</span>
    <span class="o">...</span>
    <span class="c1">###############################################################</span>
    <span class="k">return</span> <span class="n">best_omega</span><span class="p">,</span> <span class="n">best_b</span><span class="p">,</span> <span class="n">best_risk</span>
</pre></div>
</div>
<p>Testons le code prÃ©cÃ©dent.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">omega</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">fit_empirical_risk</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Obtained empirical risk:&#39;</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span>
</pre></div>
</div>
<p>Affichons maintenant le sÃ©parateur ainsi calculÃ©.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="n">x_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">)</span>
<span class="n">y_</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">b</span><span class="o">-</span><span class="n">omega</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">x_</span><span class="p">)</span><span class="o">/</span><span class="n">omega</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span> <span class="n">y_</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Our data and our model&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Malheureusement, tester toutes les combinaisons a un coÃ»t qui nous empÃªche de considÃ©rer des jeux de donnÃ©es trop grands et en trop grande dimension. Il y a en effet dans <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> <span class="math notranslate nohighlight">\(n!/(d!(n-d)!)\)</span> combinaisons possibles Ã  tester. Dans le plan, cela nous donne :</p>
<div class="math notranslate nohighlight">
\[\frac{n!}{2(n-2)!}=0.5n(n-1)\approx 0.5 n^2\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="n">fit_duration</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">dataset_sizes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">291</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">dataset_sizes</span><span class="p">:</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">construct_dataset</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">omega</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">fit_empirical_risk</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">fit_duration</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dataset_sizes</span><span class="p">,</span> <span class="n">fit_duration</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Fit time&#39;</span><span class="p">)</span>

<span class="n">scale</span> <span class="o">=</span> <span class="n">fit_duration</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="n">dataset_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">theoretical_fit_duration</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dataset_sizes</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">scale</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dataset_sizes</span><span class="p">,</span> <span class="n">theoretical_fit_duration</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Estimated fit time&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Fit duration&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Time (s)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Dataset size&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Avec un jeu de donnÃ©es de taille 150000 dans le plan (petite dim),&#39;</span>\
      <span class="s1">&#39;le temps du fit estimÃ© est d</span><span class="se">\&#39;</span><span class="s1">environ&#39;</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">scale</span> <span class="o">*</span> <span class="mi">150000</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="mi">60</span><span class="o">/</span><span class="mi">24</span><span class="p">)),</span> <span class="s1">&#39;jours&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>On peut rÃ©soudre ce problÃ¨me avec notamment deux visions diffÃ©rentesÂ :</p>
<ol class="simple">
<li><p>On estime la loi conditionnelle <span class="math notranslate nohighlight">\(\hat{\eta}\approx\eta\)</span> et on utilise une <em>plug-in</em> rule (la mÃªme rÃ¨gle que pour le classifieur de Bayes)Â :</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}\hat{g}(x)=\begin{cases}1\text{ si }\hat{\eta}(x)\geq 0.5\\0\text{ sinon.}\end{cases}\end{split}\]</div>
<ol class="simple">
<li><p>On optimise un autre â€œrisqueâ€ quâ€™on appelle un <span class="math notranslate nohighlight">\(\phi\)</span>-risk et qui nous garantit que le rÃ©sultat ne sera pas trop mauvais par rapport au vrai risque.</p></li>
</ol>
<p>La sÃ©quence de cours sur la rÃ©gression logistique montre un rÃ©sultat de convergence pour le premier cas. Cette sÃ©quence aborde le deuxiÃ¨me point.</p>
</div>
</div>
<div class="section" id="ii-construction-du-phi-risk">
<h2>II. Construction du <span class="math notranslate nohighlight">\(\phi\)</span>-risk<a class="headerlink" href="#ii-construction-du-phi-risk" title="Permalink to this headline">Â¶</a></h2>
<p>Ã€ la place de chercher Ã  optimiser notre erreur 0/1, lâ€™idÃ©e va Ãªtre de trouver une fonction avec des propriÃ©tÃ©s mathÃ©matiques intÃ©ressantes et pouvant nous offrir certaines garanties du point de vue de la classification. ConsidÃ©rons ici les labels <span class="math notranslate nohighlight">\(\mathcal{Y}=\{-1, 1\}\)</span> et supposons que notre classe de fonctions ait la forme suivanteÂ :</p>
<div class="math notranslate nohighlight">
\[\mathcal{H}=\{h:x\mapsto \text{sign}(g(x)),\ g:\mathcal{X}\rightarrow\mathbb{R}\}\]</div>
<p>Il sâ€™agit de la composition entre une fonction de score <span class="math notranslate nohighlight">\(g\)</span> et de la fonctionÂ :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{sign}(x)=\begin{cases}+1\text{ si }x\geq 0\\-1\text{ sinon.}\end{cases}\end{split}\]</div>
<p>Un classifieur <span class="math notranslate nohighlight">\(h\in\mathcal{H}\)</span> est donc une fonction de <span class="math notranslate nohighlight">\(\mathcal{X}\rightarrow \{-1,+1\}\)</span>. Un <span class="math notranslate nohighlight">\(\phi\)</span>-risk est un risque â€œfacileâ€ Ã  minimiser et qui garantit dâ€™Ãªtre un â€œbon choixâ€ du point de vue de lâ€™erreur 0/1.</p>
<div class="admonition-definition-phi-risk admonition">
<p class="admonition-title">DÃ©finition (<span class="math notranslate nohighlight">\(\phi\)</span>-risk)</p>
<p>Un <span class="math notranslate nohighlight">\(\phi\)</span>-risk est dÃ©fini commeÂ :</p>
<div class="math notranslate nohighlight">
\[L^\phi(g)=\mathbb{E}\big[\phi(Yg(X))\big],\]</div>
<p>avec les propriÃ©tÃ©s suivantesÂ :</p>
<ol class="simple">
<li><p><strong>Majoration du risque 0/1</strong> : <span class="math notranslate nohighlight">\(\phi(z)\geq \mathbf{1}\{z&lt;0\}\)</span>,</p></li>
<li><p><strong>ConvexitÃ©</strong> : La fonction <span class="math notranslate nohighlight">\(\phi:\mathbb{R}\rightarrow\mathbb{R}^+\)</span> est convexe,</p></li>
<li><p><strong>CalibrÃ©e pour la classification</strong> : La fonction <span class="math notranslate nohighlight">\(\phi\)</span> est dÃ©rivable en <span class="math notranslate nohighlight">\(0\)</span> et <span class="math notranslate nohighlight">\(\phi^\prime(0)&lt;0\)</span>.</p></li>
</ol>
</div>
<p>Nous allons bien entendu dÃ©tailler lâ€™intÃ©rÃªt de ces propriÃ©tÃ©s. Le risque empirique associÃ© devient alors :</p>
<div class="math notranslate nohighlight">
\[L_n^\phi(g)=\frac{1}{n}\sum_i \phi(Y_ig(X_i)).\]</div>
<p>Comme nous lâ€™avons vu dans la dÃ©finition, avec la fonction <span class="math notranslate nohighlight">\(\phi\)</span> suivante :</p>
<div class="math notranslate nohighlight">
\[\phi(z)=\mathbf{1}\{z&lt; 0\},\]</div>
<p>on retombe sur notre erreur <span class="math notranslate nohighlight">\(0/1\)</span>. En effet, on compte <span class="math notranslate nohighlight">\(1\)</span> si le signe de <span class="math notranslate nohighlight">\(Yg(X)\)</span> est nÃ©gatif, Ã  savoir, si notre score ne correspond pas au label et <span class="math notranslate nohighlight">\(0\)</span> sinon. Dâ€™autres choix de <span class="math notranslate nohighlight">\(\phi\)</span>-risks sont les suivants (il y en a beaucoup dâ€™autres) :</p>
<ul class="simple">
<li><p><strong>Hinge loss</strong> : <span class="math notranslate nohighlight">\(\phi(z)=\text{max}(0, 1-z)\)</span>, ici  on pÃ©nalise tous les <span class="math notranslate nohighlight">\(z\)</span> tant quâ€™ils sont infÃ©rieur Ã  <span class="math notranslate nohighlight">\(1\)</span>. Cette loss est notamment utilisÃ©e par le SVM qui cherche Ã  obtenir une marge,</p></li>
<li><p><strong>Smoothed hinge loss</strong> : <span class="math notranslate nohighlight">\(\phi(z)=\beta^{-1}\text{log}\big(1+e^{\beta(1-z)}\big)\)</span>, oÃ¹ <span class="math notranslate nohighlight">\(\beta&gt;0\)</span> est un paramÃ¨tre. On retrouve la hinge loss comme limite lorsque <span class="math notranslate nohighlight">\(\beta\rightarrow\infty\)</span>,</p></li>
<li><p><strong>Logistic loss</strong> : <span class="math notranslate nohighlight">\(\phi(z)=\text{log}(1+e^{-z})\)</span>, câ€™est la loss utilisÃ©e lorsquâ€™on fait une rÃ©gression logistique ou de la classification binaire en <em>deep learning</em>. En effet, on a :</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{log}(1+e^{-yg(x)})=-\text{log}\Big(\frac{1}{1+e^{-yg(x)}}\Big)=-\big(\mathbf{1}\{y=1\}\text{log}(\sigma(g(x)))+\mathbf{1}\{y=-1\}\text{log}(1-\sigma(g(x)))\big),\]</div>
<p>oÃ¹ <span class="math notranslate nohighlight">\(\sigma(z)=(1+e^{-z})^{-1}\)</span> est la fonction sigmoÃ¯d. Câ€™est la composition de notre fonction de score <span class="math notranslate nohighlight">\(g\)</span> avec une sigmoÃ¯d avec une log-entropie nÃ©gative (i.e. on veut maximiser la vraisemblance du modÃ¨le <span class="math notranslate nohighlight">\(\sigma(g(x))\)</span>. Si <span class="math notranslate nohighlight">\(g\)</span> est un modÃ¨le linÃ©aire, alors on retombe sur la rÃ©gression logistique.</p>
<p>Visualisons ces quelques <span class="math notranslate nohighlight">\(\phi\)</span>-risk.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">zero_one_loss</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">hinge_loss</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">v</span> <span class="o">=</span> <span class="mi">1</span><span class="o">-</span><span class="n">x</span>
    <span class="k">return</span> <span class="n">v</span> <span class="o">*</span> <span class="p">(</span><span class="n">v</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">v</span><span class="o">&lt;=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">logistic_loss</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">soft_hinge_loos</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">logistic_loss</span><span class="p">(</span><span class="n">beta</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">/</span><span class="n">beta</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">201</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">zero_one_loss</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;0/1 loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">hinge_loss</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Hinge loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">soft_hinge_loos</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Soft Hinge loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">logistic_loss</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Logistic loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\phi$-risks&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_fonctions_proxy_32_0.png" src="../_images/2_fonctions_proxy_32_0.png" />
</div>
</div>
</div>
<div class="section" id="iii-les-proprietes-d-un-phi-risk">
<h2>III. Les propriÃ©tÃ©s dâ€™un <span class="math notranslate nohighlight">\(\phi\)</span>-risk<a class="headerlink" href="#iii-les-proprietes-d-un-phi-risk" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="a-majoration-du-risque-0-1">
<h3>A. Majoration du risque 0/1<a class="headerlink" href="#a-majoration-du-risque-0-1" title="Permalink to this headline">Â¶</a></h3>
<p>La dÃ©finition dâ€™un <span class="math notranslate nohighlight">\(\phi\)</span>-risk indiquait que ce dernier devait majorer lâ€™erreur 0/1. En rÃ©alitÃ© nous pouvons allÃ©ger cette contrainte et dire quâ€™un <span class="math notranslate nohighlight">\(\phi\)</span>-risk doit majorer le risque 0/1 a un facteur proportionnel prÃ¨s. Ainsi le risque logistique ne majore pas lâ€™erreur 0/1 car <span class="math notranslate nohighlight">\(\log(1+e^0)&lt;1\)</span>. Cependant, nous avons bienÂ :</p>
<div class="math notranslate nohighlight">
\[\frac{1}{\log 2}\phi(z)\geq \mathbf{1}\{z&lt;0\}.\]</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Intuitivement, supposons que lâ€™on minimise lâ€™un des <span class="math notranslate nohighlight">\(\phi\)</span>-risk sur notre problÃ¨me de <em>machine learning</em> et quâ€™on obtienne <span class="math notranslate nohighlight">\(0\)</span> erreur Ã  la fin. Que pouvons nous dire quant au risque <span class="math notranslate nohighlight">\(0/1\)</span> ?</strong></p>
</div>
<p>De maniÃ¨re plus gÃ©nÃ©rale, notre objectif reste de minimiser la quantitÃ© dâ€™erreurs de classifications. Notons <span class="math notranslate nohighlight">\(g^\star\)</span> le classifieur de Bayes et <span class="math notranslate nohighlight">\(h_n\)</span> le minimiseur du <span class="math notranslate nohighlight">\(\phi\)</span>-risk empirique sur un jeu de donnÃ©es de taille <span class="math notranslate nohighlight">\(n\)</span>. Nous pouvons majorer le risque 0/1 de la maniÃ¨re suivanteÂ :</p>
<div class="math notranslate nohighlight">
\[L^{\text{0/1}}(h_n)\leq \underbrace{L^\text{0/1}(g^\star)}_{\text{Bayes risk}}+(\underbrace{\inf_{h\in\mathcal{H}}L^\text{0/1}(h)-L^\text{0/1}(g^\star)}_{\text{Approximation error}})+(\underbrace{L^\phi(h_n)-\inf_{h\in\mathcal{H}}L^\phi(h)}_{\text{Estimation error}})+(\underbrace{\inf_{h\in\mathcal{H}}L^\phi(h)-\inf_{h\in\mathcal{H}}L^\text{0/1}(h)}_{\text{Optimization error}})\]</div>
<p>On supposera que <span class="math notranslate nohighlight">\(L^\phi\)</span> aura Ã©tÃ© recalibrÃ©e de maniÃ¨re Ã  bien majorer le risque <span class="math notranslate nohighlight">\(L^{0/1}\)</span>.</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p>DÃ©crivez chacune des erreurs.</p>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p>DÃ©montrez lâ€™inÃ©galitÃ©.</p>
</div>
</div>
<div class="section" id="b-convexite">
<h3>B. ConvexitÃ©<a class="headerlink" href="#b-convexite" title="Permalink to this headline">Â¶</a></h3>
<p>La convexitÃ© garantit un certain nombre de propriÃ©tÃ©s dont la plus intÃ©ressante, peut-Ãªtre, est que si nous trouvons un minimiseur local de notre loss, alors celui-ci est Ã©galement un minimiseur global. Câ€™est donc une propriÃ©tÃ© idÃ©ale pour la minimisation.</p>
</div>
<div class="section" id="c-calibre-pour-la-classification">
<h3>C. CalibrÃ© pour la classification<a class="headerlink" href="#c-calibre-pour-la-classification" title="Permalink to this headline">Â¶</a></h3>
<p>La derniÃ¨re propriÃ©tÃ© que nous avons Ã©voquÃ©e est dâ€™Ãªtre calibrÃ© pour la classification. Nous avons indiquÃ© que cela Ã©tait Ã©quivalent Ã  avoir <span class="math notranslate nohighlight">\(\phi\)</span> dÃ©rivable en <span class="math notranslate nohighlight">\(0\)</span> et <span class="math notranslate nohighlight">\(\phi^\prime(0)&lt;0\)</span>.</p>
<p>Rappellons la notation <span class="math notranslate nohighlight">\(\eta(x)=\mathbb{P}(Y=1|X=x)\)</span> indique la <strong>vraie</strong> probabilitÃ© conditionnelle dâ€™obtenir le label <span class="math notranslate nohighlight">\(1\)</span> sachant quâ€™on a observÃ© la donnÃ©e <span class="math notranslate nohighlight">\(x\)</span>. Nous avons donc la probabilitÃ© du label <span class="math notranslate nohighlight">\(-1\)</span> avec <span class="math notranslate nohighlight">\(1-\eta(x)\)</span>. La fonction <span class="math notranslate nohighlight">\(\eta\)</span> nâ€™est bien sÃ»r pas connue. Cependant, nous avonsÂ :</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\Big[\phi(Yg(x))|X=x\Big]=\eta(x)\phi(g(x))+(1-\eta(x))\phi(-g(x))=:C_\eta(g(x)).\]</div>
<p>Câ€™est tout simplement lâ€™espÃ©rance de notre <span class="math notranslate nohighlight">\(\phi\)</span>-risk lorsquâ€™on a observÃ© la donnÃ©e <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>Rappelons-nous que la fonction <span class="math notranslate nohighlight">\(g\)</span> retourne un score positif si on prÃ©dit le label <span class="math notranslate nohighlight">\(y=1\)</span> et un score nÃ©gatif si on prÃ©dit le label <span class="math notranslate nohighlight">\(y=-1\)</span>. Ainsi, si <span class="math notranslate nohighlight">\(\eta(x)&gt;0.5\)</span>, alors, on veut prÃ©dire le label <span class="math notranslate nohighlight">\(1\)</span> et on veut que <span class="math notranslate nohighlight">\(g(x)&gt;0\)</span>. Ã€ lâ€™inverse, si <span class="math notranslate nohighlight">\(\eta(x)&lt;0.5\)</span>, alors on veut prÃ©dire le label <span class="math notranslate nohighlight">\(-1\)</span> et on veut <span class="math notranslate nohighlight">\(g(x)&lt;0\)</span>. Plus formellement, nous pouvons dÃ©finir cela comme suit.</p>
<div class="admonition-definition-loss-calibree-pour-la-classification admonition">
<p class="admonition-title">DÃ©finition (<em>loss</em> calibrÃ©e pour la classification)</p>
<p>Un <span class="math notranslate nohighlight">\(\phi\)</span>-risk est calibrÃ© pour la classification si et seulement siÂ :</p>
<div class="math notranslate nohighlight">
\[\eta&gt;0.5\Leftrightarrow\text{argmin}_{z}C_\eta(z)\subset\mathbb{R}^{\star+},\]</div>
<div class="math notranslate nohighlight">
\[\eta&lt;0.5\Leftrightarrow\text{argmin}_{z}C_\eta(z)\subset\mathbb{R}^{\star-}.\]</div>
</div>
<p>Les minimiseurs de notre <em>loss</em> (du point de vu du score), doivent avoir le mÃªme signe que <span class="math notranslate nohighlight">\(\eta-0.5\)</span>. Cela a bien entendu un effet sur la forme que peut prendre <span class="math notranslate nohighlight">\(\phi\)</span> et en particulier celui que nous illustrons par la proposition suivante.</p>
<div class="admonition-proposition admonition">
<p class="admonition-title">Proposition</p>
<p><span class="math notranslate nohighlight">\(\phi\)</span> est dÃ©rivable en <span class="math notranslate nohighlight">\(0\)</span> et <span class="math notranslate nohighlight">\(\phi^\prime(0)&lt;0\)</span> si et seulement si <span class="math notranslate nohighlight">\(\phi\)</span> est calibrÃ©e pour la classification.</p>
</div>
<div class="caution dropdown admonition">
<p class="admonition-title">Preuve</p>
<p>Notons <span class="math notranslate nohighlight">\(\phi_+\)</span>, <span class="math notranslate nohighlight">\(\phi_-\)</span>, <span class="math notranslate nohighlight">\((C_\eta)_+\)</span> et <span class="math notranslate nohighlight">\((C_\eta)_-\)</span> les dÃ©rivÃ©es Ã  gauche et Ã  droite. Nous avons alors, pour que les inÃ©galitÃ©s soient satisfaites, <span class="math notranslate nohighlight">\((C_\eta)_+^\prime(0)&lt;0\Leftrightarrow \eta&gt;0.5\)</span> et <span class="math notranslate nohighlight">\((C_\eta)_-^\prime(0)&gt;0\Leftrightarrow \eta&lt;0.5\)</span> puisque <span class="math notranslate nohighlight">\(C_\eta\)</span> est convexe en tant que combinaison convexe de fonctions convexes. Pour se convaincre des Ã©quivalences prÃ©cÃ©dentes, supposons <span class="math notranslate nohighlight">\(\eta&gt;0.5\)</span>, alors le minimum de <span class="math notranslate nohighlight">\(C_\eta(z)\)</span> doit Ãªtre atteint avec <span class="math notranslate nohighlight">\(z&gt;0\)</span>. Si <span class="math notranslate nohighlight">\((C_\eta)_+^\prime(0)&lt;0\)</span>, alors le minimum est â€œÃ  droiteâ€ de 0 et rÃ©pond au besoin.</p>
<p><span class="math notranslate nohighlight">\((\Leftarrow)\)</span> Supposons que les deux inÃ©galitÃ©s soient satisfaites.</p>
<p>Nous voulons montrer (1) que <span class="math notranslate nohighlight">\(\phi\)</span> est dÃ©rivable en <span class="math notranslate nohighlight">\(0\)</span> et (2) que <span class="math notranslate nohighlight">\(\phi^\prime(0)&lt;0\)</span>.</p>
<p>On a (attention Ã  la dÃ©rivÃ©e) :</p>
<div class="math notranslate nohighlight">
\[\lim_{\eta\rightarrow 0.5^+}(C_\eta)_+^\prime(0)=\lim_{\eta\rightarrow 0.5^+}\eta\phi_+^\prime(0)-(1-\eta)\phi_-^\prime(0)=0.5\big(\phi_+^\prime(0)-\phi_-^\prime(0)\big)\leq 0.\]</div>
<p>La derniÃ¨re inÃ©galitÃ© est vÃ©rifiÃ©e par hypothÃ¨se sur <span class="math notranslate nohighlight">\(C_\eta\)</span>. Cela nous donne donc <span class="math notranslate nohighlight">\(\phi_-^\prime(0)\geq\phi_+^\prime(0)\)</span>. Par hypothÃ¨se, <span class="math notranslate nohighlight">\(\phi\)</span> est convexe et on a toujours <span class="math notranslate nohighlight">\(\phi_-^\prime(0)\leq\phi_+^\prime(0)\)</span>. On a donc <span class="math notranslate nohighlight">\(\phi_+^\prime(0)=\phi_-^\prime(0)\)</span> et <span class="math notranslate nohighlight">\(\phi\)</span> est dÃ©rivable en <span class="math notranslate nohighlight">\(0\)</span>. Câ€™est le premier point.</p>
<p>Nous avons ensuite :</p>
<div class="math notranslate nohighlight">
\[C_\eta^\prime(0)=\eta\phi^\prime(0)-(1-\eta)\phi^\prime(0)=(2\eta-1)\phi^\prime(0).\]</div>
<p>Si <span class="math notranslate nohighlight">\(\eta&gt;0.5\)</span>, alors <span class="math notranslate nohighlight">\(2\eta-1&gt;0\)</span> et <span class="math notranslate nohighlight">\(C_\eta^\prime(0)&lt;0\)</span>. On a alors <span class="math notranslate nohighlight">\(\phi^\prime(0)&lt;0\)</span>. On obtient le mÃªme rÃ©sultat si <span class="math notranslate nohighlight">\(\eta&lt;0.5\)</span> !</p>
<p><span class="math notranslate nohighlight">\((\Rightarrow)\)</span> Supposons que <span class="math notranslate nohighlight">\(\phi\)</span> est dÃ©rivable en <span class="math notranslate nohighlight">\(0\)</span> et que <span class="math notranslate nohighlight">\(\phi^\prime(0)&lt;0\)</span>. On obtient alors :</p>
<div class="math notranslate nohighlight">
\[C_\eta^\prime(0)=(2\eta-1)\phi^\prime(0)\]</div>
<p>dont le signe est nÃ©gatif si <span class="math notranslate nohighlight">\(\eta&gt;0.5\)</span> et positif si <span class="math notranslate nohighlight">\(\eta&lt;0.5\)</span>. <strong><span class="math notranslate nohighlight">\(\boxed{}\)</span></strong></p>
</div>
</div>
<div class="section" id="d-construction">
<h3>D. Construction<a class="headerlink" href="#d-construction" title="Permalink to this headline">Â¶</a></h3>
<p>Maintenant que nous avons vu et dÃ©crit les propriÃ©tÃ©s dâ€™un <span class="math notranslate nohighlight">\(\phi\)</span>-risk calibrÃ© pour la classification, nous pouvons laisser notre imagination Ã  en crÃ©er de nouveaux.</p>
<div class="admonition-exercice-creer-une-loss admonition">
<p class="admonition-title">Exercice (crÃ©er une <em>loss</em>)</p>
<p>Nous avons vu les propriÃ©tÃ©s quâ€™un <span class="math notranslate nohighlight">\(\phi\)</span>-risk devait respecter pour Ãªtre â€œclassification calibratedâ€. Inventez une nouvelle fonction de perte (i.e. <em>loss</em>).</p>
</div>
<div class="hint dropdown admonition">
<p class="admonition-title">Indice</p>
<p>Utilisez des fonctions simples, connues, convexes, etc..</p>
</div>
</div>
</div>
<div class="section" id="iv-logistic-loss-et-regression-logistique">
<h2>IV. <em>Logistic loss</em> et rÃ©gression logistique<a class="headerlink" href="#iv-logistic-loss-et-regression-logistique" title="Permalink to this headline">Â¶</a></h2>
<p>La <em>logistic loss</em> est <span class="math notranslate nohighlight">\(\phi(z)=\text{log}(1+e^{-z})\)</span>. ConsidÃ©rons un jeu de donnÃ©es <span class="math notranslate nohighlight">\(S_n\)</span>, notre objectif pratique est de minimiser le <span class="math notranslate nohighlight">\(\phi\)</span>-risk empirique :</p>
<div class="math notranslate nohighlight">
\[L_n^\phi(g)=\frac{1}{n}\sum_i\phi(y_ig(x_i))=\frac{1}{n}\sum_i\text{log}(1+e^{-y_ig(x_i)})\]</div>
<p>Reprenons le cas dâ€™un modÃ¨le linÃ©aire. Cela nous permettra de comparer les coÃ»ts calculatoires avec le vrai risque empirique. Nous avons doncÂ :</p>
<div class="math notranslate nohighlight">
\[\mathcal{H}=\{x\mapsto \text{sign}(g(x)):\ g(x)=\langle \omega, x\rangle +b,\ \omega\in\mathbb{R}^d,\ b\in\mathbb{R}\}\]</div>
<p>Lâ€™optimisation du <span class="math notranslate nohighlight">\(\phi\)</span>-risk se fera bien sÃ»r Ã  partir de la fonction <span class="math notranslate nohighlight">\(g(x)\)</span> et non de <span class="math notranslate nohighlight">\(\text{sign}(g(x))\)</span>.</p>
<p>Notre objectif est, dans un premier temps, de calculer le gradient de notre <span class="math notranslate nohighlight">\(\phi\)</span>-risk empirique. La premiÃ¨re Ã©tape est de calculer la dÃ©rivÃ©e de la fonction <span class="math notranslate nohighlight">\(\phi(yz)\)</span> :</p>
<div class="math notranslate nohighlight">
\[\phi^\prime(yz)=-\frac{ye^{-yz}}{1+e^{-yz}}=-\frac{y}{1+e^{yz}}.\]</div>
<p>Observons notamment quâ€™on aÂ :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\phi^\prime((+1)z)&amp;=\frac{1}{1+e^{-z}}-1=\sigma(z)-y^{0/1}\\
\phi^\prime((-1)z)&amp;=\frac{1}{1+e^{-z}}-0=\sigma(z)-y^{0/1}
\end{aligned}\end{split}\]</div>
<p>oÃ¹ <span class="math notranslate nohighlight">\(y^{0/1}\)</span> reprÃ©sente le label <span class="math notranslate nohighlight">\(0\)</span> ou <span class="math notranslate nohighlight">\(1\)</span> plutÃ´t que <span class="math notranslate nohighlight">\(-1\)</span> et <span class="math notranslate nohighlight">\(+1\)</span>. Par simplicitÃ© de notation considÃ©rons les vecteur <span class="math notranslate nohighlight">\(x_i=[1, x_1, \ldots, x_d]^T\)</span> afin dâ€™Ã©viter la notation avec le biais <span class="math notranslate nohighlight">\(b\)</span>.</p>
<p>Nous avons donc naturellement que :</p>
<div class="math notranslate nohighlight">
\[\nabla_\omega g(x)=x\]</div>
<p>et donc :</p>
<div class="math notranslate nohighlight">
\[\nabla_\omega \phi(y_ig(x_i))=x_i\left(\frac{1}{1+e^{-\langle \omega, x_i\rangle}}-y_i^{0/1}\right),\]</div>
<p>On retrouve le gradient que nous avons calculÃ© lors de la prÃ©cÃ©dente sÃ©quence (i.e. sur la rÃ©gression logistique) oÃ¹ on avait :</p>
<div class="math notranslate nohighlight">
\[\nabla_\omega L_n(\omega)=\frac{1}{n}X^T(\sigma(X\boldsymbol{\omega})-\boldsymbol{y}^{0/1})\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CrossEntropy</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">CrossEntropy</span><span class="o">.</span><span class="n">_format_ndarray</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pos</span> <span class="o">=</span> <span class="mi">0</span>
        
    <span class="k">def</span> <span class="nf">_format_ndarray</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
        <span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="k">else</span> <span class="n">arr</span>
        <span class="k">return</span> <span class="n">arr</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">arr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">arr</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">)))</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">y_pred</span>
    
    <span class="k">def</span> <span class="nf">_sigmoid</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
        <span class="n">z_max</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="c1"># numerical stability trick</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="o">-</span><span class="n">z_max</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="o">-</span><span class="n">z_max</span><span class="p">)</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z_max</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">val</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">CrossEntropy</span><span class="o">.</span><span class="n">_format_ndarray</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">CrossEntropy</span><span class="o">.</span><span class="n">_sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
        <span class="n">log_p</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)),</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">log_p</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">_shuffle</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">batch_size</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span> <span class="k">else</span> <span class="n">batch_size</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_pos</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">_pos</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_pos</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pos</span><span class="o">+</span><span class="n">batch_size</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pos</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_shuffle</span><span class="p">()</span>
            
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">CrossEntropy</span><span class="o">.</span><span class="n">_format_ndarray</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        
        <span class="n">beta</span> <span class="o">=</span> <span class="n">CrossEntropy</span><span class="o">.</span><span class="n">_format_ndarray</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
        
        <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">CrossEntropy</span><span class="o">.</span><span class="n">_sigmoid</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>   <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">grad</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GradientDescent</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="n">init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">CrossEntropy</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">nb_iterations</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">init</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">param_trace</span> <span class="o">=</span> <span class="p">[</span><span class="n">beta</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
        <span class="n">loss_trace</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">beta</span><span class="p">)]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_iterations</span><span class="p">):</span>
            <span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
            <span class="n">param_trace</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">beta</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">loss_trace</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">beta</span><span class="p">))</span>
            
        <span class="k">return</span> <span class="n">param_trace</span><span class="p">,</span> <span class="n">loss_trace</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">construct_dataset</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>
<span class="n">gd</span> <span class="o">=</span> <span class="n">GradientDescent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">params</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">gd</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">nb_iterations</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">_</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_fonctions_proxy_40_0.png" src="../_images/2_fonctions_proxy_40_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
<span class="n">omega</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">x_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">)</span>
<span class="n">y_</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">b</span><span class="o">-</span><span class="n">omega</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">x_</span><span class="p">)</span><span class="o">/</span><span class="n">omega</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span> <span class="n">y_</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Our data and our model&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_fonctions_proxy_41_0.png" src="../_images/2_fonctions_proxy_41_0.png" />
</div>
</div>
<p>Ã‰tudions lâ€™Ã©volution du temps de calcul et comparons le au minimiseur du risque empirique.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="n">fit_duration_erm</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">fit_duration_phi_risk</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">dataset_sizes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">291</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">dataset_sizes</span><span class="p">:</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">construct_dataset</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">omega</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">fit_empirical_risk</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">fit_duration_erm</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="p">)</span>
    
    <span class="n">gd</span> <span class="o">=</span> <span class="n">GradientDescent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">params</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">gd</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">nb_iterations</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">fit_duration_phi_risk</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dataset_sizes</span><span class="p">,</span> <span class="n">fit_duration_erm</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ERM fit time&#39;</span><span class="p">)</span>

<span class="n">scale</span> <span class="o">=</span> <span class="n">fit_duration_erm</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="n">dataset_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">theoretical_fit_duration</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dataset_sizes</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">scale</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dataset_sizes</span><span class="p">,</span> <span class="n">theoretical_fit_duration</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Estimated ERM fit time&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dataset_sizes</span><span class="p">,</span> <span class="n">fit_duration_phi_risk</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$\phi$-risk fit time&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Fit duration&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Time (s)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Dataset size&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition-question admonition">
<p class="admonition-title">Question</p>
<p><strong>Quâ€™en conclure ? Dans quel cas le minimiseur du risque empirique a-t-il toujours du sens ?</strong></p>
</div>
</div>
<div class="section" id="v-optimiser-votre-loss">
<h2>V. Optimiser votre <em>loss</em><a class="headerlink" href="#v-optimiser-votre-loss" title="Permalink to this headline">Â¶</a></h2>
<div class="admonition-exercice-optimisez-votre-loss admonition">
<p class="admonition-title">Exercice (Optimisez votre <em>loss</em>)</p>
<p>Dans un exercice prÃ©cÃ©dent vous avez crÃ©Ã© votre propre <em>loss</em>. Utilisez votre <em>loss</em> sur le jeu de donnÃ©es prÃ©cÃ©dent avec un modÃ¨le linÃ©aireÂ :</p>
<div class="math notranslate nohighlight">
\[\mathcal{H}=\{x\rightarrow\text{sign}(g(x)):\ g(x)=\langle\omega, x\rangle +b,\ \omega\in\mathbb{R}^d,b\in\mathbb{R}\}.\]</div>
<p>oÃ¹Â :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{sign}(x)=\begin{cases}+1\text{ si }x\geq 0\\-1\text{ sinon.}\end{cases}\end{split}\]</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pensez a ajouter une colonne de 1 si vous voulez un biais</span>
<span class="n">X_prime</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="o">...</span>
<span class="n">beta</span> <span class="o">=</span> <span class="o">...</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
<span class="n">omega</span> <span class="o">=</span> <span class="n">beta</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">beta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">x_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">)</span>
<span class="n">y_</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">b</span><span class="o">-</span><span class="n">omega</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">x_</span><span class="p">)</span><span class="o">/</span><span class="n">omega</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span> <span class="n">y_</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Our data and our model&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./3_classification"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="1_logistic_regression.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">La rÃ©gression logistique â˜•ï¸</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="3_bayes_classifier.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Le classifieur de Bayes â˜•ï¸</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Servajean, Leveau & Chailan<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>
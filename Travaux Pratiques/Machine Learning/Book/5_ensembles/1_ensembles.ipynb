{"cells": [{"cell_type": "markdown", "id": "collectible-validation", "metadata": {}, "source": ["# M\u00e9thodes ensemblistes \u2615\ufe0f\u2615\ufe0f\n", "\n", "**<span style='color:blue'> Objectifs de la s\u00e9quence</span>** ", "\n", "* \u00catre sensibilis\u00e9&nbsp;:\n", "    * aux gains de performances possibles via ce genre de m\u00e9thodes,\n", "    * \u00e0 l'effet d'un ensemble de mod\u00e8les sur la variance de notre pr\u00e9diction.\n", "* \u00catre capable&nbsp;:\n", "    * de produire des ensembles de mod\u00e8les,\n", "    * de se servir de $\\texttt{sklearn}$ pour faire des ensembles,\n", "    * et utiliser les for\u00eats al\u00e9atoires, les boostings, etc.\n", "    \n", "\n\n ----"]}, {"cell_type": "markdown", "id": "arbitrary-beijing", "metadata": {}, "source": ["## Introduction\n", "\n", "Soit $\\mathcal{X}$ notre espace d'entr\u00e9e et $\\mathcal{Y}$ notre espace de sortie. Soit $X, Y$ deux variables al\u00e9atoires sur $\\mathcal{X}$ et $\\mathcal{Y}$ et soit $\\mathbb{P}$ leur mesure jointe. Notre objectif est de trouver une application $h:\\mathcal{X}\\mapsto\\mathcal{Y}$ qui minimise une certaine erreur qu'on notera $L$. N'ayant pas acc\u00e8s aux variables al\u00e9atoires $X$ et $Y$, nous collectons un jeu de donn\u00e9es $S_n=\\{(X_i, Y_i)\\}_{i\\leq n}\\sim\\mathbb{P}^n$ (par exemple en r\u00e9cup\u00e9rant des images sur internet) et nous construisons un risque empirique :\n", "\n", "$$L_n(h)=\\frac{1}{n}\\sum_{i}\\ell(h(X_i), Y_i),$$\n", "\n", "o\u00f9 $\\ell$ d\u00e9finit une erreur \u00e9l\u00e9mentaire (i.e. pour une unique pr\u00e9diction). Comme nous l'avons vu, ce risque empirique $L_n$ est un estimateur de $L$. La fonction $h$ est ainsi construite en utilisant le jeu de donn\u00e9es $S_n$.\n", "\n"]}, {"cell_type": "markdown", "id": "democratic-annotation", "metadata": {}, "source": ["## I. L'approche na\u00efve\n", "\n", "L'approche la plus simple lorsqu'on cherche \u00e0 combiner plusieurs mod\u00e8les consiste \u00e0 moyenner leur pr\u00e9diction. Dans le cas de la r\u00e9gression, la pr\u00e9diction est la moyenne traditionnelle. Dans le cas de la classification, on utilisera un vote \u00e0 la majorit\u00e9 simple.\n", "\n", "---\n", "\n", "Soit $\\mathcal{Y}\\subseteq\\mathbb{R}$ l'espace des labels et $\\mathcal{X}$ l'espace de nos donn\u00e9es d'entr\u00e9e. Consid\u00e9rons une famille de pr\u00e9dicteurs $\\{y_j\\}_{j\\leq m}$, l'agr\u00e9gation de ces derniers se fait de la mani\u00e8re la plus na\u00efve qui soit (comme nous l'avons vu au-dessus)&nbsp;:\n", "\n", "$$\\hat{y}=\\frac{1}{m}\\sum_j y_j(x).$$\n", "\n", "Supposons que nous ayons pour un pr\u00e9dicteur donn\u00e9 $\\hat{y}_{ij} = y(x_j)+\\epsilon_{ij}$ o\u00f9 la pr\u00e9diction se fait \u00e0 un bruit pr\u00e8s qui d\u00e9pend de l'\u00e9chantillon mais aussi du pr\u00e9dicteur. Nous avons \u00e9videmment la relation suivante&nbsp;:\n", "\n", "$$\\mathbb{E}\\big[(\\hat{y}_j(X)-y(X))^2\\big]=\\mathbb{E}\\big[\\epsilon_{j}^2\\big],$$\n", "\n", "o\u00f9 $\\epsilon_j$ d\u00e9pend de $X$. De mani\u00e8re similaire, si cette fois-ci nous consid\u00e9rons l'agr\u00e9gation de nos pr\u00e9dicteurs, nous avons&nbsp;:\n", "\n", "$$\\mathbb{E}\\big[(y(X)-\\frac{1}{m}\\sum_j \\hat{y_j}(X))^2\\big]=\\mathbb{E}\\big[(\\frac{1}{m}\\sum_j\\epsilon_{j})^2\\big].$$\n", "\n", "Si on suppose (ce qui n'est pas vraiment le cas en pratique)&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "\\mathbb{E}\\big[\\epsilon_j\\big]&=0\\\\\n", "\\mathbb{E}\\big[\\epsilon_j\\epsilon_k\\big]&=0,\\ j\\neq k,\n", "\\end{aligned}$$\n", "nous avons&nbsp;:\n", "\n", "$$\\mathbb{E}\\big[(\\frac{1}{m}\\sum_j\\epsilon_{j})^2\\big]=\\frac{1}{m^2}\\sum_i\\mathbb{E}\\big[\\epsilon^2\\big]=\\frac{1}{m}\\mathbb{E}_{\\bar{\\epsilon}},$$\n", "\n", "o\u00f9 $\\mathbb{E}_{\\bar{\\epsilon}}$ indique l'erreur moyenne de nos pr\u00e9dicteurs. L'erreur diminue au fur et \u00e0 mesure que l'on ajoute des pr\u00e9dicteurs.\n", "\n", "---"]}, {"cell_type": "code", "execution_count": null, "id": "productive-playlist", "metadata": {"tags": ["hide-input"]}, "outputs": [], "source": ["from sklearn.datasets import load_digits\n", "import matplotlib.pyplot as plt\n", "\n", "data = load_digits()\n", "\n", "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n", "for ax, image, label in zip(axes, data.images, data.target):\n", "    ax.set_axis_off()\n", "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n", "    ax.set_title('Training: %i' % label)"]}, {"cell_type": "code", "execution_count": null, "id": "meaningful-bangladesh", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "X = data.data / data.data.max()\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X, data.target, test_size=0.75)"]}, {"cell_type": "code", "execution_count": null, "id": "available-lambda", "metadata": {}, "outputs": [], "source": ["from sklearn.linear_model import LogisticRegression\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.neighbors import KNeighborsClassifier\n", "import numpy as np\n", "from scipy import stats"]}, {"cell_type": "markdown", "id": "71e71cd1", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Compl\u00e9tez la m\u00e9thode $\\texttt{predict}$ de la classe suivante afin de faire un vote \u00e0 la majorit\u00e9 simple pour combiner plusieurs mod\u00e8les.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "id": "7cc7f75b", "metadata": {}, "source": ["class MajorityVoting(object):\n", "    def __init__(self, *models):\n", "        self.models = models\n", "    \n", "    def fit(self, X, y):\n", "        for m in self.models:\n", "            m.fit(X, y)\n", "\n", "    def predict(self, X):\n", "        ####### Complete this part ######## or die ####################\n", "        ...\n", "        ...\n", "        ...\n", "        return ...\n", "        ###############################################################\n", "    \n", "    def score(self, X, y):\n", "        scores = {'model': [], 'vote': None}\n", "        for m in self.models:\n", "            scores['model'].append(m.score(X, y))\n", "        scores['vote'] = (self.predict(X) == y).astype(int).sum()/len(X)\n", "        return scores\n", "            \n", "model = MajorityVoting(\n", "    LogisticRegression(max_iter=5000), \n", "    DecisionTreeClassifier(),\n", "    KNeighborsClassifier()\n", ")\n", "model.fit(X_train, y_train)\n", "\n", "scores = model.score(X_test, y_test)\n", "\n", "print('Nos mod\u00e8les atteignent:', scores['model'])\n", "print('Notre agr\u00e9gation atteint:', scores['vote'])\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "dressed-russia", "metadata": {}, "source": ["On peut imaginer que les mod\u00e8les se trompent sur les m\u00eames images et non de mani\u00e8re al\u00e9atoire. C'est par exemple le cas si les mod\u00e8les se trompent lorsqu'un chiffre est mal dessin\u00e9 et ressemble \u00e0 un autre chiffre : tous les mod\u00e8les vont faire la m\u00eame erreur. Dans ces cas de figure, le meilleur mod\u00e8le est le meilleur mod\u00e8le et non le vote. \u00c0 l'inverse, si nos diff\u00e9rents mod\u00e8les ont tendance \u00e0 faire des erreurs diff\u00e9rentes, alors l'agr\u00e9gation permettra de gagner en score.\n", "\n", "Consid\u00e9rons maintenant le jeu de donn\u00e9es suivant."]}, {"cell_type": "code", "execution_count": null, "id": "subtle-magnitude", "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import load_boston\n", "\n", "data = load_boston()"]}, {"cell_type": "code", "execution_count": null, "id": "resident-copyright", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "X = data.data/data.data.max(axis=0)\n", "X_train, X_test, y_train, y_test = train_test_split(X, data.target, test_size=0.75)"]}, {"cell_type": "code", "execution_count": null, "id": "north-borough", "metadata": {}, "outputs": [], "source": ["from sklearn.linear_model import LinearRegression\n", "from sklearn.tree import DecisionTreeRegressor\n", "from sklearn.neighbors import KNeighborsRegressor\n", "import numpy as np\n", "from scipy import stats\n", "\n", "from sklearn.metrics import mean_squared_error"]}, {"cell_type": "markdown", "id": "fbdb9cb5", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Compl\u00e9tez la m\u00e9thode $\\texttt{predict}$ de la classe suivante afin de faire une moyenne pour combiner plusieurs mod\u00e8les.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "id": "d73d2095", "metadata": {}, "source": ["class AverageVoting(object):\n", "    def __init__(self, *models):\n", "        self.models = models\n", "    \n", "    def fit(self, X, y):\n", "        for m in self.models:\n", "            m.fit(X, y)\n", "\n", "    def predict(self, X):\n", "        ####### Complete this part ######## or die ####################\n", "        ...\n", "        ...\n", "        ...\n", "        return ...\n", "        ###############################################################\n", "    \n", "    def score(self, X, y):\n", "        scores = {'model': [], 'vote': None}\n", "        for m in self.models:\n", "            scores['model'].append(mean_squared_error(y, m.predict(X)))\n", "        scores['vote'] = mean_squared_error(y, self.predict(X))\n", "        return scores\n", "            \n", "model = AverageVoting(\n", "    LinearRegression(), \n", "    DecisionTreeRegressor(),\n", "    KNeighborsRegressor()\n", ")\n", "model.fit(X_train, y_train)\n", "\n", "scores = model.score(X_test, y_test)\n", "\n", "print('Nos mod\u00e8les atteignent:', scores['model'])\n", "print('Notre agr\u00e9gation atteint:', scores['vote'])\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "checked-daily", "metadata": {}, "source": ["On observe cette fois-ci un gain clair de performances. Les diff\u00e9rents mod\u00e8les doivent se tromper d'une mani\u00e8re au moins partiellement al\u00e9atoire et r\u00e9partie autour de la moyenne. L'agr\u00e9gation rend ce r\u00e9sultat plus stable.\n", "\n", "Ces mani\u00e8res d'agr\u00e9ger des votes est effectives mais restent \"na\u00efve\". Nous allons voir que nous pouvons formaliser tout cela dans le cadre du *framework* probabiliste."]}, {"cell_type": "markdown", "id": "wrong-vacuum", "metadata": {}, "source": ["## II. Bayesian Model Averaging\n", "\n", "Notons $z_i=(x_i, y_i)$ et une famille de $M$ mod\u00e8les probabilistes (e.g. r\u00e9gression logistique) o\u00f9 chaque mod\u00e8le est not\u00e9 $\\mathcal{M}_j$. Notons :\n", "\n", "$$p(y_i|x_i, \\mathcal{M}_j),$$\n", "\n", "la densit\u00e9 d'un point de $\\mathcal{Y}$ relativement au mod\u00e8le $\\mathcal{M}_j$ et \u00e0 une observation $x_i\\in\\mathcal{X}$. Notre objectif est de d\u00e9terminer dans un premier temps la \"qualit\u00e9\" d'un mod\u00e8le en tenant compte de notre point de vu *a priori* ainsi que des donn\u00e9es que nous avons pu observer $S_n$. Notons ainsi $p(\\mathcal{M}_j)$ notre probabilit\u00e9 *a priori* sur le mod\u00e8le $\\mathcal{M}_j$. Celle-ci peut favoriser certaines solutions parcimonieuses ou bien \u00eatre uniforme et ne favoriser aucun mod\u00e8le. En appliquant la r\u00e8gle de Bayes, nous obtenons :\n", "\n", "$$p(\\mathcal{M}_j|X, \\boldsymbol{y})=\\frac{p(\\boldsymbol{y}|X, \\mathcal{M}_j)p(\\mathcal{M}_j)}{p(\\boldsymbol{y}| X)},$$\n", "\n", "o\u00f9 $S_n=(\\boldsymbol{y}, X)$ et o\u00f9 nous avons :\n", "\n", "$$p(\\boldsymbol{y}|X, \\mathcal{M}_j)=\\prod_i p(y_i|x_i, \\mathcal{M}_j),$$\n", "\n", "ainsi que :\n", "\n", "$$p(\\boldsymbol{y}| X)=\\sum_j p(\\boldsymbol{y}|X, \\mathcal{M}_j)p(\\mathcal{M}_j).$$\n", "\n", "Nous avons bien ici toutes les informations nous permettant d'\u00e9valuer de mani\u00e8re Bay\u00e9sienne la qualit\u00e9 de nos diff\u00e9rents mod\u00e8les. Cependant, en pratique, les variations pourraient tr\u00e8s bien d\u00e9pendre d'un tirage particulier de nos donn\u00e9es. L'id\u00e9e derri\u00e8re le *Bayesian Model Averaging* consiste \u00e0 consid\u00e9rer TOUS les mod\u00e8les mais \u00e0 les pond\u00e9rer par leur qualit\u00e9. Cela ne se fait pas au doigt mouill\u00e9, mais en utilisant encore une fois le *framework* probabiliste :\n", "\n", "$$p(y_\\text{new}|x_\\text{new}, S_n)=\\sum_j p(y_\\text{new}|x_\\text{new}, \\mathcal{M}_j, S_n)p(\\mathcal{M}_j|S_n).$$\n"]}, {"cell_type": "markdown", "id": "virgin-dictionary", "metadata": {}, "source": ["**Application a une famille de r\u00e9gression logistique.**\n", "\n", "Dans le cas de la r\u00e9gression logistique, nous avons la vraisemblance suivante :\n", "\n", "$$\\mathcal{L}(\\theta_j)=\\prod_i p(y_i|x_i,\\theta_j)$$\n", "\n", "o\u00f9\n", "\n", "$$p(y_i|x_i, \\theta_j)=\\sigma_j(x_i)^y_i(1-\\sigma_j(x_i))^{1-y_i}$$\n", "\n", "et\n", "\n", "$$\\sigma_j(x)=(1+e^{-\\langle\\theta_j,x\\rangle})^{-1}.$$\n", "\n", "Ainsi, la *posterior* de notre mod\u00e8le $\\theta_j$ apr\u00e8s avoir observ\u00e9 notre jeu de donn\u00e9es est donn\u00e9e par :\n", "\n", "$$p(\\theta_j|S_n)\\propto \\prod_i p(y_i|x_i,\\theta_j)p(\\theta_j),$$\n", "\n", "o\u00f9 le symbole $\\propto$ indique la proportionnalit\u00e9."]}, {"cell_type": "markdown", "id": "8d387923", "metadata": {}, "source": ["Nous allons construire dans cette exercice un mod\u00e8le de classification d'images de chiffres \u00e0 partir de r\u00e9gressions logistiques. La particularit\u00e9 sera que chaque r\u00e9gression logistique ne verra qu'une partie de l'image tir\u00e9e al\u00e9atoirement."]}, {"cell_type": "code", "execution_count": null, "id": "numerous-darkness", "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import load_digits\n", "data = load_digits()"]}, {"cell_type": "markdown", "id": "nominated-opera", "metadata": {}, "source": ["Affichons le jeu de donn\u00e9es ainsi qu'un exemple de ce que pourrait voir un mod\u00e8le."]}, {"cell_type": "code", "execution_count": null, "id": "chief-traveler", "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "\n", "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n", "for ax, image, label in zip(axes, data.images, data.target):\n", "    ax.set_axis_off()\n", "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n", "    ax.set_title('Training: %i' % label)\n", "\n", "mask = np.random.binomial(n=1, p=0.4, size=data.images[0].shape).astype(bool)\n", "\n", "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n", "for ax, image, label in zip(axes, data.images, data.target):\n", "    ax.set_axis_off()\n", "    ax.imshow(\n", "        np.ma.masked_array(image, mask=mask), \n", "        cmap=plt.cm.gray_r, \n", "        interpolation='nearest'\n", "    )\n", "    ax.set_title('Training: %i' % label)"]}, {"cell_type": "markdown", "id": "37876ea8", "metadata": {}, "source": ["Construisons notre jeu d'apprentissage ainsi que notre jeu de test."]}, {"cell_type": "code", "execution_count": null, "id": "grave-appendix", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "X = data.data / data.data.max()\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X, data.target, test_size=0.75)"]}, {"cell_type": "markdown", "id": "8cfccc75", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Compl\u00e9tez les m\u00e9thodes $\\texttt{posterior_}$, $\\texttt{fit}$ et $\\texttt{predict}$ afin de reproduire le *framework* que nous avons introduit au-dessus.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "execution_count": null, "id": "piano-pendant", "metadata": {}, "outputs": [], "source": ["from sklearn.linear_model import LogisticRegression\n", "import numpy as np\n", "from scipy.special import logsumexp"]}, {"cell_type": "code", "id": "c382fd0f", "metadata": {}, "source": ["class BayesianLogisticAveraging(object):\n", "    def __init__(self, dim=64, p=0.5, nb_models=50, max_iter=5000):\n", "        self.mask = np.random.binomial(n=1, p=p, size=(nb_models, dim)).astype(bool)\n", "        self.nb_models = nb_models\n", "        self.models = []\n", "        self.posterior = []\n", "        self.prior = 1./nb_models\n", "        self.max_iter = max_iter\n", "        \n", "    def posterior_(self, i, X, y):\n", "        ####### Complete this part ######## or die ####################\n", "        ...\n", "        ...\n", "\n", "        return ...\n", "        ###############################################################\n", "        \n", "        \n", "    def fit(self, X_train, y_train):\n", "        ####### Complete this part ######## or die ####################\n", "        ...\n", "        ...\n", "        ...\n", "        ...\n", "        ###############################################################\n", "        \n", "    def individual_scores(self, X, y):\n", "        scores = []\n", "        for _ in range(len(self.models)):\n", "            scores.append(self.models[_].score(X[:, self.mask[_]], y))\n", "        print(np.max(scores))\n", "            \n", "    def predict(self, X):\n", "        ####### Complete this part ######## or die ####################\n", "        ...\n", "        ...\n", "        ...\n", "        ...\n", "        return ...\n", "        ###############################################################\n", "        \n", "    def score(self, X, y):\n", "        scores = {'model': [], 'vote': None}\n", "        for i, m in enumerate(self.models):\n", "            scores['model'].append(m.score(X[:, self.mask[i]], y))\n", "        scores['vote'] = (self.predict(X) == y).astype(int).sum()/len(X)\n", "        return scores\n", "\n", "model = BayesianLogisticAveraging()\n", "model.fit(X_train, y_train)\n", "\n", "scores = model.score(X_test, y_test)\n", "\n", "print('Nos mod\u00e8les atteignent:\\n -', '\\n - '.join([str(i) for i in scores['model']]))\n", "print('Notre agr\u00e9gation atteint:', scores['vote'])\n", "\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "50289e90", "metadata": {}, "source": ["De la m\u00eame mani\u00e8re que pr\u00e9c\u00e9demment un gain est observ\u00e9 si les erreurs sont ind\u00e9pendantes. De plus, il est important d'avoir une bonne estimation des probabilit\u00e9s conditionnelles afin que la vraisemblance soit un bon indicateur de la qualit\u00e9 de notre mod\u00e8le. Une strat\u00e9gie alternative, souvent utilis\u00e9e en *deep learning* consiste pour chaque pr\u00e9diction \u00e0 favoriser le mod\u00e8le dont les scores de pr\u00e9dictions sont les plus \u00e9lev\u00e9s."]}, {"cell_type": "markdown", "id": "089609b2", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Compl\u00e9tez la m\u00e9thodes $\\texttt{predict}$ afin de que la pr\u00e9diction choisie soit celle du mod\u00e8le ayant le score de pr\u00e9diction le plus \u00e9lev\u00e9.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "id": "da09d3c0", "metadata": {}, "source": ["class LogisticMaxPooling(object):\n", "    def __init__(self, dim=64, p=0.5, nb_models=50, max_iter=5000):\n", "        self.mask = np.random.binomial(n=1, p=p, size=(nb_models, dim)).astype(bool)\n", "        self.nb_models = nb_models\n", "        self.models = []\n", "        self.max_iter = max_iter\n", "        \n", "        \n", "    def fit(self, X_train, y_train):\n", "        for _ in range(self.nb_models):\n", "            model = LogisticRegression(max_iter=self.max_iter)\n", "            X = X_train[:, self.mask[_]]\n", "            model.fit(X, y_train)\n", "            self.models.append(model)\n", "            \n", "    def individual_scores(self, X, y):\n", "        scores = []\n", "        for _ in range(len(self.models)):\n", "            scores.append(self.models[_].score(X[:, self.mask[_]], y))\n", "        print('Best model:', np.max(scores), ':: Worst model:', np.min(scores))\n", "            \n", "    def predict(self, X):\n", "        ####### Complete this part ######## or die ####################\n", "        ...\n", "        ...\n", "        ...\n", "        return ...\n", "        ###############################################################\n", "        \n", "    def score(self, X, y):\n", "        scores = {'model': [], 'vote': None}\n", "        for i, m in enumerate(self.models):\n", "            scores['model'].append(m.score(X[:, self.mask[i]], y))\n", "        scores['vote'] = (self.predict(X) == y).astype(int).sum()/len(X)\n", "        return scores\n", "\n", "model = LogisticMaxPooling()\n", "model.fit(X_train, y_train)\n", "model.individual_scores(X_test, y_test)\n", "model.predict(X_test)\n", "\n", "scores = model.score(X_test, y_test)\n", "\n", "print('Nos mod\u00e8les atteignent:\\n -', '\\n - '.join([str(i) for i in scores['model']]))\n", "print('Notre agr\u00e9gation atteint:', scores['vote'])\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "dependent-faculty", "metadata": {}, "source": ["Le principe du *Bayesian Model Averaging* se g\u00e9n\u00e9ralise bien s\u00fbr lorsqu'on a une quantit\u00e9 infinie de mod\u00e8les et la *posterior* pr\u00e9dictive s'\u00e9crit :\n", "\n", "$$p(y|x_\\text{new}, S_n)=\\int p(y|x_\\text{new}, \\mathcal{M}, S_n)p(\\mathcal{M}|S_n)dM.$$\n", "\n", "Cette strat\u00e9gie demande un effort analytique cependant beaucoup plus intense. La s\u00e9quence [Bay\u00e9sienne linear regression](./2_bayesian_linear_regression.ipynb) illustrera notamment cette id\u00e9e dans le cas de la r\u00e9gression lin\u00e9aire et pointera du doigt son lien avec la notion de r\u00e9gularisation.\n"]}, {"cell_type": "markdown", "id": "3087d6e7", "metadata": {}, "source": ["## III. Bagging\n", "\n", "\n", "La difficult\u00e9 de consid\u00e9rer plusieurs mod\u00e8les est souvent que pour une classe de mod\u00e8les donn\u00e9e, un m\u00eame jeu d'apprentissage implique une m\u00eame solution et combiner plusieurs fois la m\u00eame chose ne sert \u00e0 rien. L'id\u00e9e du bagging est d'agr\u00e9ger plusieurs mod\u00e8les en trouvant justement une strat\u00e9gie pour cr\u00e9er de la variabilit\u00e9 entre ces derniers. L'agr\u00e9gation se fait de la mani\u00e8re la plus na\u00efve possible en moyennant les pr\u00e9dictions comme indiqu\u00e9 au-dessus.\n", "\n", "La strat\u00e9gie utilis\u00e9e pour cr\u00e9er de la variabilit\u00e9 est de passer par un jeu de donn\u00e9es *bootstrap*. Soit $S_n=\\{(X_i, Y_i)\\}_{i\\leq n}$ un jeu de donn\u00e9es de taille $n$. L'id\u00e9e va \u00eatre de construire $m$ nouveaux jeux de donn\u00e9es (pour chacun de nos mod\u00e8les) \u00e0 partir de $S_n$ en tirant $n$ points al\u00e9atoirement dans $S_n$ **avec** remise. Ainsi, certains points appara\u00eetront en double, et d'autres seront absents."]}, {"cell_type": "code", "execution_count": null, "id": "32f8673e", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "from sklearn.ensemble import BaggingRegressor\n", "from sklearn.tree import DecisionTreeRegressor"]}, {"cell_type": "markdown", "id": "d67e461f", "metadata": {}, "source": ["Consid\u00e9rons la fonction suivante&nbsp;:"]}, {"cell_type": "code", "execution_count": null, "id": "291abb66", "metadata": {}, "outputs": [], "source": ["def f(x):\n", "    x = x.ravel()\n", "    return np.exp(-(x-5) ** 2) + 2 * np.exp(-(x - 7) ** 2) + np.exp(-(x + 2) ** 2)"]}, {"cell_type": "code", "execution_count": null, "id": "afc0b84d", "metadata": {}, "outputs": [], "source": ["def generate(n_samples, noise=0.1, n_repeat=1):\n", "    X = np.random.rand(n_samples) * 10\n", "    X = np.sort(X)\n", "\n", "    if n_repeat == 1:\n", "        y = f(X) + np.random.normal(0.0, noise, n_samples)\n", "    else:\n", "        y = np.zeros((n_samples, n_repeat))\n", "\n", "        for i in range(n_repeat):\n", "            y[:, i] = f(X) + np.random.normal(0.0, noise, n_samples)\n", "\n", "    X = X.reshape((n_samples, 1))\n", "\n", "    return X, y\n", "\n", "\n", "X_train = []\n", "y_train = []\n", "\n", "n_repeat=50\n", "\n", "X_test, y_test = generate(n_samples=1000, n_repeat=n_repeat)\n", "\n", "for i in range(n_repeat):\n", "    X, y = generate(n_samples=50)\n", "    X_train.append(X)\n", "    y_train.append(y)"]}, {"cell_type": "markdown", "id": "397d48ab", "metadata": {}, "source": ["Nous avons vu dans la s\u00e9quence sur la r\u00e9gression lin\u00e9aire que nous pouvions d\u00e9composer notre erreur entre une composante de biais et une composante de variance&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "\\mathbb{E}\\big[(y-\\hat{f}(x))^2\\big]=\\sigma^2+\\text{Var}\\big(\\hat{f}\\big)+\\text{bias}(\\hat{f})^2.\\end{aligned}$$\n", "\n", "L'objectif ci-dessous va \u00eatre de comparer les diff\u00e9rentes briques de notre erreur. Pour cela, nous devons trouver une strat\u00e9gie pour estimer chacune de ces quantit\u00e9s.\n", "\n", "**<span style='color:blue'> Question</span>** ", "\n", "**\u00c0 votre avis, comment estimer empiriquement l'erreur totale ainsi que chacune des briques de l'erreur totale (en utilisant les jeux de donn\u00e9es cr\u00e9\u00e9s au-dessus) ?**\n", "\n", "\n\n ----", "\n", "\n", "**<span style='color:blue'> Exercice</span>** ", "\n", "**Compl\u00e9tez le code suivant afin d'entra\u00eener un arbre et un bagging construit \u00e0 partir d'arbres et avec 100 estimateurs.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "id": "a7fd803b", "metadata": {}, "source": ["####### Complete this part ######## or die ####################\n", "estimators = {\n", "    \"Tree\": ...\n", "    \"Bagging\": ...\n", "}\n", "###############################################################\n", "\n", "\n", "# Loop over estimators to compare\n", "for n, (name, estimator) in enumerate(estimators.items()):\n", "    # Compute predictions\n", "    y_predict = np.zeros((1000, n_repeat))\n", "\n", "    for i in range(n_repeat):\n", "        estimator.fit(X_train[i], y_train[i])\n", "        y_predict[:, i] = estimator.predict(X_test)\n", "\n", "    # Bias^2 + Variance + Noise decomposition of the mean squared error\n", "    y_error = np.zeros(1000)\n", "\n", "    for i in range(n_repeat):\n", "        for j in range(n_repeat):\n", "            y_error += (y_test[:, j] - y_predict[:, i]) ** 2\n", "\n", "    y_error /= (n_repeat * n_repeat)\n", "\n", "    y_noise = np.var(y_test, axis=1)\n", "    y_bias = (f(X_test) - np.mean(y_predict, axis=1)) ** 2\n", "    y_var = np.var(y_predict, axis=1)\n", "    \n", "    print(\"{0}\\t: {1:.4f} (error) = {2:.4f} (bias^2) \"\n", "          \" + {3:.4f} (var) + {4:.4f} (noise)\".format(name,\n", "                                                      np.mean(y_error),\n", "                                                      np.mean(y_bias),\n", "                                                      np.mean(y_var),\n", "                                                      np.mean(y_noise)))\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "7a65d1c1", "metadata": {}, "source": ["Vous devriez constater que l\u00e0 o\u00f9 le gain d'erreur est le plus important est la variance. C'est exactement ce que nous avions pr\u00e9alablement anticip\u00e9."]}, {"cell_type": "markdown", "id": "gross-warner", "metadata": {}, "source": ["## IV. Boosting\n", "\n", "L'id\u00e9e premi\u00e8re derri\u00e8re la m\u00e9thode de *boosting* est de construire des mod\u00e8les \"faibles\" (potentiellement \u00e0 peine meilleurs que le hasard) mais compl\u00e9mentaires entre eux poss\u00e9dant ainsi de bonnes performances en tant que groupe. Nous \u00e9tudierons ici la m\u00e9thode *AdaBoost* (i.e. Adaptive Boosting). Soit $\\mathcal{X}\\subseteq\\mathbb{R}^d$ et $\\mathcal{Y}=\\{-1, 1\\}$. On ne consid\u00e8rera que le cas de la classification binaire bien que le propos se g\u00e9n\u00e9ralise \u00e0 d'autres t\u00e2ches de *machine learning*. Notre objectif est de construire un mod\u00e8le final $h$ de $\\mathcal{X}$ vers $\\mathcal{Y}$ en s'appuyant sur un jeu de donn\u00e9es $S_n$. Notons $w_i^j$ le poids associ\u00e9 \u00e0 la donn\u00e9e $(x_i, y_i)$ pour le mod\u00e8le faible $j$. Initialisons les poids \u00e0 $w_i^j=1/n$. Notre mod\u00e8le consistera en $m$ classifieurs faibles.\n", "\n", "*AdaBoost* fonctionne de la mani\u00e8re suivante :\n", "\n", "1. On initialise notre compteur $j=1$ (i.e. on consid\u00e8re le premier mod\u00e8le)\n", "2. On optimise notre mod\u00e8le sur le jeu de donn\u00e9es : $\\sum_i w_i^j\\textbf{1}\\{h_j(x_i)\\neq y_i\\}$\n", "\n", "3.  On calcule l'erreur normalis\u00e9e : $\\epsilon_j=\\frac{\\sum_i w_i^j\\textbf{1}\\{h_j(x_i)\\neq y_i\\}}{\\sum_i w_i^j}$ et on \u00e9value l'importance du mod\u00e8le courant pour notre meta-mod\u00e8le : $\\alpha_j=\\text{ln}\\Big(\\frac{1-\\epsilon_j}{\\epsilon_j}\\Big)$\n", "4.  On met \u00e0 jour la pond\u00e9ration des donn\u00e9es pour le mod\u00e8le suivant : $w_i^{j+1}=w_i^j\\text{exp}\\big(\\alpha_j \\textbf{1}\\{h_j(x_i)\\neq y_i\\}\\big)$\n", "5.  Si $j<m$, on reprend \u00e0 l'\u00e9tape $2$ pour optimiser le mod\u00e8le suivant.\n", "\n", "Une fois les mod\u00e8les optimis\u00e9s, notre *meta-classifieur* permet de faire des pr\u00e9dictions de la mani\u00e8re suivante :\n", "\n", "$$H(x)=\\text{sign}\\Big(\\sum_j\\alpha_j h_j(x)\\Big)$$\n", "\n", "Intuitivement, on cherche \u00e0 favoriser les mod\u00e8les qui \"fonctionnent bien\" et \u00e0 donner de l'importance aux exemples d'apprentissage mal class\u00e9s pour que les futurs classifieurs faibles se concentrent dessus."]}, {"cell_type": "markdown", "id": "focused-madonna", "metadata": {}, "source": ["----\n", "\n", "**Pourquoi ces coefficients ?**\n", "\n", "\n", "Consid\u00e9rons tout d'abord la *loss* exponentielle :\n", "\n", "$$\\ell(z)=\\text{exp}\\big(-z\\big),$$\n", "\n", "o\u00f9 $z=yh(x)$, $(x, y)\\in S_n$. La *loss* est affich\u00e9e juste apr\u00e8s. De mani\u00e8re directe, on observe que si notre mod\u00e8le pr\u00e9dit un score dont le signe est le m\u00eame que celui du label, alors la *loss* est petite et, \u00e0 l'inverse, si le signe est diff\u00e9rent, notre *loss* sera grande."]}, {"cell_type": "code", "execution_count": null, "id": "turkish-engineer", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "x = np.linspace(-1, 1, 101)\n", "y = np.exp(-x)\n", "\n", "plt.figure(figsize=(12, 8))\n", "plt.plot(x, y, label='Exponential loss')\n", "y = (x<=0).astype(int)\n", "x[51] = 0\n", "plt.plot(x, y, label='0/1 loss')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "median-shannon", "metadata": {}, "source": ["On cherche \u00e0 construire un classifieur $H$ minimisant la *loss* exponentielle suivante :\n", "\n", "$$\\mathcal{L}(H)=\\sum_{i=1}^n\\ell(y_iH(x_i)),$$\n", "\n", "o\u00f9 \n", "\n", "$$H(x)=\\frac{1}{2}\\sum_j\\alpha_j h_j(x),$$\n", "\n", "tels que $h_j$ sont nos classifieurs faibles et $\\alpha_j$ les poids qui leur sont associ\u00e9s. Cependant, imaginons qu'au lieu de tout minimiser d'une seule fois, nous proc\u00e9dions it\u00e9rativement, classifieur par classifieur. Ainsi, lorsqu'on optimise le classifieur $j$, tous les classifieurs $h_{1}, \\ldots, h_{j-1}$ et leur poid $\\alpha_1, \\ldots, \\alpha_{j-1}$ restent fix\u00e9s. Notons $H_m(x)$ le classifieur total jusqu'au classifieur faible $h_m$. Notre *loss* se reformule ainsi :\n", "\n", "$$\\mathcal{L}_j(H)=\\sum_{i=1}^n\\text{exp}\\Big(-y_iH_{j-1}(x_i)-\\frac{1}{2}y_i\\alpha_jh_j(x_i)\\Big)=\\sum_{i=1}^nw_i^j\\text{exp}\\Big(-\\frac{1}{2}y_i\\alpha_jh_j(x_i)\\Big),$$\n", "\n", "o\u00f9 $w_i^j = \\text{exp}(-y_iH_{j-1}(x_i))$. Notons que si nous sommes bien entrain d'optimiser notre loss du point de vue de $h_j$ et $\\alpha_j$, alors $w_i^j, \\forall i$ sont des constantes. Notons $S_c^{m}$ l'ensemble des points correctement class\u00e9s par $H_m$ et $S_i^m$ ceux qui \u00e0 l'inverse ne le sont pas. Nous pouvons reformuler l'erreur pr\u00e9c\u00e9dente de la mani\u00e8re suivante :\n", "\n", "$$\\begin{aligned}\n", "\\mathcal{L}_j(H)&=e^{-\\alpha_j/2}\\sum_{i\\in S_\\mathcal{c}^j}w_i^m+e^{\\alpha_j/2}\\sum_{i\\in S_\\mathcal{i}^j}w_i^m\\\\\n", "&=(e^{\\alpha_j/2}-e^{-\\alpha_j/2})\\sum_{i=1}^nw_i^m\\textbf{1}\\{h_j(x_i)\\neq y_i\\}+e^{-\\alpha_j/2}\\sum_{i=1}^nw_i^m\\\\\n", "&\\propto (e^{\\alpha_j/2}-e^{-\\alpha_j/2})\\sum_{i=1}^nw_i^m\\textbf{1}\\{h_j(x_i)\\neq y_i\\}\n", "\\end{aligned}$$\n", "\n", "Du point de vu de l'optimisation de $h_j$, on remarque que cela revient \u00e0 optimiser :\n", "\n", "$$(e^{\\alpha_j/2}-e^{-\\alpha_j/2})\\sum_{i=1}^nw_i^m\\textbf{1}\\{h_j(x_i)\\neq y_i\\},$$\n", "\n", "o\u00f9 le choix de $\\alpha_j$ n'a pas d'effet. Cela revient \u00e0 r\u00e9aliser l'\u00e9tape $2$ de notre algorithme. Consid\u00e9rons maintenant l'optimisation de $\\alpha_j$. En annulant la deriv\u00e9e en fonction de $\\alpha_j$ on se rend compte que cela revient \u00e0 le calculer comme indiqu\u00e9 \u00e0 l'\u00e9tape 3 ($\\epsilon_j$ et $\\alpha_j$).\n", "\n", "----\n", "\n", "Dans le cas de la r\u00e9gression, la strat\u00e9gie va \u00eatre de travailler sur les r\u00e9sidus des mod\u00e8les pr\u00e9c\u00e9dents.\n", "\n", "---\n", "Un *stumps* est une fonction seuille. C'est une fonction qui prend une variable, et consid\u00e8re que toutes les donn\u00e9es dont la valeur de cette variable est sup\u00e9riere (ou inf\u00e9rieure) \u00e0 un seuil sont associ\u00e9es \u00e0 la classe $1$ et les autres \u00e0 la classe $-1$. Il s'agit tout simplement d'un arbre de profondeur $1$.\n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "2a5cb4e6", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "from sklearn import datasets\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.metrics import zero_one_loss\n", "from sklearn.ensemble import AdaBoostClassifier"]}, {"cell_type": "markdown", "id": "b0f831ea", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Dans le code ci-dessous, compl\u00e9tez les trois sections. La premi\u00e8re consiste \u00e0 construire un *stump*, \u00e0 le fiter et \u00e0 calculer son taux d'erreur sur le test. La seconde consiste \u00e0 calculer un arbre de d\u00e9cision de profondeur $9$ et de nombre minimum de point par feuille \u00e0 $1$, \u00e0 le fitter et \u00e0 calculer son taux d'erreur sur le test. Enfin, la troisi\u00e8me partie consiste \u00e0 construire un mod\u00e8le AdaBoost o\u00f9 le classifieur faible est un *stump*.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "id": "c7a41470", "metadata": {}, "source": ["X, y = datasets.make_hastie_10_2(n_samples=12000, random_state=1)\n", "\n", "X_test, y_test = X[2000:], y[2000:]\n", "X_train, y_train = X[:2000], y[:2000]\n", "\n", "####### Complete this part ######## or die ####################\n", "... stump instanciation\n", "... fit\n", "dt_stump_err = ...\n", "###############################################################\n", "\n", "####### Complete this part ######## or die ####################\n", "... decision tree instanciation\n", "... fit\n", "dt_err = ...\n", "###############################################################\n", "\n", "\n", "####### Complete this part ######## or die ####################\n", "n_estimators = 400\n", "learning_rate = 1.\n", "\n", "... AdaBoost instanciation\n", "... fit\n", "###############################################################\n", "\n", "fig = plt.figure(figsize=(12, 8))\n", "ax = fig.add_subplot(111)\n", "\n", "ax.plot([1, n_estimators], [dt_stump_err] * 2, 'k-',\n", "        label='Decision Stump Error')\n", "ax.plot([1, n_estimators], [dt_err] * 2, 'k--',\n", "        label='Decision Tree Error')\n", "\n", "ada_err = np.zeros((n_estimators,))\n", "for i, y_pred in enumerate(ada.staged_predict(X_test)):\n", "    ada_err[i] = zero_one_loss(y_pred, y_test)\n", "\n", "ada_err_train = np.zeros((n_estimators,))\n", "for i, y_pred in enumerate(ada.staged_predict(X_train)):\n", "    ada_err_train[i] = zero_one_loss(y_pred, y_train)\n", "\n", "ax.plot(np.arange(n_estimators) + 1, ada_err,\n", "        label='AdaBoost Test Error',\n", "        color='red')\n", "ax.plot(np.arange(n_estimators) + 1, ada_err_train,\n", "        label='AdaBoost Train Error',\n", "        color='blue')\n", "\n", "ax.set_ylim((0.0, 0.5))\n", "ax.set_xlabel('Nombre d\\'estimateurs')\n", "ax.set_ylabel('Taux d\\'erreur')\n", "\n", "leg = ax.legend(loc='upper right', fancybox=True)\n", "leg.get_frame().set_alpha(0.7)\n", "\n", "plt.show()\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "raising-martin", "metadata": {}, "source": ["## V. Les for\u00eats al\u00e9atoires"]}, {"cell_type": "markdown", "id": "6cf9be48", "metadata": {}, "source": ["Vous avez vu diff\u00e9rentes mani\u00e8res d'agr\u00e9ger plusieurs classifieurs. Vous avez \u00e9galement \u00e9tudi\u00e9 les arbres de d\u00e9cision. Un arbre pris individuellement, sans limite de profondeur poss\u00e8de une dimension VC infinie. Ou, dit autrement, il poss\u00e8de une forte capacit\u00e9 \u00e0 sur-apprendre. Les *for\u00eats al\u00e9atoires* ou *random forest* d\u00e9finissent une strat\u00e9gie permettant d'agr\u00e9ger des arbres de d\u00e9cisions compl\u00e9mentaires. Chacun des arbres ne peut pas avoir \u00e9t\u00e9 appris de la m\u00eame mani\u00e8re puisque l'apprentissage est d\u00e9terministe (on aurait ainsi $M$ arbres identiques et \u00e7a n'apporterait rien).\n", "\n", "Il existe de multiples strat\u00e9gies permettant de construire des arbres diff\u00e9rents et nous en d\u00e9taillons une ici. Il existe deux angles d'attaque que nous combinerons :\n", "* Chaque arbre ne voit pas les m\u00eames donn\u00e9es,\n", "* Chaque arbre ne voit pas les m\u00eames *features* (chaque n\u0153ud voit des features choisis al\u00e9atoirement).\n", "\n", "Une strat\u00e9gie d'apprentissage est donc la suivante pour un des arbres :\n", "\n", "1.  On construit un jeu de donn\u00e9es $S^\\prime$ en tirant uniform\u00e9ment des points de $S$ avec remise. Une fois que $|S^\\prime|=n^\\prime$, on s'arr\u00eate,\n", "2. En supposant que nous ayons $d$ variables explicatives, on tire uniform\u00e9ment $d^\\prime$ variables dans $d$,\n", "3. On construit l'arbre avec le jeu de donn\u00e9es $S^\\prime$ et les $d^\\prime$ variables s\u00e9lectionn\u00e9es (dans \\texttt{sklearn} les variables sont choisies \u00e0 chaque n\u0153ud).\n", "\n", "Cette op\u00e9ration est r\u00e9p\u00e9t\u00e9e jusqu'\u00e0 ce que tous nos arbres aient \u00e9t\u00e9 construits. Ainsi en voyant moins de *features*, nos arbres sont plus stables et leur combinaisons ayant vu des informations diff\u00e9rentes est elle-m\u00eame plus stable."]}, {"cell_type": "code", "execution_count": null, "id": "28dda641", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "from sklearn import datasets\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.metrics import zero_one_loss\n", "from sklearn.ensemble import RandomForestClassifier"]}, {"cell_type": "markdown", "id": "d7ee1be7", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**En r\u00e9cup\u00e9rant les r\u00e9sultats d\u00e9j\u00e0 calcul\u00e9s dans Boosting pour *stump* et *decision tree*, compl\u00e9tez le code suivant afin de construire et de fitter un *random forest* tel que la profondeur maximale de l'arbre soit $9$.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "id": "78b455aa", "metadata": {}, "source": ["X, y = datasets.make_hastie_10_2(n_samples=12000, random_state=1)\n", "\n", "X_test, y_test = X[2000:], y[2000:]\n", "X_train, y_train = X[:2000], y[:2000]\n", "\n", "\n", "\n", "learning_rate = 1.\n", "\n", "max_n_estimators = 50\n", "\n", "n_estimators_list = np.array(\n", "    list(range(1, max_n_estimators + 1))\n", ")\n", "\n", "rf_err = np.zeros((n_estimators_list.shape[0],))\n", "rf_err_train = np.zeros((n_estimators_list.shape[0],))\n", "\n", "for j, nb_estimators in enumerate(n_estimators_list):\n", "    ####### Complete this part ######## or die ####################\n", "    ... random forest construction\n", "    ... fit\n", "    ###############################################################\n", "    rf_err[j] = zero_one_loss(rf.predict(X_test), y_test)\n", "    rf_err_train[j] = zero_one_loss(rf.predict(X_train), y_train)\n", "\n", "fig = plt.figure(figsize=(12, 8))\n", "ax = fig.add_subplot(111)\n", "\n", "ax.plot([1, max_n_estimators], [dt_stump_err] * 2, 'k-',\n", "        label='Decision Stump Error')\n", "ax.plot([1, max_n_estimators], [dt_err] * 2, 'k--',\n", "        label='Decision Tree Error')\n", "\n", "ax.plot(n_estimators_list, rf_err,\n", "        label='RandomForest Test Error',\n", "        color='red')\n", "\n", "ax.plot(n_estimators_list, rf_err_train,\n", "        label='RandomForest Train Error',\n", "        color='blue')\n", "\n", "ax.set_ylim((0.0, 0.5))\n", "ax.set_xlabel('Nombre d\\'estimateurs')\n", "ax.set_ylabel('Taux d\\'erreur')\n", "\n", "leg = ax.legend(loc='upper right', fancybox=True)\n", "leg.get_frame().set_alpha(0.7)\n", "\n", "plt.show()\n"], "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.2"}}, "nbformat": 4, "nbformat_minor": 5}
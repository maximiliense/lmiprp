
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>M√©thodes ensemblistes ‚òïÔ∏è‚òïÔ∏è &#8212; Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Bayesian linear regression ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è" href="2_bayesian_linear_regression.html" />
    <link rel="prev" title="Les m√©thodes ensemblistes" href="0_propos_liminaire.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-72WWYCKNK6"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-72WWYCKNK6');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../0_requisite/rappels_python.html">
   Rappels de Python et Numpy
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../1_what_is_ml/0_propos_liminaire.html">
   <em>
    Machine Learning
   </em>
   , initiation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/1_introduction_ml.html">
     <em>
      Machine learning
     </em>
     et mal√©diction de la dimension
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/2_regression_and_classification_trees.html">
     Les arbres de r√©gression et de classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../2_regression/0_propos_liminaire.html">
   La
   <em>
    r√©gression
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/1_linear_regression.html">
     La r√©gression lin√©aire ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/2_optimization.html">
     L‚Äôoptimisation ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/3_interpolation.html">
     Interpolation ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/4_algo_proximal_lasso.html">
     Sous-diff√©rentiel et le cas du Lasso ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/5_least_square_qr.html">
     Les moindres carr√©s via une d√©composition QR (et plus)‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/6_ridge.html">
     Une analyse de la r√©gularisation Ridge ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../3_classification/0_propos_liminaire.html">
   La
   <em>
    classification
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/1_logistic_regression.html">
     La r√©gression logistique ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/2_fonctions_proxy.html">
     Les fonctions de perte (loss function) ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/3_bayes_classifier.html">
     Le classifieur de Bayes ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/4_VC_theory.html">
     Un mod√®le formel de l‚Äôapprentissage ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è (üíÜ‚Äç‚ôÇÔ∏è)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../4_kernel_methods/0_propos_liminaire.html">
   Les m√©thodes √† noyaux
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../4_kernel_methods/1_svm.html">
     Le SVM ou l‚Äôhypoth√®se max-margin ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="0_propos_liminaire.html">
   Les m√©thodes
   <em>
    ensemblistes
   </em>
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     M√©thodes ensemblistes ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="2_bayesian_linear_regression.html">
     Bayesian linear regression ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../6_deeplearning/0_propos_liminaire.html">
   <em>
    deep learning
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/1_autodiff.html">
     La diff√©rentiation automatique et un d√©but de
     <em>
      deep learning
     </em>
     ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/2_filters_representation.html">
     Filtres et espace de repr√©sentation des r√©seaux de neurones ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/3_probabilities_calibration.html">
     Calibration des probabilit√©s et quelques notions ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/4_regularization_deep.html">
     R√©gularisation en
     <em>
      deep learning
     </em>
     ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/5_transfer_multitask.html">
     <em>
      Transfer learning
     </em>
     et apprentissage multi-t√¢ches ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/6_adversarial.html">
     Les attaques adversaires ‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../7_unsupervised/0_propos_liminaire.html">
   L‚Äôapprentissage non-supervis√©
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_unsupervised/1_principal_component_analysis.html">
     L‚ÄôAnalyse en Composantes Principales ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_unsupervised/2_em_gaussin_mixture_model.html">
     Mod√®le de M√©lange Gaussien et algorithme
     <em>
      Expectation-Maximization
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../8_set_prediction/0_propos_liminaire.html">
   Pr√©diction d‚Äôensembles
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/1_well_defined.html">
     Jeu d‚Äôapprentissage
     <em>
      set-valued
     </em>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/2_ill_defined.html">
     Jeu d‚Äôapprentissage uniquement multi-classes ‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../biblio.html">
   Bibliographie
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/5_ensembles/1_ensembles.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/maximiliense/lmiprp"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/maximiliense/lmiprp/issues/new?title=Issue%20on%20page%20%2F5_ensembles/1_ensembles.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/maximiliense/lmiprp/blob/master/Travaux Pratiques/Machine Learning/Book/5_ensembles/1_ensembles.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#i-l-approche-naive">
   I. L‚Äôapproche na√Øve
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ii-bayesian-model-averaging">
   II. Bayesian Model Averaging
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iii-bagging">
   III. Bagging
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iv-boosting">
   IV. Boosting
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#v-les-forets-aleatoires">
   V. Les for√™ts al√©atoires
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="methodes-ensemblistes">
<h1>M√©thodes ensemblistes ‚òïÔ∏è‚òïÔ∏è<a class="headerlink" href="#methodes-ensemblistes" title="Permalink to this headline">¬∂</a></h1>
<div class="admonition-objectifs-de-la-sequence admonition">
<p class="admonition-title">Objectifs de la s√©quence</p>
<ul class="simple">
<li><p>√ätre sensibilis√©¬†:</p>
<ul>
<li><p>aux gains de performances possibles via ce genre de m√©thodes,</p></li>
<li><p>√† l‚Äôeffet d‚Äôun ensemble de mod√®les sur la variance de notre pr√©diction.</p></li>
</ul>
</li>
<li><p>√ätre capable¬†:</p>
<ul>
<li><p>de produire des ensembles de mod√®les,</p></li>
<li><p>de se servir de <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span> pour faire des ensembles,</p></li>
<li><p>et utiliser les for√™ts al√©atoires, les boostings, etc.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¬∂</a></h2>
<p>Soit <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> notre espace d‚Äôentr√©e et <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> notre espace de sortie. Soit <span class="math notranslate nohighlight">\(X, Y\)</span> deux variables al√©atoires sur <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> et <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> et soit <span class="math notranslate nohighlight">\(\mathbb{P}\)</span> leur mesure jointe. Notre objectif est de trouver une application <span class="math notranslate nohighlight">\(h:\mathcal{X}\mapsto\mathcal{Y}\)</span> qui minimise une certaine erreur qu‚Äôon notera <span class="math notranslate nohighlight">\(L\)</span>. N‚Äôayant pas acc√®s aux variables al√©atoires <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(Y\)</span>, nous collectons un jeu de donn√©es <span class="math notranslate nohighlight">\(S_n=\{(X_i, Y_i)\}_{i\leq n}\sim\mathbb{P}^n\)</span> (par exemple en r√©cup√©rant des images sur internet) et nous construisons un risque empirique :</p>
<div class="math notranslate nohighlight">
\[L_n(h)=\frac{1}{n}\sum_{i}\ell(h(X_i), Y_i),\]</div>
<p>o√π <span class="math notranslate nohighlight">\(\ell\)</span> d√©finit une erreur √©l√©mentaire (i.e. pour une unique pr√©diction). Comme nous l‚Äôavons vu, ce risque empirique <span class="math notranslate nohighlight">\(L_n\)</span> est un estimateur de <span class="math notranslate nohighlight">\(L\)</span>. La fonction <span class="math notranslate nohighlight">\(h\)</span> est ainsi construite en utilisant le jeu de donn√©es <span class="math notranslate nohighlight">\(S_n\)</span>.</p>
</div>
<div class="section" id="i-l-approche-naive">
<h2>I. L‚Äôapproche na√Øve<a class="headerlink" href="#i-l-approche-naive" title="Permalink to this headline">¬∂</a></h2>
<p>L‚Äôapproche la plus simple lorsqu‚Äôon cherche √† combiner plusieurs mod√®les consiste √† moyenner leur pr√©diction. Dans le cas de la r√©gression, la pr√©diction est la moyenne traditionnelle. Dans le cas de la classification, on utilisera un vote √† la majorit√© simple.</p>
<hr class="docutils" />
<p>Soit <span class="math notranslate nohighlight">\(\mathcal{Y}\subseteq\mathbb{R}\)</span> l‚Äôespace des labels et <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> l‚Äôespace de nos donn√©es d‚Äôentr√©e. Consid√©rons une famille de pr√©dicteurs <span class="math notranslate nohighlight">\(\{y_j\}_{j\leq m}\)</span>, l‚Äôagr√©gation de ces derniers se fait de la mani√®re la plus na√Øve qui soit (comme nous l‚Äôavons vu au-dessus)¬†:</p>
<div class="math notranslate nohighlight">
\[\hat{y}=\frac{1}{m}\sum_j y_j(x).\]</div>
<p>Supposons que nous ayons pour un pr√©dicteur donn√© <span class="math notranslate nohighlight">\(\hat{y}_{ij} = y(x_j)+\epsilon_{ij}\)</span> o√π la pr√©diction se fait √† un bruit pr√®s qui d√©pend de l‚Äô√©chantillon mais aussi du pr√©dicteur. Nous avons √©videmment la relation suivante¬†:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\big[(\hat{y}_j(X)-y(X))^2\big]=\mathbb{E}\big[\epsilon_{j}^2\big],\]</div>
<p>o√π <span class="math notranslate nohighlight">\(\epsilon_j\)</span> d√©pend de <span class="math notranslate nohighlight">\(X\)</span>. De mani√®re similaire, si cette fois-ci nous consid√©rons l‚Äôagr√©gation de nos pr√©dicteurs, nous avons¬†:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\big[(y(X)-\frac{1}{m}\sum_j \hat{y_j}(X))^2\big]=\mathbb{E}\big[(\frac{1}{m}\sum_j\epsilon_{j})^2\big].\]</div>
<p>Si on suppose (ce qui n‚Äôest pas vraiment le cas en pratique)¬†:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{E}\big[\epsilon_j\big]&amp;=0\\
\mathbb{E}\big[\epsilon_j\epsilon_k\big]&amp;=0,\ j\neq k,
\end{aligned}\end{split}\]</div>
<p>nous avons¬†:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\big[(\frac{1}{m}\sum_j\epsilon_{j})^2\big]=\frac{1}{m^2}\sum_i\mathbb{E}\big[\epsilon^2\big]=\frac{1}{m}\mathbb{E}_{\bar{\epsilon}},\]</div>
<p>o√π <span class="math notranslate nohighlight">\(\mathbb{E}_{\bar{\epsilon}}\)</span> indique l‚Äôerreur moyenne de nos pr√©dicteurs. L‚Äôerreur diminue au fur et √† mesure que l‚Äôon ajoute des pr√©dicteurs.</p>
<hr class="docutils" />
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>

<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">images</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray_r</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Training: </span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">label</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_ensembles_3_0.png" src="../_images/1_ensembles_3_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span> <span class="o">/</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Compl√©tez la m√©thode <span class="math notranslate nohighlight">\(\texttt{predict}\)</span> de la classe suivante afin de faire un vote √† la majorit√© simple pour combiner plusieurs mod√®les.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MajorityVoting</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">models</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">models</span> <span class="o">=</span> <span class="n">models</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">:</span>
            <span class="n">m</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1">####### Complete this part ######## or die ####################</span>
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="k">return</span> <span class="o">...</span>
        <span class="c1">###############################################################</span>
    
    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;vote&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">:</span>
            <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
        <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;vote&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span>
            
<span class="n">model</span> <span class="o">=</span> <span class="n">MajorityVoting</span><span class="p">(</span>
    <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">),</span> 
    <span class="n">DecisionTreeClassifier</span><span class="p">(),</span>
    <span class="n">KNeighborsClassifier</span><span class="p">()</span>
<span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Nos mod√®les atteignent:&#39;</span><span class="p">,</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Notre agr√©gation atteint:&#39;</span><span class="p">,</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;vote&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>On peut imaginer que les mod√®les se trompent sur les m√™mes images et non de mani√®re al√©atoire. C‚Äôest par exemple le cas si les mod√®les se trompent lorsqu‚Äôun chiffre est mal dessin√© et ressemble √† un autre chiffre : tous les mod√®les vont faire la m√™me erreur. Dans ces cas de figure, le meilleur mod√®le est le meilleur mod√®le et non le vote. √Ä l‚Äôinverse, si nos diff√©rents mod√®les ont tendance √† faire des erreurs diff√©rentes, alors l‚Äôagr√©gation permettra de gagner en score.</p>
<p>Consid√©rons maintenant le jeu de donn√©es suivant.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_boston</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">load_boston</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">/</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsRegressor</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Compl√©tez la m√©thode <span class="math notranslate nohighlight">\(\texttt{predict}\)</span> de la classe suivante afin de faire une moyenne pour combiner plusieurs mod√®les.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">AverageVoting</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">models</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">models</span> <span class="o">=</span> <span class="n">models</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">:</span>
            <span class="n">m</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1">####### Complete this part ######## or die ####################</span>
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="k">return</span> <span class="o">...</span>
        <span class="c1">###############################################################</span>
    
    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;vote&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">:</span>
            <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)))</span>
        <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;vote&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">scores</span>
            
<span class="n">model</span> <span class="o">=</span> <span class="n">AverageVoting</span><span class="p">(</span>
    <span class="n">LinearRegression</span><span class="p">(),</span> 
    <span class="n">DecisionTreeRegressor</span><span class="p">(),</span>
    <span class="n">KNeighborsRegressor</span><span class="p">()</span>
<span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Nos mod√®les atteignent:&#39;</span><span class="p">,</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Notre agr√©gation atteint:&#39;</span><span class="p">,</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;vote&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>On observe cette fois-ci un gain clair de performances. Les diff√©rents mod√®les doivent se tromper d‚Äôune mani√®re au moins partiellement al√©atoire et r√©partie autour de la moyenne. L‚Äôagr√©gation rend ce r√©sultat plus stable.</p>
<p>Ces mani√®res d‚Äôagr√©ger des votes est effectives mais restent ‚Äúna√Øve‚Äù. Nous allons voir que nous pouvons formaliser tout cela dans le cadre du <em>framework</em> probabiliste.</p>
</div>
<div class="section" id="ii-bayesian-model-averaging">
<h2>II. Bayesian Model Averaging<a class="headerlink" href="#ii-bayesian-model-averaging" title="Permalink to this headline">¬∂</a></h2>
<p>Notons <span class="math notranslate nohighlight">\(z_i=(x_i, y_i)\)</span> et une famille de <span class="math notranslate nohighlight">\(M\)</span> mod√®les probabilistes (e.g. r√©gression logistique) o√π chaque mod√®le est not√© <span class="math notranslate nohighlight">\(\mathcal{M}_j\)</span>. Notons :</p>
<div class="math notranslate nohighlight">
\[p(y_i|x_i, \mathcal{M}_j),\]</div>
<p>la densit√© d‚Äôun point de <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> relativement au mod√®le <span class="math notranslate nohighlight">\(\mathcal{M}_j\)</span> et √† une observation <span class="math notranslate nohighlight">\(x_i\in\mathcal{X}\)</span>. Notre objectif est de d√©terminer dans un premier temps la ‚Äúqualit√©‚Äù d‚Äôun mod√®le en tenant compte de notre point de vu <em>a priori</em> ainsi que des donn√©es que nous avons pu observer <span class="math notranslate nohighlight">\(S_n\)</span>. Notons ainsi <span class="math notranslate nohighlight">\(p(\mathcal{M}_j)\)</span> notre probabilit√© <em>a priori</em> sur le mod√®le <span class="math notranslate nohighlight">\(\mathcal{M}_j\)</span>. Celle-ci peut favoriser certaines solutions parcimonieuses ou bien √™tre uniforme et ne favoriser aucun mod√®le. En appliquant la r√®gle de Bayes, nous obtenons :</p>
<div class="math notranslate nohighlight">
\[p(\mathcal{M}_j|X, \boldsymbol{y})=\frac{p(\boldsymbol{y}|X, \mathcal{M}_j)p(\mathcal{M}_j)}{p(\boldsymbol{y}| X)},\]</div>
<p>o√π <span class="math notranslate nohighlight">\(S_n=(\boldsymbol{y}, X)\)</span> et o√π nous avons :</p>
<div class="math notranslate nohighlight">
\[p(\boldsymbol{y}|X, \mathcal{M}_j)=\prod_i p(y_i|x_i, \mathcal{M}_j),\]</div>
<p>ainsi que :</p>
<div class="math notranslate nohighlight">
\[p(\boldsymbol{y}| X)=\sum_j p(\boldsymbol{y}|X, \mathcal{M}_j)p(\mathcal{M}_j).\]</div>
<p>Nous avons bien ici toutes les informations nous permettant d‚Äô√©valuer de mani√®re Bay√©sienne la qualit√© de nos diff√©rents mod√®les. Cependant, en pratique, les variations pourraient tr√®s bien d√©pendre d‚Äôun tirage particulier de nos donn√©es. L‚Äôid√©e derri√®re le <em>Bayesian Model Averaging</em> consiste √† consid√©rer TOUS les mod√®les mais √† les pond√©rer par leur qualit√©. Cela ne se fait pas au doigt mouill√©, mais en utilisant encore une fois le <em>framework</em> probabiliste :</p>
<div class="math notranslate nohighlight">
\[p(y_\text{new}|x_\text{new}, S_n)=\sum_j p(y_\text{new}|x_\text{new}, \mathcal{M}_j, S_n)p(\mathcal{M}_j|S_n).\]</div>
<p><strong>Application a une famille de r√©gression logistique.</strong></p>
<p>Dans le cas de la r√©gression logistique, nous avons la vraisemblance suivante :</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\theta_j)=\prod_i p(y_i|x_i,\theta_j)\]</div>
<p>o√π</p>
<div class="math notranslate nohighlight">
\[p(y_i|x_i, \theta_j)=\sigma_j(x_i)^y_i(1-\sigma_j(x_i))^{1-y_i}\]</div>
<p>et</p>
<div class="math notranslate nohighlight">
\[\sigma_j(x)=(1+e^{-\langle\theta_j,x\rangle})^{-1}.\]</div>
<p>Ainsi, la <em>posterior</em> de notre mod√®le <span class="math notranslate nohighlight">\(\theta_j\)</span> apr√®s avoir observ√© notre jeu de donn√©es est donn√©e par :</p>
<div class="math notranslate nohighlight">
\[p(\theta_j|S_n)\propto \prod_i p(y_i|x_i,\theta_j)p(\theta_j),\]</div>
<p>o√π le symbole <span class="math notranslate nohighlight">\(\propto\)</span> indique la proportionnalit√©.</p>
<p>Nous allons construire dans cette exercice un mod√®le de classification d‚Äôimages de chiffres √† partir de r√©gressions logistiques. La particularit√© sera que chaque r√©gression logistique ne verra qu‚Äôune partie de l‚Äôimage tir√©e al√©atoirement.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Affichons le jeu de donn√©es ainsi qu‚Äôun exemple de ce que pourrait voir un mod√®le.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">images</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray_r</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Training: </span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">label</span><span class="p">)</span>

<span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">bool</span><span class="p">)</span>

<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">images</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">ma</span><span class="o">.</span><span class="n">masked_array</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">),</span> 
        <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray_r</span><span class="p">,</span> 
        <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span>
    <span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Training: </span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">label</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_ensembles_20_0.png" src="../_images/1_ensembles_20_0.png" />
<img alt="../_images/1_ensembles_20_1.png" src="../_images/1_ensembles_20_1.png" />
</div>
</div>
<p>Construisons notre jeu d‚Äôapprentissage ainsi que notre jeu de test.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span> <span class="o">/</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Compl√©tez les m√©thodes <span class="math notranslate nohighlight">\(\texttt{posterior_}\)</span>, <span class="math notranslate nohighlight">\(\texttt{fit}\)</span> et <span class="math notranslate nohighlight">\(\texttt{predict}\)</span> afin de reproduire le <em>framework</em> que nous avons introduit au-dessus.</strong></p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">logsumexp</span>
</pre></div>
</div>
</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">BayesianLogisticAveraging</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">nb_models</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">nb_models</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">bool</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nb_models</span> <span class="o">=</span> <span class="n">nb_models</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">models</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">posterior</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prior</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="n">nb_models</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">=</span> <span class="n">max_iter</span>
        
    <span class="k">def</span> <span class="nf">posterior_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="c1">####### Complete this part ######## or die ####################</span>
        <span class="o">...</span>
        <span class="o">...</span>

        <span class="k">return</span> <span class="o">...</span>
        <span class="c1">###############################################################</span>
        
        
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">):</span>
        <span class="c1">####### Complete this part ######## or die ####################</span>
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="c1">###############################################################</span>
        
    <span class="k">def</span> <span class="nf">individual_scores</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">)):</span>
            <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">[</span><span class="n">_</span><span class="p">]</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask</span><span class="p">[</span><span class="n">_</span><span class="p">]],</span> <span class="n">y</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">scores</span><span class="p">))</span>
            
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1">####### Complete this part ######## or die ####################</span>
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="k">return</span> <span class="o">...</span>
        <span class="c1">###############################################################</span>
        
    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;vote&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">):</span>
            <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span> <span class="n">y</span><span class="p">))</span>
        <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;vote&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">BayesianLogisticAveraging</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Nos mod√®les atteignent:</span><span class="se">\n</span><span class="s1"> -&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> - &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Notre agr√©gation atteint:&#39;</span><span class="p">,</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;vote&#39;</span><span class="p">])</span>

</pre></div>
</div>
<p>De la m√™me mani√®re que pr√©c√©demment un gain est observ√© si les erreurs sont ind√©pendantes. De plus, il est important d‚Äôavoir une bonne estimation des probabilit√©s conditionnelles afin que la vraisemblance soit un bon indicateur de la qualit√© de notre mod√®le. Une strat√©gie alternative, souvent utilis√©e en <em>deep learning</em> consiste pour chaque pr√©diction √† favoriser le mod√®le dont les scores de pr√©dictions sont les plus √©lev√©s.</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Compl√©tez la m√©thodes <span class="math notranslate nohighlight">\(\texttt{predict}\)</span> afin de que la pr√©diction choisie soit celle du mod√®le ayant le score de pr√©diction le plus √©lev√©.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LogisticMaxPooling</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">nb_models</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">nb_models</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">bool</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nb_models</span> <span class="o">=</span> <span class="n">nb_models</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">models</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">=</span> <span class="n">max_iter</span>
        
        
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nb_models</span><span class="p">):</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">)</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask</span><span class="p">[</span><span class="n">_</span><span class="p">]]</span>
            <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            
    <span class="k">def</span> <span class="nf">individual_scores</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">)):</span>
            <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">[</span><span class="n">_</span><span class="p">]</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask</span><span class="p">[</span><span class="n">_</span><span class="p">]],</span> <span class="n">y</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Best model:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="s1">&#39;:: Worst model:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">scores</span><span class="p">))</span>
            
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1">####### Complete this part ######## or die ####################</span>
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="k">return</span> <span class="o">...</span>
        <span class="c1">###############################################################</span>
        
    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;vote&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">):</span>
            <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span> <span class="n">y</span><span class="p">))</span>
        <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;vote&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticMaxPooling</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">individual_scores</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Nos mod√®les atteignent:</span><span class="se">\n</span><span class="s1"> -&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> - &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Notre agr√©gation atteint:&#39;</span><span class="p">,</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;vote&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>Le principe du <em>Bayesian Model Averaging</em> se g√©n√©ralise bien s√ªr lorsqu‚Äôon a une quantit√© infinie de mod√®les et la <em>posterior</em> pr√©dictive s‚Äô√©crit :</p>
<div class="math notranslate nohighlight">
\[p(y|x_\text{new}, S_n)=\int p(y|x_\text{new}, \mathcal{M}, S_n)p(\mathcal{M}|S_n)dM.\]</div>
<p>Cette strat√©gie demande un effort analytique cependant beaucoup plus intense. La s√©quence <a class="reference internal" href="2_bayesian_linear_regression.html"><span class="doc std std-doc">Bay√©sienne linear regression</span></a> illustrera notamment cette id√©e dans le cas de la r√©gression lin√©aire et pointera du doigt son lien avec la notion de r√©gularisation.</p>
</div>
<div class="section" id="iii-bagging">
<h2>III. Bagging<a class="headerlink" href="#iii-bagging" title="Permalink to this headline">¬∂</a></h2>
<p>La difficult√© de consid√©rer plusieurs mod√®les est souvent que pour une classe de mod√®les donn√©e, un m√™me jeu d‚Äôapprentissage implique une m√™me solution et combiner plusieurs fois la m√™me chose ne sert √† rien. L‚Äôid√©e du bagging est d‚Äôagr√©ger plusieurs mod√®les en trouvant justement une strat√©gie pour cr√©er de la variabilit√© entre ces derniers. L‚Äôagr√©gation se fait de la mani√®re la plus na√Øve possible en moyennant les pr√©dictions comme indiqu√© au-dessus.</p>
<p>La strat√©gie utilis√©e pour cr√©er de la variabilit√© est de passer par un jeu de donn√©es <em>bootstrap</em>. Soit <span class="math notranslate nohighlight">\(S_n=\{(X_i, Y_i)\}_{i\leq n}\)</span> un jeu de donn√©es de taille <span class="math notranslate nohighlight">\(n\)</span>. L‚Äôid√©e va √™tre de construire <span class="math notranslate nohighlight">\(m\)</span> nouveaux jeux de donn√©es (pour chacun de nos mod√®les) √† partir de <span class="math notranslate nohighlight">\(S_n\)</span> en tirant <span class="math notranslate nohighlight">\(n\)</span> points al√©atoirement dans <span class="math notranslate nohighlight">\(S_n\)</span> <strong>avec</strong> remise. Ainsi, certains points appara√Ætront en double, et d‚Äôautres seront absents.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
</pre></div>
</div>
</div>
</div>
<p>Consid√©rons la fonction suivante¬†:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mi">5</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">7</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_repeat</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">n_repeat</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">noise</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_repeat</span><span class="p">))</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_repeat</span><span class="p">):</span>
            <span class="n">y</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">noise</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>

    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>


<span class="n">X_train</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">n_repeat</span><span class="o">=</span><span class="mi">50</span>

<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">generate</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_repeat</span><span class="o">=</span><span class="n">n_repeat</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_repeat</span><span class="p">):</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">generate</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">X_train</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">y_train</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Nous avons vu dans la s√©quence sur la r√©gression lin√©aire que nous pouvions d√©composer notre erreur entre une composante de biais et une composante de variance¬†:</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\mathbb{E}\big[(y-\hat{f}(x))^2\big]=\sigma^2+\text{Var}\big(\hat{f}\big)+\text{bias}(\hat{f})^2.\end{aligned}\]</div>
<p>L‚Äôobjectif ci-dessous va √™tre de comparer les diff√©rentes briques de notre erreur. Pour cela, nous devons trouver une strat√©gie pour estimer chacune de ces quantit√©s.</p>
<div class="admonition-question admonition">
<p class="admonition-title">Question</p>
<p><strong>√Ä votre avis, comment estimer empiriquement l‚Äôerreur totale ainsi que chacune des briques de l‚Äôerreur totale (en utilisant les jeux de donn√©es cr√©√©s au-dessus) ?</strong></p>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Compl√©tez le code suivant afin d‚Äôentra√Æner un arbre et un bagging construit √† partir d‚Äôarbres et avec 100 estimateurs.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="n">estimators</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;Tree&quot;</span><span class="p">:</span> <span class="o">...</span>
    <span class="s2">&quot;Bagging&quot;</span><span class="p">:</span> <span class="o">...</span>
<span class="p">}</span>
<span class="c1">###############################################################</span>


<span class="c1"># Loop over estimators to compare</span>
<span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">estimator</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">estimators</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
    <span class="c1"># Compute predictions</span>
    <span class="n">y_predict</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_repeat</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_repeat</span><span class="p">):</span>
        <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">y_predict</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

    <span class="c1"># Bias^2 + Variance + Noise decomposition of the mean squared error</span>
    <span class="n">y_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_repeat</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_repeat</span><span class="p">):</span>
            <span class="n">y_error</span> <span class="o">+=</span> <span class="p">(</span><span class="n">y_test</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="n">y_predict</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span>

    <span class="n">y_error</span> <span class="o">/=</span> <span class="p">(</span><span class="n">n_repeat</span> <span class="o">*</span> <span class="n">n_repeat</span><span class="p">)</span>

    <span class="n">y_noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y_bias</span> <span class="o">=</span> <span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_predict</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">y_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">y_predict</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{0}</span><span class="se">\t</span><span class="s2">: </span><span class="si">{1:.4f}</span><span class="s2"> (error) = </span><span class="si">{2:.4f}</span><span class="s2"> (bias^2) &quot;</span>
          <span class="s2">&quot; + </span><span class="si">{3:.4f}</span><span class="s2"> (var) + </span><span class="si">{4:.4f}</span><span class="s2"> (noise)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">,</span>
                                                      <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_error</span><span class="p">),</span>
                                                      <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_bias</span><span class="p">),</span>
                                                      <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_var</span><span class="p">),</span>
                                                      <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_noise</span><span class="p">)))</span>
</pre></div>
</div>
<p>Vous devriez constater que l√† o√π le gain d‚Äôerreur est le plus important est la variance. C‚Äôest exactement ce que nous avions pr√©alablement anticip√©.</p>
</div>
<div class="section" id="iv-boosting">
<h2>IV. Boosting<a class="headerlink" href="#iv-boosting" title="Permalink to this headline">¬∂</a></h2>
<p>L‚Äôid√©e premi√®re derri√®re la m√©thode de <em>boosting</em> est de construire des mod√®les ‚Äúfaibles‚Äù (potentiellement √† peine meilleurs que le hasard) mais compl√©mentaires entre eux poss√©dant ainsi de bonnes performances en tant que groupe. Nous √©tudierons ici la m√©thode <em>AdaBoost</em> (i.e. Adaptive Boosting). Soit <span class="math notranslate nohighlight">\(\mathcal{X}\subseteq\mathbb{R}^d\)</span> et <span class="math notranslate nohighlight">\(\mathcal{Y}=\{-1, 1\}\)</span>. On ne consid√®rera que le cas de la classification binaire bien que le propos se g√©n√©ralise √† d‚Äôautres t√¢ches de <em>machine learning</em>. Notre objectif est de construire un mod√®le final <span class="math notranslate nohighlight">\(h\)</span> de <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> vers <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> en s‚Äôappuyant sur un jeu de donn√©es <span class="math notranslate nohighlight">\(S_n\)</span>. Notons <span class="math notranslate nohighlight">\(w_i^j\)</span> le poids associ√© √† la donn√©e <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> pour le mod√®le faible <span class="math notranslate nohighlight">\(j\)</span>. Initialisons les poids √† <span class="math notranslate nohighlight">\(w_i^j=1/n\)</span>. Notre mod√®le consistera en <span class="math notranslate nohighlight">\(m\)</span> classifieurs faibles.</p>
<p><em>AdaBoost</em> fonctionne de la mani√®re suivante :</p>
<ol class="simple">
<li><p>On initialise notre compteur <span class="math notranslate nohighlight">\(j=1\)</span> (i.e. on consid√®re le premier mod√®le)</p></li>
<li><p>On optimise notre mod√®le sur le jeu de donn√©es : <span class="math notranslate nohighlight">\(\sum_i w_i^j\textbf{1}\{h_j(x_i)\neq y_i\}\)</span></p></li>
<li><p>On calcule l‚Äôerreur normalis√©e : <span class="math notranslate nohighlight">\(\epsilon_j=\frac{\sum_i w_i^j\textbf{1}\{h_j(x_i)\neq y_i\}}{\sum_i w_i^j}\)</span> et on √©value l‚Äôimportance du mod√®le courant pour notre meta-mod√®le : <span class="math notranslate nohighlight">\(\alpha_j=\text{ln}\Big(\frac{1-\epsilon_j}{\epsilon_j}\Big)\)</span></p></li>
<li><p>On met √† jour la pond√©ration des donn√©es pour le mod√®le suivant : <span class="math notranslate nohighlight">\(w_i^{j+1}=w_i^j\text{exp}\big(\alpha_j \textbf{1}\{h_j(x_i)\neq y_i\}\big)\)</span></p></li>
<li><p>Si <span class="math notranslate nohighlight">\(j&lt;m\)</span>, on reprend √† l‚Äô√©tape <span class="math notranslate nohighlight">\(2\)</span> pour optimiser le mod√®le suivant.</p></li>
</ol>
<p>Une fois les mod√®les optimis√©s, notre <em>meta-classifieur</em> permet de faire des pr√©dictions de la mani√®re suivante :</p>
<div class="math notranslate nohighlight">
\[H(x)=\text{sign}\Big(\sum_j\alpha_j h_j(x)\Big)\]</div>
<p>Intuitivement, on cherche √† favoriser les mod√®les qui ‚Äúfonctionnent bien‚Äù et √† donner de l‚Äôimportance aux exemples d‚Äôapprentissage mal class√©s pour que les futurs classifieurs faibles se concentrent dessus.</p>
<hr class="docutils" />
<p><strong>Pourquoi ces coefficients ?</strong></p>
<p>Consid√©rons tout d‚Äôabord la <em>loss</em> exponentielle :</p>
<div class="math notranslate nohighlight">
\[\ell(z)=\text{exp}\big(-z\big),\]</div>
<p>o√π <span class="math notranslate nohighlight">\(z=yh(x)\)</span>, <span class="math notranslate nohighlight">\((x, y)\in S_n\)</span>. La <em>loss</em> est affich√©e juste apr√®s. De mani√®re directe, on observe que si notre mod√®le pr√©dit un score dont le signe est le m√™me que celui du label, alors la <em>loss</em> est petite et, √† l‚Äôinverse, si le signe est diff√©rent, notre <em>loss</em> sera grande.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Exponential loss&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">&lt;=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="n">x</span><span class="p">[</span><span class="mi">51</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;0/1 loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_ensembles_40_0.png" src="../_images/1_ensembles_40_0.png" />
</div>
</div>
<p>On cherche √† construire un classifieur <span class="math notranslate nohighlight">\(H\)</span> minimisant la <em>loss</em> exponentielle suivante :</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(H)=\sum_{i=1}^n\ell(y_iH(x_i)),\]</div>
<p>o√π</p>
<div class="math notranslate nohighlight">
\[H(x)=\frac{1}{2}\sum_j\alpha_j h_j(x),\]</div>
<p>tels que <span class="math notranslate nohighlight">\(h_j\)</span> sont nos classifieurs faibles et <span class="math notranslate nohighlight">\(\alpha_j\)</span> les poids qui leur sont associ√©s. Cependant, imaginons qu‚Äôau lieu de tout minimiser d‚Äôune seule fois, nous proc√©dions it√©rativement, classifieur par classifieur. Ainsi, lorsqu‚Äôon optimise le classifieur <span class="math notranslate nohighlight">\(j\)</span>, tous les classifieurs <span class="math notranslate nohighlight">\(h_{1}, \ldots, h_{j-1}\)</span> et leur poid <span class="math notranslate nohighlight">\(\alpha_1, \ldots, \alpha_{j-1}\)</span> restent fix√©s. Notons <span class="math notranslate nohighlight">\(H_m(x)\)</span> le classifieur total jusqu‚Äôau classifieur faible <span class="math notranslate nohighlight">\(h_m\)</span>. Notre <em>loss</em> se reformule ainsi :</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_j(H)=\sum_{i=1}^n\text{exp}\Big(-y_iH_{j-1}(x_i)-\frac{1}{2}y_i\alpha_jh_j(x_i)\Big)=\sum_{i=1}^nw_i^j\text{exp}\Big(-\frac{1}{2}y_i\alpha_jh_j(x_i)\Big),\]</div>
<p>o√π <span class="math notranslate nohighlight">\(w_i^j = \text{exp}(-y_iH_{j-1}(x_i))\)</span>. Notons que si nous sommes bien entrain d‚Äôoptimiser notre loss du point de vue de <span class="math notranslate nohighlight">\(h_j\)</span> et <span class="math notranslate nohighlight">\(\alpha_j\)</span>, alors <span class="math notranslate nohighlight">\(w_i^j, \forall i\)</span> sont des constantes. Notons <span class="math notranslate nohighlight">\(S_c^{m}\)</span> l‚Äôensemble des points correctement class√©s par <span class="math notranslate nohighlight">\(H_m\)</span> et <span class="math notranslate nohighlight">\(S_i^m\)</span> ceux qui √† l‚Äôinverse ne le sont pas. Nous pouvons reformuler l‚Äôerreur pr√©c√©dente de la mani√®re suivante :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathcal{L}_j(H)&amp;=e^{-\alpha_j/2}\sum_{i\in S_\mathcal{c}^j}w_i^m+e^{\alpha_j/2}\sum_{i\in S_\mathcal{i}^j}w_i^m\\
&amp;=(e^{\alpha_j/2}-e^{-\alpha_j/2})\sum_{i=1}^nw_i^m\textbf{1}\{h_j(x_i)\neq y_i\}+e^{-\alpha_j/2}\sum_{i=1}^nw_i^m\\
&amp;\propto (e^{\alpha_j/2}-e^{-\alpha_j/2})\sum_{i=1}^nw_i^m\textbf{1}\{h_j(x_i)\neq y_i\}
\end{aligned}\end{split}\]</div>
<p>Du point de vu de l‚Äôoptimisation de <span class="math notranslate nohighlight">\(h_j\)</span>, on remarque que cela revient √† optimiser :</p>
<div class="math notranslate nohighlight">
\[(e^{\alpha_j/2}-e^{-\alpha_j/2})\sum_{i=1}^nw_i^m\textbf{1}\{h_j(x_i)\neq y_i\},\]</div>
<p>o√π le choix de <span class="math notranslate nohighlight">\(\alpha_j\)</span> n‚Äôa pas d‚Äôeffet. Cela revient √† r√©aliser l‚Äô√©tape <span class="math notranslate nohighlight">\(2\)</span> de notre algorithme. Consid√©rons maintenant l‚Äôoptimisation de <span class="math notranslate nohighlight">\(\alpha_j\)</span>. En annulant la deriv√©e en fonction de <span class="math notranslate nohighlight">\(\alpha_j\)</span> on se rend compte que cela revient √† le calculer comme indiqu√© √† l‚Äô√©tape 3 (<span class="math notranslate nohighlight">\(\epsilon_j\)</span> et <span class="math notranslate nohighlight">\(\alpha_j\)</span>).</p>
<hr class="docutils" />
<p>Dans le cas de la r√©gression, la strat√©gie va √™tre de travailler sur les r√©sidus des mod√®les pr√©c√©dents.</p>
<hr class="docutils" />
<p>Un <em>stumps</em> est une fonction seuille. C‚Äôest une fonction qui prend une variable, et consid√®re que toutes les donn√©es dont la valeur de cette variable est sup√©riere (ou inf√©rieure) √† un seuil sont associ√©es √† la classe <span class="math notranslate nohighlight">\(1\)</span> et les autres √† la classe <span class="math notranslate nohighlight">\(-1\)</span>. Il s‚Äôagit tout simplement d‚Äôun arbre de profondeur <span class="math notranslate nohighlight">\(1\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">zero_one_loss</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">AdaBoostClassifier</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Dans le code ci-dessous, compl√©tez les trois sections. La premi√®re consiste √† construire un <em>stump</em>, √† le fiter et √† calculer son taux d‚Äôerreur sur le test. La seconde consiste √† calculer un arbre de d√©cision de profondeur <span class="math notranslate nohighlight">\(9\)</span> et de nombre minimum de point par feuille √† <span class="math notranslate nohighlight">\(1\)</span>, √† le fitter et √† calculer son taux d‚Äôerreur sur le test. Enfin, la troisi√®me partie consiste √† construire un mod√®le AdaBoost o√π le classifieur faible est un <em>stump</em>.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_hastie_10_2</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">12000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">2000</span><span class="p">:],</span> <span class="n">y</span><span class="p">[</span><span class="mi">2000</span><span class="p">:]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="mi">2000</span><span class="p">],</span> <span class="n">y</span><span class="p">[:</span><span class="mi">2000</span><span class="p">]</span>

<span class="c1">####### Complete this part ######## or die ####################</span>
<span class="o">...</span> <span class="n">stump</span> <span class="n">instanciation</span>
<span class="o">...</span> <span class="n">fit</span>
<span class="n">dt_stump_err</span> <span class="o">=</span> <span class="o">...</span>
<span class="c1">###############################################################</span>

<span class="c1">####### Complete this part ######## or die ####################</span>
<span class="o">...</span> <span class="n">decision</span> <span class="n">tree</span> <span class="n">instanciation</span>
<span class="o">...</span> <span class="n">fit</span>
<span class="n">dt_err</span> <span class="o">=</span> <span class="o">...</span>
<span class="c1">###############################################################</span>


<span class="c1">####### Complete this part ######## or die ####################</span>
<span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">400</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1.</span>

<span class="o">...</span> <span class="n">AdaBoost</span> <span class="n">instanciation</span>
<span class="o">...</span> <span class="n">fit</span>
<span class="c1">###############################################################</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_estimators</span><span class="p">],</span> <span class="p">[</span><span class="n">dt_stump_err</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;k-&#39;</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Decision Stump Error&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_estimators</span><span class="p">],</span> <span class="p">[</span><span class="n">dt_err</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;k--&#39;</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Decision Tree Error&#39;</span><span class="p">)</span>

<span class="n">ada_err</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_estimators</span><span class="p">,))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">y_pred</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ada</span><span class="o">.</span><span class="n">staged_predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)):</span>
    <span class="n">ada_err</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">zero_one_loss</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="n">ada_err_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_estimators</span><span class="p">,))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">y_pred</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ada</span><span class="o">.</span><span class="n">staged_predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)):</span>
    <span class="n">ada_err_train</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">zero_one_loss</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_estimators</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">ada_err</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;AdaBoost Test Error&#39;</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_estimators</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">ada_err_train</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;AdaBoost Train Error&#39;</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">((</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Nombre d</span><span class="se">\&#39;</span><span class="s1">estimateurs&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Taux d</span><span class="se">\&#39;</span><span class="s1">erreur&#39;</span><span class="p">)</span>

<span class="n">leg</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">,</span> <span class="n">fancybox</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">leg</span><span class="o">.</span><span class="n">get_frame</span><span class="p">()</span><span class="o">.</span><span class="n">set_alpha</span><span class="p">(</span><span class="mf">0.7</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="v-les-forets-aleatoires">
<h2>V. Les for√™ts al√©atoires<a class="headerlink" href="#v-les-forets-aleatoires" title="Permalink to this headline">¬∂</a></h2>
<p>Vous avez vu diff√©rentes mani√®res d‚Äôagr√©ger plusieurs classifieurs. Vous avez √©galement √©tudi√© les arbres de d√©cision. Un arbre pris individuellement, sans limite de profondeur poss√®de une dimension VC infinie. Ou, dit autrement, il poss√®de une forte capacit√© √† sur-apprendre. Les <em>for√™ts al√©atoires</em> ou <em>random forest</em> d√©finissent une strat√©gie permettant d‚Äôagr√©ger des arbres de d√©cisions compl√©mentaires. Chacun des arbres ne peut pas avoir √©t√© appris de la m√™me mani√®re puisque l‚Äôapprentissage est d√©terministe (on aurait ainsi <span class="math notranslate nohighlight">\(M\)</span> arbres identiques et √ßa n‚Äôapporterait rien).</p>
<p>Il existe de multiples strat√©gies permettant de construire des arbres diff√©rents et nous en d√©taillons une ici. Il existe deux angles d‚Äôattaque que nous combinerons :</p>
<ul class="simple">
<li><p>Chaque arbre ne voit pas les m√™mes donn√©es,</p></li>
<li><p>Chaque arbre ne voit pas les m√™mes <em>features</em> (chaque n≈ìud voit des features choisis al√©atoirement).</p></li>
</ul>
<p>Une strat√©gie d‚Äôapprentissage est donc la suivante pour un des arbres :</p>
<ol class="simple">
<li><p>On construit un jeu de donn√©es <span class="math notranslate nohighlight">\(S^\prime\)</span> en tirant uniform√©ment des points de <span class="math notranslate nohighlight">\(S\)</span> avec remise. Une fois que <span class="math notranslate nohighlight">\(|S^\prime|=n^\prime\)</span>, on s‚Äôarr√™te,</p></li>
<li><p>En supposant que nous ayons <span class="math notranslate nohighlight">\(d\)</span> variables explicatives, on tire uniform√©ment <span class="math notranslate nohighlight">\(d^\prime\)</span> variables dans <span class="math notranslate nohighlight">\(d\)</span>,</p></li>
<li><p>On construit l‚Äôarbre avec le jeu de donn√©es <span class="math notranslate nohighlight">\(S^\prime\)</span> et les <span class="math notranslate nohighlight">\(d^\prime\)</span> variables s√©lectionn√©es (dans \texttt{sklearn} les variables sont choisies √† chaque n≈ìud).</p></li>
</ol>
<p>Cette op√©ration est r√©p√©t√©e jusqu‚Äô√† ce que tous nos arbres aient √©t√© construits. Ainsi en voyant moins de <em>features</em>, nos arbres sont plus stables et leur combinaisons ayant vu des informations diff√©rentes est elle-m√™me plus stable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">zero_one_loss</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>En r√©cup√©rant les r√©sultats d√©j√† calcul√©s dans Boosting pour <em>stump</em> et <em>decision tree</em>, compl√©tez le code suivant afin de construire et de fitter un <em>random forest</em> tel que la profondeur maximale de l‚Äôarbre soit <span class="math notranslate nohighlight">\(9\)</span>.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_hastie_10_2</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">12000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">2000</span><span class="p">:],</span> <span class="n">y</span><span class="p">[</span><span class="mi">2000</span><span class="p">:]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="mi">2000</span><span class="p">],</span> <span class="n">y</span><span class="p">[:</span><span class="mi">2000</span><span class="p">]</span>



<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1.</span>

<span class="n">max_n_estimators</span> <span class="o">=</span> <span class="mi">50</span>

<span class="n">n_estimators_list</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_n_estimators</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
<span class="p">)</span>

<span class="n">rf_err</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_estimators_list</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],))</span>
<span class="n">rf_err_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_estimators_list</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],))</span>

<span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">nb_estimators</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">n_estimators_list</span><span class="p">):</span>
    <span class="c1">####### Complete this part ######## or die ####################</span>
    <span class="o">...</span> <span class="n">random</span> <span class="n">forest</span> <span class="n">construction</span>
    <span class="o">...</span> <span class="n">fit</span>
    <span class="c1">###############################################################</span>
    <span class="n">rf_err</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">zero_one_loss</span><span class="p">(</span><span class="n">rf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="n">y_test</span><span class="p">)</span>
    <span class="n">rf_err_train</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">zero_one_loss</span><span class="p">(</span><span class="n">rf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_n_estimators</span><span class="p">],</span> <span class="p">[</span><span class="n">dt_stump_err</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;k-&#39;</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Decision Stump Error&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_n_estimators</span><span class="p">],</span> <span class="p">[</span><span class="n">dt_err</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;k--&#39;</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Decision Tree Error&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">n_estimators_list</span><span class="p">,</span> <span class="n">rf_err</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;RandomForest Test Error&#39;</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">n_estimators_list</span><span class="p">,</span> <span class="n">rf_err_train</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;RandomForest Train Error&#39;</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">((</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Nombre d</span><span class="se">\&#39;</span><span class="s1">estimateurs&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Taux d</span><span class="se">\&#39;</span><span class="s1">erreur&#39;</span><span class="p">)</span>

<span class="n">leg</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">,</span> <span class="n">fancybox</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">leg</span><span class="o">.</span><span class="n">get_frame</span><span class="p">()</span><span class="o">.</span><span class="n">set_alpha</span><span class="p">(</span><span class="mf">0.7</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./5_ensembles"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="0_propos_liminaire.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Les m√©thodes <em>ensemblistes</em></p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="2_bayesian_linear_regression.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Bayesian linear regression ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Servajean, Leveau & Chailan<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>
{"cells": [{"cell_type": "markdown", "id": "popular-password", "metadata": {}, "source": ["# Bayesian linear regression \u2615\ufe0f\u2615\ufe0f\u2615\ufe0f\u2615\ufe0f\n", "\n", "**<span style='color:blue'> Objectifs de la s\u00e9quence</span>** ", "\n", "* \u00catre sensibilis\u00e9 aux liens entre approche Bay\u00e9sienne et&nbsp;:\n", "    * r\u00e9gularisation,\n", "    * ensemble de mod\u00e8les.\n", "* \u00catre capable&nbsp;:\n", "    * d'utiliser la r\u00e9gression lin\u00e9aire Bayesienne dans $\\texttt{sklearn}$.\n", "    \n", "\n\n ----", "\n", "\n", "Vous trouverez un approfondissement non n\u00e9cessaire \u00e0 cette s\u00e9quence et traitant de l'inf\u00e9rence Bay\u00e9sienne \u00e0 l'adresse&nbsp;:\n", "\n", "* [Inf\u00e9rence Bay\u00e9sienne](https://github.com/maximiliense/lmiprp/raw/main/Cours/Inf\u00e9rence%20Bay\u00e9sienne/cours.pdf)"]}, {"cell_type": "markdown", "id": "functioning-oxide", "metadata": {}, "source": ["## I. Posterior pr\u00e9dictive et mod\u00e8le\n", "\n", "Comme nous avons pu le voir, le principe du *Bayesian Model Averaging* se g\u00e9n\u00e9ralise bien s\u00fbr lorsqu'on a une quantit\u00e9 infinie de mod\u00e8les o\u00f9 la *posterior* pr\u00e9dictive s'\u00e9crit :\n", "\n", "$$p(y|x_\\text{new}, S_n)=\\int p(y|x_\\text{new}, \\mathcal{M}, S_n)p(\\mathcal{M}|S_n)dM.$$\n", "\n", "Nous pouvons formaliser une version Bay\u00e9sienne de la r\u00e9gression lin\u00e9aire en suivant ce principe.\n", "\n", "### A. Le mod\u00e8le \n", "\n", "Consid\u00e9rons le mod\u00e8le lin\u00e9aire suivant. Soit $X\\in\\mathbb{R}^d$, $\\omega\\in\\mathbb{R}^d$, $\\epsilon\\sim\\mathcal{N}(0, \\sigma^2)$ et consid\u00e9rons la d\u00e9pendence suivante :\n", "\n", "$$Y=\\langle \\omega, X\\rangle + \\epsilon.$$\n", "\n", "On supposera dans cet exercice que les matrices qu'on inverse sont inversibles.\n", "\n", "Nous avons donc $Y\\sim\\mathcal{N}(\\langle\\omega, X\\rangle, \\sigma^2)$. Ici, chaque param\u00e9trisation (chaque choix de $\\omega$) peut \u00eatre vu comme un mod\u00e8le. Comme pour le cas pr\u00e9c\u00e9dent, nous devons donner une mesure *a priori* pour chacun de nos mod\u00e8les. Nous choissons ici le *prior* suivant :\n", "\n", "$$p(\\omega)=\\mathcal{N}(0, \\sigma^2\\Lambda^{-1}),$$\n", "\n", "o\u00f9 $\\Lambda$ est la matrice de pr\u00e9cision. Par facilit\u00e9, consid\u00e9rons $\\Lambda=\\lambda I$ et notons $\\Sigma=\\sigma^2\\Lambda^{-1}$.  \n", "\n", "## II. D\u00e9tails des calculs\n", "\n", "(Si vous n'\u00eates int\u00e9ress\u00e9s que par la solution, vous pouvez sauter directement les calculs)\n", "\n", "### 1. Calcul de la posterior sur $\\omega$\n", "\n", "Nous cherchons dans un premier temps \u00e0 calculer :\n", "\n", "$$p(y|\\tilde{X}, \\omega)\\propto\\text{exp}\\Big(-\\frac{1}{2\\sigma^2}(y-\\tilde{X}\\omega )^T(y-\\tilde{X}\\omega)\\Big)$$\n", "\n", "o\u00f9\n", "\n", "$$\\tilde{X}=\\begin{pmatrix}x_1^T\\\\\\vdots\\\\x_n^T\\end{pmatrix}\\text{ et }y=\\begin{pmatrix}y_1\\\\\\vdots\\\\y_n\\end{pmatrix}.$$\n", "\n", "C'est notre likelihood. Si en maximisant cette quantit\u00e9 qu'on obtient (lorsque $\\tilde{X}^T\\tilde{X}$ est inversible) :\n", "\n", "$$\\hat{\\omega}=(\\tilde{X}^T\\tilde{X})^{-1}\\tilde{X}^ty$$\n", "\n", "On rappelle notre prior sur $\\omega$ :\n", "\n", "$$p(\\omega)\\propto\\text{exp}\\Big(-\\frac{1}{2}\\omega^T\\Sigma^{-1}\\omega\\Big)=\\text{exp}\\Big(-\\frac{\\lambda}{2\\sigma^2}\\omega^T\\omega\\Big).$$\n", "\n", "Nous avons donc tous les \u00e9l\u00e9ments nous permettant de calculer notre *posterior* :\n", "\n", "$$p(\\omega|\\tilde{X}, y)\\propto p(y|\\tilde{X},\\omega)p(\\omega).$$\n", "\n", "En rempla\u00e7ant les quantit\u00e9s ci-dessus, nous obtenons donc :\n", "\n", "$$p(\\omega|\\tilde{X}, y)\\propto \\text{exp}\\Big(-\\frac{1}{2\\sigma^2}(y-\\tilde{X}\\omega )^T(y-\\tilde{X}\\omega)-\\frac{\\lambda}{2\\sigma^2}\\omega^T\\omega\\Big).$$\n", "\n", "Notre prior sur $\\omega$ est conjugu\u00e9 avec notre vraisemblance. Cela veut dire que la *posterior* est donc n\u00e9cessairement une loi normale \u00e9galement. Ainsi, on sait qu'elle sera proportionnelle \u00e0 l'exponentielle de :\n", "\n", "$$(\\omega-\\mu)^T{\\Sigma^\\prime}^{-1}(\\omega-\\mu)=\\omega^T{\\Sigma^\\prime}^{-1}\\omega-2\\omega^T{\\Sigma^\\prime}^{-1}\\mu+\\text{constante}.$$\n", "\n", "o\u00f9 $\\mu$ sera le param\u00e8tre de moyenne et $\\Sigma^\\prime$ notre nouvelle matrice de covariance. Consid\u00e9rons donc la quantit\u00e9 $\\frac{1}{\\sigma^2}(y-\\tilde{X}\\omega )^T(y-\\tilde{X}\\omega)+\\frac{\\lambda}{\\sigma^2}\\omega^T\\omega$ (nous avons factoris\u00e9 par $-1/2$ les \u00e9l\u00e9ments de l'exponentiel) et cherchons \u00e0 obtenir la forme classique attendue d'une loi normale :\n", "\n", "$$\\begin{aligned}\n", "\\frac{1}{\\sigma^2}(y-\\tilde{X}\\omega )^T(y-\\tilde{X}\\omega)+\\frac{\\lambda}{\\sigma^2}\\omega^T\\omega&=\\sigma^{-2}(y^Ty-2\\omega^T\\tilde{X}^T y+\\omega^T\\tilde{X}^T\\tilde{X}\\omega)+\\lambda\\sigma^{-2}\\omega^T\\omega\\\\\n", "&=\\sigma^{-2}y^Ty-2\\sigma^{-2}\\omega^T\\tilde{X}^Ty+\\sigma^{-2}\\omega^T(\\tilde{X}^T\\tilde{X}+\\lambda I)\\omega\n", "\\end{aligned}$$"]}, {"cell_type": "markdown", "id": "contrary-clarity", "metadata": {}, "source": ["On remarque ainsi directement que :\n", "\n", "$$\\begin{aligned}\n", "{\\Sigma^\\prime}^{-1}&=\\frac{\\tilde{X}^T\\tilde{X}+\\lambda I}{\\sigma^2}\n", "\\end{aligned}$$\n", "\n", "Et on en d\u00e9duit que :\n", "\n", "$$\\mu=(\\tilde{X}^T\\tilde{X}+\\lambda I)^{-1}\\tilde{X}^T y.$$\n", "\n", "On remarque que l'esp\u00e9rance de notre posterior est exactement l'estimateur de Ridge. Dans notre cas de figure, la posterior est une loi normale et est ainsi sym\u00e9trique : son maximum est atteint par son esp\u00e9rance. On parle souvent de MAP, ou Maximum A Posteriori."]}, {"cell_type": "markdown", "id": "swedish-shadow", "metadata": {}, "source": ["### 2. Calcul de la posterior pr\u00e9dictive sur $y_\\text{new}$\n", "\n", "Maintenant que nous avons la posterior pour nos mod\u00e8les, nous pouvons calculer la *posterior* pr\u00e9dictive. Celle-ci s'exprime de la mani\u00e8re suivante :\n", "\n", "$$\\begin{aligned}\n", "p(y_{\\text{new}}| x_{\\text{new}}, \\tilde{X}, y)&=\\int_\\omega p(y_{\\text{new}}|x_{\\text{new}}, \\omega)p(\\omega|\\tilde{X}, y)d\\omega\\text{ (la d\u00e9pendance en $X$ disparait)}\\\\\n", "&\\propto\\int_\\omega\\text{exp}\\Big(-\\frac{1}{2\\sigma^2}(y_{\\text{new}}-\\langle \\omega, x_{\\text{new}}\\rangle)^2\\Big)\\text{exp}\\Big(-\\frac{1}{2}(\\omega-\\mu)^T{\\Sigma^\\prime}^{-1}(\\omega-\\mu)\\Big)d\\omega\\\\\n", "&=\\int_\\omega\\text{exp}\\Big(-\\frac{1}{2}\\big(\\sigma^{-2}(y_{\\text{new}}^2-2y_{\\text{new}}\\langle \\omega, x_{\\text{new}}\\rangle+(\\langle \\omega, x_{\\text{new}}\\rangle)^2+\\omega^T{\\Sigma^\\prime}^{-1}\\omega-2\\omega^T{\\Sigma^\\prime}^{-1}\\mu+\\\\&\\mu^T{\\Sigma^\\prime}^{-1}\\mu\\big)\\Big)d\\omega\n", "\\end{aligned}$$\n", "\n", "o\u00f9 $\\mu^T{\\Sigma^\\prime}^{-1}\\mu$ n'agit que comme un facteur de proportionnalit\u00e9 et peut donc \u00eatre \"\u00e9limin\u00e9\".\n", "\n", "\n", "#### \u00c9tape 2.1 : Int\u00e9gration de la variable $\\omega$\n", "\n", "Nous allons dans un premier temps regrouper tous les \u00e9l\u00e9ments faisant intervenir $\\omega$, notre variable d'int\u00e9gration.\n", "\n", "\n", "$$\\begin{aligned}\n", "p(y_{\\text{new}}| x_{\\text{new}}, \\tilde{X}, y)&\\propto\\int_\\omega\\text{exp}\\Big(-\\frac{1}{2}\\big(\\sigma^{-2}(y_{\\text{new}}^2-2y_{\\text{new}}\\langle \\omega, x_{\\text{new}}\\rangle+(\\langle \\omega, x_{\\text{new}}\\rangle)^2+\\omega^T{\\Sigma^\\prime}^{-1}\\omega-2\\omega^T{\\Sigma^\\prime}^{-1}\\mu\\big)\\Big)d\\omega\n", "\\end{aligned}$$\n", "\n", "\n", "Consid\u00e9rons \u00e0 nouveau la partie entre parenth\u00e8ses :\n", "\n", "$$\\begin{aligned}\n", "\\sigma^{-2}(y_{\\text{new}}^2-2y_{\\text{new}}&\\langle \\omega, x_{\\text{new}}\\rangle+(\\langle \\omega, x_{\\text{new}}\\rangle)^2+\\omega^T{\\Sigma^\\prime}^{-1}\\omega-2\\omega^T{\\Sigma^\\prime}^{-1}\\mu\\\\\n", "&=y_\\text{new}^2\\sigma^{-2}-2\\omega^T({\\Sigma^\\prime}^{-1}\\mu+x_\\text{new}y_\\text{new}\\sigma^{-2})+\\omega^T(x_\\text{new}x_\\text{new}^T\\sigma^{-2}+{\\Sigma^\\prime}^{-1})\\omega\\\\\n", "&=(\\omega-\\mu^\\prime){\\Sigma^{\\prime\\prime}}^{-1}(\\omega-\\mu^\\prime)-{\\mu^\\prime}^T{\\Sigma^{\\prime\\prime}}^{-1}\\mu^\\prime+y_{\\text{new}}^2\\sigma^{-2}\n", "\\end{aligned}$$\n", "\n", "\n", "o\u00f9 :\n", "\n", "$${\\Sigma^{\\prime\\prime}}^{-1}=x_\\text{new}x_\\text{new}^T\\sigma^{-2}+{\\Sigma^\\prime}^{-1},$$\n", "\n", "et :\n", "\n", "$$\\mu^\\prime={\\Sigma^{\\prime\\prime}}({\\Sigma^\\prime}^{-1}\\mu+x_{\\text{new}}y_\\text{new}\\sigma^{-2}).$$\n", "\n", "On en d\u00e9duit ainsi :\n", "\n", "$$\\begin{aligned}\n", "p(y_{\\text{new}}| x_{\\text{new}}, \\tilde{X}, y)&\\propto\\text{exp}\\Big(\\frac{1}{2}{\\mu^\\prime}^T{\\Sigma^{\\prime\\prime}}^{-1}\\mu^\\prime-\\frac{1}{2}y_{\\text{new}}^2\\sigma^{-2}\\Big)\\int_\\omega \\text{exp}\\Big(-\\frac{1}{2}(\\omega-\\mu^\\prime){\\Sigma^{\\prime\\prime}}^{-1}(\\omega-\\mu^\\prime)\\Big)d\\omega\\\\\n", "&\\propto\\text{exp}\\Big(\\frac{1}{2}{\\mu^\\prime}^T{\\Sigma^{\\prime\\prime}}^{-1}\\mu^\\prime-\\frac{1}{2}y_{\\text{new}}^2\\sigma^{-2}\\Big),\n", "\\end{aligned}$$\n", "\n", "o\u00f9 nous avons pu sortir les \u00e9l\u00e9ments constants relativement \u00e0 $\\omega$ de l'int\u00e9gral. N'y trouvant plus qu'une loi normale dont la probabilit\u00e9 vaut $1$ sur son domaine, nous avons pu \u00e9liminer l'int\u00e9grale (toujours \u00e0 un facteur proportionnel pr\u00e8s)."]}, {"cell_type": "markdown", "id": "solar-lawrence", "metadata": {}, "source": ["#### \u00c9tape 2.2 : Calcul de la loi de $y_\\text{new}$\n", "\n", "On sent que la loi de $y_\\text{new}$ sera aussi une loi normale, et on va essayer de retrouver quelque chose qui ressemble \u00e0 :\n", "On sait qu'on aura quelque chose de la forme :\n", "\n", "$$\\begin{aligned}\n", "p(y_\\text{new}|x_\\text{new}, \\tilde{X}, y)&\\propto\\text{exp}\\Big(-\\frac{1}{2{\\sigma^\\prime}^2}(y_\\text{new}-\\mu^{\\prime\\prime)^2}\\Big)\\\\\n", "&=\\text{exp}\\Big(-\\frac{1}{2}({\\sigma^\\prime}^{-2}y_\\text{new}^2-2{\\sigma^\\prime}^{-2}y_\\text{new}\\mu^{\\prime\\prime}+{\\sigma^\\prime}^{-2}{\\mu^{\\prime\\prime}}^2)\\Big),\n", "\\end{aligned}\n", "$$\n", "\n", "o\u00f9 $\\mu^{\\prime\\prime}$ sera l'esp\u00e9rance de notre pr\u00e9diction et $\\sigma^\\prime$ son \u00e9cart-type. Rappelons que notre point de d\u00e9part ici est : \n", "\n", "$$\\text{exp}\\Big(\\frac{1}{2}{\\mu^\\prime}^T{\\Sigma^{\\prime\\prime}}^{-1}\\mu^\\prime-\\frac{1}{2}y_{\\text{new}}^2\\sigma^{-2}\\Big).$$\n", "\n", "En constatant que $\\Sigma, \\Sigma^\\prime$ ou encore $\\Sigma^{\\prime\\prime}$ sont toutes sym\u00e9triques, nous avons :\n", "\n", "$$\\begin{aligned}\n", "{\\mu^\\prime}^T{\\Sigma^{\\prime\\prime}}^{-1}\\mu^\\prime&=({\\Sigma^\\prime}^{-1}\\mu+x_{\\text{new}}y_\\text{new}\\sigma^{-2})^T{\\Sigma^{\\prime\\prime}}{\\Sigma^{\\prime\\prime}}^{-1}{\\Sigma^{\\prime\\prime}}({\\Sigma^\\prime}^{-1}\\mu+x_{\\text{new}}y_\\text{new}\\sigma^{-2})\\\\\n", "&=\\sigma^{-2}y_\\text{new}x_\\text{new}^T\\Sigma^{\\prime\\prime}\\sigma^{-2}y_\\text{new}x_\\text{new}+2y_\\text{new}\\sigma^{-2}x_\\text{new}^T\\Sigma^{\\prime\\prime}{\\Sigma^\\prime}^{-1}\\mu+\\text{constante}\\\\\n", "&=(\\sigma^{-4}x_\\text{new}^T\\Sigma^{\\prime\\prime}x_\\text{new})y_\\text{new}^2+2(\\sigma^{-2}x_\\text{new}^T\\Sigma^{\\prime\\prime}{\\Sigma^\\prime}^{-1}\\mu) y_\\text{new}+\\text{constante}.\n", "\\end{aligned}$$\n", "\n", "En combinant ${\\mu^\\prime}^T{\\Sigma^{\\prime\\prime}}^{-1}\\mu^\\prime$ avec $-y_{\\text{new}}^2\\sigma^{-2}$, nous obtenons :\n", "\n", "$$\\begin{aligned}\n", "{\\mu^\\prime}^T{\\Sigma^{\\prime\\prime}}^{-1}\\mu^\\prime-y_{\\text{new}}^2\\sigma^{-2}&=(\\sigma^{-4}x_\\text{new}^T\\Sigma^{\\prime\\prime}x_\\text{new}-\\sigma^{-2})y_\\text{new}^2+2(\\sigma^{-2}x_\\text{new}^T\\Sigma^{\\prime\\prime}{\\Sigma^\\prime}^{-1}\\mu) y_\\text{new}+\\text{c}\n", "\\end{aligned}$$\n", "\n", "Remarquons que dans l'exponentielle nous n'avons pas indiqu\u00e9 le signe \"-\" classique de la loi normale. Nous obtenons ainsi :\n", "\n", "$$\\begin{aligned}\n", "y_{\\text{new}}^2\\sigma^{-2}-{\\mu^\\prime}^T{\\Sigma^{\\prime\\prime}}^{-1}\\mu^\\prime&=(\\sigma^{-2}-\\sigma^{-4}x_\\text{new}\\Sigma^{\\prime\\prime}x_\\text{new})y_\\text{new}^2-2(\\sigma^{-2}x_\\text{new}\\Sigma^{\\prime\\prime}{\\Sigma^\\prime}^{-1}\\mu)y_\\text{new}+c\n", "\\end{aligned}$$\n", "\n", "On retrouve donc :\n", "\n", "$$\\begin{aligned}\n", "{\\sigma^\\prime}^{-2}=\\sigma^{-2}(1-\\sigma^{-2}x_\\text{new}^T\\Sigma^{\\prime\\prime}x_\\text{new})\\\\\n", "\\mu^{\\prime\\prime}={\\sigma^\\prime}^2\\sigma^{-2}x_\\text{new}^T\\Sigma^{\\prime\\prime}{\\Sigma^\\prime}^{-1}\\mu.\n", "\\end{aligned}$$\n", "\n", "Nous avons :\n", "\n", "$$\\begin{aligned}\n", "p(y_\\text{new}|x_\\text{new}, \\tilde{X}, y)&\\propto\\text{exp}\\Big(-\\frac{{\\sigma^\\prime}^{-2}}{2}(y_\\text{new}-\\mu^{\\prime\\prime)^2}\\Big)\n", "\\end{aligned}$$\n", "\n", "\n"]}, {"cell_type": "markdown", "id": "pursuant-current", "metadata": {}, "source": ["#### \u00c9tape 2.3 : Un peu de nettoyage\n", "\n", "Nous avons gr\u00e2ce \u00e0 la formule de [Sherman\u2013Morrison](https://en.wikipedia.org/wiki/Sherman\u2013Morrison_formula) :\n", "\n", "$$\\Sigma^{\\prime\\prime}=(x_\\text{new}x_\\text{new}^T\\sigma^{-2}+{\\Sigma^\\prime}^{-1})^{-1}={\\Sigma^\\prime}-\\frac{{\\Sigma^\\prime}\\sigma^{-2}x_\\text{new}x_\\text{new}^T{\\Sigma^\\prime}}{1+\\sigma^{-2}x_\\text{new}^T{\\Sigma^\\prime}x_\\text{new}}$$\n", "\n", "Nous avons donc :\n", "\n", "$$x_\\text{new}^T\\Sigma^{\\prime\\prime}x_{\\text{new}}=x_\\text{new}^T\\Sigma^\\prime x_\\text{new}-\\frac{\\sigma^{-2}(x_\\text{new}^T\\Sigma^\\prime x_\\text{new})^2}{1+\\sigma^{-2}x_\\text{new}^T{\\Sigma^\\prime}x_\\text{new}}=\\frac{x_\\text{new}^T\\Sigma^\\prime x_\\text{new}}{1+\\sigma^{-2}x_\\text{new}^T\\Sigma^\\prime x_\\text{new}}.$$\n", "\n", "Reprenons $\\sigma^\\prime$. Nous avons enfin :\n", "\n", "$${\\sigma^\\prime}^{-2}=\\sigma^{-2}(1-\\sigma^{-2}\\frac{x_\\text{new}^T\\Sigma^\\prime x_\\text{new}}{1+\\sigma^{-2}x_\\text{new}^T\\Sigma^\\prime x_\\text{new}})=\\frac{\\sigma^{-2}}{1+\\sigma^{-2}x_\\text{new}^T\\Sigma^\\prime x_\\text{new}}$$\n", "\n", "Consid\u00e9rons maintenant $\\mu^{\\prime\\prime}$ :\n", "\n", "$$\\mu^{\\prime\\prime}={\\sigma^\\prime}^2\\sigma^{-2}x_\\text{new}^T\\Sigma^{\\prime\\prime}{\\Sigma^\\prime}^{-1}\\mu=\\mu^T{\\sigma^\\prime}^2\\sigma^{-2}\\Sigma^{\\prime\\prime}{\\Sigma^\\prime}^{-1}x_\\text{new}.$$\n", "\n", "Traitons la partie \u00e0 droite et montrons :\n", "\n", "$$\\begin{aligned}\n", "{\\sigma^\\prime}^2\\sigma^{-2}\\Sigma^{\\prime\\prime}{\\Sigma^\\prime}^{-1}x_\\text{new}&=x_\\text{new}\\\\\n", "\\Leftrightarrow {\\sigma^\\prime}^2\\sigma^{-2} x_\\text{new}&={\\Sigma^{\\prime\\prime}}^{-1}\\Sigma^\\prime x_\\text{new}\\\\&\n", "=(x_\\text{new}x_\\text{new}^T\\sigma^{-2}+{\\Sigma^\\prime}^{-1})\\Sigma^\\prime x_\\text{new}\\\\\n", "&=\\sigma^{-2}x_\\text{new}x_\\text{new}^T\\Sigma^\\prime x_\\text{new}+x_\\text{new}\\\\\n", "&= (\\sigma^{-2}x_\\text{new}^T\\Sigma^\\prime x_\\text{new} + 1)x_\\text{new}\\\\\n", "\\Leftrightarrow {\\sigma^\\prime}^2x_\\text{new}&=\\frac{\\sigma^{-2}x_\\text{new}^T\\Sigma^\\prime x_\\text{new} + 1}{\\sigma^{-2}}x_\\text{new}\\\\\n", "\\Leftrightarrow x_\\text{new}&=x_\\text{new}.\n", "\\end{aligned}$$\n", "\n", "Et nous obtenons ce que nous voulions montrer. Cela nous indique que nous avons : \n", "\n", "$$\\mu^{\\prime\\prime}=\\mu^T x_\\text{new},$$\n", "\n", "o\u00f9 \n", "\n", "$$\\mu=(\\tilde{X}^T\\tilde{X}+\\lambda I)^{-1}\\tilde{X}^T y.$$"]}, {"cell_type": "markdown", "id": "excellent-african", "metadata": {}, "source": ["### 3. Conclusion\n", "\n", "Nous obtenons donc, \u00e9tant donn\u00e9 une observation $x_\\text{new}$ que son label $y_\\text{new}$ est distribu\u00e9 selon une loi normale dont la moyenne est :\n", "\n", "$$\\mu^T x_\\text{new}$$\n", "\n", "avec :\n", "\n", "$$\\mu=(\\tilde{X}^T\\tilde{X}+\\lambda I)^{-1}\\tilde{X}^T y,$$\n", "\n", "et dont la variance est donn\u00e9e par :\n", "\n", "$${\\sigma^\\prime}^{-2}=\\frac{\\sigma^{-2}}{1+\\sigma^{-2}x_\\text{new}^T\\Sigma^\\prime x_\\text{new}}$$\n", "\n", "o\u00f9 \n", "\n", "$${\\Sigma^\\prime}^{-1}=\\frac{\\tilde{X}^T\\tilde{X}+\\lambda I}{\\sigma^2}.$$\n", "\n", "Cela nous dit finalement qu'une pr\u00e9diction d'un mod\u00e8lre Ridge revient \u00e0 prendre l'esp\u00e9rance d'un Bayesian Model Averaging o\u00f9 nos mod\u00e8les sont les mod\u00e8les lin\u00e9aires auquel on ajoute comme *prior* une loi normale dont la variance est contr\u00f4l\u00e9e par le param\u00e8tre $1/\\lambda$.\n", "\n", "On retrouve ce mod\u00e8le dans $\\texttt{sklearn}$ avec $\\texttt{BayesianRidge}$.\n", "\n", "---"]}, {"cell_type": "markdown", "id": "intended-rainbow", "metadata": {}, "source": ["## III. Pratique"]}, {"cell_type": "markdown", "id": "b68a6292", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "\n", "**Dans le code ci-dessous, jouez avec le *prior* de la r\u00e9gression lin\u00e9aire Bay\u00e9sienne pour en voir les effets.**\n", "\n", "\n\n ----"]}, {"cell_type": "code", "execution_count": null, "id": "suspected-alexandria", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "from sklearn.linear_model import BayesianRidge\n", "\n", "\n", "def func(x): return np.sin(2*np.pi*x)\n", "\n", "\n", "# ############\n", "# Generate sinusoidal data with noise\n", "size = 25\n", "rng = np.random.RandomState(1234)\n", "x_train = rng.uniform(0., 1., size)\n", "y_train = func(x_train) + rng.normal(scale=0.1, size=size)\n", "x_test = np.linspace(0., 1., 100)\n", "\n", "\n", "# ############\n", "# Fit by cubic polynomial\n", "n_order = 3\n", "X_train = np.vander(x_train, n_order + 1, increasing=True)\n", "X_test = np.vander(x_test, n_order + 1, increasing=True)\n", "\n", "# ############\n", "# Plot the true and predicted curves with log marginal likelihood (L)\n", "reg = BayesianRidge(tol=1e-6, fit_intercept=False, compute_score=True)\n", "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n", "for i, ax in enumerate(axes):\n", "    # Bayesian ridge regression with different initial value pairs\n", "    if i == 0:\n", "        init = [1 / np.var(y_train), 1.]  # Default values\n", "    elif i == 1:\n", "        ####### Complete this part ######## or die ####################\n", "        init = [1., 1e-3]\n", "        reg.set_params(alpha_init=init[0], lambda_init=init[1])\n", "        ###############################################################\n", "    reg.fit(X_train, y_train)\n", "    ymean, ystd = reg.predict(X_test, return_std=True)\n", "\n", "    ax.plot(x_test, func(x_test), color=\"blue\", label=\"sin($2\\\\pi x$)\")\n", "    ax.scatter(x_train, y_train, s=50, alpha=0.5, label=\"observation\")\n", "    ax.plot(x_test, ymean, color=\"red\", label=\"predict mean\")\n", "    ax.fill_between(x_test, ymean-ystd, ymean+ystd,\n", "                    color=\"pink\", alpha=0.5, label=\"predict std\")\n", "    ax.set_ylim(-1.3, 1.3)\n", "    ax.legend()\n", "    title = \"$\\\\alpha$_init$={:.2f},\\\\ \\\\lambda$_init$={}$\".format(\n", "            init[0], init[1])\n", "    if i == 0:\n", "        title += \" (Default)\"\n", "    ax.set_title(title, fontsize=12)\n", "    text = \"$\\\\alpha={:.1f}$\\n$\\\\lambda={:.3f}$\\n$L={:.1f}$\".format(\n", "           reg.alpha_, reg.lambda_, reg.scores_[-1])\n", "    ax.text(0.05, -1.0, text, fontsize=12)\n", "\n", "plt.tight_layout()\n", "plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.2"}}, "nbformat": 4, "nbformat_minor": 5}
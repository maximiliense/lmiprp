
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Machine learning et malédiction de la dimension &#8212; Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Les arbres de régression et de classification" href="2_regression_and_classification_trees.html" />
    <link rel="prev" title="Machine Learning, initiation" href="0_propos_liminaire.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-72WWYCKNK6"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-72WWYCKNK6');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../0_requisite/rappels_python.html">
   Rappels de Python et Numpy
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="0_propos_liminaire.html">
   <em>
    Machine Learning
   </em>
   , initiation
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     <em>
      Machine learning
     </em>
     et malédiction de la dimension
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="2_regression_and_classification_trees.html">
     Les arbres de régression et de classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../2_regression/0_propos_liminaire.html">
   La
   <em>
    régression
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/1_linear_regression.html">
     La régression linéaire ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/2_optimization.html">
     L’optimisation ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/3_interpolation.html">
     Interpolation ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/4_algo_proximal_lasso.html">
     Sous-différentiel et le cas du Lasso ☕️☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/5_least_square_qr.html">
     Les moindres carrés via une décomposition QR (et plus)☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/6_ridge.html">
     Une analyse de la régularisation Ridge ☕️☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../3_classification/0_propos_liminaire.html">
   La
   <em>
    classification
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/1_logistic_regression.html">
     La régression logistique ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/2_fonctions_proxy.html">
     Les fonctions de perte (loss function) ☕️☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/3_bayes_classifier.html">
     Le classifieur de Bayes ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/4_VC_theory.html">
     Un modèle formel de l’apprentissage ☕️☕️☕️☕️ (💆‍♂️)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../4_kernel_methods/0_propos_liminaire.html">
   Les méthodes à noyaux
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../4_kernel_methods/1_svm.html">
     Le SVM ou l’hypothèse max-margin ☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5_ensembles/0_propos_liminaire.html">
   Les méthodes
   <em>
    ensemblistes
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/1_ensembles.html">
     Méthodes ensemblistes ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/2_bayesian_linear_regression.html">
     Bayesian linear regression ☕️☕️☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../6_deeplearning/0_propos_liminaire.html">
   <em>
    deep learning
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/1_autodiff.html">
     La différentiation automatique et un début de
     <em>
      deep learning
     </em>
     ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/2_filters_representation.html">
     Filtres et espace de représentation des réseaux de neurones ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/3_probabilities_calibration.html">
     Calibration des probabilités et quelques notions ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/4_regularization_deep.html">
     Régularisation en
     <em>
      deep learning
     </em>
     ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/5_transfer_multitask.html">
     <em>
      Transfer learning
     </em>
     et apprentissage multi-tâches ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/6_adversarial.html">
     Les attaques adversaires ☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../7_unsupervised/0_propos_liminaire.html">
   L’apprentissage non-supervisé
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_unsupervised/1_principal_component_analysis.html">
     L’Analyse en Composantes Principales ☕️☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_unsupervised/2_em_gaussin_mixture_model.html">
     Modèle de Mélange Gaussien et algorithme
     <em>
      Expectation-Maximization
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../8_set_prediction/0_propos_liminaire.html">
   Prédiction d’ensembles
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/1_well_defined.html">
     Jeu d’apprentissage
     <em>
      set-valued
     </em>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/2_ill_defined.html">
     Jeu d’apprentissage uniquement multi-classes ☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../biblio.html">
   Bibliographie
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/1_what_is_ml/1_introduction_ml.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/maximiliense/lmiprp"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/maximiliense/lmiprp/issues/new?title=Issue%20on%20page%20%2F1_what_is_ml/1_introduction_ml.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/maximiliense/lmiprp/blob/master/Travaux Pratiques/Machine Learning/Book/1_what_is_ml/1_introduction_ml.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#i-introduction">
   I. Introduction
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-l-apprentissage-supervise">
     A. L’apprentissage supervisé
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-l-apprentissage-non-supervise">
     B. L’apprentissage non-supervisé
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ii-on-casse-une-idee-preconcue-as">
   II. On casse une idée préconçue (AS)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iii-une-autre-premiere-approche-logique-le-knn-as">
   III. Une autre première approche logique : le KNN (AS)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iv-les-arbres-de-decision-ou-classification-and-regression-tree-cart-as">
   IV. Les arbres de décision ou Classification and Regression Tree (CART) (AS)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#v-les-forets-aleatoires-ou-random-forest-rf-as">
   V. Les forêts aléatoires ou Random Forest (RF) (AS)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vi-choix-des-hyperparametres-as">
   VI. Choix des hyperparamètres (AS)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vii-l-algorithme-des-k-moyennes-ans">
   VII. L’algorithme des K-Moyennes (ANS)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#viii-la-malediction-de-la-dimension">
   VIII. La malédiction de la dimension
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#en-details">
     En détails
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#x-quelques-modeles-proposes-par-texttt-sklearn">
   X. Quelques modèles proposés par
   <span class="math notranslate nohighlight">
    \(\texttt{sklearn}\)
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#xi-le-no-free-lunch-theorem">
   XI. Le
   <em>
    no-free-lunch
   </em>
   Theorem
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="machine-learning-et-malediction-de-la-dimension">
<h1><em>Machine learning</em> et malédiction de la dimension<a class="headerlink" href="#machine-learning-et-malediction-de-la-dimension" title="Permalink to this headline">¶</a></h1>
<div class="admonition-objectifs-de-la-sequence admonition">
<p class="admonition-title">Objectifs de la séquence</p>
<ul class="simple">
<li><p>Être capable de différencier l’apprentissage supervisé et non supervisé.</p></li>
<li><p>Être initié à :</p>
<ul>
<li><p>Construire un modèle à partir d’un jeu de données,</p></li>
<li><p>Évaluer ce modèle,</p></li>
<li><p>Le sur-apprentissage,</p></li>
<li><p>La malédiction de la dimension.</p></li>
</ul>
</li>
<li><p>Manipuler :</p>
<ul>
<li><p>Quelques modèles standards de la librairie <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span>.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="i-introduction">
<h2>I. Introduction<a class="headerlink" href="#i-introduction" title="Permalink to this headline">¶</a></h2>
<p>Imaginons que nous souhaitions construire une application qui prendrait en entrée une image de chien ou de chat et qui doive prédire laquelle des deux espèces est représentée. Imaginons encore une application qui prendrait en entrée un mail qu’elle classifierait comme SPAM ou NONSPAM. Supposons qu’il existe deux catégories de clients qu’on ne connait pas <em>a priori</em> et que l’entreprise souhaite prédire pour chacun des clients sa catégorie. On peut vouloir prédire la température qu’il fera demain à partir de données relevées aujourd’hui.</p>
<p>Une constante est partagée par l’ensemble de ces scénarios. Il y a tout d’abord une donnée d’entrée plus ou moins complexe et structurée. On notera <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> l’espace auquel elle appartient. Ensuite, à partir de cette donnée, l’objectif est de faire une prédiction. Notons <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> l’espace auquel appartient cette prédiction. On appelle ça aussi nos labels ou nos variables à expliquer. Notre objectif, en tant que <em>machine learner</em> est de construire une fonction <span class="math notranslate nohighlight">\(h:\mathcal{X}\mapsto\mathcal{Y}\)</span> qui aura de <em>bonnes performances</em> “en production”, c’est-à-dire sur des données nouvelles que nous n’avons jamais vues (i.e. on ne veut pas prédire la météo d’hier à partir d’avant hier, mais bien de demain à partir d’aujourd’hui).</p>
<p>Deux types d’apprentissage sont généralement opposés : l’apprentissage supervisé (AS) et non-supervisé (ANS).</p>
<div class="section" id="a-l-apprentissage-supervise">
<h3>A. L’apprentissage supervisé<a class="headerlink" href="#a-l-apprentissage-supervise" title="Permalink to this headline">¶</a></h3>
<p>L’apprentissage supervisé part du principe que (1) nos labels (i.e. l’espace <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>) est bien défini et (2) que nous avons accès à des données associant des éléments de <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> à leur label <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>.</p>
<p>On parlera de problème de régression si par exemple <span class="math notranslate nohighlight">\(\mathcal{Y}\subseteq\mathbb{R}\)</span> ou de problème de classification si <span class="math notranslate nohighlight">\(\mathcal{Y}=\{1, \ldots, C\}\)</span> où l’ordre n’est pas important. Par exemple, prédire la température est un problème de régression alors que prédire si la photo représente un chien ou un chat est un problème de classification.</p>
<p>A fortiori, toutes les observations dans <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> ne sont pas nécessairement équiprobables. Certains clients ont peut-être, par exemple, un profil plus commun que d’autres. Afin de pouvoir définir plus rigoureusement ce qu’on entend par <em>bonnes performances</em>, notons <span class="math notranslate nohighlight">\(X\in\mathcal{X}\)</span> une variable aléatoire qui décrit nos données observées et <span class="math notranslate nohighlight">\(Y\in\mathcal{Y}\)</span> la variable aléatoire associée à nos labels. Assez naïvement, notons <span class="math notranslate nohighlight">\(\mathbb{P}\)</span> la loi de notre couple <span class="math notranslate nohighlight">\(X,Y\)</span> :</p>
<div class="math notranslate nohighlight">
\[X, Y\sim \mathbb{P}.\]</div>
<p>Notons <span class="math notranslate nohighlight">\(r:\mathcal{Y}\times\mathcal{Y}\rightarrow\mathbb{R}^+\)</span> une mesure d’erreur, un risque élémentaire. On a par exemple, dans le cas d’un problème de régression, l’erreur quadratique :</p>
<div class="math notranslate nohighlight">
\[r(\hat{y}, y)=(\hat{y}-y)^2,\]</div>
<p>où <span class="math notranslate nohighlight">\(\hat{y}\)</span> est la prédiction que ferait notre modèle. Ou encore, dans le cas de la classification nous pouvons avoir cette fois-ci l’erreur <span class="math notranslate nohighlight">\(0.1\)</span> :</p>
<div class="math notranslate nohighlight">
\[r(\hat{y}, y)=\textbf{1}\{\hat{y}\neq y\},\]</div>
<p>qui vaut <span class="math notranslate nohighlight">\(1\)</span> si la prédiction est mauvaise ou <span class="math notranslate nohighlight">\(0\)</span> sinon.</p>
<p>Notre objectif est tout naturellement de trouver une application <span class="math notranslate nohighlight">\(h:\mathcal{X}\mapsto\mathcal{Y}\)</span> telle que <span class="math notranslate nohighlight">\(R(h)=\mathbb{E}\big[r(h(X), Y)\big]\)</span> est petit. On veut un bon modèle sur de nouvelles données. L’idée va être de collecter des données représentatives (dans le sens iid) et de construire notre modèle avec ces dernières. Notons :</p>
<div class="math notranslate nohighlight">
\[S_n=\{(X_i, Y_i)\}_{i\leq n}\]</div>
<p>un jeu de données de taille <span class="math notranslate nohighlight">\(n\)</span>.</p>
</div>
<div class="section" id="b-l-apprentissage-non-supervise">
<h3>B. L’apprentissage non-supervisé<a class="headerlink" href="#b-l-apprentissage-non-supervise" title="Permalink to this headline">¶</a></h3>
<p>Ici, c’est l’inverse. Nous avons accès à l’espace des observations <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> duquel on peut collecter des données (toujours selon la loi de la variable aléatoire <span class="math notranslate nohighlight">\(X\)</span>). On sait qu’il existe un espace <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> cible mais (1) il n’est pas nécessairement connu et (2) nous ne connaissons pas d’exemple de liens entre exemples d’apprentissage et cibles associées.</p>
<p>Par exemple, si <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> représente des données clients, on peut savoir (se douter) qu’il existe des groupes de clients qui se ressemblent mais ne pas les connaître et ne pas savoir combien il y en a. Il s’agit ici d’une tâche de <em>clustering</em> où on cherche à regrouper des données entre-elles toujours de manière à ce que le regroupement se généralise à de nouvelles données.</p>
<p>On peut chercher à transformer nos données dans <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> dans un espace qu’on notera cette fois-ci <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span> où ces dernières auront de meilleures propriété. On note cet “espace de représentation” <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span> et non <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> car il s’agit souvent d’une étape intermédiaire avant une tâche supervisée où on chercherait à prédire un label dans <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>. C’est ce qu’on appelle l’apprentissage de représentation. Ainsi, si <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> est l’ensemble des photos de chiens et de chats, <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span> est l’ensemble de ces dernières où on a mis “d’un côté” les photos de chiens et de “l’autre” celles de chats. Il devient simple de construire une tâche supervisée permettant de prédire le bon label “chien/chat”.</p>
</div>
</div>
<div class="section" id="ii-on-casse-une-idee-preconcue-as">
<h2>II. On casse une idée préconçue (AS)<a class="headerlink" href="#ii-on-casse-une-idee-preconcue-as" title="Permalink to this headline">¶</a></h2>
<p>Soit <span class="math notranslate nohighlight">\(S=\{(x_i, y_i)\}_{i\leq n}\)</span> un jeu de données représentatif de taille <span class="math notranslate nohighlight">\(n\)</span>. Un modèle très performant sur ces données est-il performant sur des données nouvelles ? Réfléchissez quelques instants et écrivez votre réponse dans un coin.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">metrics</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<p>Chargeons et affichons notre jeu de données. Ce dernier consiste en des chiffres écrits à la main. L’objectif va être de faire un modèle qui permet de prédire ces derniers.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">digits</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_digits</span><span class="p">()</span>

<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">,</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray_r</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Training: </span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">label</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_introduction_ml_8_0.png" src="../_images/1_introduction_ml_8_0.png" />
</div>
</div>
<p>Construisons notre premier modèle. Ce dernier correspond à la fonction mathématique suivante :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}h(x)=\begin{cases}y&amp;\text{ si } (x, y)\in S_n\\\text{aléatoire}()&amp;\text{ sinon.}\end{cases}\end{aligned}\end{split}\]</div>
<p>On retourne le label connu si on a déjà vu le “point” et on retourne un label au hasard sinon.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Memorize</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
        
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">memorized</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
                <span class="c1"># On compare les pixels un a un</span>
                <span class="c1"># et on regarde s&#39;ils sont tous identiques</span>
                <span class="c1"># pixel wise comparison</span>
                <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">==</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                    <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
                    <span class="n">memorized</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="k">break</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">memorized</span><span class="p">:</span>
                <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<p>Le code suivant prépare nos données (des images) afin qu’elles deviennent facilement manipulables par notre modèle.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># flatten the images</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Split data into 50% train and 50% test subsets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Les données de test nous permettront de tester notre modèle sur des données qu’il n’a pas utilisé pour se construire. Cela nous permet de tester notre modèle sur de “nouvelles données”. C’est son pouvoir de généralisation qui nous intéresse.</p>
<div class="tip admonition">
<p class="admonition-title">Évaluation d’un modèle</p>
<p>Il n’existe pas une seule manière d’évaluer les performances d’un modèle. En voici quelques-une.</p>
<p><em>Précision</em></p>
<p>Considérons une tâche de classification (<span class="math notranslate nohighlight">\(y=\{1,\ldots, C\}\)</span>), la précision d’un modèle <span class="math notranslate nohighlight">\(h\)</span> pour la classe <span class="math notranslate nohighlight">\(c\)</span> est</p>
<div class="math notranslate nohighlight">
\[\text{prec}_c(h)=\frac{\sum_i \textbf{1}\{y_i=c, h(x_i)=y_i\}}{\sum_i \textbf{1}\{h(x_i)=c\}}\]</div>
<p>On veut que le score soit proche de <span class="math notranslate nohighlight">\(1\)</span>.</p>
<hr class="docutils" />
<p><em>Rappel</em></p>
<p>Considérons une tâche de classification (<span class="math notranslate nohighlight">\(y=\{1,\ldots, C\}\)</span>), le rappel d’un modèle <span class="math notranslate nohighlight">\(h\)</span> pour la classe <span class="math notranslate nohighlight">\(c\)</span> est</p>
<div class="math notranslate nohighlight">
\[\text{rec}_c(h)=\frac{\sum_i \textbf{1}\{y_i=c, h(x_i)=y_i\}}{\sum_i \textbf{1}\{y_i=c\}}\]</div>
<p>On veut que le score soit proche de <span class="math notranslate nohighlight">\(1\)</span>.</p>
<hr class="docutils" />
<p><em>Score f1</em></p>
<p>Ce dernier combine le rappel et la précision :</p>
<div class="math notranslate nohighlight">
\[\text{f1}_c(h)=2\frac{\text{prec}_c(h)\cdot\text{rec}_c(h)}{\text{prec}_c(h)+\text{rec}_c(h)}\]</div>
<p>On veut que le score soit proche de <span class="math notranslate nohighlight">\(1\)</span>.</p>
<hr class="docutils" />
<p><em>L’accuracy</em></p>
<p>C’est une sorte de généralisation de la précision :</p>
<div class="math notranslate nohighlight">
\[\text{acc}(h)=\frac{\sum_i\textbf{1}\{h(x_i)=y_i\}}{n},\]</div>
<p>c’est le ratio de bonnes prédictions.</p>
<hr class="docutils" />
<p><em>Le support</em></p>
<p>C’est le nombre de points de notre jeu de données qui sont concernées par la métrique.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Memorize</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>On commence par tester les performances de notre modèle sur notre jeu d’apprentissage.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Classification report for classifier Memorize on train:</span><span class="se">\n</span><span class="s1">&#39;</span>
      <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Classification report for classifier Memorize on train:
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        90
           1       1.00      1.00      1.00        91
           2       1.00      1.00      1.00        91
           3       1.00      1.00      1.00        92
           4       1.00      1.00      1.00        89
           5       1.00      1.00      1.00        91
           6       1.00      1.00      1.00        90
           7       1.00      1.00      1.00        90
           8       1.00      1.00      1.00        86
           9       1.00      1.00      1.00        88

    accuracy                           1.00       898
   macro avg       1.00      1.00      1.00       898
weighted avg       1.00      1.00      1.00       898
</pre></div>
</div>
</div>
</div>
<p>Notre modèle est parfait ! Aucune erreur. On ne peut pas faire mieux ! Et du côté du test ?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Classification report for classifier Memorize on test:</span><span class="se">\n</span><span class="s1">&#39;</span>
      <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Classification report for classifier Memorize on test:
              precision    recall  f1-score   support

           0       0.11      0.10      0.10        88
           1       0.10      0.09      0.09        91
           2       0.08      0.09      0.09        86
           3       0.13      0.11      0.12        91
           4       0.06      0.05      0.06        92
           5       0.07      0.07      0.07        91
           6       0.09      0.09      0.09        91
           7       0.09      0.10      0.10        89
           8       0.13      0.14      0.13        88
           9       0.11      0.14      0.13        92

    accuracy                           0.10       899
   macro avg       0.10      0.10      0.10       899
weighted avg       0.10      0.10      0.10       899
</pre></div>
</div>
</div>
</div>
<p>C’est ridiculement mauvais : on se trompe neuf fois sur dix, soit exactement ce qu’on attendrait d’une réponse aléatoire.</p>
<p>Oui mais on a fait exprès de construire le modèle de cette manière ! En réalité, il existe une infinité de fonctions, paramétriques ou non, qu’on peut rendre aussi bonne qu’on veut sur nos données mais qui seraient particulièrement mauvaises sur de nouvelles données (cela inclut les modèles usuels et c’est pour cela qu’on a besoin d’experts !)… Toute la difficulté du <em>machine learner</em> va être de contrôler cela.</p>
<div class="note admonition">
<p class="admonition-title"><span class="math notranslate nohighlight">\(\texttt{memorize}\)</span> en pratique</p>
<p>Cette approche n’est pas totalement absurde. Si <span class="math notranslate nohighlight">\(|\mathcal{X}|&lt;\infty\)</span>, ou si <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> est discret, alors plus on collectera de données plus les nouvelles données auront déjà été vues et nos prédictions deviendront intéressantes. C’est exactement ce que nous faisons face à un nouveau paquet de bonbons dont nous ne connaissons pas le goût.</p>
</div>
</div>
<div class="section" id="iii-une-autre-premiere-approche-logique-le-knn-as">
<h2>III. Une autre première approche logique : le KNN (AS)<a class="headerlink" href="#iii-une-autre-premiere-approche-logique-le-knn-as" title="Permalink to this headline">¶</a></h2>
<p>Intuitivement, on a envie de dire que nos données ne sont pas complètement déstructurées. Deux clients très similaires achèteront très probablement des produits très similaires. Un <em>trois</em> ressemble plus à un <em>trois</em> qu’à un <em>cinq</em> et un <em>cinq</em> ressemble plus à un <em>cinq</em> qu’à un <em>trois</em>. Finalement, on généralise un petit peu l’exemple précédent. Au lieu de répondre aléatoirement si je ne connais pas la donnée, je cherche l’exemple le plus proche et je prédis le même label ! Plus rigoureusement, notre modèle de prédiction fonctionne comme suit :</p>
<div class="math notranslate nohighlight">
\[\hat{y}_\text{new}=y\text{ avec }(x, y)=\text{argmin}_{(x, y)\in S}\lVert x-x_{\text{new}}\rVert_2.\]</div>
<p>On peut imaginer que si plusieurs points sont équidistants, la réponse se fait aléatoirement entre les labels possibles.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Utilisez l’objet <span class="math notranslate nohighlight">\(\texttt{KNeighborsClassifier}\)</span> avec le paramètre <span class="math notranslate nohighlight">\(\texttt{n}\_\texttt{neighbors=1}\)</span> et entraînez le sur <span class="math notranslate nohighlight">\(\texttt{X}\_\texttt{train}\)</span>.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="n">model</span> <span class="o">=</span> 
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="c1">###############################################################</span>

<span class="n">predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Classification report for classifier </span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s1"> on train:</span><span class="se">\n</span><span class="s1">&#39;</span>
      <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>On a testé le modèle sur le jeu d’apprentissage et on est toujours aussi bon sur le jeu d’apprentissage ! Cependant, c’est attendu car l’image qui ressemble le plus à une autre est l’image elle-même. Prédisons maintenant sur le test.</p>
<div class="tip admonition">
<p class="admonition-title">La matrice de confusion</p>
<p>Les métriques précédentes nous donnent les performances du modèle en moyenne ainsi que par classe. Cependant, cela ne nous donne que très peu d’information quant au type d’erreur qu’il fait. La <strong>matrice de confusion</strong> permet de comprendre le type d’erreurs que fait notre modèle. La cellule <span class="math notranslate nohighlight">\(i,j\)</span> indique le nombre d’éléments de la classe <span class="math notranslate nohighlight">\(i\)</span> qui ont été prédits de la classe <span class="math notranslate nohighlight">\(j\)</span>.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Classification report for classifier </span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s1"> on test:</span><span class="se">\n</span><span class="s1">&#39;</span>
      <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">figure_</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Is Machine learning solved ? Minute papillon ! Ce modèle est très sensible au bruit ! Supposons qu’une de nos données soit bruitée (e.g. un 3 qui ressemble à un 8). Si une nouvelle donnée représentant un <span class="math notranslate nohighlight">\(8\)</span> se retrouve à côté de cette anomalie, elle sera mal prédite. Nous pouvons adresser cette limite de la manière suivante : au lieu de regarder le point le plus proche, on regarde les <span class="math notranslate nohighlight">\(k\)</span> points les plus proches et on fait un vote à la majorité. Plus formellement la prédiction est faite comme suit :</p>
<div class="math notranslate nohighlight">
\[\hat{y}_\text{new}=\text{majority$\_$voting}(\texttt{KNN}.\texttt{labels})\text{  où  }\texttt{KNN}=\text{argmin}_{S^\prime\subset S,\ |S^\prime|=K}\sum_i \lVert x_i- x_{\text{new}}\rVert.\]</div>
<p>Dans le cas où on chercherait à faire une régression, on remplace le vote à la majorité par une moyenne.</p>
<p>Récupérons le jeu de données <span class="math notranslate nohighlight">\(\texttt{iris}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="c1"># décommentez la ligne suivante pour obtenir des informations</span>
<span class="c1"># sur le dataset iris.</span>
<span class="c1"># print(iris.DESCR)</span>
</pre></div>
</div>
</div>
</div>
<p>De la même manière que précédemment, on construit notre jeu d’apprentissage pour construire notre modèle et notre jeu de test pour en tester les performances.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Utilisez l’objet <span class="math notranslate nohighlight">\(\texttt{KNeighborsClassifier}\)</span> avec le paramètre <span class="math notranslate nohighlight">\(\texttt{n}\_\texttt{neighbors=1}\)</span> et entraînez le sur <span class="math notranslate nohighlight">\(\texttt{X}\_\texttt{train}\)</span>. Faites une prédiction sur <span class="math notranslate nohighlight">\(\texttt{predicted=X}\_\texttt{test}\)</span>.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="n">model</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="o">...</span>

<span class="n">predicted</span> <span class="o">=</span> <span class="o">...</span> 
<span class="c1">###############################################################</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">figure_</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Les performances sont déjà très bonnes ! Mais il est possible de gagner un tout petit peu de performances en considérant plus de voisins :</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Utilisez l’objet <span class="math notranslate nohighlight">\(\texttt{KNeighborsClassifier}\)</span> avec le paramètre <span class="math notranslate nohighlight">\(\texttt{n}\_\texttt{neighbors=10}\)</span> et entraînez le sur <span class="math notranslate nohighlight">\(\texttt{X}\_\texttt{train}\)</span>. Faites une prédiction sur <span class="math notranslate nohighlight">\(\texttt{predicted=X}\_\texttt{test}\)</span>.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="n">model</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="o">...</span>

<span class="n">predicted</span> <span class="o">=</span> <span class="o">...</span>
<span class="c1">###############################################################</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">figure_</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="iv-les-arbres-de-decision-ou-classification-and-regression-tree-cart-as">
<h2>IV. Les arbres de décision ou Classification and Regression Tree (CART) (AS)<a class="headerlink" href="#iv-les-arbres-de-decision-ou-classification-and-regression-tree-cart-as" title="Permalink to this headline">¶</a></h2>
<p>De la même manière que pour l’algorithme KNN, on supposera ici que nos données admettent une certaine structure et que des points proches possèdent probablement le même label ou une prediction proche dans le cas de la régression. Ici, à la différence du KNN, la notion de voisinage se construit au travers d’hyperrectangles parallèles aux axes. Afin de bien comprendre le fonctionnement, supposons que notre arbre de décision soit déjà construit et soit celui représenté par la figure ci-dessous. Prenons une nouvelle donnée :</p>
<div class="math notranslate nohighlight">
\[x_{\text{new}}=[\text{ecoute:}1, \text{math:}12,\text{info:}18]^T,\]</div>
<p>et partons de la racine de notre arbre. Cette racine possède deux branches sortantes. Le choix de la branche se fait à partir d’un critère sur une des variables explicatives de <span class="math notranslate nohighlight">\(x\)</span>. Dans notre exemple la variable explicative est l’écoute en cours. Si l’étudiant écoute, on prend la branche de droite, sinon celle de gauche. Dans notre, ce sera donc la branche de droite. La nouvelle variable à regarder est la note de math et sa valeur doit être supérieure à 10. C’est notre cas et nous prenons la branche de droite. Nous sommes à une feuille et la prédiction à faire est “oui, l’étudiant s’en sortira”.</p>
<p><img alt="Decision tree" src="https://raw.githubusercontent.com/maximiliense/lmiprp/main/Travaux%20Pratiques/Machine%20Learning/Introduction/data/Introduction/cart.jpg" /></p>
<p>Le choix de la règle de décision à chaque noeud peut être adapté afin d’obtenir des régions à la géométrie variable. La construction d’un arbre se fait en partant de la racine vers les feuilles et en choisissant intérativement les variables explicatives qui ont le plus d’effet sur notre prédiction (via diverses critères).</p>
<div class="note admonition">
<p class="admonition-title">Les feuilles de l’arbre</p>
<p>En pratique, les feuilles de notre arbre contiennent les exemples d’apprentissage de notre jeu de données. La prédiction dépend des exemples contenus dans la feuille qui nous concerne.</p>
</div>
<p>La séquence suivante abordera plus en détails les arbres de classification et de régression.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Utilisez l’objet <span class="math notranslate nohighlight">\(\texttt{DecisionTreeClassifier}\)</span> avec le paramètre <span class="math notranslate nohighlight">\(\texttt{max}\_\texttt{depth=2}\)</span> et entraînez le sur <span class="math notranslate nohighlight">\(\texttt{X}\_\texttt{train}\)</span>. Faites une prédiction sur <span class="math notranslate nohighlight">\(\texttt{predicted=X}\_\texttt{test}\)</span>.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="n">model</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="o">...</span>

<span class="n">predicted</span> <span class="o">=</span> <span class="o">...</span>
<span class="c1">###############################################################</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">figure_</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="v-les-forets-aleatoires-ou-random-forest-rf-as">
<h2>V. Les forêts aléatoires ou Random Forest (RF) (AS)<a class="headerlink" href="#v-les-forets-aleatoires-ou-random-forest-rf-as" title="Permalink to this headline">¶</a></h2>
<p>Les arbres de décision peuvent être sujets au surapprentissage. Une manière de compenser le problème est d’en construire plusieurs où chaque arbre est construit en ne voyant qu’une partie des données. Enfin leurs prédictions sont aggrégées. Ces approches sont généralement beaucoup plus performantes que les arbres simples. Malheureusement, autant avec un arbre simple on pouvait essayer de comprendre la prédiction, autant ici, cela devient difficile.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Utilisez l’objet <span class="math notranslate nohighlight">\(\texttt{RandomForestClassifier}\)</span> et entraînez le sur <span class="math notranslate nohighlight">\(\texttt{X}\_\texttt{train}\)</span>. Faites une prédiction sur <span class="math notranslate nohighlight">\(\texttt{predicted=X}\_\texttt{test}\)</span>.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="c1">###############################################################</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">figure_</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="vi-choix-des-hyperparametres-as">
<h2>VI. Choix des hyperparamètres (AS)<a class="headerlink" href="#vi-choix-des-hyperparametres-as" title="Permalink to this headline">¶</a></h2>
<p>Pour les modèles précédents, nous avons dû choisir différents paramètres qui affectaient les performances de notre modèle. Nous les avons choisis en regardant les performances de notre modèle sur le jeu de test. Cependant, les bonnes performances étaient peut-être un coup de chance !</p>
<p>Il existe deux stratégies d’évaluation sans biais de la qualité de notre modèle :</p>
<ul class="simple">
<li><p>La validation non croisée où une partie de notre jeu de données est cachée pendant l’apprentissage puis utilisée afin d’évaluer les performances du modèle. Il s’agit du découpage train/test. Cette stratégie est un estimateur sans biais de la qualité de notre modèle mais possède une variance plus forte que la validation croisée. Elle peut-être particulièrement utile lorsque le coup d’apprentissage d’un modèle est très élevé (e.g. <em>deep learning</em>)</p></li>
<li><p>La validation croisée où notre jeu de données est divisé en <em>k</em> parties (on parle aussi de <em>k-fold</em>). Évidemment, <span class="math notranslate nohighlight">\(k\in\{2, ..., n\}\)</span> où <span class="math notranslate nohighlight">\(n\)</span> est la taille du jeu de données. Chacune des parties jouera successivement le rôle de jeu de test pendant que les <span class="math notranslate nohighlight">\(k-1\)</span> autres parties serviront à calculer notre modèle. Le résultat de cette procédure est un vecteur de <span class="math notranslate nohighlight">\(k\)</span> scores dont on peut calculer la moyenne, la variance, etc.</p></li>
</ul>
<p>On peut illustrer la méthode des <em>k-folds</em> via l’exemple suivant :</p>
<div class="math notranslate nohighlight">
\[\begin{align*}
\text{Appartient au train set: } \color{red}{\boxed{}}&amp;\text{ et appartient au test set: }\color{green}{\boxed{}}
\end{align*}\]</div>
<div class="math notranslate nohighlight">
\[\begin{align*}
\text{Step 1: }\color{green}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align*}\]</div>
<div class="math notranslate nohighlight">
\[\begin{align*}
\text{Step 2: }\color{red}{\boxed{}}\color{green}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align*}\]</div>
<div class="math notranslate nohighlight">
\[\begin{align*}
\text{Step 3: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{green}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align*}\]</div>
<div class="math notranslate nohighlight">
\[\begin{align*}
\text{Step 4: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{green}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align*}\]</div>
<div class="math notranslate nohighlight">
\[\begin{align*}
\text{Step 5: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{green}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align*}\]</div>
<div class="math notranslate nohighlight">
\[\begin{align*}
\text{Step 6: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{green}{\boxed{}}\color{red}{\boxed{}}
\end{align*}\]</div>
<div class="math notranslate nohighlight">
\[\begin{align*}
\text{Step 7: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{green}{\boxed{}}
\end{align*}\]</div>
<div class="warning admonition">
<p class="admonition-title">Overfitter à la main</p>
<p>Attention, quand on fait quelques tests à la main et que l’on évalue nos performances sur le jeu de test, on est déjà entrain de faire de la sélection de modèle. On se sert alors du test comme d’un ensemble de validation. C’est d’autant plus vrai lorsqu’on a peu de données d’apprentissage.</p>
</div>
<p>La méthode <span class="math notranslate nohighlight">\(\texttt{cross_val_score}\)</span> de <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span> permet de réaliser cette procédure. On pourra renseigner le paramètre <span class="math notranslate nohighlight">\(\texttt{cv}\)</span> qui indique le nombre <span class="math notranslate nohighlight">\(k\)</span> et le paramètre <span class="math notranslate nohighlight">\(\texttt{scoring}\)</span> qui donne la métrique que l’on souhaite calculer.</p>
<p>Si on cherche à trouver une valeur d’un hyper-paramètre du modèle, l’objet <span class="math notranslate nohighlight">\(\texttt{𝙶𝚛𝚒𝚍𝚂𝚎𝚊𝚛𝚌𝚑𝙲𝚅}\)</span> applique une validation croisée en cherchant différentes valeurs de cet hyper-paramètre.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Utilisez l’objet <span class="math notranslate nohighlight">\(\texttt{GridSearchCV}\)</span> pour faire une recherche par grille sur le modèle <span class="math notranslate nohighlight">\(\texttt{RandomForestClassifier}\)</span> et entraînez le sur <span class="math notranslate nohighlight">\(\texttt{X}\_\texttt{train}\)</span>.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="c1">###############################################################</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">figure_</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Notre RandomForest de base était déjà très bon !</p>
</div>
<div class="section" id="vii-l-algorithme-des-k-moyennes-ans">
<h2>VII. L’algorithme des K-Moyennes (ANS)<a class="headerlink" href="#vii-l-algorithme-des-k-moyennes-ans" title="Permalink to this headline">¶</a></h2>
<p>Il s’agit ici d’un algorithme non supervisé. Imaginons que nous ayons collecté un jeu de données <span class="math notranslate nohighlight">\(S_n=\{(X_i)\}_{i\leq n}\)</span>. On sait qu’il existe des groupes dans nos données. Supposons même qu’on sâche qu’il existe <span class="math notranslate nohighlight">\(K\)</span> groupes. L’idée de l’algorithme des K-Moyennes va être de détecter ces <span class="math notranslate nohighlight">\(K\)</span> groupes en trouvant une solution au problème d’optimisation suivant :</p>
<div class="math notranslate nohighlight">
\[\text{KMeans}=\text{argmin}_{m_1, \ldots, m_K\in\mathcal{X}, c_1,\ldots,c_n\in\{1,\ldots,K\}}\sum_{i=1}^K\sum_{j=1}^n\textbf{1}\{c_j=i\}\lVert m_i-x_j\rVert_2=\text{argmin}_{m_1, \ldots, m_K\in\mathcal{X}, c_1,\ldots,c_n\in\{1,\ldots,K\}}\sum_{j=1}^n\lVert x_j-m_{c_j}\rVert_2.\]</div>
<p>Dit autrement, chaque groupe est représenté par une coordonnée <span class="math notranslate nohighlight">\(m_i\)</span> (qui s’avère être la moyenne des éléments du groupe) et chaque élément de notre jeu de données n’est associé qu’à un seul groupe <span class="math notranslate nohighlight">\(c_j\)</span>.</p>
<p>Considérons le jeu de données suivant.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">sample_data</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">]])</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="mi">15</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">means</span>
    <span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="o">//</span><span class="n">n</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="o">*</span><span class="n">n</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
    
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_data</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="k">def</span> <span class="nf">plot_clusters</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">means</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Nos données et leur label inconnu&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">means</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">means</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="p">[</span><span class="n">m</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="n">s</span><span class="o">=</span><span class="mi">55</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">path</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">p</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plot_clusters</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_introduction_ml_60_0.png" src="../_images/1_introduction_ml_60_0.png" />
</div>
</div>
<p>Le problème de K-Means est NP-Difficile. Pour cela, nous utilisons en pratique un heuristique appelé “algorithme de LLoyd” qui fonctionnde la manière suivante :</p>
<ol class="simple">
<li><p>On initialise les k moyennes</p></li>
<li><p>On assigne tous nos points à leur moyenne la plus proche</p></li>
<li><p>On met à jour les moyennes avec les nouveaux points</p></li>
<li><p>Si le déplacement des moyennes est significatif, on reprend à l’étape 2</p></li>
</ol>
<div class="tip admonition">
<p class="admonition-title">Fixer le <span class="math notranslate nohighlight">\(k\)</span></p>
<p>Le <span class="math notranslate nohighlight">\(k\)</span> peut être fixé via une selection de paramètres et une validation croisée.</p>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Le problème est NP-Difficile et un algorithme permettant de le résoudre est l’algorithme de LLoyd. Implémentez le.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">KMeans</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.001</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1">####### Complete this part ######## or die ####################</span>
        <span class="c1"># K means initialization</span>
        <span class="o">...</span>
        <span class="c1"># Step 1</span>
        <span class="o">...</span>
        <span class="c1"># Step 2</span>
        <span class="o">...</span>
        <span class="c1">###############################################################</span>
        <span class="k">return</span> <span class="n">assignment</span><span class="p">,</span> <span class="n">means</span><span class="p">,</span> <span class="n">path</span>
            

<span class="n">model</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plot_clusters</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="o">*</span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</pre></div>
</div>
<p>On se rend compte qu’on arrive à retrouver les 3 groupes automatiquement (Les couleurs sont celles calculées par notre modèle des K-Moyennes) !</p>
</div>
<div class="section" id="viii-la-malediction-de-la-dimension">
<h2>VIII. La malédiction de la dimension<a class="headerlink" href="#viii-la-malediction-de-la-dimension" title="Permalink to this headline">¶</a></h2>
<p>Nous avons pu observer des scénarios où l’erreur sur notre jeu de données d’apprentissage était <span class="math notranslate nohighlight">\(0\)</span> alors que notre modèle n’était pas si bon que cela sur notre jeu de test. Cet écart peut même devenir catastrophique ! De manière plus rigoureuse, le gap de généralisation de notre estimateur <span class="math notranslate nohighlight">\(\hat{h}\)</span> est la quantité suivante :</p>
<div class="math notranslate nohighlight">
\[\text{gap}(\hat{h})=|Re(\hat{h})-R(\hat{h})|.\]</div>
<p>Où <span class="math notranslate nohighlight">\(Re\)</span> fait référence à notre risque empirique, c’est-à-dire l’erreur sur le jeu d’apprentissage et <span class="math notranslate nohighlight">\(R\)</span> à l’erreur en espérance.</p>
<p>Il est possible d’avoir une idée de <span class="math notranslate nohighlight">\(R(\hat{h})\)</span> en passant par un jeu de test ou par une autre stratégie d’évaluation via un jeu de test par exemple, comme nous avons pu le voir. Nous allons ici nous rendre compte que les modèles qui regardent le voisinage de nos données souffrent d’une grosse limite liée à ce qu’on appelle <em>la malédiction de la dimension</em> et qui affecte grandement ce <em>gap</em>.</p>
<div class="section" id="en-details">
<h3>En détails<a class="headerlink" href="#en-details" title="Permalink to this headline">¶</a></h3>
<p>La malédiction de la dimension fait référence aux résultats contre-intuitifs qui apparaissent lorsque la dimension augmente.</p>
<div class="note admonition">
<p class="admonition-title">Une illustration avec l’orange multi-dimensionnelle</p>
<p>Cette exemple donne une image plus visuelle de la malédiction de la dimension. Modélisons une orange comme une boule parfaite de rayon <span class="math notranslate nohighlight">\(8\text{cm}\)</span> avec une enveloppe (i.e. la peau) d’une épaisseur de <span class="math notranslate nohighlight">\(5\text{mm}\)</span> partout. Le volume d’une boule <span class="math notranslate nohighlight">\(\mathcal{B}_r\)</span> de rayon <span class="math notranslate nohighlight">\(r\)</span> dans un espace de dimension <span class="math notranslate nohighlight">\(d\)</span> est donné par :</p>
<div class="math notranslate nohighlight">
\[\text{vol}(\mathcal{B}_r)=K r^d,\]</div>
<p>où <span class="math notranslate nohighlight">\(K\)</span> est une constante qui dépend de la dimension. La question qu’on peut se poser est celle du volume occupé par la peau de l’orange relativement au volume total de l’orange. C’est le volume total de l’orange auquel on soustrait le volume de l’orange sans la peau divisé par le volume total de l’orange :</p>
<div class="math notranslate nohighlight">
\[\text{ratio}=\frac{\text{vol}(\mathcal{B}_8)-\text{vol}(\mathcal{B}_{8-0.05})}{\text{vol}(\mathcal{B}_8)}.\]</div>
<p>En dimension <span class="math notranslate nohighlight">\(3\)</span>, cela nous donne <span class="math notranslate nohighlight">\(\text{ratio}=0.018\)</span>. Moins de <span class="math notranslate nohighlight">\(2\%\)</span> du volume de l’orange est occupé par la peau. Que se passe-t-il en dimension 500 ? On obtient <span class="math notranslate nohighlight">\(\text{ratio}\approx 96\%\)</span>. Presque tout le volume de l’orange est occupé par la peau ! Et <span class="math notranslate nohighlight">\(500\)</span> ne représente pas la très grande dimension (une image en couleur de 200px<span class="math notranslate nohighlight">\(\times\)</span>200px a une dimension de 120000).</p>
</div>
<p>Une première manière de l’observer en <em>machine learning</em> est possible grâce au KNN. Ce dernier classe un nouvel élément en fonction de ses voisins dans le jeu d’apprentissage. Nous allons en particulier étudier l’évolution du risque de généralisation en fonction de la dimension. Plus précisément, les données synthétiques sont construites de la manière suivante :</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="k">def</span> <span class="nf">sample_data</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">))</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">y</span><span class="o">*</span><span class="n">X</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">X</span> <span class="c1"># positive have mean mu and negative, -mu</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="o">-</span><span class="n">k</span><span class="p">))</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">noise</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<p>Dit autrement, <span class="math notranslate nohighlight">\(k\)</span> dimensions contiennent le signal intéressant pour notre tâche et <span class="math notranslate nohighlight">\(d\)</span> dimensions ne servent à rien. Nous observons ci-dessous ce qui se passe lorsqu’on rajouter des dimensions de bruits (i.e. qui ne servent à rien). C’est typiquement ce pourrait se passer avec des images. Une photo de chien ne contient pas que des pixels descriptifs du concept de chien.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>

<span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">redo</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">max_dim</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">first_dim</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">steps</span> <span class="o">=</span> <span class="mi">100</span>

<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">first_dim</span><span class="p">,</span> <span class="n">max_dim</span><span class="p">,</span> <span class="n">steps</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">redo</span><span class="p">):</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_data</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="n">d</span><span class="p">)</span>
        <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">sample_data</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="n">d</span><span class="p">)</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>
        <span class="n">c</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],)))</span>
        <span class="n">s</span> <span class="o">+=</span> <span class="n">c</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],)))</span><span class="o">/</span><span class="n">redo</span>
    <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># configuration generale de matplotlib</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">12.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">first_dim</span><span class="p">,</span> <span class="n">max_dim</span><span class="p">,</span> <span class="n">steps</span><span class="p">)),</span> <span class="n">scores</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Evolution de l</span><span class="se">\&#39;</span><span class="s1">accuracy en fonction de la dimension du probleme&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_introduction_ml_72_0.png" src="../_images/1_introduction_ml_72_0.png" />
</div>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Quelle est l’accuracy d’un classifieur aléatoire ?</strong></p>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Expliquez pourquoi l’accuracy diminue lorsqu’on rajoute des dimensions sans signal.</strong></p>
</div>
<p>De manière similaire, étudions l’évolution des distances lorsque la dimension évolue. Nous allons tirer aléatoirement un ensemble de vecteurs dans <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> et nous calculerons la norme <span class="math notranslate nohighlight">\(\lVert x\rVert^2_2\)</span> moyenne, maximale et minimale de notre tirage. De la même manière nous calculerons la distance <span class="math notranslate nohighlight">\(\lVert x-y\rVert_2\)</span> moyenne, maximale et minimale entre les couples de points de notre jeu de données. Enfin, l’objectif sera d’étudier ces quantités en faisant évoluer la dimension <span class="math notranslate nohighlight">\(d\)</span> du problème.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.spatial</span> <span class="kn">import</span> <span class="n">distance_matrix</span>

<span class="k">def</span> <span class="nf">sample_data</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sample_data</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">redo</span> <span class="o">=</span> <span class="mi">50</span>
<span class="k">def</span> <span class="nf">experiment_</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
    <span class="n">min_</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">max_</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">mean_</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">redo</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">sample_data</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
        <span class="n">vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">X</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">min_</span> <span class="o">+=</span> <span class="n">vec</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">/</span><span class="n">redo</span>
        <span class="n">max_</span> <span class="o">+=</span> <span class="n">vec</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">/</span><span class="n">redo</span>
        <span class="n">mean_</span> <span class="o">+=</span> <span class="n">vec</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">/</span><span class="n">redo</span>
        
        <span class="n">mat</span> <span class="o">=</span> <span class="n">distance_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">25</span><span class="p">],</span> <span class="n">X</span><span class="p">[:</span><span class="mi">25</span><span class="p">])</span>
        <span class="n">triu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">mat</span><span class="p">)</span>
        <span class="n">triu</span> <span class="o">=</span> <span class="n">triu</span><span class="p">[</span><span class="n">triu</span><span class="o">!=</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">dist_max</span> <span class="o">=</span> <span class="n">triu</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="n">dist_min</span> <span class="o">=</span> <span class="n">triu</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
        <span class="n">dist_mean</span> <span class="o">=</span> <span class="n">triu</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">min_</span><span class="p">,</span> <span class="n">max_</span><span class="p">,</span> <span class="n">mean_</span><span class="p">,</span> <span class="n">dist_min</span><span class="p">,</span> <span class="n">dist_max</span><span class="p">,</span> <span class="n">dist_mean</span>
<span class="n">idx</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">val</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
    <span class="n">idx</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">d</span><span class="p">])</span>
    <span class="n">val</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">experiment_</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>
<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">):</span>
    <span class="n">idx</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">d</span><span class="p">])</span>
    <span class="n">val</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">experiment_</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>
<span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">idx</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">val</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">arr</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">arr</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Min&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">arr</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">arr</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Max&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">arr</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">arr</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Moy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Dimension du problème&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Norme $\ell_2$ des vecteurs&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Evolution des normes&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">arr</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">arr</span><span class="p">[:,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Min&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">arr</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">arr</span><span class="p">[:,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Max&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">arr</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">arr</span><span class="p">[:,</span> <span class="mi">6</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Moy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Dimension du problème&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Distance $\ell_2$ entre nos vecteurs&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Evolution des distances&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Evolution des distances&#39;)
</pre></div>
</div>
<img alt="../_images/1_introduction_ml_77_1.png" src="../_images/1_introduction_ml_77_1.png" />
</div>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Quel phénomène mathématique pouvons nous invoquer afin d’expliquer cela ?</strong></p>
</div>
<p>Soit <span class="math notranslate nohighlight">\(x_\text{new}\)</span> une nouvelle donnée. Une petite perturbation du point de notre jeu d’apprentissage le plus différent de <span class="math notranslate nohighlight">\(x_\text{new}\)</span> peut le transformer en le point le plus proche est inversement… C’est une grosse limite des modèles précédents. Il faut soit réfléchir à réduire la dimension, soit injecter de la connaissance dans nos modèles, etc.</p>
</div>
</div>
<div class="section" id="x-quelques-modeles-proposes-par-texttt-sklearn">
<h2>X. Quelques modèles proposés par <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span><a class="headerlink" href="#x-quelques-modeles-proposes-par-texttt-sklearn" title="Permalink to this headline">¶</a></h2>
<p>Voici une liste (absolument non exhaustive) de quelques modèles proposés par la librairie <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span>. Certains (pas nécessairement présents dans cette liste) seront approfondis dans les prochaines séquences.</p>
<ul class="simple">
<li><p>Supervisé :</p></li>
<li><p>Régression dans <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> :</p>
<ul>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html">Régression linéaire</a> et ses variantes :</p>
<ul>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html">Ridge</a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html">Lasso</a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html">Elastic-Net</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html">Régression KNN</a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html">Arbre de régression</a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor">Forêts aléatoires</a></p></li>
<li><p>etc.</p></li>
</ul>
</li>
<li><p>Classification (ou assimilés) :</p>
<ul>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html">SVC</a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html">Classification KNN</a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">Régression Logistique</a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html">Arbre de classification</a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">Forêts aléatoires</a></p></li>
</ul>
</li>
<li><p>Non-supervisé :</p>
<ul>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html">K-means</a> (clustering)</p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html">t-SNE</a> (apprentissage de <em>manifold</em>)</p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html">OneclassSVM</a> (détection de nouveauté)</p></li>
</ul>
</li>
</ul>
<p>Pour une liste plus exhaustive des différents outils <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span> merci de consulter le lien suivant  :</p>
<ul class="simple">
<li><p><a class="reference external" href="https://scikit-learn.org/stable/supervised_learning.html">https://scikit-learn.org/stable/supervised_learning.html</a>.</p></li>
</ul>
</div>
<div class="section" id="xi-le-no-free-lunch-theorem">
<h2>XI. Le <em>no-free-lunch</em> Theorem<a class="headerlink" href="#xi-le-no-free-lunch-theorem" title="Permalink to this headline">¶</a></h2>
<p>Nous avons vu plus haut que le modèle <span class="math notranslate nohighlight">\(\texttt{memorize}\)</span> pouvait être parfaitement bon sur les données d’apprentissage mais échouer sur de nouvelles données : il ne généralise pas. Pour cela, nous avons rajouté certaines hypothèses et construit de nouveaux modèles comme le <span class="math notranslate nohighlight">\(\texttt{KNN}\)</span> qui exploite le voisinage d’un point pour pouvoir faire une prédiction. La malédiction de la dimension, malheureusement, pénalise ce fonctionnement dès que la dimension de l’espace d’entrée devient trop grande.</p>
<p>Une question que nous pouvons nous poser est la suivante : pouvons nous construire une règle de classification générique, optimisée à partir d’un jeu de données, qui puisse offrir la garantie de toujours fonctionner ?</p>
<p>La réponse est non, et au-delà de cela, pour chaque modèle que nous allons utiliser, nous devrons faire des hypothèses sur les données. Dans les cas où ces hypothèses seraient falsifiées alors notre stratégie échouera.</p>
<div class="admonition-no-free-lunch-theorem admonition">
<p class="admonition-title"><em>No-free-lunch Theorem</em></p>
<p>Soit <span class="math notranslate nohighlight">\(\mathcal{D}_n=\{(X_i, Y_i)\}_{i\leq n}\)</span> un jeu de données et <span class="math notranslate nohighlight">\(\hat{h}_n:\mathcal{X}\mapsto\mathcal{Y}\)</span> un modèle de classification (un classifieur) construit à partir de <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> selon une règle au choix. L’indice <span class="math notranslate nohighlight">\(n\)</span> montre la dépendence sur la taille de notre jeu de données. Soit <span class="math notranslate nohighlight">\(1/16\geq\{a_n\}_{n&gt;0}&gt;0\)</span> une suite qui converge vers <span class="math notranslate nohighlight">\(0\)</span> mais à une vitesse aussi lente qu’on le veuille. Alors, il existe un problème (i.e. une distribution sur <span class="math notranslate nohighlight">\(X\times Y\)</span>) tel que le meilleur classifieur <span class="math notranslate nohighlight">\(h^\star\)</span> fasse <span class="math notranslate nohighlight">\(0\)</span>, mais que notre estimateur ait une erreur satisfaisant l’inégalité :</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\big[R(h_n)\big]&gt;a_n.\]</div>
</div>
<p>Dit autrement, plus le jeu de données sera grand plus l’erreur sera faible. Mais cette décroissance de l’erreur peut être arbitrairement lente.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./1_what_is_ml"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="0_propos_liminaire.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><em>Machine Learning</em>, initiation</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="2_regression_and_classification_trees.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Les arbres de régression et de classification</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Servajean, Leveau & Chailan<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>
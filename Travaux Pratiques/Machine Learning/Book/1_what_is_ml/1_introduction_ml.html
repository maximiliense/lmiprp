
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Machine learning et malÃ©diction de la dimension &#8212; Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Les arbres de rÃ©gression et de classification" href="2_regression_and_classification_trees.html" />
    <link rel="prev" title="Machine Learning, initiation" href="0_propos_liminaire.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-72WWYCKNK6"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-72WWYCKNK6');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../0_requisite/rappels_python.html">
   Rappels de Python et Numpy
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="0_propos_liminaire.html">
   <em>
    Machine Learning
   </em>
   , initiation
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     <em>
      Machine learning
     </em>
     et malÃ©diction de la dimension
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="2_regression_and_classification_trees.html">
     Les arbres de rÃ©gression et de classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../2_regression/0_propos_liminaire.html">
   La
   <em>
    rÃ©gression
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/1_linear_regression.html">
     La rÃ©gression linÃ©aire â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/2_optimization.html">
     Lâ€™optimisation â˜•ï¸â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/3_interpolation.html">
     Interpolation â˜•ï¸â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/4_algo_proximal_lasso.html">
     Sous-diffÃ©rentiel et le cas du Lasso â˜•ï¸â˜•ï¸â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/5_least_square_qr.html">
     Les moindres carrÃ©s via une dÃ©composition QR (et plus)â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_regression/6_ridge.html">
     Une analyse de la rÃ©gularisation Ridge â˜•ï¸â˜•ï¸â˜•ï¸
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../3_classification/0_propos_liminaire.html">
   La
   <em>
    classification
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/1_logistic_regression.html">
     La rÃ©gression logistique â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/2_fonctions_proxy.html">
     Les fonctions de perte (loss function) â˜•ï¸â˜•ï¸â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/3_bayes_classifier.html">
     Le classifieur de Bayes â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_classification/4_VC_theory.html">
     Un modÃ¨le formel de lâ€™apprentissage â˜•ï¸â˜•ï¸â˜•ï¸â˜•ï¸ (ğŸ’†â€â™‚ï¸)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../4_kernel_methods/0_propos_liminaire.html">
   Les mÃ©thodes Ã  noyaux
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../4_kernel_methods/1_svm.html">
     Le SVM ou lâ€™hypothÃ¨se max-margin â˜•ï¸â˜•ï¸
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5_ensembles/0_propos_liminaire.html">
   Les mÃ©thodes
   <em>
    ensemblistes
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/1_ensembles.html">
     MÃ©thodes ensemblistes â˜•ï¸â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/2_bayesian_linear_regression.html">
     Bayesian linear regression â˜•ï¸â˜•ï¸â˜•ï¸â˜•ï¸
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../6_deeplearning/0_propos_liminaire.html">
   <em>
    deep learning
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/1_autodiff.html">
     La diffÃ©rentiation automatique et un dÃ©but de
     <em>
      deep learning
     </em>
     â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/2_filters_representation.html">
     Filtres et espace de reprÃ©sentation des rÃ©seaux de neurones â˜•ï¸â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/3_probabilities_calibration.html">
     Calibration des probabilitÃ©s et quelques notions â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/4_regularization_deep.html">
     RÃ©gularisation en
     <em>
      deep learning
     </em>
     â˜•ï¸â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/5_transfer_multitask.html">
     <em>
      Transfer learning
     </em>
     et apprentissage multi-tÃ¢ches â˜•ï¸â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_deeplearning/6_adversarial.html">
     Les attaques adversaires â˜•ï¸
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../7_unsupervised/0_propos_liminaire.html">
   Lâ€™apprentissage non-supervisÃ©
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_unsupervised/1_principal_component_analysis.html">
     Lâ€™Analyse en Composantes Principales â˜•ï¸â˜•ï¸â˜•ï¸
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_unsupervised/2_em_gaussin_mixture_model.html">
     ModÃ¨le de MÃ©lange Gaussien et algorithme
     <em>
      Expectation-Maximization
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../8_set_prediction/0_propos_liminaire.html">
   PrÃ©diction dâ€™ensembles
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/1_well_defined.html">
     Jeu dâ€™apprentissage
     <em>
      set-valued
     </em>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_set_prediction/2_ill_defined.html">
     Jeu dâ€™apprentissage uniquement multi-classes â˜•ï¸
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../biblio.html">
   Bibliographie
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/1_what_is_ml/1_introduction_ml.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/maximiliense/lmiprp"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/maximiliense/lmiprp/issues/new?title=Issue%20on%20page%20%2F1_what_is_ml/1_introduction_ml.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/maximiliense/lmiprp/blob/master/Travaux Pratiques/Machine Learning/Book/1_what_is_ml/1_introduction_ml.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#i-introduction">
   I. Introduction
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-l-apprentissage-supervise">
     A. Lâ€™apprentissage supervisÃ©
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-l-apprentissage-non-supervise">
     B. Lâ€™apprentissage non-supervisÃ©
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ii-on-casse-une-idee-preconcue-as">
   II. On casse une idÃ©e prÃ©conÃ§ue (AS)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iii-une-autre-premiere-approche-logique-le-knn-as">
   III. Une autre premiÃ¨re approche logiqueÂ : le KNN (AS)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iv-les-arbres-de-decision-ou-classification-and-regression-tree-cart-as">
   IV. Les arbres de dÃ©cision ou Classification and Regression Tree (CART) (AS)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#v-les-forets-aleatoires-ou-random-forest-rf-as">
   V. Les forÃªts alÃ©atoires ou Random Forest (RF) (AS)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vi-choix-des-hyperparametres-as">
   VI. Choix des hyperparamÃ¨tres (AS)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vii-l-algorithme-des-k-moyennes-ans">
   VII. Lâ€™algorithme des K-Moyennes (ANS)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#viii-la-malediction-de-la-dimension">
   VIII. La malÃ©diction de la dimension
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#en-details">
     En dÃ©tails
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#x-quelques-modeles-proposes-par-texttt-sklearn">
   X. Quelques modÃ¨les proposÃ©s par
   <span class="math notranslate nohighlight">
    \(\texttt{sklearn}\)
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#xi-le-no-free-lunch-theorem">
   XI. Le
   <em>
    no-free-lunch
   </em>
   Theorem
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="machine-learning-et-malediction-de-la-dimension">
<h1><em>Machine learning</em> et malÃ©diction de la dimension<a class="headerlink" href="#machine-learning-et-malediction-de-la-dimension" title="Permalink to this headline">Â¶</a></h1>
<div class="admonition-objectifs-de-la-sequence admonition">
<p class="admonition-title">Objectifs de la sÃ©quence</p>
<ul class="simple">
<li><p>ÃŠtre capable de diffÃ©rencier lâ€™apprentissage supervisÃ© et non supervisÃ©.</p></li>
<li><p>ÃŠtre initiÃ© Ã Â :</p>
<ul>
<li><p>Construire un modÃ¨le Ã  partir dâ€™un jeu de donnÃ©es,</p></li>
<li><p>Ã‰valuer ce modÃ¨le,</p></li>
<li><p>Le sur-apprentissage,</p></li>
<li><p>La malÃ©diction de la dimension.</p></li>
</ul>
</li>
<li><p>ManipulerÂ :</p>
<ul>
<li><p>Quelques modÃ¨les standards de la librairie <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span>.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="i-introduction">
<h2>I. Introduction<a class="headerlink" href="#i-introduction" title="Permalink to this headline">Â¶</a></h2>
<p>Imaginons que nous souhaitions construire une application qui prendrait en entrÃ©e une image de chien ou de chat et qui doive prÃ©dire laquelle des deux espÃ¨ces est reprÃ©sentÃ©e. Imaginons encore une application qui prendrait en entrÃ©e un mail quâ€™elle classifierait comme SPAM ou NONSPAM. Supposons quâ€™il existe deux catÃ©gories de clients quâ€™on ne connait pas <em>a priori</em> et que lâ€™entreprise souhaite prÃ©dire pour chacun des clients sa catÃ©gorie. On peut vouloir prÃ©dire la tempÃ©rature quâ€™il fera demain Ã  partir de donnÃ©es relevÃ©es aujourdâ€™hui.</p>
<p>Une constante est partagÃ©e par lâ€™ensemble de ces scÃ©narios. Il y a tout dâ€™abord une donnÃ©e dâ€™entrÃ©e plus ou moins complexe et structurÃ©e. On notera <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> lâ€™espace auquel elle appartient. Ensuite, Ã  partir de cette donnÃ©e, lâ€™objectif est de faire une prÃ©diction. Notons <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> lâ€™espace auquel appartient cette prÃ©diction. On appelle Ã§a aussi nos labels ou nos variables Ã  expliquer. Notre objectif, en tant que <em>machine learner</em> est de construire une fonction <span class="math notranslate nohighlight">\(h:\mathcal{X}\mapsto\mathcal{Y}\)</span> qui aura de <em>bonnes performances</em> â€œen productionâ€, câ€™est-Ã -dire sur des donnÃ©es nouvelles que nous nâ€™avons jamais vues (i.e. on ne veut pas prÃ©dire la mÃ©tÃ©o dâ€™hier Ã  partir dâ€™avant hier, mais bien de demain Ã  partir dâ€™aujourdâ€™hui).</p>
<p>Deux types dâ€™apprentissage sont gÃ©nÃ©ralement opposÃ©sÂ : lâ€™apprentissage supervisÃ© (AS) et non-supervisÃ© (ANS).</p>
<div class="section" id="a-l-apprentissage-supervise">
<h3>A. Lâ€™apprentissage supervisÃ©<a class="headerlink" href="#a-l-apprentissage-supervise" title="Permalink to this headline">Â¶</a></h3>
<p>Lâ€™apprentissage supervisÃ© part du principe que (1) nos labels (i.e. lâ€™espace <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>) est bien dÃ©fini et (2) que nous avons accÃ¨s Ã  des donnÃ©es associant des Ã©lÃ©ments de <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> Ã  leur label <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>.</p>
<p>On parlera de problÃ¨me de rÃ©gression si par exemple <span class="math notranslate nohighlight">\(\mathcal{Y}\subseteq\mathbb{R}\)</span> ou de problÃ¨me de classification si <span class="math notranslate nohighlight">\(\mathcal{Y}=\{1, \ldots, C\}\)</span> oÃ¹ lâ€™ordre nâ€™est pas important. Par exemple, prÃ©dire la tempÃ©rature est un problÃ¨me de rÃ©gression alors que prÃ©dire si la photo reprÃ©sente un chien ou un chat est un problÃ¨me de classification.</p>
<p>A fortiori, toutes les observations dans <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> ne sont pas nÃ©cessairement Ã©quiprobables. Certains clients ont peut-Ãªtre, par exemple, un profil plus commun que dâ€™autres. Afin de pouvoir dÃ©finir plus rigoureusement ce quâ€™on entend par <em>bonnes performances</em>, notons <span class="math notranslate nohighlight">\(X\in\mathcal{X}\)</span> une variable alÃ©atoire qui dÃ©crit nos donnÃ©es observÃ©es et <span class="math notranslate nohighlight">\(Y\in\mathcal{Y}\)</span> la variable alÃ©atoire associÃ©e Ã  nos labels. Assez naÃ¯vement, notons <span class="math notranslate nohighlight">\(\mathbb{P}\)</span> la loi de notre couple <span class="math notranslate nohighlight">\(X,Y\)</span>Â :</p>
<div class="math notranslate nohighlight">
\[X, Y\sim \mathbb{P}.\]</div>
<p>Notons <span class="math notranslate nohighlight">\(r:\mathcal{Y}\times\mathcal{Y}\rightarrow\mathbb{R}^+\)</span> une mesure dâ€™erreur, un risque Ã©lÃ©mentaire. On a par exemple, dans le cas dâ€™un problÃ¨me de rÃ©gression, lâ€™erreur quadratiqueÂ :</p>
<div class="math notranslate nohighlight">
\[r(\hat{y}, y)=(\hat{y}-y)^2,\]</div>
<p>oÃ¹ <span class="math notranslate nohighlight">\(\hat{y}\)</span> est la prÃ©diction que ferait notre modÃ¨le. Ou encore, dans le cas de la classification nous pouvons avoir cette fois-ci lâ€™erreur <span class="math notranslate nohighlight">\(0.1\)</span>Â :</p>
<div class="math notranslate nohighlight">
\[r(\hat{y}, y)=\textbf{1}\{\hat{y}\neq y\},\]</div>
<p>qui vaut <span class="math notranslate nohighlight">\(1\)</span> si la prÃ©diction est mauvaise ou <span class="math notranslate nohighlight">\(0\)</span> sinon.</p>
<p>Notre objectif est tout naturellement de trouver une application <span class="math notranslate nohighlight">\(h:\mathcal{X}\mapsto\mathcal{Y}\)</span> telle que <span class="math notranslate nohighlight">\(R(h)=\mathbb{E}\big[r(h(X), Y)\big]\)</span> est petit. On veut un bon modÃ¨le sur de nouvelles donnÃ©es. Lâ€™idÃ©e va Ãªtre de collecter des donnÃ©es reprÃ©sentatives (dans le sens iid) et de construire notre modÃ¨le avec ces derniÃ¨res. NotonsÂ :</p>
<div class="math notranslate nohighlight">
\[S_n=\{(X_i, Y_i)\}_{i\leq n}\]</div>
<p>un jeu de donnÃ©es de taille <span class="math notranslate nohighlight">\(n\)</span>.</p>
</div>
<div class="section" id="b-l-apprentissage-non-supervise">
<h3>B. Lâ€™apprentissage non-supervisÃ©<a class="headerlink" href="#b-l-apprentissage-non-supervise" title="Permalink to this headline">Â¶</a></h3>
<p>Ici, câ€™est lâ€™inverse. Nous avons accÃ¨s Ã  lâ€™espace des observations <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> duquel on peut collecter des donnÃ©es (toujours selon la loi de la variable alÃ©atoire <span class="math notranslate nohighlight">\(X\)</span>). On sait quâ€™il existe un espace <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> cible mais (1) il nâ€™est pas nÃ©cessairement connu et (2) nous ne connaissons pas dâ€™exemple de liens entre exemples dâ€™apprentissage et cibles associÃ©es.</p>
<p>Par exemple, si <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> reprÃ©sente des donnÃ©es clients, on peut savoir (se douter) quâ€™il existe des groupes de clients qui se ressemblent mais ne pas les connaÃ®tre et ne pas savoir combien il y en a. Il sâ€™agit ici dâ€™une tÃ¢che de <em>clustering</em> oÃ¹ on cherche Ã  regrouper des donnÃ©es entre-elles toujours de maniÃ¨re Ã  ce que le regroupement se gÃ©nÃ©ralise Ã  de nouvelles donnÃ©es.</p>
<p>On peut chercher Ã  transformer nos donnÃ©es dans <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> dans un espace quâ€™on notera cette fois-ci <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span> oÃ¹ ces derniÃ¨res auront de meilleures propriÃ©tÃ©. On note cet â€œespace de reprÃ©sentationâ€ <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span> et non <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> car il sâ€™agit souvent dâ€™une Ã©tape intermÃ©diaire avant une tÃ¢che supervisÃ©e oÃ¹ on chercherait Ã  prÃ©dire un label dans <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>. Câ€™est ce quâ€™on appelle lâ€™apprentissage de reprÃ©sentation. Ainsi, si <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> est lâ€™ensemble des photos de chiens et de chats, <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span> est lâ€™ensemble de ces derniÃ¨res oÃ¹ on a mis â€œdâ€™un cÃ´tÃ©â€ les photos de chiens et de â€œlâ€™autreâ€ celles de chats. Il devient simple de construire une tÃ¢che supervisÃ©e permettant de prÃ©dire le bon label â€œchien/chatâ€.</p>
</div>
</div>
<div class="section" id="ii-on-casse-une-idee-preconcue-as">
<h2>II. On casse une idÃ©e prÃ©conÃ§ue (AS)<a class="headerlink" href="#ii-on-casse-une-idee-preconcue-as" title="Permalink to this headline">Â¶</a></h2>
<p>Soit <span class="math notranslate nohighlight">\(S=\{(x_i, y_i)\}_{i\leq n}\)</span> un jeu de donnÃ©es reprÃ©sentatif de taille <span class="math notranslate nohighlight">\(n\)</span>. Un modÃ¨le trÃ¨s performant sur ces donnÃ©es est-il performant sur des donnÃ©es nouvelles ? RÃ©flÃ©chissez quelques instants et Ã©crivez votre rÃ©ponse dans un coin.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">metrics</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<p>Chargeons et affichons notre jeu de donnÃ©es. Ce dernier consiste en des chiffres Ã©crits Ã  la main. Lâ€™objectif va Ãªtre de faire un modÃ¨le qui permet de prÃ©dire ces derniers.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">digits</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_digits</span><span class="p">()</span>

<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">,</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray_r</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Training: </span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">label</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_introduction_ml_8_0.png" src="../_images/1_introduction_ml_8_0.png" />
</div>
</div>
<p>Construisons notre premier modÃ¨le. Ce dernier correspond Ã  la fonction mathÃ©matique suivanteÂ :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}h(x)=\begin{cases}y&amp;\text{ si } (x, y)\in S_n\\\text{alÃ©atoire}()&amp;\text{ sinon.}\end{cases}\end{aligned}\end{split}\]</div>
<p>On retourne le label connu si on a dÃ©jÃ  vu le â€œpointâ€ et on retourne un label au hasard sinon.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Memorize</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
        
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">memorized</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
                <span class="c1"># On compare les pixels un a un</span>
                <span class="c1"># et on regarde s&#39;ils sont tous identiques</span>
                <span class="c1"># pixel wise comparison</span>
                <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">==</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                    <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
                    <span class="n">memorized</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="k">break</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">memorized</span><span class="p">:</span>
                <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<p>Le code suivant prÃ©pare nos donnÃ©es (des images) afin quâ€™elles deviennent facilement manipulables par notre modÃ¨le.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># flatten the images</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Split data into 50% train and 50% test subsets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Les donnÃ©es de test nous permettront de tester notre modÃ¨le sur des donnÃ©es quâ€™il nâ€™a pas utilisÃ© pour se construire. Cela nous permet de tester notre modÃ¨le sur de â€œnouvelles donnÃ©esâ€. Câ€™est son pouvoir de gÃ©nÃ©ralisation qui nous intÃ©resse.</p>
<div class="tip admonition">
<p class="admonition-title">Ã‰valuation dâ€™un modÃ¨le</p>
<p>Il nâ€™existe pas une seule maniÃ¨re dâ€™Ã©valuer les performances dâ€™un modÃ¨le. En voici quelques-une.</p>
<p><em>PrÃ©cision</em></p>
<p>ConsidÃ©rons une tÃ¢che de classification (<span class="math notranslate nohighlight">\(y=\{1,\ldots, C\}\)</span>), la prÃ©cision dâ€™un modÃ¨le <span class="math notranslate nohighlight">\(h\)</span> pour la classe <span class="math notranslate nohighlight">\(c\)</span> est</p>
<div class="math notranslate nohighlight">
\[\text{prec}_c(h)=\frac{\sum_i \textbf{1}\{y_i=c, h(x_i)=y_i\}}{\sum_i \textbf{1}\{h(x_i)=c\}}\]</div>
<p>On veut que le score soit proche de <span class="math notranslate nohighlight">\(1\)</span>.</p>
<hr class="docutils" />
<p><em>Rappel</em></p>
<p>ConsidÃ©rons une tÃ¢che de classification (<span class="math notranslate nohighlight">\(y=\{1,\ldots, C\}\)</span>), le rappel dâ€™un modÃ¨le <span class="math notranslate nohighlight">\(h\)</span> pour la classe <span class="math notranslate nohighlight">\(c\)</span> est</p>
<div class="math notranslate nohighlight">
\[\text{rec}_c(h)=\frac{\sum_i \textbf{1}\{y_i=c, h(x_i)=y_i\}}{\sum_i \textbf{1}\{y_i=c\}}\]</div>
<p>On veut que le score soit proche de <span class="math notranslate nohighlight">\(1\)</span>.</p>
<hr class="docutils" />
<p><em>Score f1</em></p>
<p>Ce dernier combine le rappel et la prÃ©cisionÂ :</p>
<div class="math notranslate nohighlight">
\[\text{f1}_c(h)=2\frac{\text{prec}_c(h)\cdot\text{rec}_c(h)}{\text{prec}_c(h)+\text{rec}_c(h)}\]</div>
<p>On veut que le score soit proche de <span class="math notranslate nohighlight">\(1\)</span>.</p>
<hr class="docutils" />
<p><em>Lâ€™accuracy</em></p>
<p>Câ€™est une sorte de gÃ©nÃ©ralisation de la prÃ©cisionÂ :</p>
<div class="math notranslate nohighlight">
\[\text{acc}(h)=\frac{\sum_i\textbf{1}\{h(x_i)=y_i\}}{n},\]</div>
<p>câ€™est le ratio de bonnes prÃ©dictions.</p>
<hr class="docutils" />
<p><em>Le support</em></p>
<p>Câ€™est le nombre de points de notre jeu de donnÃ©es qui sont concernÃ©es par la mÃ©trique.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Memorize</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>On commence par tester les performances de notre modÃ¨le sur notre jeu dâ€™apprentissage.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Classification report for classifier Memorize on train:</span><span class="se">\n</span><span class="s1">&#39;</span>
      <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Classification report for classifier Memorize on train:
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        90
           1       1.00      1.00      1.00        91
           2       1.00      1.00      1.00        91
           3       1.00      1.00      1.00        92
           4       1.00      1.00      1.00        89
           5       1.00      1.00      1.00        91
           6       1.00      1.00      1.00        90
           7       1.00      1.00      1.00        90
           8       1.00      1.00      1.00        86
           9       1.00      1.00      1.00        88

    accuracy                           1.00       898
   macro avg       1.00      1.00      1.00       898
weighted avg       1.00      1.00      1.00       898
</pre></div>
</div>
</div>
</div>
<p>Notre modÃ¨le est parfait ! Aucune erreur. On ne peut pas faire mieux ! Et du cÃ´tÃ© du test ?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Classification report for classifier Memorize on test:</span><span class="se">\n</span><span class="s1">&#39;</span>
      <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Classification report for classifier Memorize on test:
              precision    recall  f1-score   support

           0       0.11      0.10      0.10        88
           1       0.10      0.09      0.09        91
           2       0.08      0.09      0.09        86
           3       0.13      0.11      0.12        91
           4       0.06      0.05      0.06        92
           5       0.07      0.07      0.07        91
           6       0.09      0.09      0.09        91
           7       0.09      0.10      0.10        89
           8       0.13      0.14      0.13        88
           9       0.11      0.14      0.13        92

    accuracy                           0.10       899
   macro avg       0.10      0.10      0.10       899
weighted avg       0.10      0.10      0.10       899
</pre></div>
</div>
</div>
</div>
<p>Câ€™est ridiculement mauvais : on se trompe neuf fois sur dix, soit exactement ce quâ€™on attendrait dâ€™une rÃ©ponse alÃ©atoire.</p>
<p>Oui mais on a fait exprÃ¨s de construire le modÃ¨le de cette maniÃ¨re ! En rÃ©alitÃ©, il existe une infinitÃ© de fonctions, paramÃ©triques ou non, quâ€™on peut rendre aussi bonne quâ€™on veut sur nos donnÃ©es mais qui seraient particuliÃ¨rement mauvaises sur de nouvelles donnÃ©es (cela inclut les modÃ¨les usuels et câ€™est pour cela quâ€™on a besoin dâ€™experts !)â€¦ Toute la difficultÃ© du <em>machine learner</em> va Ãªtre de contrÃ´ler cela.</p>
<div class="note admonition">
<p class="admonition-title"><span class="math notranslate nohighlight">\(\texttt{memorize}\)</span> en pratique</p>
<p>Cette approche nâ€™est pas totalement absurde. Si <span class="math notranslate nohighlight">\(|\mathcal{X}|&lt;\infty\)</span>, ou si <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> est discret, alors plus on collectera de donnÃ©es plus les nouvelles donnÃ©es auront dÃ©jÃ  Ã©tÃ© vues et nos prÃ©dictions deviendront intÃ©ressantes. Câ€™est exactement ce que nous faisons face Ã  un nouveau paquet de bonbons dont nous ne connaissons pas le goÃ»t.</p>
</div>
</div>
<div class="section" id="iii-une-autre-premiere-approche-logique-le-knn-as">
<h2>III. Une autre premiÃ¨re approche logiqueÂ : le KNN (AS)<a class="headerlink" href="#iii-une-autre-premiere-approche-logique-le-knn-as" title="Permalink to this headline">Â¶</a></h2>
<p>Intuitivement, on a envie de dire que nos donnÃ©es ne sont pas complÃ¨tement dÃ©structurÃ©es. Deux clients trÃ¨s similaires achÃ¨teront trÃ¨s probablement des produits trÃ¨s similaires. Un <em>trois</em> ressemble plus Ã  un <em>trois</em> quâ€™Ã  un <em>cinq</em> et un <em>cinq</em> ressemble plus Ã  un <em>cinq</em> quâ€™Ã  un <em>trois</em>. Finalement, on gÃ©nÃ©ralise un petit peu lâ€™exemple prÃ©cÃ©dent. Au lieu de rÃ©pondre alÃ©atoirement si je ne connais pas la donnÃ©e, je cherche lâ€™exemple le plus proche et je prÃ©dis le mÃªme label ! Plus rigoureusement, notre modÃ¨le de prÃ©diction fonctionne comme suitÂ :</p>
<div class="math notranslate nohighlight">
\[\hat{y}_\text{new}=y\text{ avec }(x, y)=\text{argmin}_{(x, y)\in S}\lVert x-x_{\text{new}}\rVert_2.\]</div>
<p>On peut imaginer que si plusieurs points sont Ã©quidistants, la rÃ©ponse se fait alÃ©atoirement entre les labels possibles.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Utilisez lâ€™objet <span class="math notranslate nohighlight">\(\texttt{KNeighborsClassifier}\)</span> avec le paramÃ¨tre <span class="math notranslate nohighlight">\(\texttt{n}\_\texttt{neighbors=1}\)</span> et entraÃ®nez le sur <span class="math notranslate nohighlight">\(\texttt{X}\_\texttt{train}\)</span>.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="n">model</span> <span class="o">=</span> 
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="c1">###############################################################</span>

<span class="n">predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Classification report for classifier </span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s1"> on train:</span><span class="se">\n</span><span class="s1">&#39;</span>
      <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>On a testÃ© le modÃ¨le sur le jeu dâ€™apprentissage et on est toujours aussi bon sur le jeu dâ€™apprentissage ! Cependant, câ€™est attendu car lâ€™image qui ressemble le plus Ã  une autre est lâ€™image elle-mÃªme. PrÃ©disons maintenant sur le test.</p>
<div class="tip admonition">
<p class="admonition-title">La matrice de confusion</p>
<p>Les mÃ©triques prÃ©cÃ©dentes nous donnent les performances du modÃ¨le en moyenne ainsi que par classe. Cependant, cela ne nous donne que trÃ¨s peu dâ€™information quant au type dâ€™erreur quâ€™il fait. La <strong>matrice de confusion</strong> permet de comprendre le type dâ€™erreurs que fait notre modÃ¨le. La cellule <span class="math notranslate nohighlight">\(i,j\)</span> indique le nombre dâ€™Ã©lÃ©ments de la classe <span class="math notranslate nohighlight">\(i\)</span> qui ont Ã©tÃ© prÃ©dits de la classe <span class="math notranslate nohighlight">\(j\)</span>.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Classification report for classifier </span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s1"> on test:</span><span class="se">\n</span><span class="s1">&#39;</span>
      <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">figure_</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Is Machine learning solved ? Minute papillon ! Ce modÃ¨le est trÃ¨s sensible au bruit ! Supposons quâ€™une de nos donnÃ©es soit bruitÃ©e (e.g. un 3 qui ressemble Ã  un 8). Si une nouvelle donnÃ©e reprÃ©sentant un <span class="math notranslate nohighlight">\(8\)</span> se retrouve Ã  cÃ´tÃ© de cette anomalie, elle sera mal prÃ©dite. Nous pouvons adresser cette limite de la maniÃ¨re suivanteÂ : au lieu de regarder le point le plus proche, on regarde les <span class="math notranslate nohighlight">\(k\)</span> points les plus proches et on fait un vote Ã  la majoritÃ©. Plus formellement la prÃ©diction est faite comme suitÂ :</p>
<div class="math notranslate nohighlight">
\[\hat{y}_\text{new}=\text{majority$\_$voting}(\texttt{KNN}.\texttt{labels})\text{  oÃ¹  }\texttt{KNN}=\text{argmin}_{S^\prime\subset S,\ |S^\prime|=K}\sum_i \lVert x_i- x_{\text{new}}\rVert.\]</div>
<p>Dans le cas oÃ¹ on chercherait Ã  faire une rÃ©gression, on remplace le vote Ã  la majoritÃ© par une moyenne.</p>
<p>RÃ©cupÃ©rons le jeu de donnÃ©es <span class="math notranslate nohighlight">\(\texttt{iris}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="c1"># dÃ©commentez la ligne suivante pour obtenir des informations</span>
<span class="c1"># sur le dataset iris.</span>
<span class="c1"># print(iris.DESCR)</span>
</pre></div>
</div>
</div>
</div>
<p>De la mÃªme maniÃ¨re que prÃ©cÃ©demment, on construit notre jeu dâ€™apprentissage pour construire notre modÃ¨le et notre jeu de test pour en tester les performances.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Utilisez lâ€™objet <span class="math notranslate nohighlight">\(\texttt{KNeighborsClassifier}\)</span> avec le paramÃ¨tre <span class="math notranslate nohighlight">\(\texttt{n}\_\texttt{neighbors=1}\)</span> et entraÃ®nez le sur <span class="math notranslate nohighlight">\(\texttt{X}\_\texttt{train}\)</span>. Faites une prÃ©diction sur <span class="math notranslate nohighlight">\(\texttt{predicted=X}\_\texttt{test}\)</span>.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="n">model</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="o">...</span>

<span class="n">predicted</span> <span class="o">=</span> <span class="o">...</span> 
<span class="c1">###############################################################</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">figure_</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Les performances sont dÃ©jÃ  trÃ¨s bonnes ! Mais il est possible de gagner un tout petit peu de performances en considÃ©rant plus de voisins :</p>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Utilisez lâ€™objet <span class="math notranslate nohighlight">\(\texttt{KNeighborsClassifier}\)</span> avec le paramÃ¨tre <span class="math notranslate nohighlight">\(\texttt{n}\_\texttt{neighbors=10}\)</span> et entraÃ®nez le sur <span class="math notranslate nohighlight">\(\texttt{X}\_\texttt{train}\)</span>. Faites une prÃ©diction sur <span class="math notranslate nohighlight">\(\texttt{predicted=X}\_\texttt{test}\)</span>.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="n">model</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="o">...</span>

<span class="n">predicted</span> <span class="o">=</span> <span class="o">...</span>
<span class="c1">###############################################################</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">figure_</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="iv-les-arbres-de-decision-ou-classification-and-regression-tree-cart-as">
<h2>IV. Les arbres de dÃ©cision ou Classification and Regression Tree (CART) (AS)<a class="headerlink" href="#iv-les-arbres-de-decision-ou-classification-and-regression-tree-cart-as" title="Permalink to this headline">Â¶</a></h2>
<p>De la mÃªme maniÃ¨re que pour lâ€™algorithme KNN, on supposera ici que nos donnÃ©es admettent une certaine structure et que des points proches possÃ¨dent probablement le mÃªme label ou une prediction proche dans le cas de la rÃ©gression. Ici, Ã  la diffÃ©rence du KNN, la notion de voisinage se construit au travers dâ€™hyperrectangles parallÃ¨les aux axes. Afin de bien comprendre le fonctionnement, supposons que notre arbre de dÃ©cision soit dÃ©jÃ  construit et soit celui reprÃ©sentÃ© par la figure ci-dessous. Prenons une nouvelle donnÃ©eÂ :</p>
<div class="math notranslate nohighlight">
\[x_{\text{new}}=[\text{ecoute:}1, \text{math:}12,\text{info:}18]^T,\]</div>
<p>et partons de la racine de notre arbre. Cette racine possÃ¨de deux branches sortantes. Le choix de la branche se fait Ã  partir dâ€™un critÃ¨re sur une des variables explicatives de <span class="math notranslate nohighlight">\(x\)</span>. Dans notre exemple la variable explicative est lâ€™Ã©coute en cours. Si lâ€™Ã©tudiant Ã©coute, on prend la branche de droite, sinon celle de gauche. Dans notre, ce sera donc la branche de droite. La nouvelle variable Ã  regarder est la note de math et sa valeur doit Ãªtre supÃ©rieure Ã  10. Câ€™est notre cas et nous prenons la branche de droite. Nous sommes Ã  une feuille et la prÃ©diction Ã  faire est â€œoui, lâ€™Ã©tudiant sâ€™en sortiraâ€.</p>
<p><img alt="Decision tree" src="https://raw.githubusercontent.com/maximiliense/lmiprp/main/Travaux%20Pratiques/Machine%20Learning/Introduction/data/Introduction/cart.jpg" /></p>
<p>Le choix de la rÃ¨gle de dÃ©cision Ã  chaque noeud peut Ãªtre adaptÃ© afin dâ€™obtenir des rÃ©gions Ã  la gÃ©omÃ©trie variable. La construction dâ€™un arbre se fait en partant de la racine vers les feuilles et en choisissant intÃ©rativement les variables explicatives qui ont le plus dâ€™effet sur notre prÃ©diction (via diverses critÃ¨res).</p>
<div class="note admonition">
<p class="admonition-title">Les feuilles de lâ€™arbre</p>
<p>En pratique, les feuilles de notre arbre contiennent les exemples dâ€™apprentissage de notre jeu de donnÃ©es. La prÃ©diction dÃ©pend des exemples contenus dans la feuille qui nous concerne.</p>
</div>
<p>La sÃ©quence suivante abordera plus en dÃ©tails les arbres de classification et de rÃ©gression.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Utilisez lâ€™objet <span class="math notranslate nohighlight">\(\texttt{DecisionTreeClassifier}\)</span> avec le paramÃ¨tre <span class="math notranslate nohighlight">\(\texttt{max}\_\texttt{depth=2}\)</span> et entraÃ®nez le sur <span class="math notranslate nohighlight">\(\texttt{X}\_\texttt{train}\)</span>. Faites une prÃ©diction sur <span class="math notranslate nohighlight">\(\texttt{predicted=X}\_\texttt{test}\)</span>.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="n">model</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="o">...</span>

<span class="n">predicted</span> <span class="o">=</span> <span class="o">...</span>
<span class="c1">###############################################################</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">figure_</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="v-les-forets-aleatoires-ou-random-forest-rf-as">
<h2>V. Les forÃªts alÃ©atoires ou Random Forest (RF) (AS)<a class="headerlink" href="#v-les-forets-aleatoires-ou-random-forest-rf-as" title="Permalink to this headline">Â¶</a></h2>
<p>Les arbres de dÃ©cision peuvent Ãªtre sujets au surapprentissage. Une maniÃ¨re de compenser le problÃ¨me est dâ€™en construire plusieurs oÃ¹ chaque arbre est construit en ne voyant quâ€™une partie des donnÃ©es. Enfin leurs prÃ©dictions sont aggrÃ©gÃ©es. Ces approches sont gÃ©nÃ©ralement beaucoup plus performantes que les arbres simples. Malheureusement, autant avec un arbre simple on pouvait essayer de comprendre la prÃ©diction, autant ici, cela devient difficile.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Utilisez lâ€™objet <span class="math notranslate nohighlight">\(\texttt{RandomForestClassifier}\)</span> et entraÃ®nez le sur <span class="math notranslate nohighlight">\(\texttt{X}\_\texttt{train}\)</span>. Faites une prÃ©diction sur <span class="math notranslate nohighlight">\(\texttt{predicted=X}\_\texttt{test}\)</span>.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="c1">###############################################################</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">figure_</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="vi-choix-des-hyperparametres-as">
<h2>VI. Choix des hyperparamÃ¨tres (AS)<a class="headerlink" href="#vi-choix-des-hyperparametres-as" title="Permalink to this headline">Â¶</a></h2>
<p>Pour les modÃ¨les prÃ©cÃ©dents, nous avons dÃ» choisir diffÃ©rents paramÃ¨tres qui affectaient les performances de notre modÃ¨le. Nous les avons choisis en regardant les performances de notre modÃ¨le sur le jeu de test. Cependant, les bonnes performances Ã©taient peut-Ãªtre un coup de chance !</p>
<p>Il existe deux stratÃ©gies dâ€™Ã©valuation sans biais de la qualitÃ© de notre modÃ¨leÂ :</p>
<ul class="simple">
<li><p>La validation non croisÃ©e oÃ¹ une partie de notre jeu de donnÃ©es est cachÃ©e pendant lâ€™apprentissage puis utilisÃ©e afin dâ€™Ã©valuer les performances du modÃ¨le. Il sâ€™agit du dÃ©coupage train/test. Cette stratÃ©gie est un estimateur sans biais de la qualitÃ© de notre modÃ¨le mais possÃ¨de une variance plus forte que la validation croisÃ©e. Elle peut-Ãªtre particuliÃ¨rement utile lorsque le coup dâ€™apprentissage dâ€™un modÃ¨le est trÃ¨s Ã©levÃ© (e.g. <em>deep learning</em>)</p></li>
<li><p>La validation croisÃ©e oÃ¹ notre jeu de donnÃ©es est divisÃ© en <em>k</em> parties (on parle aussi de <em>k-fold</em>). Ã‰videmment, <span class="math notranslate nohighlight">\(k\in\{2, ..., n\}\)</span> oÃ¹ <span class="math notranslate nohighlight">\(n\)</span> est la taille du jeu de donnÃ©es. Chacune des parties jouera successivement le rÃ´le de jeu de test pendant que les <span class="math notranslate nohighlight">\(k-1\)</span> autres parties serviront Ã  calculer notre modÃ¨le. Le rÃ©sultat de cette procÃ©dure est un vecteur de <span class="math notranslate nohighlight">\(k\)</span> scores dont on peut calculer la moyenne, la variance, etc.</p></li>
</ul>
<p>On peut illustrer la mÃ©thode des <em>k-folds</em> via lâ€™exemple suivantÂ :</p>
<div class="math notranslate nohighlight">
\[\begin{align*}
\text{Appartient au train set: } \color{red}{\boxed{}}&amp;\text{ et appartient au test set: }\color{green}{\boxed{}}
\end{align*}\]</div>
<div class="math notranslate nohighlight">
\[\begin{align*}
\text{Step 1: }\color{green}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align*}\]</div>
<div class="math notranslate nohighlight">
\[\begin{align*}
\text{Step 2: }\color{red}{\boxed{}}\color{green}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align*}\]</div>
<div class="math notranslate nohighlight">
\[\begin{align*}
\text{Step 3: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{green}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align*}\]</div>
<div class="math notranslate nohighlight">
\[\begin{align*}
\text{Step 4: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{green}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align*}\]</div>
<div class="math notranslate nohighlight">
\[\begin{align*}
\text{Step 5: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{green}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align*}\]</div>
<div class="math notranslate nohighlight">
\[\begin{align*}
\text{Step 6: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{green}{\boxed{}}\color{red}{\boxed{}}
\end{align*}\]</div>
<div class="math notranslate nohighlight">
\[\begin{align*}
\text{Step 7: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{green}{\boxed{}}
\end{align*}\]</div>
<div class="warning admonition">
<p class="admonition-title">Overfitter Ã  la main</p>
<p>Attention, quand on fait quelques tests Ã  la main et que lâ€™on Ã©value nos performances sur le jeu de test, on est dÃ©jÃ  entrain de faire de la sÃ©lection de modÃ¨le. On se sert alors du test comme dâ€™un ensemble de validation. Câ€™est dâ€™autant plus vrai lorsquâ€™on a peu de donnÃ©es dâ€™apprentissage.</p>
</div>
<p>La mÃ©thode <span class="math notranslate nohighlight">\(\texttt{cross_val_score}\)</span> de <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span> permet de rÃ©aliser cette procÃ©dure. On pourra renseigner le paramÃ¨tre <span class="math notranslate nohighlight">\(\texttt{cv}\)</span> qui indique le nombre <span class="math notranslate nohighlight">\(k\)</span> et le paramÃ¨tre <span class="math notranslate nohighlight">\(\texttt{scoring}\)</span> qui donne la mÃ©trique que lâ€™on souhaite calculer.</p>
<p>Si on cherche Ã  trouver une valeur dâ€™un hyper-paramÃ¨tre du modÃ¨le, lâ€™objet <span class="math notranslate nohighlight">\(\texttt{ğ™¶ğš›ğš’ğšğš‚ğšğšŠğš›ğšŒğš‘ğ™²ğš…}\)</span> applique une validation croisÃ©e en cherchant diffÃ©rentes valeurs de cet hyper-paramÃ¨tre.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Utilisez lâ€™objet <span class="math notranslate nohighlight">\(\texttt{GridSearchCV}\)</span> pour faire une recherche par grille sur le modÃ¨le <span class="math notranslate nohighlight">\(\texttt{RandomForestClassifier}\)</span> et entraÃ®nez le sur <span class="math notranslate nohighlight">\(\texttt{X}\_\texttt{train}\)</span>.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="c1">###############################################################</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">figure_</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Notre RandomForest de base Ã©tait dÃ©jÃ  trÃ¨s bon !</p>
</div>
<div class="section" id="vii-l-algorithme-des-k-moyennes-ans">
<h2>VII. Lâ€™algorithme des K-Moyennes (ANS)<a class="headerlink" href="#vii-l-algorithme-des-k-moyennes-ans" title="Permalink to this headline">Â¶</a></h2>
<p>Il sâ€™agit ici dâ€™un algorithme non supervisÃ©. Imaginons que nous ayons collectÃ© un jeu de donnÃ©es <span class="math notranslate nohighlight">\(S_n=\{(X_i)\}_{i\leq n}\)</span>. On sait quâ€™il existe des groupes dans nos donnÃ©es. Supposons mÃªme quâ€™on sÃ¢che quâ€™il existe <span class="math notranslate nohighlight">\(K\)</span> groupes. Lâ€™idÃ©e de lâ€™algorithme des K-Moyennes va Ãªtre de dÃ©tecter ces <span class="math notranslate nohighlight">\(K\)</span> groupes en trouvant une solution au problÃ¨me dâ€™optimisation suivantÂ :</p>
<div class="math notranslate nohighlight">
\[\text{KMeans}=\text{argmin}_{m_1, \ldots, m_K\in\mathcal{X}, c_1,\ldots,c_n\in\{1,\ldots,K\}}\sum_{i=1}^K\sum_{j=1}^n\textbf{1}\{c_j=i\}\lVert m_i-x_j\rVert_2=\text{argmin}_{m_1, \ldots, m_K\in\mathcal{X}, c_1,\ldots,c_n\in\{1,\ldots,K\}}\sum_{j=1}^n\lVert x_j-m_{c_j}\rVert_2.\]</div>
<p>Dit autrement, chaque groupe est reprÃ©sentÃ© par une coordonnÃ©e <span class="math notranslate nohighlight">\(m_i\)</span> (qui sâ€™avÃ¨re Ãªtre la moyenne des Ã©lÃ©ments du groupe) et chaque Ã©lÃ©ment de notre jeu de donnÃ©es nâ€™est associÃ© quâ€™Ã  un seul groupe <span class="math notranslate nohighlight">\(c_j\)</span>.</p>
<p>ConsidÃ©rons le jeu de donnÃ©es suivant.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">sample_data</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">]])</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="mi">15</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">means</span>
    <span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="o">//</span><span class="n">n</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="o">*</span><span class="n">n</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
    
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_data</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="k">def</span> <span class="nf">plot_clusters</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">means</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Nos donnÃ©es et leur label inconnu&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">means</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">means</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="p">[</span><span class="n">m</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="n">s</span><span class="o">=</span><span class="mi">55</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">path</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">p</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plot_clusters</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_introduction_ml_60_0.png" src="../_images/1_introduction_ml_60_0.png" />
</div>
</div>
<p>Le problÃ¨me de K-Means est NP-Difficile. Pour cela, nous utilisons en pratique un heuristique appelÃ© â€œalgorithme de LLoydâ€ qui fonctionnde la maniÃ¨re suivanteÂ :</p>
<ol class="simple">
<li><p>On initialise les k moyennes</p></li>
<li><p>On assigne tous nos points Ã  leur moyenne la plus proche</p></li>
<li><p>On met Ã  jour les moyennes avec les nouveaux points</p></li>
<li><p>Si le dÃ©placement des moyennes est significatif, on reprend Ã  lâ€™Ã©tape 2</p></li>
</ol>
<div class="tip admonition">
<p class="admonition-title">Fixer le <span class="math notranslate nohighlight">\(k\)</span></p>
<p>Le <span class="math notranslate nohighlight">\(k\)</span> peut Ãªtre fixÃ© via une selection de paramÃ¨tres et une validation croisÃ©e.</p>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Le problÃ¨me est NP-Difficile et un algorithme permettant de le rÃ©soudre est lâ€™algorithme de LLoyd. ImplÃ©mentez le.</strong></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">KMeans</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.001</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1">####### Complete this part ######## or die ####################</span>
        <span class="c1"># K means initialization</span>
        <span class="o">...</span>
        <span class="c1"># Step 1</span>
        <span class="o">...</span>
        <span class="c1"># Step 2</span>
        <span class="o">...</span>
        <span class="c1">###############################################################</span>
        <span class="k">return</span> <span class="n">assignment</span><span class="p">,</span> <span class="n">means</span><span class="p">,</span> <span class="n">path</span>
            

<span class="n">model</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plot_clusters</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="o">*</span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</pre></div>
</div>
<p>On se rend compte quâ€™on arrive Ã  retrouver les 3 groupes automatiquement (Les couleurs sont celles calculÃ©es par notre modÃ¨le des K-Moyennes) !</p>
</div>
<div class="section" id="viii-la-malediction-de-la-dimension">
<h2>VIII. La malÃ©diction de la dimension<a class="headerlink" href="#viii-la-malediction-de-la-dimension" title="Permalink to this headline">Â¶</a></h2>
<p>Nous avons pu observer des scÃ©narios oÃ¹ lâ€™erreur sur notre jeu de donnÃ©es dâ€™apprentissage Ã©tait <span class="math notranslate nohighlight">\(0\)</span> alors que notre modÃ¨le nâ€™Ã©tait pas si bon que cela sur notre jeu de test. Cet Ã©cart peut mÃªme devenir catastrophique ! De maniÃ¨re plus rigoureuse, le gap de gÃ©nÃ©ralisation de notre estimateur <span class="math notranslate nohighlight">\(\hat{h}\)</span> est la quantitÃ© suivanteÂ :</p>
<div class="math notranslate nohighlight">
\[\text{gap}(\hat{h})=|Re(\hat{h})-R(\hat{h})|.\]</div>
<p>OÃ¹ <span class="math notranslate nohighlight">\(Re\)</span> fait rÃ©fÃ©rence Ã  notre risque empirique, câ€™est-Ã -dire lâ€™erreur sur le jeu dâ€™apprentissage et <span class="math notranslate nohighlight">\(R\)</span> Ã  lâ€™erreur en espÃ©rance.</p>
<p>Il est possible dâ€™avoir une idÃ©e de <span class="math notranslate nohighlight">\(R(\hat{h})\)</span> en passant par un jeu de test ou par une autre stratÃ©gie dâ€™Ã©valuation via un jeu de test par exemple, comme nous avons pu le voir. Nous allons ici nous rendre compte que les modÃ¨les qui regardent le voisinage de nos donnÃ©es souffrent dâ€™une grosse limite liÃ©e Ã  ce quâ€™on appelle <em>la malÃ©diction de la dimension</em> et qui affecte grandement ce <em>gap</em>.</p>
<div class="section" id="en-details">
<h3>En dÃ©tails<a class="headerlink" href="#en-details" title="Permalink to this headline">Â¶</a></h3>
<p>La malÃ©diction de la dimension fait rÃ©fÃ©rence aux rÃ©sultats contre-intuitifs qui apparaissent lorsque la dimension augmente.</p>
<div class="note admonition">
<p class="admonition-title">Une illustration avec lâ€™orange multi-dimensionnelle</p>
<p>Cette exemple donne une image plus visuelle de la malÃ©diction de la dimension. ModÃ©lisons une orange comme une boule parfaite de rayon <span class="math notranslate nohighlight">\(8\text{cm}\)</span> avec une enveloppe (i.e. la peau) dâ€™une Ã©paisseur de <span class="math notranslate nohighlight">\(5\text{mm}\)</span> partout. Le volume dâ€™une boule <span class="math notranslate nohighlight">\(\mathcal{B}_r\)</span> de rayon <span class="math notranslate nohighlight">\(r\)</span> dans un espace de dimension <span class="math notranslate nohighlight">\(d\)</span> est donnÃ© parÂ :</p>
<div class="math notranslate nohighlight">
\[\text{vol}(\mathcal{B}_r)=K r^d,\]</div>
<p>oÃ¹ <span class="math notranslate nohighlight">\(K\)</span> est une constante qui dÃ©pend de la dimension. La question quâ€™on peut se poser est celle du volume occupÃ© par la peau de lâ€™orange relativement au volume total de lâ€™orange. Câ€™est le volume total de lâ€™orange auquel on soustrait le volume de lâ€™orange sans la peau divisÃ© par le volume total de lâ€™orangeÂ :</p>
<div class="math notranslate nohighlight">
\[\text{ratio}=\frac{\text{vol}(\mathcal{B}_8)-\text{vol}(\mathcal{B}_{8-0.05})}{\text{vol}(\mathcal{B}_8)}.\]</div>
<p>En dimension <span class="math notranslate nohighlight">\(3\)</span>, cela nous donne <span class="math notranslate nohighlight">\(\text{ratio}=0.018\)</span>. Moins de <span class="math notranslate nohighlight">\(2\%\)</span> du volume de lâ€™orange est occupÃ© par la peau. Que se passe-t-il en dimension 500 ? On obtient <span class="math notranslate nohighlight">\(\text{ratio}\approx 96\%\)</span>. Presque tout le volume de lâ€™orange est occupÃ© par la peau ! Et <span class="math notranslate nohighlight">\(500\)</span> ne reprÃ©sente pas la trÃ¨s grande dimension (une image en couleur de 200px<span class="math notranslate nohighlight">\(\times\)</span>200px a une dimension de 120000).</p>
</div>
<p>Une premiÃ¨re maniÃ¨re de lâ€™observer en <em>machine learning</em> est possible grÃ¢ce au KNN. Ce dernier classe un nouvel Ã©lÃ©ment en fonction de ses voisins dans le jeu dâ€™apprentissage. Nous allons en particulier Ã©tudier lâ€™Ã©volution du risque de gÃ©nÃ©ralisation en fonction de la dimension. Plus prÃ©cisÃ©ment, les donnÃ©es synthÃ©tiques sont construites de la maniÃ¨re suivanteÂ :</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="k">def</span> <span class="nf">sample_data</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">))</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">y</span><span class="o">*</span><span class="n">X</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">X</span> <span class="c1"># positive have mean mu and negative, -mu</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="o">-</span><span class="n">k</span><span class="p">))</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">noise</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<p>Dit autrement, <span class="math notranslate nohighlight">\(k\)</span> dimensions contiennent le signal intÃ©ressant pour notre tÃ¢che et <span class="math notranslate nohighlight">\(d\)</span> dimensions ne servent Ã  rien. Nous observons ci-dessous ce qui se passe lorsquâ€™on rajouter des dimensions de bruits (i.e. qui ne servent Ã  rien). Câ€™est typiquement ce pourrait se passer avec des images. Une photo de chien ne contient pas que des pixels descriptifs du concept de chien.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>

<span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">redo</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">max_dim</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">first_dim</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">steps</span> <span class="o">=</span> <span class="mi">100</span>

<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">first_dim</span><span class="p">,</span> <span class="n">max_dim</span><span class="p">,</span> <span class="n">steps</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">redo</span><span class="p">):</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_data</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="n">d</span><span class="p">)</span>
        <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">sample_data</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="n">d</span><span class="p">)</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>
        <span class="n">c</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],)))</span>
        <span class="n">s</span> <span class="o">+=</span> <span class="n">c</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],)))</span><span class="o">/</span><span class="n">redo</span>
    <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># configuration generale de matplotlib</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">12.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">first_dim</span><span class="p">,</span> <span class="n">max_dim</span><span class="p">,</span> <span class="n">steps</span><span class="p">)),</span> <span class="n">scores</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Evolution de l</span><span class="se">\&#39;</span><span class="s1">accuracy en fonction de la dimension du probleme&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_introduction_ml_72_0.png" src="../_images/1_introduction_ml_72_0.png" />
</div>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Quelle est lâ€™accuracy dâ€™un classifieur alÃ©atoire ?</strong></p>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Expliquez pourquoi lâ€™accuracy diminue lorsquâ€™on rajoute des dimensions sans signal.</strong></p>
</div>
<p>De maniÃ¨re similaire, Ã©tudions lâ€™Ã©volution des distances lorsque la dimension Ã©volue. Nous allons tirer alÃ©atoirement un ensemble de vecteurs dans <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> et nous calculerons la norme <span class="math notranslate nohighlight">\(\lVert x\rVert^2_2\)</span> moyenne, maximale et minimale de notre tirage. De la mÃªme maniÃ¨re nous calculerons la distance <span class="math notranslate nohighlight">\(\lVert x-y\rVert_2\)</span> moyenne, maximale et minimale entre les couples de points de notre jeu de donnÃ©es. Enfin, lâ€™objectif sera dâ€™Ã©tudier ces quantitÃ©s en faisant Ã©voluer la dimension <span class="math notranslate nohighlight">\(d\)</span> du problÃ¨me.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.spatial</span> <span class="kn">import</span> <span class="n">distance_matrix</span>

<span class="k">def</span> <span class="nf">sample_data</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sample_data</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">redo</span> <span class="o">=</span> <span class="mi">50</span>
<span class="k">def</span> <span class="nf">experiment_</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
    <span class="n">min_</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">max_</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">mean_</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">redo</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">sample_data</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
        <span class="n">vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">X</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">min_</span> <span class="o">+=</span> <span class="n">vec</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">/</span><span class="n">redo</span>
        <span class="n">max_</span> <span class="o">+=</span> <span class="n">vec</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">/</span><span class="n">redo</span>
        <span class="n">mean_</span> <span class="o">+=</span> <span class="n">vec</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">/</span><span class="n">redo</span>
        
        <span class="n">mat</span> <span class="o">=</span> <span class="n">distance_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">25</span><span class="p">],</span> <span class="n">X</span><span class="p">[:</span><span class="mi">25</span><span class="p">])</span>
        <span class="n">triu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">mat</span><span class="p">)</span>
        <span class="n">triu</span> <span class="o">=</span> <span class="n">triu</span><span class="p">[</span><span class="n">triu</span><span class="o">!=</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">dist_max</span> <span class="o">=</span> <span class="n">triu</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="n">dist_min</span> <span class="o">=</span> <span class="n">triu</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
        <span class="n">dist_mean</span> <span class="o">=</span> <span class="n">triu</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">min_</span><span class="p">,</span> <span class="n">max_</span><span class="p">,</span> <span class="n">mean_</span><span class="p">,</span> <span class="n">dist_min</span><span class="p">,</span> <span class="n">dist_max</span><span class="p">,</span> <span class="n">dist_mean</span>
<span class="n">idx</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">val</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
    <span class="n">idx</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">d</span><span class="p">])</span>
    <span class="n">val</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">experiment_</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>
<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">):</span>
    <span class="n">idx</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">d</span><span class="p">])</span>
    <span class="n">val</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">experiment_</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>
<span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">idx</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">val</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">arr</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">arr</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Min&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">arr</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">arr</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Max&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">arr</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">arr</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Moy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Dimension du problÃ¨me&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Norme $\ell_2$ des vecteurs&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Evolution des normes&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">arr</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">arr</span><span class="p">[:,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Min&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">arr</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">arr</span><span class="p">[:,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Max&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">arr</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">arr</span><span class="p">[:,</span> <span class="mi">6</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Moy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Dimension du problÃ¨me&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Distance $\ell_2$ entre nos vecteurs&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Evolution des distances&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Evolution des distances&#39;)
</pre></div>
</div>
<img alt="../_images/1_introduction_ml_77_1.png" src="../_images/1_introduction_ml_77_1.png" />
</div>
</div>
<div class="admonition-exercice admonition">
<p class="admonition-title">Exercice</p>
<p><strong>Quel phÃ©nomÃ¨ne mathÃ©matique pouvons nous invoquer afin dâ€™expliquer cela ?</strong></p>
</div>
<p>Soit <span class="math notranslate nohighlight">\(x_\text{new}\)</span> une nouvelle donnÃ©e. Une petite perturbation du point de notre jeu dâ€™apprentissage le plus diffÃ©rent de <span class="math notranslate nohighlight">\(x_\text{new}\)</span> peut le transformer en le point le plus proche est inversementâ€¦ Câ€™est une grosse limite des modÃ¨les prÃ©cÃ©dents. Il faut soit rÃ©flÃ©chir Ã  rÃ©duire la dimension, soit injecter de la connaissance dans nos modÃ¨les, etc.</p>
</div>
</div>
<div class="section" id="x-quelques-modeles-proposes-par-texttt-sklearn">
<h2>X. Quelques modÃ¨les proposÃ©s par <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span><a class="headerlink" href="#x-quelques-modeles-proposes-par-texttt-sklearn" title="Permalink to this headline">Â¶</a></h2>
<p>Voici une liste (absolument non exhaustive) de quelques modÃ¨les proposÃ©s par la librairie <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span>. Certains (pas nÃ©cessairement prÃ©sents dans cette liste) seront approfondis dans les prochaines sÃ©quences.</p>
<ul class="simple">
<li><p>SupervisÃ©Â :</p></li>
<li><p>RÃ©gression dans <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>Â :</p>
<ul>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html">RÃ©gression linÃ©aire</a> et ses variantesÂ :</p>
<ul>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html">Ridge</a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html">Lasso</a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html">Elastic-Net</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html">RÃ©gression KNN</a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html">Arbre de rÃ©gression</a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor">ForÃªts alÃ©atoires</a></p></li>
<li><p>etc.</p></li>
</ul>
</li>
<li><p>Classification (ou assimilÃ©s)Â :</p>
<ul>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html">SVC</a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html">Classification KNN</a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">RÃ©gression Logistique</a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html">Arbre de classification</a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">ForÃªts alÃ©atoires</a></p></li>
</ul>
</li>
<li><p>Non-supervisÃ©Â :</p>
<ul>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html">K-means</a> (clustering)</p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html">t-SNE</a> (apprentissage de <em>manifold</em>)</p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html">OneclassSVM</a> (dÃ©tection de nouveautÃ©)</p></li>
</ul>
</li>
</ul>
<p>Pour une liste plus exhaustive des diffÃ©rents outils <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span> merci de consulter le lien suivantÂ  :</p>
<ul class="simple">
<li><p><a class="reference external" href="https://scikit-learn.org/stable/supervised_learning.html">https://scikit-learn.org/stable/supervised_learning.html</a>.</p></li>
</ul>
</div>
<div class="section" id="xi-le-no-free-lunch-theorem">
<h2>XI. Le <em>no-free-lunch</em> Theorem<a class="headerlink" href="#xi-le-no-free-lunch-theorem" title="Permalink to this headline">Â¶</a></h2>
<p>Nous avons vu plus haut que le modÃ¨le <span class="math notranslate nohighlight">\(\texttt{memorize}\)</span> pouvait Ãªtre parfaitement bon sur les donnÃ©es dâ€™apprentissage mais Ã©chouer sur de nouvelles donnÃ©es : il ne gÃ©nÃ©ralise pas. Pour cela, nous avons rajoutÃ© certaines hypothÃ¨ses et construit de nouveaux modÃ¨les comme le <span class="math notranslate nohighlight">\(\texttt{KNN}\)</span> qui exploite le voisinage dâ€™un point pour pouvoir faire une prÃ©diction. La malÃ©diction de la dimension, malheureusement, pÃ©nalise ce fonctionnement dÃ¨s que la dimension de lâ€™espace dâ€™entrÃ©e devient trop grande.</p>
<p>Une question que nous pouvons nous poser est la suivante : pouvons nous construire une rÃ¨gle de classification gÃ©nÃ©rique, optimisÃ©e Ã  partir dâ€™un jeu de donnÃ©es, qui puisse offrir la garantie de toujours fonctionner ?</p>
<p>La rÃ©ponse est non, et au-delÃ  de cela, pour chaque modÃ¨le que nous allons utiliser, nous devrons faire des hypothÃ¨ses sur les donnÃ©es. Dans les cas oÃ¹ ces hypothÃ¨ses seraient falsifiÃ©es alors notre stratÃ©gie Ã©chouera.</p>
<div class="admonition-no-free-lunch-theorem admonition">
<p class="admonition-title"><em>No-free-lunch Theorem</em></p>
<p>Soit <span class="math notranslate nohighlight">\(\mathcal{D}_n=\{(X_i, Y_i)\}_{i\leq n}\)</span> un jeu de donnÃ©es et <span class="math notranslate nohighlight">\(\hat{h}_n:\mathcal{X}\mapsto\mathcal{Y}\)</span> un modÃ¨le de classification (un classifieur) construit Ã  partir de <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> selon une rÃ¨gle au choix. Lâ€™indice <span class="math notranslate nohighlight">\(n\)</span> montre la dÃ©pendence sur la taille de notre jeu de donnÃ©es. Soit <span class="math notranslate nohighlight">\(1/16\geq\{a_n\}_{n&gt;0}&gt;0\)</span> une suite qui converge vers <span class="math notranslate nohighlight">\(0\)</span> mais Ã  une vitesse aussi lente quâ€™on le veuille. Alors, il existe un problÃ¨me (i.e. une distribution sur <span class="math notranslate nohighlight">\(X\times Y\)</span>) tel que le meilleur classifieur <span class="math notranslate nohighlight">\(h^\star\)</span> fasse <span class="math notranslate nohighlight">\(0\)</span>, mais que notre estimateur ait une erreur satisfaisant lâ€™inÃ©galitÃ©Â :</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\big[R(h_n)\big]&gt;a_n.\]</div>
</div>
<p>Dit autrement, plus le jeu de donnÃ©es sera grand plus lâ€™erreur sera faible. Mais cette dÃ©croissance de lâ€™erreur peut Ãªtre arbitrairement lente.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./1_what_is_ml"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="0_propos_liminaire.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><em>Machine Learning</em>, initiation</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="2_regression_and_classification_trees.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Les arbres de rÃ©gression et de classification</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Servajean, Leveau & Chailan<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>
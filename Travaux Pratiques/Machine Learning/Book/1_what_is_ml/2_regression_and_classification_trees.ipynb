{"cells": [{"cell_type": "markdown", "id": "potential-feature", "metadata": {}, "source": ["# Les arbres de r\u00e9gression et de classification\n", "\n", "**<span style='color:blue'> Objectifs de la s\u00e9quence</span>** ", "\n", "* Comprendre&nbsp;:\n", "    * Le principe d'un arbre de d\u00e9cision,\n", "    * La construction d'un arbre de d\u00e9cision.\n", "* Savoir interpr\u00e9ter un arbre de d\u00e9cision.\n", "* Manipuler&nbsp;:\n", "    * Les arbres de d\u00e9cision via la librairie $\\texttt{sklearn}$.\n", "\n", "\n\n ----"]}, {"cell_type": "markdown", "id": "subjective-saver", "metadata": {}, "source": ["## I. Introduction\n", "\n", "Les arbres de d\u00e9cision forment une cat\u00e9gorie de mod\u00e8les cl\u00e9s en *marchine learning*. Bien que n'\u00e9tant pas les mod\u00e8les les plus performants par eux-m\u00eames, ils forment la brique de base des for\u00eats al\u00e9atoires, souvent sur le podium des mod\u00e8les les plus performants. De plus, leur facilit\u00e9 d'interpr\u00e9tation leur offre un atout non n\u00e9gligeable.\n", "\n", "Les [arbres de d\u00e9cision](https://fr.wikipedia.org/wiki/Arbre_de_d%C3%A9cision) s\u2019appuient sur une structure de donn\u00e9es qu\u2019on appelle un [arbre](https://fr.wikipedia.org/wiki/Arbre_(th%C3%A9orie_des_graphes)) (thanks captain obvious). Le principe d'un arbre de d\u00e9cision est illustr\u00e9 par la figure suivante&nbsp;:\n", "\n", "\n", "![Decision tree](https://raw.githubusercontent.com/maximiliense/lmiprp/main/Travaux%20Pratiques/Machine%20Learning/Introduction/data/Introduction/cart.jpg)\n", "\n", "Imaginons l'\u00e9tudiant John Smith caract\u00e9ris\u00e9 par une note en licence en informatique de 14, une note en math\u00e9matiques de 15 et qui \u00e9coute en cours. Sa classification se fera comme suit&nbsp;:\n", "1.  On regarde la racine&nbsp;: l'\u00e9tudiant \u00e9coute en cours, on prend donc la branche de droite,\n", "2.  On regarde sa note en math&nbsp;: elle est sup\u00e9rieure \u00e0 10, on en conclut qu'il obtiendra son module de *machine learning*.\n", "\n", "L'id\u00e9e est qu'\u00e0 chaque n\u0153ud notre arbre va d\u00e9couper le sous-espace dont \"il s'occupe par un hyperplan de mani\u00e8re \u00e0 d\u00e9finir deux sous-espaces. L'un sera trait\u00e9 par les n\u0153uds de la branche droite et l'autre par ceux de la branche gauche. Cr\u00e9ons maintenant un jeu de donn\u00e9es synth\u00e9tique illustrant le sc\u00e9nario pr\u00e9c\u00e9dent."]}, {"cell_type": "code", "execution_count": null, "id": "devoted-closing", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "\n", "def create_student_dataset(n):\n", "    # la premiere colonne est la note d'informatique,\n", "    # la seconde la note de math\n", "    X = np.random.uniform(0, 20, size=(n, 2))\n", "    conditions = np.stack([\n", "        X[:, 0] > 11,\n", "        X[:, 1] > 8\n", "    ]).T\n", "    y = np.any(\n", "        np.stack(\n", "         [np.all(conditions[:, 0:2], axis=1), X[:, 1] > 18]\n", "        ), \n", "        axis=0)\n", "    return X, y\n", "    \n", "X, y = create_student_dataset(45)"]}, {"cell_type": "code", "execution_count": null, "id": "numerical-relations", "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "\n", "def plot_student(X, y, tree=None):\n", "    plt.figure(figsize=(12, 8))\n", "    plt.scatter(X[y, 0], X[y, 1], color='yellow', edgecolors='black', \n", "                label='R\u00e9ussi le module de machine learning')\n", "    plt.scatter(X[(1-y).astype(bool), 0], X[(1-y).astype(bool), 1], \n", "                c='purple', edgecolors='black', \n", "                label='\u00c9choue le module de machine learning')\n", "    plt.xlabel('Informatique')\n", "    plt.ylabel('Math\u00e9matiques')\n", "    plt.legend()\n", "    plt.show()\n", "\n", "plot_student(X, y)"]}, {"cell_type": "markdown", "id": "least-competition", "metadata": {}, "source": ["Construisons avec le *framework* $\\texttt{sklearn}$ quelques arbres de profondeurs diff\u00e9rentes sur ce probl\u00e8me et observons le d\u00e9coupage de l'espace qui en r\u00e9sulte."]}, {"cell_type": "code", "execution_count": null, "id": "heated-bookmark", "metadata": {}, "outputs": [], "source": ["from sklearn.tree import DecisionTreeClassifier, plot_tree\n", "import matplotlib.pyplot as plt\n", "\n", "\n", "fig_x_min, fig_x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n", "fig_y_min, fig_y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n", "xx, yy = np.meshgrid(\n", "    np.arange(fig_x_min, fig_x_max, 0.1),\n", "    np.arange(fig_y_min, fig_y_max, 0.1)\n", ")\n", "\n", "plt.figure(figsize=(15, 5))\n", "for i in range(1, 4):\n", "    ax = plt.subplot(1, 3, i)\n", "    tree = DecisionTreeClassifier(max_depth=i)\n", "    tree.fit(X, y)\n", "    Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n", "    Z = Z.reshape(xx.shape)\n", "\n", "    plt.contourf(xx, yy, Z, alpha=0.4)\n", "    plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k')\n", "    plt.xlabel('Informatique')\n", "    plt.ylabel('Math\u00e9matiques')\n", "    plt.title('Arbre de d\u00e9cision avec une profondeur de ' + str(i))\n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "knowing-paradise", "metadata": {}, "source": ["\n", "On voit tr\u00e8s bien \u00e0 chaque \u00e9tape suppl\u00e9mentaire le fait qu'un hyperplan s\u00e9pare l'espace en deux (ou qu'on s'arr\u00eate). Visualison maintenant l'arbre du graphique de droite ainsi construit."]}, {"cell_type": "code", "execution_count": null, "id": "incident-footage", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(12, 8))\n", "_ = plot_tree(tree, rounded=True, fontsize=14)"]}, {"cell_type": "markdown", "id": "existing-disposition", "metadata": {}, "source": ["La premi\u00e8re ligne de chaque n\u0153ud repr\u00e9sente le crit\u00e8re qui nous fera aller \u00e0 droite ou \u00e0 gauche. La seconde ligne \"gini\" indique la condition qui a permis \u00e0 l'algorithme de d\u00e9cider de cr\u00e9er de nouveaux n\u0153uds ou de s'arr\u00eater (i.e. lorsque gini vaut $0$ ou que la profondeur est atteinte, on s'arr\u00eate). *Samples* est le nombre de points de notre jeu de donn\u00e9es qui se retrouvent concern\u00e9s par le n\u0153ud et enfin, *value* est la pr\u00e9diction. Il s'agit ici d'un probl\u00e8me de classification binaire, et la premi\u00e8re case repr\u00e9sente le label $0$ et la seconde le label $1$. \u00c9tant donn\u00e9 un n\u0153ud feuille, la pr\u00e9diction \u00e0 faire est le vote \u00e0 la majorit\u00e9 simple. On peut \u00e9galement demander une estimation des probabilit\u00e9s et le mod\u00e8le retournera le vecteur suivant&nbsp;:\n", "\n", "```python\n", "value / value.sum()\n", "```"]}, {"cell_type": "markdown", "id": "43fb60b3", "metadata": {}, "source": ["**<span style='color:blue'> Crit\u00e8re d'arr\u00eat</span>** ", "Vous remarquerez peut-\u00eatre selon le tirage du jeu de donn\u00e9es que l'arbre de profondeur 2 est le m\u00eame que l'arbre de profondeur 3. C'est normal, nous minimisons d\u00e9j\u00e0 l'erreur et il n'est pas utile d'aller plus loin. Cela est r\u00e9pr\u00e9sent\u00e9 par une valeur *gini* qui vaut 0 dans le sch\u00e9ma de l'arbre.\n", "\n\n ----"]}, {"cell_type": "markdown", "id": "traditional-longer", "metadata": {}, "source": ["## II. Construction d'un arbre de classification\n", "\n", "La racine de notre arbre regroupe tous les points de notre jeu de donn\u00e9es. Comment d\u00e9cider de quelle mani\u00e8re nous devons ensuite cr\u00e9er de nouveaux n\u0153uds voire m\u00eame si nous devons en cr\u00e9er ? \u00c0 la premi\u00e8re it\u00e9ration, l'id\u00e9e va \u00eatre de consid\u00e9rer toutes les valeurs possibles de seuil $t$ et des variables $j$ qui s\u00e9pareraient notre jeu de donn\u00e9es $S$ en deux groupes $S^{(l)}$ et $S^{(r)}$ tels que $\\forall x\\in S_n^{(l)}\\Rightarrow x_j \\leq t$ et $\\forall x\\in S_n^{(r)}\\Rightarrow x_j > t$. Le choix du seuil se fera sur un crit\u00e8re qu'on appelle \"le gain d'information\"&nbsp;:\n", "\n", "\n", "$$\\mathcal{IG}(S^{l}, S^{r})=\\mathcal{I}(S)-\\frac{n_l}{n}\\mathcal{I}(S^{l})-\\frac{n_r}{n}\\mathcal{I}(S^{r}),$$\n", "\n", "O\u00f9 $n_r=|S^{(r)}|$ et $n_l=|S^{(l)}|$ et $\\mathcal{I}$ est une \"mesure d'impuret\u00e9\" d'un groupe que nous d\u00e9finirons plus tard. L'id\u00e9e est de mesurer \u00e0 quel point notre seuil $t$ et le choix de la variable $j$ ont bien s\u00e9par\u00e9 notre jeu de donn\u00e9es en deux groupes o\u00f9 les classes sont moins m\u00e9lang\u00e9es que dans le jeu complet $S$.\n", "\n", "Plusieurs m\u00e9triques d'impuret\u00e9 existent dont nous listons quelques exemples&nbsp;:\n", "\n", "* Impuret\u00e9 de Gini\n", "* Entropie\n", "* Erreur de classification\n", "\n", "L'impuret\u00e9 de Gini se calcule de la mani\u00e8re suivante&nbsp;:\n", "\n", "$$\\mathcal{I}_{\\mathcal{G}}(S)=\\sum_k p_k(1-p_k),$$\n", "\n", "o\u00f9 $p_i$ indique la proportion d'\u00e9l\u00e9ments de la classe $i$ dans l'ensemble $S$. L'entropie est donn\u00e9e par la formule $\\mathcal{I}_\\mathcal{E}(S)=-\\sum_k p_k \\text{log}_2 p_k$ et l'erreur de classification par $\\mathcal{I}_{EC}(S)=1-\\text{max}_k(p_k)$. On remarque que dans les figures de l'exemple ci-dessus, nous avons utilis\u00e9 l'impuret\u00e9 de Gini ! \n", "\n", "L'\u00e9tape pr\u00e9c\u00e9dente est ensuite r\u00e9p\u00e9t\u00e9e pour chaque sous-groupe. Plusieurs strat\u00e9gies d'arr\u00eat sont possibles. Tout d'abord, lorsque chaque feuille ne contient qu'un seul \u00e9l\u00e9ment et ne peut plus \u00eatre divis\u00e9e en deux. Ou lorsque le *split* n'am\u00e9liore pas l'*accuracy* d'un gain relatif d'au moins $\\alpha$ (hyperparam\u00e8tre qu'on fixe). Une autre strat\u00e9gie consiste \u00e0 construire l'arbre complet et \u00e0 ensuite \u00e9liminer les branches qui ne satisfont pas certains crit\u00e8res, par exemple au travers d\u2019une validation crois\u00e9e. Et bien s\u00fbr, lorsque l'arbre a atteint la profondeur maximum fix\u00e9e par l'utilisateur.\n", "\n", "## III. Construction d'un arbre de r\u00e9gression\n", "Dans le cas d'un arbre de r\u00e9gression, le crit\u00e8re de *split* est diff\u00e9rent du cas pr\u00e9c\u00e9dent. Nous consid\u00e9rons cette fois-ci comme crit\u00e8re d'erreur (l'impuret\u00e9 pr\u00e9c\u00e9dente) le *mean-squared error* ou MSE&nbsp;:\n", "\n", "\n", "$$MSE = \\sum_i(\\bar{y} \u2013 y_i)^2/n.$$\n", "\n", "Le split est choisi minimisant la quantit\u00e9 suivante&nbsp;: \n", "\n", "$$\\frac{n_l}{n}MSE(S^{(l)})+\\frac{n_r}{n}MSE(S^{(r)}).$$\n", "\n", "Ce n'est rien d'autre que la moyenne pond\u00e9r\u00e9e des erreurs.\n", "\n", "## IV. Applications\n", "\n", "**<span style='color:blue'> Exercice</span>** ", "\n", "**Pour les diff\u00e9rents jeux de donn\u00e9es suivants, proposez un type d'arbre de classification (r\u00e9gression ou classification). Quel crit\u00e8re de split a \u00e9t\u00e9 utilis\u00e9 ? Quelle est la profondeur de l\u2019arbre calcul\u00e9 ? Quelle r\u00e8gle de d\u00e9cision (i.e. le chemin de d\u00e9cision dans l'arbre) a \u00e9t\u00e9 utilis\u00e9e pour le premier \u00e9l\u00e9ment du jeu de test (uniquement les jeux de donn\u00e9es 1 avec max_depth=3 et 2) ?**\n", "\n\n ----"]}, {"cell_type": "code", "execution_count": null, "id": "hydraulic-collins", "metadata": {}, "outputs": [], "source": ["from sklearn import datasets\n", "from sklearn.model_selection import train_test_split"]}, {"cell_type": "markdown", "id": "filled-wrist", "metadata": {}, "source": ["**Jeu de donn\u00e9es 1**"]}, {"cell_type": "code", "execution_count": null, "id": "missing-spell", "metadata": {}, "outputs": [], "source": ["data = datasets.fetch_california_housing()\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(\n", "    data.data, data.target, test_size=0.5, shuffle=True\n", ")"]}, {"cell_type": "code", "id": "august-mapping", "metadata": {}, "source": ["####### Complete this part ######## or die ####################\n", "...\n", "print('Le score R2:', model.score(X_test, y_test))\n", "...\n", "print('On pr\u00e9dit:', model.predict(X_test[:1]), 'pour', X_test[:1])\n", "###############################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "immediate-press", "metadata": {}, "source": ["**Jeu de donn\u00e9es 2**"]}, {"cell_type": "code", "execution_count": null, "id": "amateur-mechanism", "metadata": {}, "outputs": [], "source": ["data = datasets.load_iris()\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(\n", "    data.data, data.target, test_size=0.5, shuffle=True\n", ")"]}, {"cell_type": "code", "id": "unlimited-samuel", "metadata": {}, "source": ["####### Complete this part ######## or die ####################\n", "...\n", "print('L\\'accuracy de notre mod\u00e8le:', model.score(X_test, y_test))\n", "...\n", "print('On pr\u00e9dit:', model.predict(X_test[:1]), 'pour', X_test[:1])\n", "###############################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "wicked-burden", "metadata": {}, "source": ["**Jeu de donn\u00e9es 3**"]}, {"cell_type": "code", "execution_count": null, "id": "immediate-shade", "metadata": {}, "outputs": [], "source": ["data = datasets.fetch_covtype()\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(\n", "    data.data, data.target, test_size=0.5, shuffle=True\n", ")"]}, {"cell_type": "code", "id": "attractive-boulder", "metadata": {}, "source": ["####### Complete this part ######## or die ####################\n", "......\n", "\n", "print('L\\'accuracy de notre mod\u00e8le:', model.score(X_test, y_test))\n", "\n", "print('On pr\u00e9dit:', model.predict(X_test[:1]), 'pour', X_test[:1])\n", "###############################################################\n"], "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.2"}}, "nbformat": 4, "nbformat_minor": 5}
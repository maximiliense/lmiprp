{"cells": [{"cell_type": "markdown", "id": "c9d67c8f", "metadata": {}, "source": ["# Jeu d'apprentissage uniquement multi-classes \u2615\ufe0f\n", "\n", "\n", "**<span style='color:blue'> Objectifs de la s\u00e9quence</span>** ", "\n", "* \u00catre sensibilis\u00e9&nbsp;:\n", "    * \u00e0 l'id\u00e9e de pr\u00e9diction d'ensembles.\n", "* \u00catre capable&nbsp;:\n", "    * de formaliser la pr\u00e9diction *average-K*,\n", "    * d'impl\u00e9menter un *average-K* avec $\\texttt{pytorch}$.\n", "\n", "\n\n ----", "\n", "Dans la plupart des donn\u00e9es et y-compris sur des probl\u00e8mes clairement \"ensemblistes\", nous ne disposons g\u00e9n\u00e9ralement QUE d'une seule classe par exemple d'apprentissage. Le probl\u00e8me n'est pas tant qu'il n'y en ait qu'une mais que nous n'avons AUCUN exemple d'absence. Ainsi, nous pouvons construire un probl\u00e8me d'optimisation qui r\u00e9compense les pr\u00e9dictions d'ensembles contenant la bonne r\u00e9ponse mais qui ne ferait rien dans le cas contraire. Le probl\u00e8me est que finalement, en faisant cela, il suffit que notre mod\u00e8le retourne toujours toutes les classes pour minimiser notre *loss*. Hors, ce n'est pas ce que nous souhaitons.\n", "\n", "Soit $\\mathcal{X}$ notre espace d'entr\u00e9e, $\\mathcal{Y}=\\{1, \\ldots, C\\}$ celui des pr\u00e9dictions et notons $S_n=\\{(X_i, Y_i)\\}_{i\\leq n}$ notre jeu de donn\u00e9es sur $\\mathcal{X}\\times\\mathcal{Y}$. Nous traitons bien s\u00fbr ici d'un probl\u00e8me de classification (cf. $\\mathcal{Y}$). Comme c'est presque tout le temps le cas, chaque exemple d'apprentissage est associ\u00e9 \u00e0 un unique label et nous souhaiterions construire un mod\u00e8le capable de pr\u00e9dire un ensemble plut\u00f4t qu'un seul \u00e9l\u00e9ment. Nous cherchons ainsi des applications :\n", "\n", "$$h:\\mathcal{X}\\rightarrow\\mathcal{P}(\\mathcal{Y}).$$\n", "\n", "Cette s\u00e9quence se concentrera sur deux strat\u00e9gies : les pr\u00e9dictions *Top-K* et les pr\u00e9dictions *Average-K*. Nous formaliserons le probl\u00e8me d'optimisation qu'elles cherchent \u00e0 r\u00e9soudre, nous en d\u00e9duirons le classifieur de Bayes et nous illustrerons ces strat\u00e9gies via des exemples."]}, {"cell_type": "markdown", "id": "18fe4452", "metadata": {}, "source": ["## I. Pr\u00e9diction *top-K*\n", "\n", "\n", "### A. Une introduction th\u00e9orique\n", "\n", "L'approche la plus na\u00efve consiste \u00e0 se dire que s'il y a de l'ambigu\u00eft\u00e9 dans nos donn\u00e9es, alors il de ne pas pr\u00e9dire qu'une seule classe mais $K$ classes \u00e0 chaque fois. Le probl\u00e8me n'est pas n\u00e9cessairement \u00e9vident et nous situerons donc dans un cadre particulier o\u00f9 notre mod\u00e8le est un estimateur de la probabilit\u00e9 conditionnelle de chaque classe :\n", "\n", "$$\\hat{\\eta}_k(x)=\\mathbb{P}(Y=k|X=x),$$\n", "\n", "sachant que par d\u00e9finition de la notion de probabilit\u00e9, nous avons bien s\u00fbr :\n", "\n", "$$\\sum_k \\hat{\\eta}_k(x)=1.$$\n", "\n", "Notre objectif est donc de construire un classifieur $\\mathcal{S}:\\mathcal{X}\\rightarrow\\mathcal{P}(\\mathcal{Y})$ tel que $|\\mathcal{S}(x)|=K,\\ \\forall x\\in\\mathcal{X}$. Nous cherchons ici \u00e0 minimiser l'erreur suivante :\n", "\n", "$$R(\\mathcal{S})=\\mathbb{P}(Y\\not\\in \\mathcal{S}(X)).$$"]}, {"cell_type": "markdown", "id": "d1c33c70", "metadata": {}, "source": ["\n", "**<span style='color:blue'> Question</span>** ", "**\u00c0 chaque pr\u00e9diction exactement $K$ classes sont retourn\u00e9es. Cette fois-ci, nos pr\u00e9dictions se font dans l'espace $\\mathcal{P}(\\mathcal{Y})$ l'ensemble des parties de $\\mathcal{Y}$. Notons $\\eta_k(x)=\\mathbb{P}(Y=k|X=x)$. Trouvez le classifieur de Bayes**\n", "\n\n ----", "\n"]}, {"cell_type": "markdown", "id": "1cd0ba63", "metadata": {}, "source": ["### B. En pratique\n", "\n"]}, {"cell_type": "markdown", "id": "b3c6e05c", "metadata": {}, "source": ["En pratique, on peut chercher \u00e0 optimiser une *loss* directement d\u00e9di\u00e9e au *Top-K* ou, \u00e0 l'inverse, optimiser un mod\u00e8le avec une *loss* classique permettant d'avoir un estimateur $\\hat{\\eta}$ et ensuite d'utiliser la r\u00e8gle de classification sur cet estimateur. C'est la seconde approche que nous choisissons ici."]}, {"cell_type": "code", "execution_count": null, "id": "7a1c184c", "metadata": {}, "outputs": [], "source": ["import torch\n", "from torch.utils.data import DataLoader \n", "from torchvision import datasets\n", "import torchvision.transforms as transforms"]}, {"cell_type": "code", "execution_count": null, "id": "77488117", "metadata": {}, "outputs": [], "source": ["transform = transforms.Compose(\n", "  [\n", "      transforms.ToTensor(),\n", "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n", "  ]\n", ")\n", "\n", "cifar_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n", "cifar_test = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n", "\n", "batch_size = 128\n", "\n", "trainloader = DataLoader(\n", "  cifar_train, batch_size=batch_size\n", ")\n", "\n", "testloader = DataLoader(\n", "  cifar_test, batch_size=batch_size, shuffle=True\n", ")\n", "\n", "print('Nb test batchs:', len(testloader))"]}, {"cell_type": "markdown", "id": "f9b5cff6", "metadata": {}, "source": ["Notre estimateur de la probabilit\u00e9 conditionnelle $\\hat{\\eta}$ est tout simplement notre r\u00e9seau de neurones. Notez cependant que les sorties ne sont pas normalis\u00e9es car, pour des raisons de stabilit\u00e9, nous pr\u00e9f\u00e9rons laisser le soin de la normalisation \u00e0 la *loss* *cross-entropy*. Ce n'est pas un probl\u00e8me car la normalisation se fait par la fonction :\n", "\n", "$$\\texttt{softmax}_k(z)=\\frac{e^{z_k}}{\\sum_j e^{z_j}},$$\n", "\n", "qui est monotone."]}, {"cell_type": "code", "execution_count": null, "id": "980250f4", "metadata": {}, "outputs": [], "source": ["import torch.nn as nn\n", "import torch.nn.functional as F\n", "from torchvision import models, transforms\n", "\n", "class Net(nn.Module):\n", "    def __init__(self):\n", "        super(Net, self).__init__()\n", "        self.conv1 = nn.Conv2d(3, 16, 5)\n", "        self.pool = nn.MaxPool2d(2, 2)\n", "        self.conv2 = nn.Conv2d(16, 32, 5)\n", "        self.fc1 = nn.Linear(32 * 5 * 5, 120)\n", "        self.fc2 = nn.Linear(120, 84)\n", "        self.fc = nn.Linear(84, 10)\n", "\n", "    def forward(self, x):\n", "        x = self.pool(F.relu(self.conv1(x)))\n", "        x = self.pool(F.relu(self.conv2(x)))\n", "        x = x.view(-1, 32 * 5 * 5)\n", "        x = F.relu(self.fc1(x))\n", "        x = F.relu(self.fc2(x))\n", "        x = self.fc(x)\n", "        return x"]}, {"cell_type": "code", "execution_count": null, "id": "c12933f8", "metadata": {}, "outputs": [], "source": ["model = Net()"]}, {"cell_type": "code", "execution_count": null, "id": "43a1d843", "metadata": {}, "outputs": [], "source": ["import torch.optim as optim\n", "from torch.optim.lr_scheduler import MultiStepLR"]}, {"cell_type": "code", "execution_count": null, "id": "d502c2ca", "metadata": {}, "outputs": [], "source": ["#Choose the loss function\n", "criterion = nn.CrossEntropyLoss()\n", "\n", "#Optimizer\n", "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=0.)"]}, {"cell_type": "code", "execution_count": null, "id": "fdce10dc", "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt"]}, {"cell_type": "code", "execution_count": null, "id": "fdd3a4e5", "metadata": {}, "outputs": [], "source": ["loss_history = []\n", "for epoch in range(2):  # loop over the dataset multiple times\n", "\n", "    running_loss = 0.0\n", "    for i, data in enumerate(trainloader, 0):\n", "        # get the inputs; data is a list of [inputs, labels]\n", "        inputs, labels = data\n", "\n", "        # zero the parameter gradients\n", "        optimizer.zero_grad()\n", "\n", "        # forward + backward + optimize\n", "        outputs = model(inputs)\n", "        loss = criterion(outputs, labels)\n", "        loss.backward()\n", "        optimizer.step()\n", "\n", "        # print statistics\n", "        running_loss += loss.item()\n", "        if i % 10 == 9:  # print every 2000 mini-batches\n", "            print('\\r[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000), end='')\n", "            loss_history.append(running_loss / 2000)\n", "            running_loss = 0.0\n", "print('\\r**** Finished Training ****')\n", "plt.figure(figsize=(12, 8))\n", "plt.plot([i for i in range(1, len(loss_history)+1)], loss_history, \n", "         label='My set-valued model')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "ae3a5490", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "**Adaptez le code suivant afin que votre mod\u00e8le soit \u00e9valu\u00e9 avec une approche *Top-K*.**\n", "\n\n ----"]}, {"cell_type": "code", "id": "33baaece", "metadata": {}, "source": ["####### Complete this part ######## or die ####################\n", "def test(model, loader, k, title, timeout=5):\n", "    model.eval()  # on passe le modele en mode evaluation\n", "    correct = 0\n", "    total = 0\n", "    set_size = 0\n", "    with torch.no_grad():\n", "        for i, data in enumerate(loader):\n", "            images, labels = data\n", "            outputs = model(images)\n", "\n", "            correct += (outputs>threshold).float().gather(1, labels.view(-1,1)).sum()\n", "            set_size += (outputs>threshold).float().sum()\n", "            total += labels.size(0)\n", "            if i >= timeout:\n", "                break\n", "\n", "    model.train()  # on remet le modele en mode apprentissage\n", "    return correct / total, set_size / total\n", "###############################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "1fb359ae", "metadata": {}, "source": ["## II. Pr\u00e9diction *Average-K*"]}, {"cell_type": "markdown", "id": "89aebba3", "metadata": {}, "source": ["### A. Une introduction th\u00e9orique\n", "On sent cependant assez rapidement que l'approche *Top-K* n'est pas totalement satisfaisante. On souhaiterait par exemple ne retourner qu'un seul \u00e9l\u00e9ment lorsque la classe de l'image est \u00e9vidente et simple et plus de $K$ \u00e9l\u00e9ments lorsque l'image est difficile \u00e0 identifier. Nous cherchons toujours ici \u00e0 minimiser l'erreur :\n", "\n", "$$R(\\mathcal{S})=\\mathbb{P}(Y\\not\\in \\mathcal{S}(X)).$$\n", "\n", "\n", "La diff\u00e9rence principale est la contrainte que l'on souhaite imposer sur notre mod\u00e8le. Le classifieur de Bayes est ainsi la solution du probl\u00e8me :\n", "\n", "$$\\mathcal{S}^\\star \\in \\text{argmin}_{\\mathcal{S}}\\mathbb{P}\\big(Y\\not\\in \\mathcal{S}(X)\\big),\\ \\text{s.t. }\\mathbb{E}\\big[|S(X)|\\big]\\leq K,$$\n", "\n", "o\u00f9 la premi\u00e8re appartenance vient du fait que le classifieur de Bayes n'est pas n\u00e9cessairement unique. En reprenant la formulation avec p\u00e9nalit\u00e9, $\\exists\\lambda > 0$ tel que :\n", "\n", "$$\\mathcal{S}^\\star_\\lambda \\in \\text{argmin}_{\\mathcal{S}}\\mathbb{P}\\big(Y\\not\\in \\mathcal{S}(X)\\big)+\\lambda\\mathbb{E}\\big[|S(X)|\\big],$$\n", "\n", "o\u00f9 on affiche la d\u00e9pendance en $\\lambda$. Il se trouve que ce probl\u00e8me poss\u00e8de une forme close que nous donnons imm\u00e9diatement et qui revient \u00e0 seuiller les probabilit\u00e9 :\n", "\n", "$$\\mathcal{S}_\\lambda^\\star(x)=\\{k\\in\\{1, \\ldots, C\\}:\\\u00a0\\eta_k(x)\\geq \\lambda\\}.$$\n", "\n", "\n", "Il s'agit maintenant de choisir une valeur correcte de $\\lambda$ en se rappelant que notre point de d\u00e9part est notre souhait de retourner **en moyenne** $K$ \u00e9l\u00e9ments. Notons :\n", "\n", "$$G(t)=\\sum_{k=1}^C\\mathbb{P}(\\eta_k(X)\\geq t).$$\n", "\n", "C'est l'esp\u00e9rance de la taille de notre ensemble pr\u00e9dit si on seuil \u00e0 $t$. Consid\u00e9rons son inverge g\u00e9n\u00e9ralis\u00e9e, not\u00e9e $G^{-1}(k)$. Notre classifieur de Bayes devient via ce dernier :\n", "\n", "$$\\mathcal{S}_\\bar{k}^\\star(x)=\\{k\\in\\{1, \\ldots, C\\}:\\\u00a0\\eta_k(x)\\geq G^{-1}(\\bar{k})\\}.$$\n", "\n", "### B. En pratique\n", "\n", "C'est bon, nous avons toutes les billes : \n", "\n", "*  On entra\u00eene notre mod\u00e8le sur un jeu d'apprentissage pour estimer $\\hat{\\eta}$,\n", "*  On estime $G^{-1}(\\bar{k})$ sur un jeu de validation.\n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "04b522ac", "metadata": {}, "outputs": [], "source": ["import torch\n", "from torch.utils.data import DataLoader \n", "from torchvision import datasets\n", "import torchvision.transforms as transforms"]}, {"cell_type": "markdown", "id": "a4bfaf48", "metadata": {}, "source": ["**Attention,** le code ci-dessous est tr\u00e8s proche du code pr\u00e9c\u00e9dent, mais nous avons du cr\u00e9er un ensemble de validation \u00e0 partir de notre jeu d'apprentissage."]}, {"cell_type": "code", "execution_count": null, "id": "c0143b94", "metadata": {}, "outputs": [], "source": ["transform = transforms.Compose(\n", "  [\n", "      transforms.ToTensor(),\n", "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n", "  ]\n", ")\n", "\n", "cifar_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n", "\n", "# Attention on split en train et val\n", "train_set, val_set = torch.utils.data.random_split(cifar_train, [49000, 1000])\n", "\n", "cifar_test = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n", "\n", "batch_size = 128\n", "\n", "trainloader = DataLoader(\n", "  train_set, batch_size=batch_size\n", ")\n", "\n", "valloader = DataLoader(\n", "  val_set, batch_size=batch_size\n", ")\n", "\n", "testloader = DataLoader(\n", "  cifar_test, batch_size=batch_size, shuffle=True\n", ")\n", "\n", "print('Nb test batchs:', len(testloader))"]}, {"cell_type": "code", "execution_count": null, "id": "a4883910", "metadata": {}, "outputs": [], "source": ["import torch.nn as nn\n", "import torch.nn.functional as F\n", "from torchvision import models, transforms\n", "\n", "class Net(nn.Module):\n", "    def __init__(self):\n", "        super(Net, self).__init__()\n", "        self.conv1 = nn.Conv2d(3, 16, 5)\n", "        self.pool = nn.MaxPool2d(2, 2)\n", "        self.conv2 = nn.Conv2d(16, 32, 5)\n", "        self.fc1 = nn.Linear(32 * 5 * 5, 120)\n", "        self.fc2 = nn.Linear(120, 84)\n", "        self.fc = nn.Linear(84, 10)\n", "\n", "    def forward(self, x):\n", "        x = self.pool(F.relu(self.conv1(x)))\n", "        x = self.pool(F.relu(self.conv2(x)))\n", "        x = x.view(-1, 32 * 5 * 5)\n", "        x = F.relu(self.fc1(x))\n", "        x = F.relu(self.fc2(x))\n", "        x = self.fc(x)\n", "        return x"]}, {"cell_type": "code", "execution_count": null, "id": "e5e26fd4", "metadata": {}, "outputs": [], "source": ["model = Net()"]}, {"cell_type": "code", "execution_count": null, "id": "53f51275", "metadata": {}, "outputs": [], "source": ["import torch.optim as optim\n", "from torch.optim.lr_scheduler import MultiStepLR"]}, {"cell_type": "code", "execution_count": null, "id": "e6b62770", "metadata": {}, "outputs": [], "source": ["#Choose the loss function\n", "criterion = nn.CrossEntropyLoss()\n", "\n", "#Optimizer\n", "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=0.)"]}, {"cell_type": "code", "execution_count": null, "id": "2ac5914d", "metadata": {}, "outputs": [], "source": ["loss_history = []\n", "for epoch in range(2):  # loop over the dataset multiple times\n", "\n", "    running_loss = 0.0\n", "    for i, data in enumerate(trainloader, 0):\n", "        # get the inputs; data is a list of [inputs, labels]\n", "        inputs, labels = data\n", "\n", "        # zero the parameter gradients\n", "        optimizer.zero_grad()\n", "\n", "        # forward + backward + optimize\n", "        outputs = model(inputs)\n", "        loss = criterion(outputs, labels)\n", "        loss.backward()\n", "        optimizer.step()\n", "\n", "        # print statistics\n", "        running_loss += loss.item()\n", "        if i % 10 == 9:  # print every 2000 mini-batches\n", "            print('\\r[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000), end='')\n", "            loss_history.append(running_loss / 2000)\n", "            running_loss = 0.0\n", "print('\\r**** Finished Training ****')\n", "plt.figure(figsize=(12, 8))\n", "plt.plot([i for i in range(1, len(loss_history)+1)], loss_history, \n", "         label='My set-valued model')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "e72d1a52", "metadata": {}, "source": ["**<span style='color:blue'> Exercice</span>** ", "**Adaptez le code suivant afin que votre mod\u00e8le soit \u00e9valu\u00e9 avec une approche *Average-K*.**\n", "\n\n ----"]}, {"cell_type": "code", "id": "fa2d1c15", "metadata": {}, "source": ["####### Complete this part ######## or die ####################\n", "def test(model, loader, k, title, timeout=5):\n", "    model.eval()  # on passe le modele en mode evaluation\n", "    correct = 0\n", "    total = 0\n", "    set_size = 0\n", "    with torch.no_grad():\n", "        for i, data in enumerate(loader):\n", "            images, labels = data\n", "            outputs = model(images)\n", "\n", "            correct += (outputs>threshold).float().gather(1, labels.view(-1,1)).sum()\n", "            set_size += (outputs>threshold).float().sum()\n", "            total += labels.size(0)\n", "            if i >= timeout:\n", "                break\n", "\n", "    model.train()  # on remet le modele en mode apprentissage\n", "    return correct / total, set_size / total\n", "###############################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "8baa2460", "metadata": {}, "source": ["**<span style='color:green'> Remarque</span>** ", "\n", "La m\u00e9thode *average-k* fonctionne *a priori* mieux, en ayant vu moins de donn\u00e9es !\n", "\n", "\n\n ----"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.2"}}, "nbformat": 4, "nbformat_minor": 5}
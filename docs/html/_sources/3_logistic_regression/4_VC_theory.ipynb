{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La th√©orie de Vapnik et Chervonenkis ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è (üíÜ‚Äç‚ôÇÔ∏è)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "La th√©orie de Vapnik et Chervonenkis, ou th√©orie VC, cherche √† expliquer via des approches statistiques pourquoi l'apprentissage automatique fonctionne dans certains cas. Cette section du cours de *machine learning*, plus th√©orique, nous permettra de d√©montrer un r√©sultat fondamental dans le cas d'un probl√®me de classification binaire.\n",
    "\n",
    "Notons $\\mathcal{X}\\subseteq\\mathbb{R}^d$ notre espace d'entr√©e de dimension $d$ et $\\mathcal{Y}=\\{0, 1\\}$ l'espace de nos labels que nous restreindrons dans cette section au cas binaire. La th√©orie VC se g√©n√©ralise bien s√ªr √† bien d'autres choses que la classification binaire. Notons $\\mathcal{H}$ l'ensemble des fonctions de $\\mathcal{X}$ dans $\\mathcal{Y}$ parmi lesquelles nous voulons r√©aliser notre apprentissage. Comme toujours, notre objectif est de trouver une fonction $h\\in\\mathcal{H}$ qui fait peu d'erreur relativement au risque suivant :\n",
    "\n",
    "$$L(h)=\\mathbb{E}\\big[\\textbf{1}\\{h(X)\\neq Y\\}\\big]=\\mathbb{P}\\big(h(X)\\neq Y\\big).$$\n",
    "\n",
    "C'est tout simplement la probabilit√© que notre fonction $h\\in\\mathcal{H}$ pr√©dise le mauvais label pour un $X$ observ√©. Bien s√ªr, nous ne connaissons pas le processus g√©n√©ratifs de nos donn√©es ni le mod√®le probabiliste le d√©crivant. Pour cela, nous devons estimer ce risque en √©chantillonnant. Notons $S_n=\\{(X_i, Y_i)\\}_{i\\leq n}\\sim\\mathbb{P}^n$ un jeu de donn√©es de taille $n$ o√π les couples sont iid. Nous pouvons maintenant utiliser ce jeu de donn√©es afin de d√©terminer empiriquement le risque de chaque fonction $h\\in\\mathcal{H}$ de la mani√®re suivante :\n",
    "\n",
    "$$L_n(h)=\\frac{1}{n}\\sum_i \\textbf{1}\\{h(X_i)\\neq Y_i\\}.$$\n",
    "\n",
    "C'est juste le nombre moyen d'erreurs commises par la fonction $h$ sur le jeu de donn√©es $S_n$. Notre objectif de vient donc :\n",
    "\n",
    "$$h_n=\\text{argmin}_{h\\in\\mathcal{H}}L_n(h),$$\n",
    "\n",
    "o√π on appellera $h_n$ le minimiseur du risque empirique. C'est la fonction dans $\\mathcal{H}$ qui fait le moins d'erreur en moyenne sur notre jeu de donn√©es. On esp√®re qu'elle ne fera pas non plus beaucoup d'erreurs sur le vrai risque $L$. Nous pouvons quantifier ce \"mauvais choix\" via la formule suivante :\n",
    "\n",
    "$$L(h_n)-\\inf_{h\\in\\mathcal{H}}L(h).$$\n",
    "\n",
    "C'est l'√©cart entre le vrai risque de notre minimiseur du risque empirique et le vrai risque de la meilleure fonction dans $\\mathcal{H}$.\n",
    "\n",
    "L'id√©e de la th√©orie VC est que finalement, si nous sommes capable d'estimer pr√©cis√©ment le risque de chacune des fonctions en utilisant notre jeu de donn√©es, *a priori*, nous ne devrions pas faire un mauvais choix en choisissant le minimiseur du risque empirique. Le lemme suivant valide cette id√©e :\n",
    "\n",
    "----\n",
    "**Lemme :**\n",
    "\n",
    "$$L(h_n)-\\inf_{h\\in\\mathcal{H}}L(h)\\leq 2\\sup_{h\\in\\mathcal{H}}|L_n(h)-L(h)|$$\n",
    "\n",
    "**Preuve :**\n",
    "\n",
    "$$\\begin{aligned}\n",
    "L(h_n)-\\inf_{h\\in\\mathcal{H}}L(h)&=L(h_n)-L_n(h_n)+L_n(h_n)-\\inf_{h\\in\\mathcal{H}}L(h)\\\\\n",
    "&\\leq L(h_n)-L_n(h_n)+\\sup_{h\\in\\mathcal{H}}|L_n(h)-L(h)|\\\\\n",
    "&\\leq 2\\sup_{h\\in\\mathcal{H}}|L_n(h)-L(h)|\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "Dit autrement, $\\sup_{h\\in\\mathcal{H}}|L_n(h)-L(h)|$ est la plus grosse erreur d'estimation du risque sur notre jeu de donn√©es $S_n$ dans notre classe de fonctions $\\mathcal{H}$. Cette derni√®re permet de majorer le choix du minimiseur du risque empirique. Na√Øvement, si $\\sup_{h\\in\\mathcal{H}}|L_n(h)-L(h)|=0$ (on estime parfaitement nos fonctions), alors $L(h_n)-\\inf_{h\\in\\mathcal{H}}L(h)=0$ et on choisit la meilleure fonction par rapport au vrai risque.\n",
    "\n",
    "\n",
    "L'objectif de la th√©orie VC va √™tre de trouver des strat√©gies permettant de quantifier l'√©volution de $\\sup_{h\\in\\mathcal{H}}|L_n(h)-L(h)|$. La calculer exactement √©tant impossible, nous allons donc la majorer par des quantit√©s plus simples √† estimer. Et si nos quantit√©s convergent vers $0$, alors $\\sup_{h\\in\\mathcal{H}}|L_n(h)-L(h)|$, en √©tant major√© par quelque chose qui converge vers $0$ et en restant positif devra n√©cessairement converger vers $0$ √©galement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Apart√© sur les in√©galit√©s de concentration\n",
    "\n",
    "La th√©orie VC fait souvent appel √† ce qu'on appelle une in√©galit√© de concentration. L'id√©e d'une in√©galit√© de concentration est de constater que nos variables al√©atoires ne peuvent finalement pas trop s'√©carter de certaines valeurs o√π elles se retrouvent \"concentr√©es\". La plus simple d'entre elle est [in√©galit√© de Markov](https://fr.wikipedia.org/wiki/In√©galit√©_de_Markov):\n",
    "\n",
    "----\n",
    "**L'in√©galit√© de Markov :** Soit $X$ une variable al√©atoire r√©elle **positive**, nous avons :\n",
    "\n",
    "$$\\forall a\\in\\mathbb{R}^+,\\ \\mathbb{P}\\big(X\\geq a\\big)\\leq \\frac{\\mathbb{E}\\big[X\\big]}{a}.$$\n",
    "\n",
    "**Preuve :**\n",
    "\n",
    "Soit $a\\in\\mathbb{R}^+$ et $\\textbf{1}\\{X\\geq a\\}$ la fonction qui vaut $1$ si $X$ est sup√©rieur ou √©gal √† $a$ et $0$ sinon. Nous avons n√©cessairement :\n",
    "\n",
    "$$a \\textbf{1}\\{X\\geq a\\}\\leq X\\textbf{1}\\{X\\geq a\\}\\leq X,$$\n",
    "\n",
    "quelque soit la r√©alisation de $X$. En effet si $X<a$ alors $\\textbf{1}\\{X\\geq a\\}$ vaut $0$. L'esp√©rance √©tant croissante, nous avons :\n",
    "\n",
    "$$\\mathbb{E}\\Big[a\\textbf{1}\\{X\\geq a\\}\\Big]\\leq\\mathbb{E}\\Big[X\\Big].$$\n",
    "\n",
    "Par d√©finition de l'esp√©rance, nous avons :\n",
    "\n",
    "$$\\mathbb{E}\\Big[a\\textbf{1}\\{X\\geq a\\}\\Big]=a \\mathbb{P}\\big(X\\geq a\\big) + 0\\mathbb{P}\\big(X< a\\big)=a \\mathbb{P}\\big(X\\geq a\\big).$$\n",
    "\n",
    "Ainsi, en combinant le tout, nous avons : \n",
    "\n",
    "$$a \\mathbb{P}\\big(X\\geq a\\big)\\leq \\mathbb{E}\\Big[X\\Big]\\Leftrightarrow \\mathbb{P}\\big(X\\geq a\\big)\\leq \\frac{\\mathbb{E}\\big[X\\big]}{a}.$$\n",
    "\n",
    "----\n",
    "\n",
    "En d'autres termes, pour une variable al√©atoire $X$ de loi fix√©e sa masse ne se situe pas √† l'infinie et plus $a$ sera grand, plus il sera improbable que $X$ soit au-del√†.\n",
    "\n",
    "Convainquons-nous du r√©sultat pr√©c√©dent via une simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# on simule une variable uniforme entre 0 et 100\n",
    "def simulate_random_variable(a, nb_times=1000):\n",
    "    X = 100 * np.random.random(size=nb_times)\n",
    "    empirical_expectation = X.mean()\n",
    "    empirical_probability = (X>a).sum()/nb_times\n",
    "    print('P(X>=a)<= E[X]/a :', empirical_probability, '<=', empirical_expectation/a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_random_variable(a=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et c'est l√† tout l'int√©r√™t ! Supposons que nous ne sachions pas calculer $\\mathbb{P}\\big(X\\geq a)$ mais que nous sachions que $\\mathbb{E}\\big[X\\big]$ alors on peut garantir un \"pire sc√©nario\" de la vitesse √† la quelle la probabilit√© converge vers $0$ lorsque $a$ augmente. Pour une variable uniforme entre $0$ et $100$, son esp√©rance et $50$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a = np.linspace(1, 100, 999)\n",
    "\n",
    "probability = 1 - a/100\n",
    "expectation = 50/a\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(a, probability, label='$\\mathbb{P}(X\\geq a)$ (inconnue)')\n",
    "plt.plot(a, expectation, label=r'$\\mathbb{E}[X]/a$')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La quantit√© $\\mathbb{E}\\big[X\\big]/a$ est potentiellement sup√©rieure √† $1$ et nous avons contraint matplotlib √† n'afficher que les valeurs entre $0$ et $1$. Pour cette exemple, nous aurions pu tr√®s na√Øvement calculer la probabilit√© nous-m√™me et nous n'avions aucun int√©r√™t √† passer par une in√©galit√© de concentration. Cependant, dans certains probl√®mes, c'est in√©vitable !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. En supposant $|\\mathcal{H}|<\\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Le Union Bound.**\n",
    "\n",
    "Nous avons tous vu au lyc√©e la formule suivante :\n",
    "\n",
    "$$\\mathbb{P}\\big(A\\cup B\\big)=\\mathbb{P}\\big(A\\big)+\\mathbb{P}\\big(B\\big)-\\mathbb{P}\\big(A\\cap B\\big).$$\n",
    "\n",
    "Si on √©limine la probabilit√© li√©e √† l'intersection, nous avons :\n",
    "\n",
    "$$\\mathbb{P}\\big(A\\cup B\\big)\\leq\\mathbb{P}\\big(A\\big)+\\mathbb{P}\\big(B\\big)$$\n",
    "\n",
    "\n",
    "De mani√®re plus g√©n√©rale, imaginons une famille d'√©v√®nements $A_i$, $i\\leq N$, tels que $\\forall i\\leq N$, nous avons $\\mathbb{P}(A_i)\\leq K$ (la probabilit√© est major√©e par une quantit√© $K$. Nous avons alors l'in√©galit√© suivante :\n",
    "\n",
    "$$\\mathbb{P}\\big(\\cup_i A_i\\big)\\leq \\sum_i \\mathbb{P}\\big(A_i\\big)\\leq \\sum_i K\\leq KN.$$\n",
    "\n",
    "C'est ce qu'on appelle un *union bound* : on majore gr√¢ce √† une in√©galit√© li√©e √† l'union.\n",
    "\n",
    "---\n",
    "\n",
    "Soit une famille de fonction $\\mathcal{H}$ de $\\mathcal{X}$ dans $\\mathcal{Y}$ telle que $|\\mathcal{H}|<\\infty$. Et soit $h\\in\\mathcal{H}$. Nous avons d√©fini le risque et le risque empirique not√©s respectivement $L_n$ et $L$. La probabilit√© que nous souhaiterions voir la plus petite possible est $\\mathbb{P}\\big(|L_n(h)-L(h)|>\\epsilon)$. Nous pouvons la majorer gr√¢ce √† l'[in√©galit√© d'Hoeffding](https://fr.wikipedia.org/wiki/In√©galit√©_de_Hoeffding) : \n",
    "\n",
    "$$\\mathbb{P}\\big(|L_n(h)-L(h)|>\\epsilon) \\leq 2 e^{-2\\epsilon^2n}.$$\n",
    "\n",
    "Cela nous dit que la probabit√© que notre estimateur empirique s'√©carte de plus de $\\epsilon$ de son esp√©rance d√©cro√Æt exponentiellement vite lorsque la taille du jeu de donn√©es augmente ! \n",
    "\n",
    "Cependant, pour pouvoir garantir que l'apprentissage se fera bien (i.e. qu'on choisira un bon minimiseur du risque empirique), nous devons majorer la probabilit√© que TOUTES les fonctions soient bien estim√©es. Pour cela, nous utilisons le *union bound* :\n",
    "\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{P}\\big(\\sup_{h\\in\\mathcal{H}} |L_n(h)-L(h)|>\\epsilon\\big)&=\\mathbb{P}\\big(\\cup_{h\\in\\mathcal{H}} |L_n(h)-L(h)|>\\epsilon\\big)\\\\\n",
    "&\\leq \\sum_{h\\in\\mathcal{H}}\\mathbb{P}\\big(\\sup_{h\\in\\mathcal{H}} |L_n(h)-L(h)|>\\epsilon\\big)\\\\\n",
    "&\\leq |\\mathcal{H}|2 e^{-2\\epsilon^2n}.\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "C'est un premier r√©sultat fondamental qui nous permet de dire que si $|\\mathcal{H}|<\\infty$, alors on peut r√©duire uniform√©ment (i.e. pour toutes fonctions de $\\mathcal{H}$) les d√©viations entre le risque empirique et son esp√©rance et donc que le choix du minimiseur empirique ne sera pas mauvais (pour peu que le jeu de donn√©es soit assez grand)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(scale):\n",
    "    H_size = np.linspace(10, 100, 3)\n",
    "    epsilon = np.linspace(0.04, 0.1, 3)\n",
    "    n = np.linspace(10, 2000, 100)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for k in H_size:\n",
    "        for i, e in enumerate(epsilon):\n",
    "            plt.plot(n, k*2*np.exp(-2*(e**2)*n), label=r'$\\epsilon=${}, $|H|=${}'.format(e, int(k)))\n",
    "    plt.legend()\n",
    "    plt.yscale(scale)\n",
    "    plt.ylim(None if scale == 'log' else 0, 1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot('linear')\n",
    "plot('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Le cas g√©n√©ral : $|\\mathcal{H}|=\\infty$\n",
    "\n",
    "Le cas g√©n√©ral permet √©galement de conclure pour des cas o√π le cardinal serait fini. \n",
    "\n",
    "\n",
    "L'intuition derri√®re le cas g√©n√©ral est qu'au lieu de consid√©rer toutes les fonctions de $\\mathcal{H}$, nous pouvons discr√©tiser $\\sup_{h\\in\\mathcal{H}}|L_n(h)-L(h)|$ de mani√®re √† ne consid√©rer que les \"valeurs effectives\" que notre classe de fonctions peut prendre sur notre jeu de donn√©e. Ainsi, si on consid√®re $S_1=\\{(X_1, Y_1)\\}$, alors, quelque soit $\\mathcal{H}$, il ne peut y avoir au plus que deux fonctions diff√©rentes sur $S_1$ : celle qui retourne $1$ pour $X_1$ et celle qui retourne $0$ pour $X_1$.\n",
    "\n",
    "*A priori*, le nombre de valeur que peut prendre $\\mathcal{H}$ sur un jeu de donn√©es $S_n$ d√©pendra du tirage du jeu de donn√©es. Pour cela, nous pouvons majorer cette quantit√© en ne consid√©rant que le \"pire des cas\". C'est l'objet de la d√©finition suivante :\n",
    "\n",
    "---\n",
    "**La fonction de croissance :**\n",
    "\n",
    "$$\\tau_\\mathcal{H}(n)=\\max_{\\{X_1, ..., X_n\\}}|\\{h(X_1), ..., h(X_n):\\ h\\in\\mathcal{H}\\}|.$$\n",
    "\n",
    "C'est le plus grand nombre de mani√®res diff√©rentes qu'une classe de fonction pourrait lab√©liser un jeu de donn√©es de taille $n$. C'est le plus grand nombre configuration de taille $n$ atteignables par notre classe de fonctions.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Notre objectif ici est de d√©montrer le r√©sultat suivant : \n",
    "\n",
    "$$\\mathbb{P}\\Big(\\sup_{h\\in\\mathcal{H}}|L_n(h)-L(h)|>\\epsilon\\Big)\\leq 8 \\tau_{\\mathcal{H}}(n)e^{-n\\epsilon^2/32}$$\n",
    "\n",
    "---\n",
    "\n",
    "### PREUVE\n",
    "\n",
    "\n",
    "On remarque que si $n\\epsilon^2<2$, alors la partie droite de l'in√©galit√© est sup√©rieure √† $1$ et l'in√©galit√© est trivialement vraie. Supposons $n\\epsilon^2\\geq 2$.\n",
    "\n",
    "\n",
    "**√âtape 1 : Sym√©trisation par un √©chantillon fant√¥me (√©chantillon de test).**\n",
    "\n",
    "Construisons un jeu de donn√©es de test virtuel $S_n^\\prime=\\{(X_i^\\prime, Y_i^\\prime)\\}_{i\\leq n}$ tel que $S_n\\sim S_n^\\prime$. Notons $L_n^\\prime$ le risque empirique associ√© √† cet √©chantillon. Supposons $n\\epsilon^2\\geq 2$, alors nous avons : \n",
    "\n",
    "$$\\mathbb{P}\\Big(\\sup_{h\\in\\mathcal{H}}|L_n(h)-L(h)|>\\epsilon\\Big)\\leq 2 \\mathbb{P}\\Big(\\sup_{h\\in\\mathcal{H}}|L_n(h)-L_n^\\prime(h)|>\\epsilon/2\\Big)$$\n",
    "\n",
    "Pour voir cela, notons $h^\\star$ une fonction telle que $|L_n(h^\\star)-L(h^\\star)|>\\epsilon$ si une telle fonction existe, sinon une fonction fix√©e au hasard. Nous avons alors :\n",
    "\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{P}\\Big(\\sup_{h\\in\\mathcal{H}}|L_n(h)-L_n^\\prime(h)|>\\epsilon/2\\Big)&\\geq \\mathbb{P}\\Big(|L_n(h^\\star)-L_n^\\prime(h^\\star)|>\\epsilon/2\\Big)\\\\\n",
    "&\\geq\\mathbb{P}\\Big(|L_n(h^\\star)-L(h^\\star)|>\\epsilon, |L_n^\\prime(h^\\star)-L(h^\\star)|<\\epsilon/2\\Big)\\\\\n",
    "&=\\mathbb{E}\\Big[\\textbf{1}\\{|L_n(h^\\star)-L(h^\\star)|>\\epsilon\\}\\mathbb{P}\\Big(|L_n^\\prime(h^\\star)-L(h^\\star)|<\\epsilon/2|Z_1,\\ldots, Z_n\\Big)\\Big]\n",
    "\\end{aligned}$$\n",
    "\n",
    "o√π $Z_i=(X_i, Y_i)$. En utilisant l'[In√©galit√© de Bienaym√©-Tchebychev](https://fr.wikipedia.org/wiki/In√©galit√©_de_Bienaym√©-Tchebychev), nous avons :\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{P}\\Big(|L_n^\\prime(h^\\star)-L(h^\\star)|<\\epsilon/2|Z_1,\\ldots, Z_n\\Big)&\\geq 1-\\frac{L(h^\\star)(1-L(h^\\star))/n}{(\\epsilon/2)^2}\\\\\n",
    "&\\geq 1-\\frac{1/4}{n\\epsilon^2/4}\\geq \\frac{1}{2}.\n",
    "\\end{aligned}$$\n",
    "\n",
    "(car $n\\epsilon^2>2$).\n",
    "Ainsi : \n",
    "\n",
    "$$\\mathbb{E}\\Big[\\textbf{1}\\{|L_n(h^\\star)-L(h^\\star)|>\\epsilon\\}\\mathbb{P}\\Big(|L_n^\\prime(h^\\star)-L(h^\\star)|<\\epsilon/2|Z_1,\\ldots, Z_n\\Big)\\Big] \\geq \\mathbb{E}\\Big[\\textbf{1}\\{|L_n(h^\\star)-L(h^\\star)|>\\epsilon\\}\\Big]\\frac{1}{2}=\\mathbb{P}\\Big(|L_n(h^\\star)-L(h^\\star)|>\\epsilon\\Big)\\frac{1}{2},$$\n",
    "\n",
    "et nous obtenons le r√©sultat voulu.\n",
    "\n",
    "Cette premi√®re √©tape nous dit que comparer le score empirique de notre risque par rapport √† son esp√©rance est √† peu pr√®s la m√™me chose que le comparer avec un jeu de test. Testons cela au travers d'une petite exp√©riences avec des tirages binomiaux normalis√©s.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "H = [0.25, 0.5, 0.75]\n",
    "n = 50\n",
    "epsilon_list = (0.01, 0.1, 0.2)\n",
    "# on r√©p√®te l'exp√©rience pour calculer empiriquement la probabilit√©\n",
    "size = 1000\n",
    "\n",
    "for epsilon in epsilon_list:\n",
    "    print('*' * 20)\n",
    "    print('Epsilon={}'.format(epsilon))\n",
    "    results = []\n",
    "    for h in H:\n",
    "        Ln = np.random.binomial(n=n, p=h, size=size)/n\n",
    "        results.append(np.abs(Ln-h)>epsilon)\n",
    "    empirical_probability_1 = ((np.array(results).sum(axis=0)>0).sum()) / size\n",
    "    print('* P(sup |L_n-L|>epsilon)=', empirical_probability_1)\n",
    "\n",
    "    results = []\n",
    "    for h in H:\n",
    "        binom = np.random.binomial(n=n, p=h, size=size)/n\n",
    "        binom_ghost = np.random.binomial(n=n, p=h, size=size)/n\n",
    "        results.append(np.abs(binom-binom_ghost)>epsilon/2)\n",
    "\n",
    "    empirical_probability_2 = (np.array(results).sum(axis=0)>0).sum() / size\n",
    "    print('* P(sup |L_n-L_n\\'|>epsilon)=', empirical_probability_2)\n",
    "    print('* 2xP(sup |L_n-L_n\\'|>epsilon)=', 2*empirical_probability_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La seconde √©tape va nous permettre d'√©limiter ce jeu fant√¥me auquel nous n'avons pas acc√®s. Aussit√¥t mis, aussit√¥t retir√©.\n",
    "\n",
    "---\n",
    "**√âtape 2 : Sym√©trisation avec des signes al√©atoires.**\n",
    "\n",
    "Nous avons donc la variable suivante :\n",
    "\n",
    "$$\\sup_{h\\in\\mathcal{H}}|L_n(h)-L_n^\\prime(h)|=\\sup_{h\\in\\mathcal{H}}\\frac{1}{n}|\\sum_i\\textbf{1}\\{h(X_i)\\neq Y_i\\}-\\textbf{1}\\{h(X_i^\\prime)\\neq Y_i^\\prime\\}|$$\n",
    "\n",
    "Puisque nos variables $\\textbf{1}\\{h(X_i)\\neq Y_i\\}$ et $\\textbf{1}\\{h(X_i^\\prime)\\neq Y_i^\\prime\\}$ sont iid, cela revient exactement √† \n",
    "\n",
    "$$\\sup_{h\\in\\mathcal{H}}\\frac{1}{n}|\\sum_i\\sigma_i(\\textbf{1}\\{h(X_i)\\neq Y_i\\}-\\textbf{1}\\{h(X_i^\\prime)\\neq Y_i^\\prime\\})|,$$\n",
    "\n",
    "o√π $\\sigma_i$ est une variable de Rademacher (i.e. $\\mathbb{P}\\big(\\sigma_i=-1\\big)=\\mathbb{P}\\big(\\sigma_i=+1\\big)=0.5$).\n",
    "\n",
    "Nous avons donc : \n",
    "\n",
    "$$\\mathbb{P}\\Big(\\sup_{h\\in\\mathcal{H}}\\frac{1}{n}|\\sum_i\\sigma_i\\textbf{1}\\{h(X_i)\\neq Y_i\\}-\\sigma_i\\textbf{1}\\{h(X_i^\\prime\\neq Y_i^\\prime\\})|>\\epsilon/2\\Big)$$\n",
    "\n",
    "Et au moins l'un deux deux termes $|\\frac{1}{n}\\sum_i\\sigma_i\\textbf{1}\\{h(X_i)\\neq Y_i\\}|$ doit √™tre sup√©rieur √† $\\epsilon/4$ pour que la somme soit sup√©rieure √† $\\epsilon/2$ (v√©rifier par contradiction). Ainsi, en appliquant le *union bound*, nous avons : \n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{P}\\Big(\\sup_{h\\in\\mathcal{H}}\\frac{1}{n}|\\sum_i\\sigma_i\\textbf{1}\\{h(X_i)\\neq Y_i\\}-\\sigma_i\\textbf{1}\\{h(X_i^\\prime\\neq Y_i^\\prime\\})&|>\\epsilon/2\\Big) \\\\\n",
    "&\\leq\\mathbb{P}\\Big(\\sup_{h\\in\\mathcal{H}}\\frac{1}{n}|\\sum_i\\sigma_i\\textbf{1}\\{h(X_i)\\neq Y_i\\}|>\\epsilon/4\\Big)+\\mathbb{P}\\Big(\\sup_{h\\in\\mathcal{H}}\\frac{1}{n}|\\sigma_i\\textbf{1}\\{h(X_i^\\prime\\neq Y_i^\\prime\\}|>\\epsilon/4\\Big)\\\\\n",
    "&=2\\mathbb{P}\\Big(\\sup_{h\\in\\mathcal{H}}\\frac{1}{n}|\\sum_i\\sigma_i\\textbf{1}\\{h(X_i)\\neq Y_i\\}|>\\epsilon/4\\Big)\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**√âtape 3 : Conditionnement.**\n",
    "Jusqu'ici, nous consid√©rions le jeu de donn√©es comme une variable al√©atoire. Fixons le et √©tudions un cas particulier. Notons $z_1, \\ldots, z_n\\in\\mathcal{X}$ cette r√©alisation.\n",
    "\n",
    "\n",
    "Nous avons donc :\n",
    "\n",
    "$$\\mathbb{P}\\Big(\\sup_{h\\in\\mathcal{H}}\\frac{1}{n}|\\sum_i\\sigma_i\\textbf{1}\\{h(X_i)\\neq Y_i\\}|>\\epsilon/4|Z_1=z_1, \\ldots, Z_n=z_n\\Big).$$\n",
    "\n",
    "Le nombre de configuration √† tester est justement la fonction de croissance qui nous indique toutes les valeurs que peut prendre notre classe de fonctions $\\mathcal{H}$ sur un jeu de donn√©es fix√© de taille $n$. En appliquant le *union bound* √† nouveau, nous obtenons donc :\n",
    "\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{P}\\Big(\\sup_{h\\in\\mathcal{H}}\\frac{1}{n}|\\sum_i\\sigma_i\\textbf{1}\\{h(X_i)\\neq Y_i\\}|>\\epsilon/4|Z_1, \\ldots, Z_n\\Big)\\leq\\tau_\\mathcal{H}(n)\\sup_{h\\in\\mathcal{H}}\\mathbb{P}\\Big(\\frac{1}{n}|\\sum_i\\sigma_i\\textbf{1}\\{h(X_i)\\neq Y_i\\}|>\\epsilon/4|Z_1, \\ldots, Z_n\\Big).\n",
    "\\end{aligned}$$\n",
    "\n",
    "De plus, en appliquant [l'in√©galit√© d'Hoeffding](https://fr.wikipedia.org/wiki/In√©galit√©_de_Hoeffding) , nous pouvons majorer la probabilit√© de droite quelque soit la fonction :\n",
    "\n",
    "$$\\mathbb{P}\\Big(\\frac{1}{n}|\\sum_i\\sigma_i\\textbf{1}\\{h(X_i)\\neq Y_i\\}|>\\epsilon/4|Z_1, \\ldots, Z_n\\Big)\\leq 2e^{-n\\epsilon^2/32}$$\n",
    "\n",
    "Cette variable ne d√©pend pas du conditionnement et nous pouvons donc prendre l'esp√©rance :\n",
    "\n",
    "$$\\mathbb{P}\\Big(\\frac{1}{n}|\\sum_i\\sigma_i\\textbf{1}\\{h(X_i)\\neq Y_i\\}|>\\epsilon/4\\Big)=\\mathbb{E}\\Big[\\mathbb{P}\\Big(\\frac{1}{n}|\\sum_i\\sigma_i\\textbf{1}\\{h(X_i)\\neq Y_i\\}|>\\epsilon/4|Z_1, \\ldots, Z_n\\Big)\\Big] \\leq 2e^{-n\\epsilon^2/32}$$\n",
    "\n",
    "**Conclusion.**\n",
    "\n",
    "En combinait les 3 √©tapes pr√©c√©dentes, nous obtenons ainsi : \n",
    "\n",
    "$$\\mathbb{P}\\Big(\\sup_{h\\in\\mathcal{H}}|L_n(h)-L(h)|>\\epsilon\\Big)\\leq 8 \\tau_{\\mathcal{H}}(n)e^{-n\\epsilon^2/32}$$\n",
    "\n",
    "----\n",
    "\n",
    "Des r√©sultats beaucoup plus stricts existent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. La dimension VC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le th√©or√®me pr√©c√©dent est tr√®s int√©ressant, mais sans hypoth√®se sur notre classe de fonction $\\mathcal{H}$, nous pourrions tr√®s bien avoir :\n",
    "\n",
    "$$\\tau_\\mathcal{H}(n)=2^n,$$\n",
    "\n",
    "et cela nous donnerait :\n",
    "\n",
    "$$\\lim_{n\\rightarrow\\infty}8 \\cdot 2^ne^{-n\\epsilon^2/32}=\\infty,$$\n",
    "\n",
    "ce qui nous emp√™cherait de conclure !\n",
    "\n",
    "Il se trouve que pour certaines classes de fonctions (et nous verront des exemples), quand bien m√™me $|\\mathcal{H}|=\\infty$, nous n'aurions pas $\\tau_\\mathcal{H}(n)=2^n$.\n",
    "\n",
    "---\n",
    "\n",
    "<span style=\"color:blue\">**Exercice :**</span> **Soit $\\mathcal{X}=\\mathbb{R}$, $\\mathcal{Y}=\\{0, 1\\}$ et $\\mathcal{H}=\\{h_s(x)=\\textbf{1}\\{x>s\\}:\\ s\\in\\mathbb{R}\\}$, l'ensemble des fonctions seuil (on retourne $1$ si $x>s$ et $0$ sinon). Montrer que $\\tau_\\mathcal{H}(2)=3$ et non $2^2=4$.**\n",
    "\n",
    "<span style=\"color:green\">**R√©ponse :**</span> **Soit $x_1, x_2\\in\\mathcal{X}$ tels que $x_1<x_2$ sans perte de g√©n√©ralit√©. La configuration (1, 0) est atteignable si $s<x_1$ et $x_2 < s$ entra√Ænant une contradiction. √Ä l'inverse les configurations (0, 0), (0, 1) et (1, 1) sont facilement atteignables.**\n",
    "\n",
    "---\n",
    "\n",
    "Nous appelons la dimension VC ou VCdim d'une classe de fonctions $\\mathcal{H}$ le plus grand jeu de donn√©es S_n tel que $\\tau_\\mathcal{H}(n)=2^n$. Plus formellement, nous avons :\n",
    "\n",
    "$$\\text{VCdim}(\\mathcal{H})=\\max_{\\tau_{\\mathcal{H}}(n)=2^n}n$$\n",
    "\n",
    "L'int√©r√™t cl√© de cette propri√©t√© nous vient du lemme suivant :\n",
    "\n",
    "---\n",
    "**Lemme de Sauer.**\n",
    "\n",
    "Soit $\\mathcal{H}$ notre classe de fonctions telle que $\\text{VCdim}(\\mathcal{H})\\leq d<\\infty$. Nous avons alors $\\forall n>0$ :\n",
    "\n",
    "$$\\tau_\\mathcal{H}(n)\\leq \\sum_{i=0}^d{n\\choose i}.$$\n",
    "\n",
    "De plus, si $n>d+1$, alors :\n",
    "\n",
    "$$\\tau_\\mathcal{H}(n)\\leq\\Bigg(\\frac{en}{d}\\Bigg)^d.$$\n",
    "\n",
    "---\n",
    "\n",
    "Ainsi, la fonction de croissance ne cro√Æt plus exponentiellement vite mais polynomialement d√®s qu'on d√©passe la dimension VC de notre classe de fonctions. Cela implique que notre majorant de g√©n√©ralisation converge vers $0$, qu'on puisse estimer l'erreur de nos fonctions correctement et donc que le choix du minimiseur du risque empirique est un bon choix : ce r√©sultat est ce qu'on appelle le *th√©or√®me fondamental du machine learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:blue\">**Exercice :**</span> **Soit $\\mathcal{X}=\\mathbb{R}^2$, $\\mathcal{Y}=\\{0, 1\\}$ et $\\mathcal{H}$ l'ensemble des classifieurs lin√©aires de $\\mathbb{R}^2$. D√©montrer que la dimension VC de $\\mathcal{H}$ est au moins $3$.**\n",
    "```{toggle}\n",
    "\n",
    "<span style=\"color:green\">**R√©ponse :**</span> **On v√©rifie facilement avec un dessin que $\\mathcal{H}$ classifie bien toutes les configurations d'un jeu de donn√©es de taille $3$.**\n",
    "```\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:blue\">**Exercice :**</span> **Soit $\\mathcal{H}_{=k}$ l'ensemble des classifieurs qui ne peuvent associer le label $1$ qu'√† *exactement* $k$ √©l√©ments de $\\mathcal{X}$. Trouver la dimension VC. On supposera que $|\\mathcal{X}|\\geq k$.**\n",
    "\n",
    "\n",
    "```{toggle}\n",
    "<span style=\"color:green\">**R√©ponse :**</span> **La dimension VC d'un tel ensemble est $\\text{VCdim}(\\mathcal{H})=\\text{min}(k, |\\mathcal{X}|-k)$. Montrons (1) que $\\text{VCdim}(\\mathcal{H})\\leq k$. Soit un jeu de donn√©es $S_{k+1}$, alors il n'est pas possible de mettre $1$ √† tous les points. Montrons (2) que $\\text{VCdim}(\\mathcal{H})\\leq |\\mathcal{X}|-k$. Soit $S_{|\\mathcal{X}|-k+1}$, alors, il n'est pas possible de mettre $0$ √† tous les points. Montrons (3) maintenant que $\\text{VCdim}(\\mathcal{H})\\geq \\text{min}(k, |\\mathcal{X}|-k)$. Soit $S_n$ tel que $n\\leq \\text{min}(k, |\\mathcal{X}|-k)$ alors toutes les configurations sont atteignables. En effet, il y a au plus $k$ $1$ √† mettre et il restera toujours au moins $k$ $1$ √† l'ext√©rieur de $S_k$.**\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

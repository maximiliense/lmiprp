{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "resistant-diving",
   "metadata": {},
   "source": [
    "# Le classifieur de Bayes ☕️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-contribution",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Reformalisons notre problème de classification. Soit $\\mathcal{X}$ l'espace des données d'entrée et $\\mathcal{Y}$ celui des données de sorties. On aurait par exemple $\\mathcal{X}$ l'ensemble de toutes les photos et $\\mathcal{Y}$ celui des labels \"chien\" et \"chat\". A priori, dans le cadre d'une application de détection \"chien/chat\", on ne s'attend pas à trouver n'importe quelle image (e.g. on ne s'attend pas à voir des photos de pizzas). L'idée est d'interpréter cela de manière probabiliste en considérons le couple de variables aléatoires suivant : \n",
    "\n",
    "$$X, Y\\in\\mathcal{X}\\times\\mathcal{Y}.$$\n",
    "\n",
    "On appellera $\\mu$ la mesure de $X$ dans le sens où étant donné $A\\subseteq\\mathcal{X}$, on a $\\mathbb{P}(X\\in A)=\\mu(A)$ et $\\eta(x)=\\mathbb{P}(Y=1|X=x)$ (dans le cas de la classification binaire) la \"probabilité a posteriori\". La fonction $\\eta$ revête d'autres formes lorsqu'on travaille sur d'autres problèmes (e.g. classification, top-k). Il est important de constater que $\\mu$ et $\\eta$ décrivent totalement les deux variables aléatoires $X, Y$. En effet, soit $A\\subseteq \\mathcal{X}\\times \\mathcal{Y}$, nous avons :\n",
    "\n",
    "$$\\mathbb{P}(X, Y\\in A)=\\int_{A\\cap \\mathcal{X}\\times \\{1\\}}\\eta(x)d\\mu+\\int_{A\\cap \\mathcal{X}\\times \\{0\\}}(1-\\eta(x))d\\mu.$$\n",
    "\n",
    "Notre objectif est de construire une application $h:\\mathcal{X}\\mapsto\\mathcal{Y}$ telle que les \"prédictions\" de $h$ soient bonnes dans le sens du risque suivant :\n",
    "\n",
    "$$L(h)=\\mathbb{P}(h(X)\\neq Y)=\\mathbb{E}\\big[\\textbf{1}\\{h(X)\\neq Y\\}\\big].$$\n",
    "\n",
    "On veut que la probabilité que $h$ fasse des erreurs soient la plus faible possible.\n",
    "\n",
    "C'est parque ne connaissant ni $\\mu$ ni $\\eta$ que nous ne pouvons pas calculer $L$ et donc trouver le meilleur $h$. Habituellement, en *machine learning*, nous collectons des données afin d'estimer $L$ et construire le classifieur approprié.\n",
    "\n",
    "## Le classifieur de Bayes\n",
    "\n",
    "Supposons que nous connaissions $\\eta$ (nous sommes un oracle). Quel est le meilleur classifieur que nous puissions construire ?\n",
    "\n",
    "Il s'agit du classifieur suivant :\n",
    "\n",
    "$$g^\\star(x)=\\begin{cases}1&\\text{ si }\\eta(x)\\geq 0.5\\\\ 0&\\text{ sinon.}\\end{cases}$$\n",
    "\n",
    "C'est ce qu'on appelle le classifieur de Bayes. C'est lui qui fait le moins d'erreurs (notons qu'il peut exister plusieurs classifieurs qui font aussi peu d'erreurs). On peut quantifier le risque atteint par ce classifieur :\n",
    "\n",
    "$$L^\\star=\\mathbb{E}\\big[\\text{min}(\\eta(X), 1-\\eta(X))\\big].$$\n",
    "\n",
    "Si les labels sont déterministes (i.e. $\\eta\\in\\{0, 1\\}$), alors $L^\\star=0$.\n",
    "\n",
    "----\n",
    "**Proposition.**\n",
    "\n",
    "$\\not\\exists g:\\mathcal{X}\\mapsto\\mathcal{Y}$ tel que $L(g)<L^\\star$. Dit autrement, on ne peut pas faire mieux que le classifieur de Bayes.\n",
    "\n",
    "**Preuve.**\n",
    "\n",
    "Soit $x\\in\\mathcal{X}$ et $g:\\mathcal{X}\\mapsto\\mathcal{Y}$ un classifieur quelconque. On a :\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{P}(g(X)\\neq Y|X=x)&=1-\\mathbb{P}(g(X)=Y|X=x)\\\\\n",
    "&= 1-(\\mathbb{P}(Y=1, g(X)=1|X=x)+\\mathbb{P}(Y=0, g(X)=1|X=x))\\\\\n",
    "&=1-(\\textbf{1}\\{g(x)=1\\}\\mathbb{P}(Y=1|X=x)+\\textbf{1}\\{g(x)=0\\}\\mathbb{P}(Y=0|X=x))\\\\\n",
    "&=1-(\\textbf{1}\\{g(x)=1\\}\\eta(x)+\\textbf{1}\\{g(x)=0\\}(1-\\eta(x))).\n",
    "\\end{aligned}$$\n",
    "\n",
    "Ainsi, nous avons :\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{P}(g(X)\\neq Y|X=x)-\\mathbb{P}(g^\\star(X)\\neq Y|X=x)&=\\eta(x)(\\textbf{1}\\{g(x)=1\\}-\\textbf{1}\\{g^\\star(x)=1\\})+(1-\\eta(x))(\\textbf{1}\\{g(x)=0\\}-\\textbf{1}\\{g^\\star(x)=0\\})\\\\\n",
    "&=(2\\eta(x)-1)(\\textbf{1}\\{g(x)=1\\}-\\textbf{1}\\{g^\\star(x)=1\\})\\\\\n",
    "&\\geq 0\n",
    "\\end{aligned}$$\n",
    "\n",
    "Il suffit maintenant de prendre l'espérance sur tous les \"x\" et le résultat est là.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-perry",
   "metadata": {},
   "source": [
    "## Un petit exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-dress",
   "metadata": {},
   "source": [
    "Construisons un jeu de données de classification binaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-nurse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "beta = np.random.uniform(-10, 10, size=(2, 1))\n",
    "\n",
    "def class_probability(x):\n",
    "    logit = np.dot(x, beta)\n",
    "    proba = sigmoid(logit)\n",
    "    \n",
    "    return proba\n",
    "\n",
    "def dataset(n):\n",
    "    X = np.random.uniform(-1, 1, size=(n, 2))\n",
    "    y = np.random.binomial(n=1, p=class_probability(X))\n",
    "    return X, y\n",
    "\n",
    "X, y = dataset(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-potter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-composition",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:blue\">**Exercice :**</span> **En vous appuyant sur le code précédent (en réutilisant éventuellement du code fourni), implémentez le classifieur de Bayes**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-church",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_classifier(x):\n",
    "    ####### Complete this part ######## or die ####################\n",
    "    return (class_probability(x)>=0.5).astype(int)\n",
    "    ###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-value",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "XX, YY = np.mgrid[-1:1:500j, -1:1:500j]\n",
    "predictions = bayes_classifier(np.stack([XX, YY], axis=2).reshape(500*500, 2))\n",
    "\n",
    "plt.pcolormesh(XX, YY, predictions.reshape(500, 500), cmap=plt.cm.Paired, shading='auto')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(-1, 1)\n",
    "plt.title('Le classifieur de Bayes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-momentum",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Les modèles max-margin ☕️☕️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques liens pour plus de détails :\n",
    "* [Overfitting](https://github.com/maximiliense/lmpirp/blob/main/Notes/Overfitting.pdf)\n",
    "* [SVM](https://github.com/maximiliense/lmpirp/blob/main/Notes/SVM.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine à vecteurs de support ou l'hypothèse du *max-margin*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'hypothèse implicite derrière le *max-margin* est qu'entre deux frontières de même complexité, la plus robuste aux perturbations (dans le sens où si on perturbe un élément du train la probabilité qu'il change de classe est la plus faible) est la meilleure. L'idée fait sens car on peut supposer que les échantillons nouveaux peuvent être vus comme des perturbations des échantillons du jeu d'apprentissage. \n",
    "\n",
    "La frontière la plus robuste aux perturbations est celle qui maximise la distance entre le point le plus proche et elle-même. C'est l'hypothèse du *max-margin*.\n",
    "\n",
    "Le SVM, ou machine à vecteurs de support adopte cette stratégie. Une présentation plus détaillée du SVM est disponible à l'adresse suivante : [SVM](https://github.com/maximiliense/lmpirp/blob/main/Notes/SVM.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un problème de classification linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le SVM est un classifieur linéaire. Soit $\\mathcal{X}\\subset\\mathbb{R}^d$ nos variables d'entrées et $\\mathcal{Y}=\\{-1,+1\\}$ nos variable à prédire. Un classifieur linéaire sépare les éléments de notre jeu de données par un hyperplan. Comme vous avez pu le voir dans le TP précédent, un hyperplan décrit par le vecteur normal $w$ est défini par les solutions de l'équations suivantes :\n",
    "\n",
    "$$\\langle w, x\\rangle = 0$$\n",
    "\n",
    "\n",
    "Si le produit scalaire est positif, on dira que notre échantillon $x$ appartient à la classe positive et inversement.\n",
    "\n",
    "Un classifieur linéaire peut donc être décrit de la manière suivante :\n",
    "\n",
    "\n",
    "$$\\begin{aligned}\n",
    "h_w:\\mathcal{X}&\\mapsto\\mathcal{Y}=\\{-1,+1\\}\\\\\n",
    "x&\\rightarrow \\text{sign}(\\langle w, x\\rangle)\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "De la même manière que pour les TPs précédents, on peut introduire la notion de biais en rajoutant une dimension de $1$ aux vecteurs $x$.\n",
    "\n",
    "---\n",
    "<span style=\"color:blue\">**Petite question d'algèbre :**</span> **Trouvez le projecteur orthogonal de $\\mathcal{X}$ sur l'hyperplan décrit par le vecteur $w$, noté $\\text{proj}_w(x)$. Démontrez que $\\forall x\\in\\mathcal{X},\\ \\text{proj}_w(x)\\in\\{z:\\langle w, z\\rangle=0\\}$ (autrement dit, démontrez que la projection de $x$ sur la frontière est bien sur la frontière).**\n",
    "\n",
    "<span style=\"color:green\">**Réponse :**</span> \n",
    "\n",
    "On suppose que $||w||=1$ (on normalisera si besoin).\n",
    "\n",
    "$$\\text{proj}_w(x)=x-\\langle w, x\\rangle w$$\n",
    "\n",
    "Et pour la démonstration :\n",
    "\n",
    "$$\\langle w, \\text{proj}_w(x)\\rangle=\\langle w, x-\\langle w, x\\rangle w\\rangle=\\langle w, x\\rangle -\\langle w, x\\rangle\\langle w, w\\rangle = 0$$\n",
    "\n",
    "<span style=\"color:blue\">**Petite question d'algèbre 2 :**</span> **Montrer que $\\text{proj}_w^2=\\text{proj}_w$ (le carré est pris dans le sens de la composition). Cela vous semble-t-il logique ?**\n",
    "\n",
    "<span style=\"color:green\">**Réponse :**</span> \n",
    "\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\text{proj}^2_w(x)&=x-\\langle w, x\\rangle w-\\langle w, x-\\langle w, x\\rangle w\\rangle w\\\\\n",
    "&=x-\\langle w, x\\rangle w-(\\langle w, x\\rangle - \\langle w, x\\rangle \\langle w, w \\rangle)w=x-\\langle w, x\\rangle w\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "La projection sur l'hyperplan défini par le vecteur normal $w$ d'un vecteur $x$ déjà sur ce dernier n'a aucune effet (puisqu'il est déjà sur l'hyperplan). Ainsi, projeter une fois ou deux fois revient à la même chose.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le primal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme dit plus haut, on ne cherche pas n'importe quel hyperplan, mais bien celui qui rang la marge maximale. La marge est définie par la plus patite distance entre un point du jeu de données et la frontière de décision.\n",
    "\n",
    "La distance d'un point à la frontière est donnée par $|\\langle x_i, w\\rangle|$ ($w$ unitaire). La quantité $y_i\\langle x_i, w\\rangle$ est positive et indique la distance à la frontière si le point est bien classé et donne la distance négative si le point est mal classé. Ainsi $\\min_{i\\leq m} y_i\\langle x_i, w\\rangle$ nous donne le point la plus petite distance (négative si mal classé).\n",
    "\n",
    "On souhaite donc trouver $w$ tel que cette distance soit maximale (i.e. le point le plus proche est le plus loin possible de la frontière) :\n",
    "\n",
    "$$\\hat{w}=\\text{argmax}_{w, ||w||=1}\\min_{i\\leq m}y_i\\langle x_i, w\\rangle$$\n",
    "\n",
    "\n",
    "Il est possible de montrer que le vecteur $w=w_0/||w_0||$ tel que :\n",
    "\n",
    "$$w_0=\\text{argmin}_{w}||w||^2_2,\\ s.t. \\forall i\\leq m,\\ y_i\\langle x_i, w\\rangle \\geq 1$$\n",
    "\n",
    "est solution de ce problème de minimisation.\n",
    "\n",
    "Remarquez que cela fait penser à la régularisation : parmi toutes les solutions possibles, on cherche celle de norme minimale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le dual (optionnel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le problème d'optimisation si dessus est ce qu'on appelle un problème d'optimisation sous contrainte. Un tel problème est associé à ce qu'on appelle un Lagrangien :\n",
    "\n",
    "$$\\mathcal{L}(w, \\alpha)=||w||_2^2+\\sum_{i=1}^m\\alpha_i(1-y_i\\langle x_i, w\\rangle),\\ \\alpha_j\\geq 0, j\\leq m$$\n",
    "\n",
    "Notons $g(w)=\\max_{\\alpha,\\alpha\\geq 0}\\mathcal{L}(w,\\alpha)$. On observe assez rapidement que $g(w)=\\infty$ si une des contraintes n'est pas satisfaite et vaut $||w||^2_2$ sinon.\n",
    "\n",
    "Ainsi, minimiser $g(w)$ revient à minimiser la norme du vecteur $w$ en respectant les contraintes. C'est ce qu'on appelle le *primal* qu'on note $p^\\star$ :\n",
    "\n",
    "$$p^\\star=\\min_w\\max_{\\alpha,\\alpha\\geq 0}\\mathcal{L}(w,\\alpha)$$\n",
    "\n",
    "Le passage au dual permet d'inverser la minimisation et la maximisation. Il n'est pas évident de montrer que les deux problèmes sont équivalents. C'est ici le cas et on note $d^\\star$ le dual (on parle donc de dualité forte) :\n",
    "\n",
    "$$d^\\star=\\max_{\\alpha,\\alpha\\geq 0}\\min_w\\mathcal{L}(w,\\alpha).$$\n",
    "\n",
    "Quelques éléments de calculs plus loin (le minimum est un point critique, on annule les dérivées partielles, etc.), on reformule le dual de la manière suivante :\n",
    "\n",
    "$$\\max_{\\alpha,\\alpha\\geq 0}\\sum_i\\alpha_i-\\frac{1}{2}\\sum_i\\sum_j\\alpha_i\\alpha_jy_iy_j\\langle x_i, x_j\\rangle$$\n",
    "\n",
    "et\n",
    "\n",
    "$$w=\\frac{1}{2}\\sum_i \\alpha_iy_ix_i$$\n",
    "\n",
    "Ainsi, notre modèle prédictif prend la forme suivante :\n",
    "\n",
    "\n",
    "$$\\begin{aligned}\n",
    "h:\\mathcal{X}&\\mapsto\\mathcal{Y}\\\\\n",
    "x&\\rightarrow\\text{sign}(\\sum_i\\alpha_iy_i\\langle x_i, x\\rangle)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'astuce du noyau (optionnel, suite du dual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'astuce du noyau découle de la formation duale et notamment du fait que celle-ci n'est liée aux données qu'au travers du produit $y_iy_i$ et du produit scalaire $\\langle x_i, x_j\\rangle$. Si le problème de classification est non linéaire, il est possible de passer par une transformation non linéaire $\\phi:\\mathcal{X}\\mapsto\\mathcal{F}$ de nos données d'entrées. Le problème devient donc :\n",
    "\n",
    "$$\\max_{\\alpha,\\alpha\\geq 0}\\sum_i\\alpha_i-\\frac{1}{2}\\sum_i\\sum_j\\alpha_i\\alpha_jy_iy_j\\langle \\phi(x_i), \\phi(x_j)\\rangle$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sans rentrer dans les détails, l'astuce du noyau vient de l'existence de fonctions :\n",
    "\n",
    "$$\\begin{aligned}\n",
    "k:\\mathcal{X}\\times\\mathcal{X}&\\mapsto\\mathbb{R}\\\\\n",
    "x_i,x_j&\\rightarrow k(x_i,x_j)=\\langle\\phi(x_i),\\phi(x_j)\\rangle.\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "Ces fonctions $k$ ne nécessitent pas de projeter les $x$ dans un espace de plus grande dimension et permettent d'obtenir le résultat du produit scalaire directement dans l'espace d'origine. Ainsi, on peut même calculer le produit scalaire dans des espaces de dimensions infinies.\n",
    "\n",
    "Par exemple le noyau :\n",
    "\n",
    "$$k(x_i, x_j)=(\\langle x_i, x_j\\rangle+c)^n,$$\n",
    "\n",
    "nous permet de faire une transformations polynomiales de degré $n$ directement dans l'espace d'origine ; on remarque que la puissance $n$ est calculée sur le résultat du produit scalaire ($+c$) qui est donc un sclaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation de la frontière de décision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de ce premier exercice est de visualiser la frontière de décision d'un SVM en jouant sur un exemple simple avec les noyaux offerts par la librairie $\\texttt{scikit-learn}$.\n",
    "\n",
    "La visualisation suivante permet d'observer la marge et notamment les vecteurs de supports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "np.random.seed(0)\n",
    "\n",
    "dataset_size = 20\n",
    "\n",
    "X = np.r_[np.random.randn(dataset_size, 2) - [2, 2], np.random.randn(dataset_size, 2) + [2, 2]]\n",
    "Y = [0] * dataset_size + [1] * dataset_size\n",
    "\n",
    "# figure number\n",
    "fignum = 1\n",
    "\n",
    "kernel = 'linear'\n",
    "clf = svm.SVC(kernel=kernel)\n",
    "clf.fit(X, Y)\n",
    "\n",
    "# get the separating hyperplane\n",
    "w = clf.coef_[0]\n",
    "a = -w[0] / w[1]\n",
    "xx = np.linspace(-5, 5)\n",
    "yy = a * xx - (clf.intercept_[0]) / w[1]\n",
    "\n",
    "# plot the parallels to the separating hyperplane that pass through the\n",
    "# support vectors (margin away from hyperplane in direction\n",
    "# perpendicular to hyperplane). This is sqrt(1+a^2) away vertically in\n",
    "# 2-d.\n",
    "margin = 1 / np.sqrt(np.sum(clf.coef_ ** 2))\n",
    "yy_down = yy - np.sqrt(1 + a ** 2) * margin\n",
    "yy_up = yy + np.sqrt(1 + a ** 2) * margin\n",
    "\n",
    "# plot the line, the points, and the nearest vectors to the plane\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.clf()\n",
    "plt.plot(xx, yy, 'k-')\n",
    "plt.plot(xx, yy_down, 'k--')\n",
    "plt.plot(xx, yy_up, 'k--')\n",
    "\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,\n",
    "            facecolors='none', zorder=10, edgecolors='k')\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired,\n",
    "            edgecolors='k')\n",
    "\n",
    "plt.axis('tight')\n",
    "x_min = -4.8\n",
    "x_max = 4.2\n",
    "y_min = -6\n",
    "y_max = 6\n",
    "\n",
    "XX, YY = np.mgrid[x_min:x_max:500j, y_min:y_max:500j]\n",
    "Z = clf.predict(np.c_[XX.ravel(), YY.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(XX.shape)\n",
    "plt.pcolormesh(XX, YY, Z, cmap=plt.cm.Paired, shading='auto')\n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(size=200):\n",
    "    X = np.random.uniform(-1, 1, size=(size, 2))\n",
    "    y = X[:, 0]**3 < X[:, 1]\n",
    "    return X, y\n",
    "X, y = sample_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(X, y, clf=None):\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    if clf is not None:\n",
    "\n",
    "        XX, YY = np.mgrid[-1:1:500j, -1:1:500j]\n",
    "        Z = clf.predict(np.c_[XX.ravel(), YY.ravel()])\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(XX.shape)\n",
    "        plt.pcolormesh(XX, YY, Z, cmap=plt.cm.Paired, shading='auto')\n",
    "\n",
    "    plt.xlim(-1, 1)\n",
    "    plt.ylim(-1, 1)\n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici la visualisation d'un SVM avec un noyau linéaire (un produit scalaire $\\langle\\cdot,\\cdot\\rangle$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 'linear'\n",
    "clf = svm.SVC(kernel=kernel)\n",
    "clf.fit(X, y)\n",
    "\n",
    "plot(X, y, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme vous avez pu le voir, si vous avez lu la section concernant le dual, le SVM permet de remplacer les comparaisons linéaires (i.e. le produit scalaire), par des comparaisons non-linéaires (i.e. produit scalaire dans un espace où les données sont projetées non linéairement). La librairie $\\texttt{scikir-learn}$ permet de jouer avec ce paramètre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:blue\">**Exercice 4 :**</span> **Jouez avec plusieurs noyaux et observez la forme de la frontière de décision.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Complete this part ######## or die ####################\n",
    "kernels = ['poly', 'rbf', 'sigmoid']\n",
    "params = [{'degree': 4, 'coef0': 5, 'gamma': 'auto'}, {'gamma': 'scale'}, {'gamma': 'auto', 'coef0': 1.}]\n",
    "for k, p in zip(kernels, params):\n",
    "    print('SVM with kernel: ' + k)\n",
    "    clf = svm.SVC(kernel=k, **p)\n",
    "    clf.fit(X, y)\n",
    "    plot(X, y, clf)\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:blue\">**Question :**</span> **Comparez la robustesse d'un SVM par rapport à un 1NN relativement à la dimension du problème. Le SVM est-il plus ou moins robuste que le 1NN ?**\n",
    "\n",
    "<span style=\"color:green\">**Réponse :**</span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Complete this part ######## or die ####################\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "def sample_data(n, k=3, d=3, mu=1):\n",
    "    y = np.random.randint(0, 2, size=(n, 1))\n",
    "    \n",
    "    X = np.random.normal(mu, 1, size=(n, k))\n",
    "    X = y*X-(1-y)*X # positive have mean mu and negative, -mu\n",
    "    noise = np.random.normal(0, 1, size=(n, d-k))\n",
    "    X = np.concatenate([X, noise], axis=1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "scores_svm = []\n",
    "scores = []\n",
    "redo = 5\n",
    "max_dim = 5000\n",
    "first_dim = 10\n",
    "steps = 100\n",
    "\n",
    "for d in range(first_dim, max_dim, steps):\n",
    "    s_svm = 0\n",
    "    s = 0\n",
    "    for _ in range(redo):\n",
    "        X, y = sample_data(100, d=d)\n",
    "        X_test, y_test = sample_data(200, d=d)\n",
    "        \n",
    "        model = SVC(kernel='linear', C=1.)\n",
    "        model.fit(X, y.reshape((y.shape[0],)))\n",
    "        s_svm += model.score(X_test, y_test.reshape((y_test.shape[0],)))/redo\n",
    "        \n",
    "        c = KNeighborsClassifier()\n",
    "        c.fit(X, y.reshape((y.shape[0],)))\n",
    "        s += c.score(X_test, y_test.reshape((y_test.shape[0],)))/redo\n",
    "    scores.append(s)\n",
    "    scores_svm.append(s_svm)\n",
    "    \n",
    "plt.plot(list(range(first_dim, max_dim, steps)), scores_svm, label='SVM')\n",
    "plt.plot(list(range(first_dim, max_dim, steps)), scores, label='KNN')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sur de vrais données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:blue\">**Exercice 5 :**</span> **Utilisez le SVM, la régression logistique ou encore le KNN pour résoudre les problèmes ci-dessous.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris['data']\n",
    "y = iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Complete this part ######## or die ####################\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "kernels = ['poly', 'rbf', 'sigmoid']\n",
    "params = [\n",
    "    {'kernel': ['linear']},\n",
    "    {'kernel': ['poly'], 'degree': [1, 2, 3, 4, 5], 'coef0': [1, 2, 3, 4, 5], 'gamma': ['auto']}, \n",
    "    {'kernel': ['rbf'], 'gamma': ['scale']}, \n",
    "    {'kernel': ['sigmoid'], 'gamma': ['auto'], 'coef0': [0.1, 1., 10.]}\n",
    "]\n",
    "\n",
    "search = GridSearchCV(svm.SVC(), params, cv=5)\n",
    "search.fit(Xtrain, ytrain)\n",
    "print('Train ' + '*' * 50)\n",
    "print(classification_report(ytrain.reshape((ytrain.shape[0], 1)), search.predict(Xtrain)))\n",
    "print('Test ' + '*' * 50)\n",
    "print(classification_report(ytest.reshape((ytest.shape[0], 1)), search.predict(Xtest)))\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\">**On observe ci-dessous qu'un modèle qui ne fait aucune erreur n'est pas nécessairement un bon modèle.**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Complementary answer ###############################################################\n",
    "model = svm.SVC(kernel = 'rbf', gamma=2000.)\n",
    "model.fit(Xtrain, ytrain)\n",
    "print('Train ' + '*' * 50)\n",
    "print(classification_report(ytrain.reshape((ytrain.shape[0], 1)), model.predict(Xtrain)))\n",
    "print('Test ' + '*' * 50)\n",
    "print(classification_report(ytest.reshape((ytest.shape[0], 1)), model.predict(Xtest), zero_division=0))\n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digit = datasets.load_digits()\n",
    "\n",
    "X = digit['data']\n",
    "Y = digit['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(14, 8))\n",
    "fig.tight_layout()\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    img = X[i].reshape((8, 8))\n",
    "    plt.gca().set_axis_off()\n",
    "    \n",
    "    plt.imshow(img, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Label: '+str(Y[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Complete this part ######## or die ####################\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.3)\n",
    "\n",
    "kernels = ['poly', 'rbf', 'sigmoid']\n",
    "params = [\n",
    "    {'kernel': ['poly'], 'degree': [1, 2, 3, 4, 5], 'coef0': [1, 2, 3, 4, 5], 'gamma': ['auto']}, \n",
    "    {'kernel': ['rbf'], 'gamma': ['scale', 0.01, 1., 10., 100., 1000.]}, \n",
    "    {'kernel': ['sigmoid'], 'gamma': ['auto'], 'coef0': [0.1, 1., 10.]}\n",
    "]\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "\n",
    "search = GridSearchCV(svm.SVC(), params, cv=5)\n",
    "search.fit(Xtrain, ytrain)\n",
    "\n",
    "print('Train ' + '*' * 50)\n",
    "print(classification_report(ytrain.reshape((ytrain.shape[0], 1)), search.predict(Xtrain)))\n",
    "print('Test ' + '*' * 50)\n",
    "print(classification_report(ytest.reshape((ytest.shape[0], 1)), search.predict(Xtest)))\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\">**On observe ci-dessous qu'un modèle qui ne fait aucune erreur n'est pas nécessairement un bon modèle.**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Complementary answer ###############################################################\n",
    "model = svm.SVC(kernel = 'rbf', gamma=2000.)\n",
    "model.fit(Xtrain, ytrain)\n",
    "print('Train ' + '*' * 50)\n",
    "print(classification_report(ytrain.reshape((ytrain.shape[0], 1)), model.predict(Xtrain)))\n",
    "print('Test ' + '*' * 50)\n",
    "print(classification_report(ytest.reshape((ytest.shape[0], 1)), model.predict(Xtest), zero_division=0))\n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "wine = datasets.load_wine()\n",
    "\n",
    "X = wine['data']\n",
    "Y = wine['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Complete this part ######## or die ####################\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.3)\n",
    "kernels = ['poly', 'rbf', 'sigmoid']\n",
    "params = [\n",
    "    {'kernel': ['poly'], 'degree': [1, 2, 3, 4, 5], 'coef0': [1, 2, 3, 4, 5], 'gamma': ['auto']}, \n",
    "    {'kernel': ['rbf'], 'gamma': ['scale', 0.01, 1., 10., 100., 1000.]}, \n",
    "    {'kernel': ['sigmoid'], 'gamma': ['auto'], 'coef0': [0.1, 1., 10.]}\n",
    "]\n",
    "\n",
    "search = GridSearchCV(svm.SVC(), params, cv=5)\n",
    "search.fit(Xtrain, ytrain)\n",
    "\n",
    "print('Train ' + '*' * 50)\n",
    "print(classification_report(ytrain.reshape((ytrain.shape[0], 1)), search.predict(Xtrain)))\n",
    "print('Test ' + '*' * 50)\n",
    "print(classification_report(ytest.reshape((ytest.shape[0], 1)), search.predict(Xtest)))\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\">**On observe ci-dessous qu'un modèle qui ne fait aucune erreur n'est pas nécessairement un bon modèle.**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Complementary answer ###############################################################\n",
    "model = svm.SVC(kernel = 'rbf', gamma=2000.)\n",
    "model.fit(Xtrain, ytrain)\n",
    "print('Train ' + '*' * 50)\n",
    "print(classification_report(ytrain.reshape((ytrain.shape[0], 1)), model.predict(Xtrain)))\n",
    "print('Test ' + '*' * 50)\n",
    "print(classification_report(ytest.reshape((ytest.shape[0], 1)), model.predict(Xtest), zero_division=0))\n",
    "############################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

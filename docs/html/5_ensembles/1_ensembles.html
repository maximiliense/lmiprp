
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Les m√©thodes ensemblistes ‚òïÔ∏è‚òïÔ∏è &#8212; Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="La diff√©rentiation automatique" href="../6_autodiff/0_propos_liminaire.html" />
    <link rel="prev" title="Les m√©thodes ensemblistes" href="0_propos_liminaire.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Machine Learning
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../0_requisite/rappels_python.html">
   Rappels de Python et Numpy
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../1_what_is_ml/0_propos_liminaire.html">
   Le
   <em>
    Machine Learning
   </em>
   , c‚Äôest quoi ?
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/1_introduction_ml.html">
     <em>
      Machine learning
     </em>
     et mal√©diction de la dimension
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/2_regression_and_classification_trees.html">
     Les arbres de r√©gression et de classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../2_linear_regression/0_propos_liminaire.html">
   La
   <em>
    r√©gression
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_linear_regression/1_linear_regression.html">
     La r√©gression lin√©aire ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_linear_regression/2_optimization.html">
     L‚Äôoptimisation ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_linear_regression/3_interpolation.html">
     Interpolation ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_linear_regression/4_algo_proximal_lasso.html">
     Sous-diff√©rentiel et le cas du Lasso ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../3_logistic_regression/0_propos_liminaire.html">
   La
   <em>
    classification
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_logistic_regression/1_logistic_regression.html">
     La R√©gression Logistique ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_logistic_regression/2_fonctions_proxy.html">
     Les fonctions proxy ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_logistic_regression/3_bayes_classifier.html">
     Le classifieur de Bayes ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_logistic_regression/4_VC_theory.html">
     La th√©orie de Vapnik et Chervonenkis ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è (üíÜ‚Äç‚ôÇÔ∏è)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../4_max_margin/0_propos_liminaire.html">
   Les mod√®les
   <em>
    max-margin
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../4_max_margin/1_max_margin.html">
     Les mod√®les max-margin ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="0_propos_liminaire.html">
   Les m√©thodes
   <em>
    ensemblistes
   </em>
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Les m√©thodes ensemblistes ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../6_autodiff/0_propos_liminaire.html">
   La diff√©rentiation automatique
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_autodiff/1_autodiff.html">
     La diff√©rentiation automatique et un d√©but de
     <em>
      deep learning
     </em>
     ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_autodiff/2_filters_representation.html">
     Filtres et espace de repr√©sentation des r√©seaux de neurones ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../7_regularization/0_propos_liminaire.html">
   La r√©gularisation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/1_regularization_deep.html">
     R√©gularisation en deep learning ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/2_unsupervised_representation_learning.html">
     Unsupervised representation learning ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/3_multitask.html">
     L‚Äôapprentissage multi-t√¢ches ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/4_adversarial.html">
     Les attaques adversaires ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/5_ridge.html">
     Une analyse de la r√©gularisation Ridge ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../content.html">
   Content in Jupyter Book
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../markdown.html">
     Markdown Files
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../notebooks.html">
     Content with notebooks
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/5_ensembles/1_ensembles.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/maximiliense/lmiprp"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/maximiliense/lmiprp/issues/new?title=Issue%20on%20page%20%2F5_ensembles/1_ensembles.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/maximiliense/lmiprp/master?urlpath=tree/5_ensembles/1_ensembles.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#i-l-approche-naive">
   I. L‚Äôapproche na√Øve
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ii-bayesian-model-averaging-et-son-lien-avec-la-regularisation">
   II. Bayesian Model Averaging (et son lien avec la r√©gularisation)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#la-regression-lineaire-bayesienne">
     La r√©gression lin√©aire Bay√©sienne
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iii-boosting">
   III. Boosting
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iv-les-forets-aleatoires">
   IV. Les for√™ts al√©atoires
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="les-methodes-ensemblistes">
<h1>Les m√©thodes ensemblistes ‚òïÔ∏è‚òïÔ∏è<a class="headerlink" href="#les-methodes-ensemblistes" title="Permalink to this headline">¬∂</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¬∂</a></h2>
<p>Soit <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> notre espace d‚Äôentr√©e et <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> notre espace de sortie. Soit <span class="math notranslate nohighlight">\(X, Y\)</span> deux variables al√©atoires sur <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> et <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> et soit <span class="math notranslate nohighlight">\(\mathbb{P}\)</span> leur mesure jointe. Notre objectif est de trouver une application <span class="math notranslate nohighlight">\(h:\mathcal{X}\mapsto\mathcal{Y}\)</span> minimise une certaine erreur qu‚Äôon notera <span class="math notranslate nohighlight">\(L\)</span>. N‚Äôayant pas acc√®s aux variables al√©atoires <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(Y\)</span>, nous collectons un jeu de donn√©es <span class="math notranslate nohighlight">\(S_n=\{(X_i, Y_i)\}_{i\leq n}\sim\mathbb{P}^n\)</span> et nous construisons un risque empirique :</p>
<div class="math notranslate nohighlight">
\[L_n(h)=\frac{1}{n}\sum_{i}\ell(h(x_i), y_i),\]</div>
<p>o√π <span class="math notranslate nohighlight">\(\ell\)</span> d√©finit une erreur √©l√©mentaire (i.e. pour une unique pr√©diction). La fonction <span class="math notranslate nohighlight">\(h\)</span> est ainsi construite en utilisant le jeu de donn√©es <span class="math notranslate nohighlight">\(S_n\)</span>.</p>
</div>
<div class="section" id="i-l-approche-naive">
<h2>I. L‚Äôapproche na√Øve<a class="headerlink" href="#i-l-approche-naive" title="Permalink to this headline">¬∂</a></h2>
<p>L‚Äôapproche la plus simple lorsqu‚Äôon cherche √† combiner plusieurs mod√®les consiste √† moyenner leur pr√©diction. Dans le cas de la r√©gression, la pr√©diction est la moyenne traditionnelle. Dans le cas de la classification, on utilisera un vote √† la majorit√© simple.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">images</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray_r</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Training: </span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">label</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_ensembles_4_0.png" src="../_images/1_ensembles_4_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span> <span class="o">/</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MajorityVoting</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">models</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">models</span> <span class="o">=</span> <span class="n">models</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">:</span>
            <span class="n">m</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">:</span>
            <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">stats</span><span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">mode</span>
    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;vote&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">:</span>
            <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
        <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;vote&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span>
            
<span class="n">model</span> <span class="o">=</span> <span class="n">MajorityVoting</span><span class="p">(</span>
    <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">),</span> 
    <span class="n">DecisionTreeClassifier</span><span class="p">(),</span>
    <span class="n">KNeighborsClassifier</span><span class="p">()</span>
<span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;model&#39;: [0.9391691394658753, 0.7603857566765578, 0.9584569732937686],
 &#39;vote&#39;: 0.9451038575667656}
</pre></div>
</div>
</div>
</div>
<p>On peut imaginer que les mod√®les se trompent sur les m√™mes images et non de mani√®re al√©atoire. C‚Äôest par exemple le cas si les mod√®les se trompent lorsqu‚Äôun chiffre est mal dessin√© et ressemble √† un autre chiffre : tous les mod√®les vont faire la m√™me erreur. Ainsi, le meilleur mod√®le est le meilleur mod√®le et non le vote.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_boston</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">load_boston</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">/</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsRegressor</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">AverageVoting</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">models</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">models</span> <span class="o">=</span> <span class="n">models</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">:</span>
            <span class="n">m</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">:</span>
            <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">results</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;vote&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">:</span>
            <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)))</span>
        <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;vote&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">scores</span>
            
<span class="n">model</span> <span class="o">=</span> <span class="n">AverageVoting</span><span class="p">(</span>
    <span class="n">LinearRegression</span><span class="p">(),</span> 
    <span class="n">DecisionTreeRegressor</span><span class="p">(),</span>
    <span class="n">KNeighborsRegressor</span><span class="p">()</span>
<span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;model&#39;: [26.477467262772567, 28.380394736842103, 46.27930421052632],
 &#39;vote&#39;: 25.226370600637846}
</pre></div>
</div>
</div>
</div>
<p>On observe cette fois-ci un gain clair des performances. Les diff√©rents mod√®les doivent se tromper de mani√®re al√©atoire r√©partie autour de la moyenne. L‚Äôagr√©gation rend ce r√©sultat plus stable.</p>
</div>
<div class="section" id="ii-bayesian-model-averaging-et-son-lien-avec-la-regularisation">
<h2>II. Bayesian Model Averaging (et son lien avec la r√©gularisation)<a class="headerlink" href="#ii-bayesian-model-averaging-et-son-lien-avec-la-regularisation" title="Permalink to this headline">¬∂</a></h2>
<p>Notons <span class="math notranslate nohighlight">\(z_i=(x_i, y_i)\)</span> et une famille de <span class="math notranslate nohighlight">\(M\)</span> mod√®les probabilistes (e.g. r√©gression logistique) o√π chaque mod√®le est not√© <span class="math notranslate nohighlight">\(\mathcal{M}_j\)</span>. Notons :</p>
<div class="math notranslate nohighlight">
\[p(y_i|x_i, \mathcal{M}_j),\]</div>
<p>la densit√© d‚Äôun point de <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> relativement au mod√®le <span class="math notranslate nohighlight">\(\mathcal{M}_j\)</span> et √† une observation <span class="math notranslate nohighlight">\(x_i\in\mathcal{X}\)</span>. Notre objectif est de d√©terminer dans un premier temps la ‚Äúqualit√©‚Äù d‚Äôun mod√®le en tenant compte de notre point de vu <em>a priori</em> ainsi que des donn√©es que nous avons pu observer <span class="math notranslate nohighlight">\(S_n\)</span>. Notons ainsi <span class="math notranslate nohighlight">\(p(\mathcal{M}_j)\)</span> notre probabilit√© <em>a priori</em> sur le mod√®le <span class="math notranslate nohighlight">\(\mathcal{M}_j\)</span>. Celle-ci peut favoriser certaines solutions parcimonieuses ou bien √™tre uniforme et ne favoriser aucun mod√®le. En appliquant la r√®gle de Bayes, nous obtenons :</p>
<div class="math notranslate nohighlight">
\[p(\mathcal{M}_j|S_n)=\frac{p(S_n|\mathcal{M}_j)p(\mathcal{M}_j)}{p(S_n)},\]</div>
<p>o√π nous avons :</p>
<div class="math notranslate nohighlight">
\[p(S_n|\mathcal{M}_j)=\prod_i p(y_i|x_i, \mathcal{M}_j),\]</div>
<p>ainsi que :</p>
<div class="math notranslate nohighlight">
\[p(S_n)=\sum_j p(S_n| \mathcal{M}_j)p(\mathcal{M}_j).\]</div>
<p>Nous avons bien ici toutes les informations nous permettant d‚Äô√©valuer de mani√®re Bay√©sienne la qualit√© de nos diff√©rents mod√®les. Cependant, en pratique, les variations pourraient tr√®s bien d√©pendre d‚Äôun tirage particulier de nos donn√©es. L‚Äôid√©e derri√®re le <em>Bayesian Model Averaging</em> consiste √† consid√©rer TOUS les mod√®les mais √† les pond√©rer par leur qualit√©. Cela ne se fait pas au doigt mouill√©, mais en utilisant encore une fois le <em>framework</em> probabiliste :</p>
<div class="math notranslate nohighlight">
\[p(y|x_\text{new}, S_n)=\sum_j p(y|x_\text{new}, \mathcal{M}_j, S_n)p(\mathcal{M}_j|S_n)\]</div>
<p>Dans le cas de la r√©gression logistique, nous avons la vraisemblance suivante :</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\theta_j)=\prod_i p(y_i|x_i,\theta_j)\]</div>
<p>o√π</p>
<div class="math notranslate nohighlight">
\[p(y_i|x_i, \theta_j)=\sigma_j(x_i)^y_i(1-\sigma_j(x_i))^{1-y_i}\]</div>
<p>et</p>
<div class="math notranslate nohighlight">
\[\sigma_j(x)=(1+e^{-\langle\theta_j,x\rangle})^{-1}.\]</div>
<p>Ainsi, la <em>posterior</em> de notre mod√®le <span class="math notranslate nohighlight">\(\theta_j\)</span> apr√®s avoir observ√© notre jeu de donn√©es est donn√©e par :</p>
<div class="math notranslate nohighlight">
\[p(\theta_j|S_n)\propto \prod_i p(y_i|x_i,\theta_j)p(\theta_j).\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># TODO exercice o√π des mod√®les logistiques prennent en compte que des param√®tres choisis al√©atoirement</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>On affiche les pixels que verront par exemple un mod√®le.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">images</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray_r</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Training: </span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">label</span><span class="p">)</span>

<span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">bool</span><span class="p">)</span>

<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">images</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">ma</span><span class="o">.</span><span class="n">masked_array</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">),</span> 
        <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray_r</span><span class="p">,</span> 
        <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span>
    <span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Training: </span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">label</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_ensembles_21_0.png" src="../_images/1_ensembles_21_0.png" />
<img alt="../_images/1_ensembles_21_1.png" src="../_images/1_ensembles_21_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span> <span class="o">/</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">logsumexp</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">BayesianLogisticAveraging</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">nb_models</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">nb_models</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">bool</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nb_models</span> <span class="o">=</span> <span class="n">nb_models</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">models</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">posterior</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prior</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="n">nb_models</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">=</span> <span class="n">max_iter</span>
        
    <span class="k">def</span> <span class="nf">posterior_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="p">)),</span> <span class="n">y</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">)</span>
        
        
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nb_models</span><span class="p">):</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">)</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask</span><span class="p">[</span><span class="n">_</span><span class="p">]]</span>
            <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">posterior</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">posterior_</span><span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
            <span class="p">)</span>
    <span class="k">def</span> <span class="nf">individual_scores</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">)):</span>
            <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">[</span><span class="n">_</span><span class="p">]</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask</span><span class="p">[</span><span class="n">_</span><span class="p">]],</span> <span class="n">y</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">scores</span><span class="p">))</span>
            
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">):</span>
            <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask</span><span class="p">[</span><span class="n">i</span><span class="p">]]))</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">posterior</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="p">)</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logsumexp</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">pred</span><span class="o">==</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">acc</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">BayesianLogisticAveraging</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">individual_scores</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="c1"># print(np.argsort(model.posterior))</span>
<span class="c1">#print(model.posterior)</span>
<span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.9272997032640949
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.9235905044510386
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LogisticMaxPooling</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">nb_models</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">nb_models</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">bool</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nb_models</span> <span class="o">=</span> <span class="n">nb_models</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">models</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">=</span> <span class="n">max_iter</span>
        
        
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nb_models</span><span class="p">):</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">)</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask</span><span class="p">[</span><span class="n">_</span><span class="p">]]</span>
            <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            
    <span class="k">def</span> <span class="nf">individual_scores</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">)):</span>
            <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">[</span><span class="n">_</span><span class="p">]</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask</span><span class="p">[</span><span class="n">_</span><span class="p">]],</span> <span class="n">y</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Best model:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="s1">&#39;:: Worst model:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">scores</span><span class="p">))</span>
            
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">):</span>
            <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>
            <span class="p">)</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">predictions</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">pred</span><span class="o">==</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">acc</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticMaxPooling</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">individual_scores</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="c1"># print(np.argsort(model.posterior))</span>
<span class="c1">#print(model.posterior)</span>
<span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Best model: 0.9191394658753709 :: Worst model: 0.8130563798219584
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.9176557863501483
</pre></div>
</div>
</div>
</div>
<p>Le principe du <em>Bayesian Model Averaging</em> se g√©n√©ralise bien s√ªr lorsqu‚Äôon a une quantit√© infinie de mod√®les o√π la <em>posterior</em> pr√©dictive s‚Äô√©crit :</p>
<div class="math notranslate nohighlight">
\[p(y|x_\text{new}, S_n)=\int p(y|x_\text{new}, \mathcal{M}, S_n)p(\mathcal{M}|S_n)dM.\]</div>
<hr class="docutils" />
<div class="section" id="la-regression-lineaire-bayesienne">
<h3>La r√©gression lin√©aire Bay√©sienne<a class="headerlink" href="#la-regression-lineaire-bayesienne" title="Permalink to this headline">¬∂</a></h3>
<p><strong>Le mod√®le</strong></p>
<p>Consid√©rons le mod√®le lin√©aire suivant. Soit <span class="math notranslate nohighlight">\(X\in\mathbb{R}^d\)</span>, <span class="math notranslate nohighlight">\(\omega\in\mathbb{R}^d\)</span>, <span class="math notranslate nohighlight">\(\epsilon\sim\mathcal{N}(0, \sigma^2)\)</span> et consid√©rons la d√©pendence suivante :</p>
<div class="math notranslate nohighlight">
\[Y=\langle \omega, X\rangle + \epsilon.\]</div>
<p>On supposera dans cet exercice que les matrices qu‚Äôon inverse sont inversibles.</p>
<p>Nous avons donc <span class="math notranslate nohighlight">\(Y\sim\mathcal{N}(\langle\omega, X\rangle, \sigma^2)\)</span>. Ici, chaque param√©trisation (chaque choix de <span class="math notranslate nohighlight">\(\omega\)</span>) peut √™tre vu comme un mod√®le. Comme pour le cas pr√©c√©dent, nous devons donner une mesure <em>a priori</em> pour chacun de nos mod√®les. Nous choissons ici le <em>prior</em> suivant :</p>
<div class="math notranslate nohighlight">
\[p(\omega)=\mathcal{N}(0, \sigma^2\Lambda^{-1}),\]</div>
<p>o√π <span class="math notranslate nohighlight">\(\Lambda\)</span> est la matrice de pr√©cision. Par facilit√©, consid√©rons <span class="math notranslate nohighlight">\(\Lambda=\lambda I\)</span> et notons <span class="math notranslate nohighlight">\(\Sigma=\sigma^2\Lambda^{-1}\)</span>.</p>
<p><strong>D√©tail des calculs pour la r√©gression lin√©aire Bay√©sienne (ou Bayesian Model Averaging with Linear Regression</strong></p>
<p>(Si vous n‚Äô√™tes int√©ress√©s que par la solution, vous pouvez sauter directement les calculs)</p>
<p><em><strong>√âtape 1 : Calcul de la posterior sur <span class="math notranslate nohighlight">\(\omega\)</span></strong></em></p>
<p>Nous cherchons dans un premier temps √† calculer :</p>
<div class="math notranslate nohighlight">
\[p(y|\tilde{X}, \omega)\propto\text{exp}\Big(-\frac{1}{2\sigma^2}(y-\tilde{X}\omega )^T(y-\tilde{X}\omega)\Big)\]</div>
<p>o√π</p>
<div class="math notranslate nohighlight">
\[\begin{split}\tilde{X}=\begin{pmatrix}x_1^T\\\vdots\\x_n^T\end{pmatrix}\text{ et }y=\begin{pmatrix}y_1\\\vdots\\y_n\end{pmatrix}.\end{split}\]</div>
<p>C‚Äôest notre likelihood. Si en maximisant cette quantit√© qu‚Äôon obtient (lorsque <span class="math notranslate nohighlight">\(\tilde{X}^T\tilde{X}\)</span> est inversible) :</p>
<div class="math notranslate nohighlight">
\[\hat{\omega}=(\tilde{X}^T\tilde{X})^{-1}\tilde{X}^ty\]</div>
<p>On rappelle notre prior sur <span class="math notranslate nohighlight">\(\omega\)</span> :</p>
<div class="math notranslate nohighlight">
\[p(\omega)\propto\text{exp}\Big(-\frac{1}{2}\omega^T\Sigma^{-1}\omega\Big)=\text{exp}\Big(-\frac{\lambda}{2\sigma^2}\omega^T\omega\Big).\]</div>
<p>Nous avons donc tous les √©l√©ments nous permettant de calculer notre <em>posterior</em> :</p>
<div class="math notranslate nohighlight">
\[p(\omega|\tilde{X}, y)\propto p(y|\tilde{X},\omega)p(\omega).\]</div>
<p>En rempla√ßant les quantit√©s ci-dessus, nous obtenons donc :</p>
<div class="math notranslate nohighlight">
\[p(\omega|\tilde{X}, y)\propto \text{exp}\Big(-\frac{1}{2\sigma^2}(y-\tilde{X}\omega )^T(y-\tilde{X}\omega)-\frac{\lambda}{2\sigma^2}\omega^T\omega\Big).\]</div>
<p>Notre prior sur <span class="math notranslate nohighlight">\(\omega\)</span> est conjugu√© avec notre vraisemblance. Cela veut dire que la <em>posterior</em> est donc n√©cessairement une loi normale √©galement. Ainsi, on sait qu‚Äôelle sera proportionnelle √† l‚Äôexponentielle de :</p>
<div class="math notranslate nohighlight">
\[(\omega-\mu)^T{\Sigma^\prime}^{-1}(\omega-\mu)=\omega^T{\Sigma^\prime}^{-1}\omega-2\omega^T{\Sigma^\prime}^{-1}\mu+\text{constante}.\]</div>
<p>o√π <span class="math notranslate nohighlight">\(\mu\)</span> sera le param√®tre de moyenne et <span class="math notranslate nohighlight">\(\Sigma^\prime\)</span> notre nouvelle matrice de covariance. Consid√©rons donc la quantit√© <span class="math notranslate nohighlight">\(\frac{1}{\sigma^2}(y-\tilde{X}\omega )^T(y-\tilde{X}\omega)+\frac{\lambda}{\sigma^2}\omega^T\omega\)</span> (nous avons factoris√© par <span class="math notranslate nohighlight">\(-1/2\)</span> les √©l√©ments de l‚Äôexponentiel) et cherchons √† obtenir la forme classique attendue d‚Äôune loi normale :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{1}{\sigma^2}(y-\tilde{X}\omega )^T(y-\tilde{X}\omega)+\frac{\lambda}{\sigma^2}\omega^T\omega&amp;=\sigma^{-2}(y^Ty-2\omega^T\tilde{X}^T y+\omega^T\tilde{X}^T\tilde{X}\omega)+\lambda\sigma^{-2}\omega^T\omega\\
&amp;=\sigma^{-2}y^Ty-2\sigma^{-2}\omega^T\tilde{X}^Ty+\sigma^{-2}\omega^T(\tilde{X}^T\tilde{X}+\lambda I)\omega
\end{aligned}\end{split}\]</div>
<p>On remarque ainsi directement que :</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
{\Sigma^\prime}^{-1}&amp;=\frac{\tilde{X}^T\tilde{X}+\lambda I}{\sigma^2}
\end{aligned}\]</div>
<p>Et on en d√©duit que :</p>
<div class="math notranslate nohighlight">
\[\mu=(\tilde{X}^T\tilde{X}+\lambda I)^{-1}\tilde{X}^T y.\]</div>
<p>On remarque que l‚Äôesp√©rance de notre posterior est exactement l‚Äôestimateur de Ridge. Dans notre cas de figure, la posterior est une loi normale et est ainsi sym√©trique : son maximum est atteint par son esp√©rance. On parle souvent de MAP, ou Maximum A Posteriori.</p>
<p><em><strong>√âtape 2 : Calcul de la posterior pr√©dictive sur <span class="math notranslate nohighlight">\(y_\text{new}\)</span></strong></em></p>
<p>Maintenant que nous avons la posterior pour nos mod√®les, nous pouvons calculer la <em>posterior</em> pr√©dictive. Celle-ci s‚Äôexprime de la mani√®re suivante :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
p(y_{\text{new}}| x_{\text{new}}, \tilde{X}, y)&amp;=\int_\omega p(y_{\text{new}}|x_{\text{new}}, \omega)p(\omega|\tilde{X}, y)d\omega\text{ (la d√©pendance en $X$ disparait)}\\
&amp;\propto\int_\omega\text{exp}\Big(-\frac{1}{2\sigma^2}(y_{\text{new}}-\langle \omega, x_{\text{new}}\rangle)^2\Big)\text{exp}\Big(-\frac{1}{2}(\omega-\mu)^T{\Sigma^\prime}^{-1}(\omega-\mu)\Big)d\omega\\
&amp;=\int_\omega\text{exp}\Big(-\frac{1}{2}\big(\sigma^{-2}(y_{\text{new}}^2-2y_{\text{new}}\langle \omega, x_{\text{new}}\rangle+(\langle \omega, x_{\text{new}}\rangle)^2+\omega^T{\Sigma^\prime}^{-1}\omega-2\omega^T{\Sigma^\prime}^{-1}\mu+\\&amp;\mu^T{\Sigma^\prime}^{-1}\mu\big)\Big)d\omega
\end{aligned}\end{split}\]</div>
<p>o√π <span class="math notranslate nohighlight">\(\mu^T{\Sigma^\prime}^{-1}\mu\)</span> n‚Äôagit que comme un facteur de proportionnalit√© et peut donc √™tre ‚Äú√©limin√©‚Äù.</p>
<p><em><strong>√âtape 2.1 : Int√©gration de la variable <span class="math notranslate nohighlight">\(\omega\)</span></strong></em></p>
<p>Nous allons dans un premier temps regrouper tous les √©l√©ments faisant intervenir <span class="math notranslate nohighlight">\(\omega\)</span>, notre variable d‚Äôint√©gration.</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
p(y_{\text{new}}| x_{\text{new}}, \tilde{X}, y)&amp;\propto\int_\omega\text{exp}\Big(-\frac{1}{2}\big(\sigma^{-2}(y_{\text{new}}^2-2y_{\text{new}}\langle \omega, x_{\text{new}}\rangle+(\langle \omega, x_{\text{new}}\rangle)^2+\omega^T{\Sigma^\prime}^{-1}\omega-2\omega^T{\Sigma^\prime}^{-1}\mu\big)\Big)d\omega
\end{aligned}\]</div>
<p>Consid√©rons √† nouveau la partie entre parenth√®ses :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\sigma^{-2}(y_{\text{new}}^2-2y_{\text{new}}&amp;\langle \omega, x_{\text{new}}\rangle+(\langle \omega, x_{\text{new}}\rangle)^2+\omega^T{\Sigma^\prime}^{-1}\omega-2\omega^T{\Sigma^\prime}^{-1}\mu\\
&amp;=y_\text{new}^2\sigma^{-2}-2\omega^T({\Sigma^\prime}^{-1}\mu+x_\text{new}y_\text{new}\sigma^{-2})+\omega^T(x_\text{new}x_\text{new}^T\sigma^{-2}+{\Sigma^\prime}^{-1})\omega\\
&amp;=(\omega-\mu^\prime){\Sigma^{\prime\prime}}^{-1}(\omega-\mu^\prime)-{\mu^\prime}^T{\Sigma^{\prime\prime}}^{-1}\mu^\prime+y_{\text{new}}^2\sigma^{-2}
\end{aligned}\end{split}\]</div>
<p>o√π :</p>
<div class="math notranslate nohighlight">
\[{\Sigma^{\prime\prime}}^{-1}=x_\text{new}x_\text{new}^T\sigma^{-2}+{\Sigma^\prime}^{-1},\]</div>
<p>et :</p>
<div class="math notranslate nohighlight">
\[\mu^\prime={\Sigma^{\prime\prime}}({\Sigma^\prime}^{-1}\mu+x_{\text{new}}y_\text{new}\sigma^{-2}).\]</div>
<p>On en d√©duit ainsi :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
p(y_{\text{new}}| x_{\text{new}}, \tilde{X}, y)&amp;\propto\text{exp}\Big(\frac{1}{2}{\mu^\prime}^T{\Sigma^{\prime\prime}}^{-1}\mu^\prime-\frac{1}{2}y_{\text{new}}^2\sigma^{-2}\Big)\int_\omega \text{exp}\Big(-\frac{1}{2}(\omega-\mu^\prime){\Sigma^{\prime\prime}}^{-1}(\omega-\mu^\prime)\Big)d\omega\\
&amp;\propto\text{exp}\Big(\frac{1}{2}{\mu^\prime}^T{\Sigma^{\prime\prime}}^{-1}\mu^\prime-\frac{1}{2}y_{\text{new}}^2\sigma^{-2}\Big),
\end{aligned}\end{split}\]</div>
<p>o√π nous avons pu sortir les √©l√©ments constants relativement √† <span class="math notranslate nohighlight">\(\omega\)</span> de l‚Äôint√©gral. N‚Äôy trouvant plus qu‚Äôune loi normale dont la probabilit√© vaut <span class="math notranslate nohighlight">\(1\)</span> sur son domaine, nous avons pu √©liminer l‚Äôint√©grale (toujours √† un facteur proportionnel pr√®s).</p>
<p><em><strong>√âtape 2.2 : Calcul de la loi de <span class="math notranslate nohighlight">\(y_\text{new}\)</span></strong></em></p>
<p>On sent que la loi de <span class="math notranslate nohighlight">\(y_\text{new}\)</span> sera aussi une loi normale, et on va essayer de retrouver quelque chose qui ressemble √† :
On sait qu‚Äôon aura quelque chose de la forme :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
p(y_\text{new}|x_\text{new}, \tilde{X}, y)&amp;\propto\text{exp}\Big(-\frac{1}{2{\sigma^\prime}^2}(y_\text{new}-\mu^{\prime\prime)^2}\Big)\\
&amp;=\text{exp}\Big(-\frac{1}{2}({\sigma^\prime}^{-2}y_\text{new}^2-2{\sigma^\prime}^{-2}y_\text{new}\mu^{\prime\prime}+{\sigma^\prime}^{-2}{\mu^{\prime\prime}}^2)\Big),
\end{aligned}
\end{split}\]</div>
<p>o√π <span class="math notranslate nohighlight">\(\mu^{\prime\prime}\)</span> sera l‚Äôesp√©rance de notre pr√©diction et <span class="math notranslate nohighlight">\(\sigma^\prime\)</span> son √©cart-type. Rappelons que notre point de d√©part ici est :</p>
<div class="math notranslate nohighlight">
\[\text{exp}\Big(\frac{1}{2}{\mu^\prime}^T{\Sigma^{\prime\prime}}^{-1}\mu^\prime-\frac{1}{2}y_{\text{new}}^2\sigma^{-2}\Big).\]</div>
<p>En constatant que <span class="math notranslate nohighlight">\(\Sigma, \Sigma^\prime\)</span> ou encore <span class="math notranslate nohighlight">\(\Sigma^{\prime\prime}\)</span> sont toutes sym√©triques, nous avons :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
{\mu^\prime}^T{\Sigma^{\prime\prime}}^{-1}\mu^\prime&amp;=({\Sigma^\prime}^{-1}\mu+x_{\text{new}}y_\text{new}\sigma^{-2})^T{\Sigma^{\prime\prime}}{\Sigma^{\prime\prime}}^{-1}{\Sigma^{\prime\prime}}({\Sigma^\prime}^{-1}\mu+x_{\text{new}}y_\text{new}\sigma^{-2})\\
&amp;=\sigma^{-2}y_\text{new}x_\text{new}^T\Sigma^{\prime\prime}\sigma^{-2}y_\text{new}x_\text{new}+2y_\text{new}\sigma^{-2}x_\text{new}^T\Sigma^{\prime\prime}{\Sigma^\prime}^{-1}\mu+\text{constante}\\
&amp;=(\sigma^{-4}x_\text{new}^T\Sigma^{\prime\prime}x_\text{new})y_\text{new}^2+2(\sigma^{-2}x_\text{new}^T\Sigma^{\prime\prime}{\Sigma^\prime}^{-1}\mu) y_\text{new}+\text{constante}.
\end{aligned}\end{split}\]</div>
<p>En combinant <span class="math notranslate nohighlight">\({\mu^\prime}^T{\Sigma^{\prime\prime}}^{-1}\mu^\prime\)</span> avec <span class="math notranslate nohighlight">\(-y_{\text{new}}^2\sigma^{-2}\)</span>, nous obtenons :</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
{\mu^\prime}^T{\Sigma^{\prime\prime}}^{-1}\mu^\prime-y_{\text{new}}^2\sigma^{-2}&amp;=(\sigma^{-4}x_\text{new}^T\Sigma^{\prime\prime}x_\text{new}-\sigma^{-2})y_\text{new}^2+2(\sigma^{-2}x_\text{new}^T\Sigma^{\prime\prime}{\Sigma^\prime}^{-1}\mu) y_\text{new}+\text{c}
\end{aligned}\]</div>
<p>Remarquons que dans l‚Äôexponentielle nous n‚Äôavons pas indiqu√© le signe ‚Äú-‚Äù classique de la loi normale. Nous obtenons ainsi :</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
y_{\text{new}}^2\sigma^{-2}-{\mu^\prime}^T{\Sigma^{\prime\prime}}^{-1}\mu^\prime&amp;=(\sigma^{-2}-\sigma^{-4}x_\text{new}\Sigma^{\prime\prime}x_\text{new})y_\text{new}^2-2(\sigma^{-2}x_\text{new}\Sigma^{\prime\prime}{\Sigma^\prime}^{-1}\mu)y_\text{new}+c
\end{aligned}\]</div>
<p>On retrouve donc :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
{\sigma^\prime}^{-2}=\sigma^{-2}(1-\sigma^{-2}x_\text{new}^T\Sigma^{\prime\prime}x_\text{new})\\
\mu^{\prime\prime}={\sigma^\prime}^2\sigma^{-2}x_\text{new}^T\Sigma^{\prime\prime}{\Sigma^\prime}^{-1}\mu.
\end{aligned}\end{split}\]</div>
<p>Nous avons :</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
p(y_\text{new}|x_\text{new}, \tilde{X}, y)&amp;\propto\text{exp}\Big(-\frac{{\sigma^\prime}^{-2}}{2}(y_\text{new}-\mu^{\prime\prime)^2}\Big)
\end{aligned}\]</div>
<p><em><strong>√âtape 2.3 : Un peu de nettoyage</strong></em></p>
<p>Nous avons gr√¢ce √† la formule de <a class="reference external" href="https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula">Sherman‚ÄìMorrison</a> :</p>
<div class="math notranslate nohighlight">
\[\Sigma^{\prime\prime}=(x_\text{new}x_\text{new}^T\sigma^{-2}+{\Sigma^\prime}^{-1})^{-1}={\Sigma^\prime}-\frac{{\Sigma^\prime}\sigma^{-2}x_\text{new}x_\text{new}^T{\Sigma^\prime}}{1+\sigma^{-2}x_\text{new}^T{\Sigma^\prime}x_\text{new}}\]</div>
<p>Nous avons donc :</p>
<div class="math notranslate nohighlight">
\[x_\text{new}^T\Sigma^{\prime\prime}x_{\text{new}}=x_\text{new}^T\Sigma^\prime x_\text{new}-\frac{\sigma^{-2}(x_\text{new}^T\Sigma^\prime x_\text{new})^2}{1+\sigma^{-2}x_\text{new}^T{\Sigma^\prime}x_\text{new}}=\frac{x_\text{new}^T\Sigma^\prime x_\text{new}}{1+\sigma^{-2}x_\text{new}^T\Sigma^\prime x_\text{new}}.\]</div>
<p>Reprenons <span class="math notranslate nohighlight">\(\sigma^\prime\)</span>. Nous avons enfin :</p>
<div class="math notranslate nohighlight">
\[{\sigma^\prime}^{-2}=\sigma^{-2}(1-\sigma^{-2}\frac{x_\text{new}^T\Sigma^\prime x_\text{new}}{1+\sigma^{-2}x_\text{new}^T\Sigma^\prime x_\text{new}})=\frac{\sigma^{-2}}{1+\sigma^{-2}x_\text{new}^T\Sigma^\prime x_\text{new}}\]</div>
<p>Consid√©rons maintenant <span class="math notranslate nohighlight">\(\mu^{\prime\prime}\)</span> :</p>
<div class="math notranslate nohighlight">
\[\mu^{\prime\prime}={\sigma^\prime}^2\sigma^{-2}x_\text{new}^T\Sigma^{\prime\prime}{\Sigma^\prime}^{-1}\mu=\mu^T{\sigma^\prime}^2\sigma^{-2}\Sigma^{\prime\prime}{\Sigma^\prime}^{-1}x_\text{new}.\]</div>
<p>Traitons la partie √† droite et montrons :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
{\sigma^\prime}^2\sigma^{-2}\Sigma^{\prime\prime}{\Sigma^\prime}^{-1}x_\text{new}&amp;=x_\text{new}\\
\Leftrightarrow {\sigma^\prime}^2\sigma^{-2} x_\text{new}&amp;={\Sigma^{\prime\prime}}^{-1}\Sigma^\prime x_\text{new}\\&amp;
=(x_\text{new}x_\text{new}^T\sigma^{-2}+{\Sigma^\prime}^{-1})\Sigma^\prime x_\text{new}\\
&amp;=\sigma^{-2}x_\text{new}x_\text{new}^T\Sigma^\prime x_\text{new}+x_\text{new}\\
&amp;= (\sigma^{-2}x_\text{new}^T\Sigma^\prime x_\text{new} + 1)x_\text{new}\\
\Leftrightarrow {\sigma^\prime}^2x_\text{new}&amp;=\frac{\sigma^{-2}x_\text{new}^T\Sigma^\prime x_\text{new} + 1}{\sigma^{-2}}x_\text{new}\\
\Leftrightarrow x_\text{new}&amp;=x_\text{new}.
\end{aligned}\end{split}\]</div>
<p>Et nous obtenons ce que nous voulions montrer. Cela nous indique que nous avons :</p>
<div class="math notranslate nohighlight">
\[\mu^{\prime\prime}=\mu^T x_\text{new},\]</div>
<p>o√π</p>
<div class="math notranslate nohighlight">
\[\mu=(\tilde{X}^T\tilde{X}+\lambda I)^{-1}\tilde{X}^T y.\]</div>
<p><strong>Conclusion</strong></p>
<p>Nous obtenons donc, √©tant donn√© une observation <span class="math notranslate nohighlight">\(x_\text{new}\)</span> que son label <span class="math notranslate nohighlight">\(y_\text{new}\)</span> est distribu√© selon une loi normale dont la moyenne est :</p>
<div class="math notranslate nohighlight">
\[\mu^T x_\text{new}\]</div>
<p>avec :</p>
<div class="math notranslate nohighlight">
\[\mu=(\tilde{X}^T\tilde{X}+\lambda I)^{-1}\tilde{X}^T y,\]</div>
<p>et dont la variance est donn√©e par :</p>
<div class="math notranslate nohighlight">
\[{\sigma^\prime}^{-2}=\frac{\sigma^{-2}}{1+\sigma^{-2}x_\text{new}^T\Sigma^\prime x_\text{new}}\]</div>
<p>o√π</p>
<div class="math notranslate nohighlight">
\[{\Sigma^\prime}^{-1}=\frac{\tilde{X}^T\tilde{X}+\lambda I}{\sigma^2}.\]</div>
<p>Cela nous dit finalement qu‚Äôune pr√©diction d‚Äôun mod√®lre Ridge revient √† prendre l‚Äôesp√©rance d‚Äôun Bayesian Model Averaging o√π nos mod√®les sont les mod√®les lin√©aires auquel on ajoute comme <em>prior</em> une loi normale dont la variance est contr√¥l√©e par le param√®tre <span class="math notranslate nohighlight">\(1/\lambda\)</span>.</p>
<p>On retrouve ce mod√®le dans <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span> avec <span class="math notranslate nohighlight">\(\texttt{BayesianRidge}\)</span>.</p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">BayesianRidge</span>


<span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">x</span><span class="p">)</span>


<span class="c1"># #############################################################################</span>
<span class="c1"># Generate sinusoidal data with noise</span>
<span class="n">size</span> <span class="o">=</span> <span class="mi">25</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span> <span class="o">+</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>


<span class="c1"># #############################################################################</span>
<span class="c1"># Fit by cubic polynomial</span>
<span class="n">n_order</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vander</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">n_order</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">increasing</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vander</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">n_order</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">increasing</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># #############################################################################</span>
<span class="c1"># Plot the true and predicted curves with log marginal likelihood (L)</span>
<span class="n">reg</span> <span class="o">=</span> <span class="n">BayesianRidge</span><span class="p">(</span><span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">compute_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="p">):</span>
    <span class="c1"># Bayesian ridge regression with different initial value pairs</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">init</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">y_train</span><span class="p">),</span> <span class="mf">1.</span><span class="p">]</span>  <span class="c1"># Default values</span>
    <span class="k">elif</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">init</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">]</span>
        <span class="n">reg</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">alpha_init</span><span class="o">=</span><span class="n">init</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">lambda_init</span><span class="o">=</span><span class="n">init</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">ymean</span><span class="p">,</span> <span class="n">ystd</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">func</span><span class="p">(</span><span class="n">x_test</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;sin($2</span><span class="se">\\</span><span class="s2">pi x$)&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;observation&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">ymean</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;predict mean&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">ymean</span><span class="o">-</span><span class="n">ystd</span><span class="p">,</span> <span class="n">ymean</span><span class="o">+</span><span class="n">ystd</span><span class="p">,</span>
                    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;pink&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;predict std&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">1.3</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">title</span> <span class="o">=</span> <span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">alpha$_init$=</span><span class="si">{:.2f}</span><span class="s2">,</span><span class="se">\\</span><span class="s2"> </span><span class="se">\\</span><span class="s2">lambda$_init$=</span><span class="si">{}</span><span class="s2">$&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">init</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">init</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">title</span> <span class="o">+=</span> <span class="s2">&quot; (Default)&quot;</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">alpha=</span><span class="si">{:.1f}</span><span class="s2">$</span><span class="se">\n</span><span class="s2">$</span><span class="se">\\</span><span class="s2">lambda=</span><span class="si">{:.3f}</span><span class="s2">$</span><span class="se">\n</span><span class="s2">$L=</span><span class="si">{:.1f}</span><span class="s2">$&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
           <span class="n">reg</span><span class="o">.</span><span class="n">alpha_</span><span class="p">,</span> <span class="n">reg</span><span class="o">.</span><span class="n">lambda_</span><span class="p">,</span> <span class="n">reg</span><span class="o">.</span><span class="n">scores_</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_ensembles_33_0.png" src="../_images/1_ensembles_33_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="iii-boosting">
<h2>III. Boosting<a class="headerlink" href="#iii-boosting" title="Permalink to this headline">¬∂</a></h2>
<p>L‚Äôid√©e primaire derri√®re la m√©thode de <em>boosting</em> est de construire des mod√®les ‚Äúfaibles‚Äù (potentiellement √† peine meilleurs que le hasard) mais compl√©mentaires entre eux poss√©dant ainsi de bonnes performances en tant que groupe. Nous √©tudierons ici la m√©thode <em>AdaBoost</em> (i.e. Adaptive Boosting). Soit <span class="math notranslate nohighlight">\(\mathcal{X}\subseteq\mathbb{R}^d\)</span> et <span class="math notranslate nohighlight">\(\mathcal{Y}=\{-1, 1\}\)</span>. On ne consid√®rera que le cas de la classification binaire bien que le propos se g√©n√©ralise √† d‚Äôautres t√¢ches de <em>machine learning</em>. Notre objectif est de construire un mod√®le final <span class="math notranslate nohighlight">\(h\)</span> de <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> vers <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> en s‚Äôappuyant sur un jeu de donn√©es <span class="math notranslate nohighlight">\(S_n\)</span>. Notons <span class="math notranslate nohighlight">\(w_i^j\)</span> le poids associ√© √† la donn√©e <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> pour le mod√®le faible <span class="math notranslate nohighlight">\(j\)</span>. Initialisons les poids √† <span class="math notranslate nohighlight">\(w_i^j=1/n\)</span>. Notre mod√®le consistera en <span class="math notranslate nohighlight">\(m\)</span> classifieurs faibles.</p>
<p><em>AdaBoost</em> fonctionne de la mani√®re suivante :</p>
<ol class="simple">
<li><p>On initialise notre compteur <span class="math notranslate nohighlight">\(j=1\)</span></p></li>
<li><p>On optimise notre mod√®le sur le jeu de donn√©es :</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\sum_i w_i^j\textbf{1}\{h_j(x_i)\neq y_i\}\]</div>
<ol class="simple">
<li><p>On calcule l‚Äôerreur normalis√©e :</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\epsilon_j=\frac{\sum_i w_i^j\textbf{1}\{h_j(x_i)\neq y_i\}}{\sum_i w_i^j}.\]</div>
<p>et on √©value l‚Äôimportance du mod√®le courant pour les futurs mod√®les :</p>
<div class="math notranslate nohighlight">
\[\alpha_j=\text{ln}\Big(\frac{1-\epsilon_j}{\epsilon_j}\Big)\]</div>
<ol class="simple">
<li><p>On met √† jour la pond√©ration des donn√©es pour le mod√®le suivant :</p></li>
</ol>
<div class="math notranslate nohighlight">
\[w_i^{j+1}=w_i^j\text{exp}\big(\alpha_j \textbf{1}\{h_j(x_i)\neq y_i\}\big)\]</div>
<ol class="simple">
<li><p>Si <span class="math notranslate nohighlight">\(j&lt;m\)</span>, on reprend √† l‚Äô√©tape <span class="math notranslate nohighlight">\(2\)</span> pour optimiser le mod√®le suivant.</p></li>
</ol>
<p>Une fois les mod√®les optimis√©s, notre <em>meta-classifieur</em> permet de faire des pr√©dictions de la mani√®re suivante :</p>
<div class="math notranslate nohighlight">
\[H(x)=\text{sign}\Big(\sum_j\alpha_j h_j(x)\Big)\]</div>
<p>Intuitivement, on cherche √† favoriser les mod√®les qui ‚Äúfonctionnent bien‚Äù et √† donner de l‚Äôimportance aux exemples d‚Äôapprentissage mal class√©s pour que les futurs classifieurs faibles se concentrent dessus.</p>
<hr class="docutils" />
<p><strong>Pourquoi ces coefficients ?</strong></p>
<p>Consid√©rons tout d‚Äôabord la <em>loss</em> exponentielle :</p>
<div class="math notranslate nohighlight">
\[\ell(z)=\text{exp}\big(-z\big),\]</div>
<p>o√π <span class="math notranslate nohighlight">\(z=yh(x)\)</span>, <span class="math notranslate nohighlight">\((x, y)\in S_n\)</span>. La <em>loss</em> est affich√©e juste apr√®s.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Exponential loss&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">&lt;=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="n">x</span><span class="p">[</span><span class="mi">51</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;0/1 loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_ensembles_36_0.png" src="../_images/1_ensembles_36_0.png" />
</div>
</div>
<p>On cherche √† construire un classifieur <span class="math notranslate nohighlight">\(H\)</span> minimisant la <em>loss</em> exponentielle suivante :</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(H)=\sum_{i=1}^n\ell(y_iH(x_i)),\]</div>
<p>o√π</p>
<div class="math notranslate nohighlight">
\[H(x)=\frac{1}{2}\sum_j\alpha_j h_j(x),\]</div>
<p>tels que <span class="math notranslate nohighlight">\(h_j\)</span> sont nos classifieurs faibles et <span class="math notranslate nohighlight">\(\alpha_j\)</span> les poids qui leur sont associ√©s. Cependant, imaginons qu‚Äôau lieu de tout minimiser d‚Äôune seule fois, nous proc√©dions it√©rativement, classifieur par classifieur. Ainsi, lorsqu‚Äôon optimise le classifieur <span class="math notranslate nohighlight">\(j\)</span>, tous les classifieurs <span class="math notranslate nohighlight">\(h_{1}, \ldots, h_{j-1}\)</span> et leur poid <span class="math notranslate nohighlight">\(\alpha_1, \ldots, \alpha_{j-1}\)</span> restent fix√©s. Notons <span class="math notranslate nohighlight">\(H_m(x)\)</span> le classifieur total jusqu‚Äôau classifieur faible <span class="math notranslate nohighlight">\(h_m\)</span>. Notre <em>loss</em> se reformule ainsi :</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_j(H)=\sum_{i=1}^n\text{exp}\Big(-y_iH_{j-1}(x_i)-\frac{1}{2}y_i\alpha_jh_j(x_i)\Big)=\sum_{i=1}^nw_i^j\text{exp}\Big(-\frac{1}{2}y_i\alpha_jh_j(x_i)\Big),\]</div>
<p>o√π <span class="math notranslate nohighlight">\(w_i^j = \text{exp}(-y_iH_{j-1}(x_i))\)</span>. Notons que si nous sommes bien entrain d‚Äôoptimiser notre loss du point de vue de <span class="math notranslate nohighlight">\(h_j\)</span> et <span class="math notranslate nohighlight">\(\alpha_j\)</span>, alors <span class="math notranslate nohighlight">\(w_i^j, \forall i\)</span> sont des constantes. Notons <span class="math notranslate nohighlight">\(S_c^{m}\)</span> l‚Äôensemble des points correctement class√©s par <span class="math notranslate nohighlight">\(H_m\)</span> et <span class="math notranslate nohighlight">\(S_i^m\)</span> ceux qui √† l‚Äôinverse ne le sont pas. Nous pouvons reformuler l‚Äôerreur pr√©c√©dente de la mani√®re suivante :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathcal{L}_j(H)&amp;=e^{-\alpha_j/2}\sum_{i\in S_\mathcal{c}^j}w_i^m+e^{\alpha_j/2}\sum_{i\in S_\mathcal{i}^j}w_i^m\\
&amp;=(e^{\alpha_j/2}-e^{-\alpha_j/2})\sum_{i=1}^nw_i^m\textbf{1}\{h_j(x_i)\neq y_i\}+e^{-\alpha_j/2}\sum_{i=1}^nw_i^m\\
&amp;\propto (e^{\alpha_j/2}-e^{-\alpha_j/2})\sum_{i=1}^nw_i^m\textbf{1}\{h_j(x_i)\neq y_i\}
\end{aligned}\end{split}\]</div>
<p>Du point de vu de l‚Äôoptimisation de <span class="math notranslate nohighlight">\(h_j\)</span>, on remarque que cela revient √† optimiser :</p>
<div class="math notranslate nohighlight">
\[(e^{\alpha_j/2}-e^{-\alpha_j/2})\sum_{i=1}^nw_i^m\textbf{1}\{h_j(x_i)\neq y_i\},\]</div>
<p>o√π le choix de <span class="math notranslate nohighlight">\(\alpha_j\)</span> n‚Äôa pas d‚Äôeffet. Cela revient √† r√©aliser l‚Äô√©tape <span class="math notranslate nohighlight">\(2\)</span> de notre algorithme. Consid√©rons maintenant l‚Äôoptimisation de <span class="math notranslate nohighlight">\(\alpha_j\)</span>. En annulant la deriv√©e en fonction de <span class="math notranslate nohighlight">\(\alpha_j\)</span> on se rend compte que cela revient √† le calculer comme indiqu√© √† l‚Äô√©tape 3 (<span class="math notranslate nohighlight">\(\epsilon_j\)</span> et <span class="math notranslate nohighlight">\(\alpha_j\)</span>).</p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># exercice avec des stumps...</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">zero_one_loss</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">AdaBoostClassifier</span>


<span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">400</span>
<span class="c1"># A learning rate of 1. may not be optimal for both SAMME and SAMME.R</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1.</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_hastie_10_2</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">12000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">2000</span><span class="p">:],</span> <span class="n">y</span><span class="p">[</span><span class="mi">2000</span><span class="p">:]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="mi">2000</span><span class="p">],</span> <span class="n">y</span><span class="p">[:</span><span class="mi">2000</span><span class="p">]</span>

<span class="n">dt_stump</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">dt_stump</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">dt_stump_err</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">dt_stump</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">dt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">dt_err</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">dt</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="n">ada_discrete</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span>
    <span class="n">base_estimator</span><span class="o">=</span><span class="n">dt_stump</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
    <span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;SAMME&quot;</span><span class="p">)</span>
<span class="n">ada_discrete</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">ada_real</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span>
    <span class="n">base_estimator</span><span class="o">=</span><span class="n">dt_stump</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
    <span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;SAMME.R&quot;</span><span class="p">)</span>
<span class="n">ada_real</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_estimators</span><span class="p">],</span> <span class="p">[</span><span class="n">dt_stump_err</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;k-&#39;</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Decision Stump Error&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_estimators</span><span class="p">],</span> <span class="p">[</span><span class="n">dt_err</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;k--&#39;</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Decision Tree Error&#39;</span><span class="p">)</span>

<span class="n">ada_discrete_err</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_estimators</span><span class="p">,))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">y_pred</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ada_discrete</span><span class="o">.</span><span class="n">staged_predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)):</span>
    <span class="n">ada_discrete_err</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">zero_one_loss</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="n">ada_discrete_err_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_estimators</span><span class="p">,))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">y_pred</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ada_discrete</span><span class="o">.</span><span class="n">staged_predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)):</span>
    <span class="n">ada_discrete_err_train</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">zero_one_loss</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">ada_real_err</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_estimators</span><span class="p">,))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">y_pred</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ada_real</span><span class="o">.</span><span class="n">staged_predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)):</span>
    <span class="n">ada_real_err</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">zero_one_loss</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="n">ada_real_err_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_estimators</span><span class="p">,))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">y_pred</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ada_real</span><span class="o">.</span><span class="n">staged_predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)):</span>
    <span class="n">ada_real_err_train</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">zero_one_loss</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_estimators</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">ada_discrete_err</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Discrete AdaBoost Test Error&#39;</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_estimators</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">ada_discrete_err_train</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Discrete AdaBoost Train Error&#39;</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_estimators</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">ada_real_err</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Real AdaBoost Test Error&#39;</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_estimators</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">ada_real_err_train</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Real AdaBoost Train Error&#39;</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">((</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;n_estimators&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;error rate&#39;</span><span class="p">)</span>

<span class="n">leg</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">,</span> <span class="n">fancybox</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">leg</span><span class="o">.</span><span class="n">get_frame</span><span class="p">()</span><span class="o">.</span><span class="n">set_alpha</span><span class="p">(</span><span class="mf">0.7</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_ensembles_39_0.png" src="../_images/1_ensembles_39_0.png" />
</div>
</div>
</div>
<div class="section" id="iv-les-forets-aleatoires">
<h2>IV. Les for√™ts al√©atoires<a class="headerlink" href="#iv-les-forets-aleatoires" title="Permalink to this headline">¬∂</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(10000, 10)
</pre></div>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./5_ensembles"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="0_propos_liminaire.html" title="previous page">Les m√©thodes <em>ensemblistes</em></a>
    <a class='right-next' id="next-link" href="../6_autodiff/0_propos_liminaire.html" title="next page">La diff√©rentiation automatique</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By The LMIPR team<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>
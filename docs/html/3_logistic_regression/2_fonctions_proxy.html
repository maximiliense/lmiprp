
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Les fonctions proxy ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è &#8212; Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Le classifieur de Bayes ‚òïÔ∏è" href="3_bayes_classifier.html" />
    <link rel="prev" title="La R√©gression Logistique ‚òïÔ∏è" href="1_logistic_regression.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Machine Learning
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../0_requisite/rappels_python.html">
   Rappels de Python et Numpy
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../1_what_is_ml/0_propos_liminaire.html">
   Le
   <em>
    Machine Learning
   </em>
   , c‚Äôest quoi ?
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/1_introduction_ml.html">
     <em>
      Machine learning
     </em>
     et mal√©diction de la dimension
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/2_regression_and_classification_trees.html">
     Les arbres de r√©gression et de classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../2_linear_regression/0_propos_liminaire.html">
   La
   <em>
    r√©gression
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_linear_regression/1_linear_regression.html">
     La r√©gression lin√©aire ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_linear_regression/2_optimization.html">
     L‚Äôoptimisation ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_linear_regression/3_interpolation.html">
     Interpolation ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_linear_regression/4_algo_proximal_lasso.html">
     Sous-diff√©rentiel et le cas du Lasso ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="0_propos_liminaire.html">
   La
   <em>
    classification
   </em>
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="1_logistic_regression.html">
     La R√©gression Logistique ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Les fonctions proxy ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3_bayes_classifier.html">
     Le classifieur de Bayes ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="4_VC_theory.html">
     La th√©orie de Vapnik et Chervonenkis ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è (üíÜ‚Äç‚ôÇÔ∏è)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../4_max_margin/0_propos_liminaire.html">
   Les mod√®les
   <em>
    max-margin
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../4_max_margin/1_max_margin.html">
     Les mod√®les max-margin ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5_ensembles/0_propos_liminaire.html">
   Les m√©thodes
   <em>
    ensemblistes
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/1_ensembles.html">
     Les m√©thodes ensemblistes ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../6_autodiff/0_propos_liminaire.html">
   La diff√©rentiation automatique
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_autodiff/1_autodiff.html">
     La diff√©rentiation automatique et un d√©but de
     <em>
      deep learning
     </em>
     ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_autodiff/2_filters_representation.html">
     Filtres et espace de repr√©sentation des r√©seaux de neurones ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../7_regularization/0_propos_liminaire.html">
   La r√©gularisation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/1_regularization_deep.html">
     R√©gularisation en deep learning ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/2_unsupervised_representation_learning.html">
     Unsupervised representation learning ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/3_multitask.html">
     L‚Äôapprentissage multi-t√¢ches ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/4_adversarial.html">
     Les attaques adversaires ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/5_ridge.html">
     Une analyse de la r√©gularisation Ridge ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../content.html">
   Content in Jupyter Book
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../markdown.html">
     Markdown Files
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../notebooks.html">
     Content with notebooks
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/3_logistic_regression/2_fonctions_proxy.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/maximiliense/lmiprp"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/maximiliense/lmiprp/issues/new?title=Issue%20on%20page%20%2F3_logistic_regression/2_fonctions_proxy.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/maximiliense/lmiprp/master?urlpath=tree/3_logistic_regression/2_fonctions_proxy.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#i-une-premiere-minimisation-du-risque-empirique">
   I. Une premi√®re minimisation du risque empirique
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#construction-du-jeu-de-donnees">
     Construction du jeu de donn√©es
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#affichage-du-dataset">
     Affichage du dataset
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#notre-classe-de-fonctions-lineaires-mathcal-h">
     Notre classe de fonctions lin√©aires
     <span class="math notranslate nohighlight">
      \(\mathcal{H}\)
     </span>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ii-construction-du-phi-risk">
   II. Construction du
   <span class="math notranslate nohighlight">
    \(\phi\)
   </span>
   -risk
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iii-proprietes-d-un-phi-risk">
   III. Propri√©t√©s d‚Äôun
   <span class="math notranslate nohighlight">
    \(\phi\)
   </span>
   -risk
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iv-logistic-loss-et-regression-logistique">
   IV.
   <em>
    Logistic loss
   </em>
   et r√©gression logistique
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="les-fonctions-proxy">
<h1>Les fonctions proxy ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è<a class="headerlink" href="#les-fonctions-proxy" title="Permalink to this headline">¬∂</a></h1>
<p>Situons-nous dans le cadre d‚Äôun probl√®me de classification supervis√© et notons <span class="math notranslate nohighlight">\(\mathcal{X}\subseteq\mathbb{R}^d\)</span> et <span class="math notranslate nohighlight">\(\mathcal{Y}=\{0, 1\}\)</span> ou <span class="math notranslate nohighlight">\(\mathcal{Y}=\{-1,+1\}\)</span> selon le contexte et afin de simplifier les notations. Notons √©galement <span class="math notranslate nohighlight">\(X,Y\in\mathcal{X}\times\mathcal{Y}\)</span> deux variables al√©atoires telles que <span class="math notranslate nohighlight">\(\mu\)</span> est la mesure de <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(\eta(x)=\mathbb{P}(Y=1|X=x)\)</span>. La connaissance de <span class="math notranslate nohighlight">\(\mu\)</span> et de <span class="math notranslate nohighlight">\(\eta\)</span> nous donne toutes les informations propres au processus g√©n√©rateur de notre probl√®me. Nous pouvons d‚Äôailleurs construire la mesure jointe. Soit <span class="math notranslate nohighlight">\(A\subset \mathcal{X}\times\mathcal{Y}\)</span>, nous avons :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}(X, Y \in A)=\int_{A\cap \mathcal{X}\times \{1\}}\eta(x)d\mu+\int_{A\cap \mathcal{X}\times \{0\}}(1-\eta(x))d\mu.\]</div>
<p>Notre objectif est de trouver une application <span class="math notranslate nohighlight">\(h:\mathcal{X}\mapsto\mathcal{Y}\)</span> tel qu‚Äôun risque est minimis√©. <em>A priori</em> ce que nous souhaitons faire est de minimiser le nombre d‚Äôerreurs. Pour cela, nous d√©finissons le risque dit <span class="math notranslate nohighlight">\(0/1\)</span> :</p>
<div class="math notranslate nohighlight">
\[L(h)=\mathbb{P}(h(X)\neq Y)=\mathbb{E}\big[1\{X\neq Y\}\big].\]</div>
<p>Nous ne connaissons malheureusement ni <span class="math notranslate nohighlight">\(\mu\)</span> ni <span class="math notranslate nohighlight">\(\eta\)</span> et pour cela, nous devons estimer le risque <span class="math notranslate nohighlight">\(L\)</span> sur un jeu de donn√©es :</p>
<div class="math notranslate nohighlight">
\[S_n=\{(X_i, Y_i)\}_{i\leq n}\sim \mathbb{P}^n,\]</div>
<p>qu‚Äôon supposera repr√©sentatif du probl√®me dans le sens o√π les couples <span class="math notranslate nohighlight">\((X, Y)\)</span> sont iid. √Ä partir de l√†, nous pouvons estimer notre risque empirique :</p>
<div class="math notranslate nohighlight">
\[L_n(h)=\frac{1}{n}\sum_i 1\{h(X_i)\neq Y_i\}.\]</div>
<p>Il s‚Äôagit tout simplement de la moyenne des erreurs.</p>
<p>Notre objectif, en tant que <em>machine learner</em> est de minimiser cette erreur. Bien s√ªr (#nofreelunchtheorem), nous ne pouvons pas consid√©rer toutes les fonctions de <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> dans <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>. Soit <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> la classe de fonctions que nous souhaitons consid√©rer. Le probl√®me √† r√©soudre est :</p>
<div class="math notranslate nohighlight">
\[h_n=\text{argmin}_{h\in\mathcal{H}}L_n(h).\]</div>
<p>C‚Äôest ce qu‚Äôon appelle le minimiseur du risque empirique. Remarquez que ce n‚Äôest pas exactement ce qu‚Äôon minimise en pratique. Nous allons voir pourquoi.</p>
<div class="section" id="i-une-premiere-minimisation-du-risque-empirique">
<h2>I. Une premi√®re minimisation du risque empirique<a class="headerlink" href="#i-une-premiere-minimisation-du-risque-empirique" title="Permalink to this headline">¬∂</a></h2>
<div class="section" id="construction-du-jeu-de-donnees">
<h3>Construction du jeu de donn√©es<a class="headerlink" href="#construction-du-jeu-de-donnees" title="Permalink to this headline">¬∂</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">h_star</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">y</span><span class="p">[</span><span class="n">x</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">&gt;</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">y</span><span class="p">[</span><span class="n">x</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">&lt;=</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="k">return</span> <span class="n">y</span>

<span class="k">def</span> <span class="nf">construct_dataset</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">h_star</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">construct_dataset</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="affichage-du-dataset">
<h3>Affichage du dataset<a class="headerlink" href="#affichage-du-dataset" title="Permalink to this headline">¬∂</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_fonctions_proxy_8_0.png" src="../_images/2_fonctions_proxy_8_0.png" />
</div>
</div>
</div>
<div class="section" id="notre-classe-de-fonctions-lineaires-mathcal-h">
<h3>Notre classe de fonctions lin√©aires <span class="math notranslate nohighlight">\(\mathcal{H}\)</span><a class="headerlink" href="#notre-classe-de-fonctions-lineaires-mathcal-h" title="Permalink to this headline">¬∂</a></h3>
<p>Consid√©rons le cas tr√®s simple des mod√®les lin√©aires. Ici, notre ensemble de fonctions <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> est l‚Äôensemble de cardinal infini dont la fronti√®re de d√©cision sont les droites dans le plan :</p>
<div class="math notranslate nohighlight">
\[\mathcal{H}=\{x\mapsto\text{sign}(\langle \omega, x\rangle+b):\ \omega\in\mathbb{R}^2,\ b\in\mathbb{R}\}.\]</div>
<p>Ici, nous supposerons que <span class="math notranslate nohighlight">\(\mathcal{Y}=\{-1, 1\}\)</span>. Notre probl√®me de minimisation devient donc :</p>
<div class="math notranslate nohighlight">
\[(\omega, b)=\text{argmin}_{\omega, b\in \mathbb{R}^2\times\mathbb{R}}\frac{1}{n}\sum_i 1\{\text{sign}(\langle \omega, x_i\rangle+b)=y_i\}.\]</div>
<p>Comment faire cela ? La fonction √† optimiser est constante par morceau et les optimiseurs √† base d‚Äôinformation du premier ordre (i.e. d√©riv√©e, gradient) ne peuvent pas nous aider. Une strat√©gie est de constater que dans le plan, une droite n‚Äôa besoin que de deux points pour se positionner. √Ä partir de deux points <span class="math notranslate nohighlight">\(x_1\)</span> et <span class="math notranslate nohighlight">\(x_2\)</span>, il est possible d‚Äôobtenir une valeur de <span class="math notranslate nohighlight">\(\omega\)</span> et de <span class="math notranslate nohighlight">\(b\)</span> en r√©solvant le syst√®me d‚Äô√©quations suivant :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{cases}\langle\omega, x_1\rangle + b=0\\\langle\omega, x_2\rangle +b=0\end{cases}.\end{split}\]</div>
<p>Il y a √©videmment une infinit√© de solutions (deux √©quations et trois inconnues). On peut r√©soudre ce probl√®me en contraignant la norme de <span class="math notranslate nohighlight">\(\omega\)</span> √† valoir <span class="math notranslate nohighlight">\(1\)</span>. Il reste deux solutions au syst√®me pr√©c√©dent. Il faut consid√©rer le cas o√π la classe <span class="math notranslate nohighlight">\(1\)</span> est d‚Äôun c√¥t√© de la droite et le cas o√π la classe <span class="math notranslate nohighlight">\(1\)</span> est de l‚Äôautre c√¥t√©.</p>
<p>Il suffit maintenant de tester toutes les valeurs possibles de <span class="math notranslate nohighlight">\(\omega\)</span> et de <span class="math notranslate nohighlight">\(b\)</span> s‚Äôappuyant sur des points de notre jeu de donn√©es et de prendre celle qui fait le moins d‚Äôerreurs !</p>
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Impl√©mentez notre mod√®le de classification <span class="math notranslate nohighlight">\(h\)</span> (i.e. la fonction qui fait une pr√©diction quant √† la classe).</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">h</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">omega</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="c1">####### Complete this part ######## or die ####################</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">omega</span><span class="p">)</span><span class="o">+</span><span class="n">b</span>
    <span class="n">predictions</span><span class="p">[</span><span class="n">predictions</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">predictions</span><span class="p">[</span><span class="n">predictions</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="c1">###############################################################</span>
    <span class="k">return</span> <span class="n">predictions</span>
</pre></div>
</div>
</div>
</div>
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Impl√©mentez une methode qui prend en param√®tre deux points ainsi que le sens (le c√¥t√© de l‚Äôhyperplan) via un boolean et retourne un vecteur <span class="math notranslate nohighlight">\(\omega\)</span> qui d√©crit l‚Äôorientation de l‚Äôhyperplan et le biais <span class="math notranslate nohighlight">\(b\)</span>.</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parameterize</span><span class="p">(</span><span class="n">x_1</span><span class="p">,</span> <span class="n">x_2</span><span class="p">,</span> <span class="n">omega_positive</span><span class="p">):</span>
    <span class="c1">####### Complete this part ######## or die ####################</span>
    <span class="n">_a</span> <span class="o">=</span> <span class="n">x_2</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">x_1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">_b</span> <span class="o">=</span> <span class="n">x_1</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">x_2</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">omega</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">_a</span><span class="o">/</span><span class="n">_b</span><span class="p">])</span>
    <span class="n">omega</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">omega</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">omega_positive</span><span class="p">:</span>
        <span class="n">omega</span> <span class="o">*=-</span><span class="mi">1</span>
    <span class="n">b</span> <span class="o">=</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">omega</span><span class="p">,</span> <span class="n">x_1</span><span class="p">)</span>
    <span class="c1">###############################################################</span>
    <span class="k">return</span> <span class="n">omega</span><span class="p">,</span> <span class="n">b</span>
</pre></div>
</div>
</div>
</div>
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Impl√©mentez une methode qui calcule le risque empirique.</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">empirical_risk</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">omega</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="c1">####### Complete this part ######## or die ####################</span>
    <span class="n">risk</span> <span class="o">=</span> <span class="p">(</span><span class="n">h</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">omega</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">!=</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1">###############################################################</span>
    <span class="k">return</span> <span class="n">risk</span>
</pre></div>
</div>
</div>
</div>
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Impl√©mentez une methode qui calcule le minimiseur du risque empirique.</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fit_empirical_risk</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1">####### Complete this part ######## or die ####################</span>
    <span class="n">best_omega</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">best_b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">best_risk</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">j</span><span class="p">:</span>
                <span class="n">omega</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">parameterize</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">j</span><span class="o">&gt;</span><span class="n">i</span><span class="p">)</span>
                <span class="n">risk</span> <span class="o">=</span> <span class="n">empirical_risk</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">omega</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">best_omega</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">risk</span> <span class="o">&lt;</span> <span class="n">best_risk</span><span class="p">:</span>
                    <span class="n">best_risk</span> <span class="o">=</span> <span class="n">risk</span>
                    <span class="n">best_omega</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">omega</span><span class="p">)</span>
                    <span class="n">best_b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="c1">###############################################################</span>
    <span class="k">return</span> <span class="n">best_omega</span><span class="p">,</span> <span class="n">best_b</span><span class="p">,</span> <span class="n">best_risk</span>
</pre></div>
</div>
</div>
</div>
<p>Testons le code pr√©c√©dent.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">omega</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">fit_empirical_risk</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Obtained empirical risk:&#39;</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Obtained empirical risk: 0.0
</pre></div>
</div>
</div>
</div>
<p>Affichons maintenant le s√©parateur ainsi calcul√©.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="n">x_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">)</span>
<span class="n">y_</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">b</span><span class="o">-</span><span class="n">omega</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">x_</span><span class="p">)</span><span class="o">/</span><span class="n">omega</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span> <span class="n">y_</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Our data and our model&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_fonctions_proxy_22_0.png" src="../_images/2_fonctions_proxy_22_0.png" />
</div>
</div>
<p>Malheureusement, tester toutes les combinaisons a un co√ªt qui nous emp√™che de consid√©rer des jeux de donn√©es trop grands et en trop grande dimension. Il y a en effet dans <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> <span class="math notranslate nohighlight">\(n!/(d!(n-d)!)\)</span> combinaisons possibles √† tester. Dans le plan, cela nous donne :</p>
<div class="math notranslate nohighlight">
\[\frac{n!}{2(n-2)!}=0.5n(n-1)\approx 0.5 n^2\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="n">fit_duration</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">dataset_sizes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">291</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">dataset_sizes</span><span class="p">:</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">construct_dataset</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">omega</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">fit_empirical_risk</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">fit_duration</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dataset_sizes</span><span class="p">,</span> <span class="n">fit_duration</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Fit time&#39;</span><span class="p">)</span>

<span class="n">scale</span> <span class="o">=</span> <span class="n">fit_duration</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="n">dataset_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">theoretical_fit_duration</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dataset_sizes</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">scale</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dataset_sizes</span><span class="p">,</span> <span class="n">theoretical_fit_duration</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Estimated fit time&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Fit duration&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Time (s)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Dataset size&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_fonctions_proxy_25_0.png" src="../_images/2_fonctions_proxy_25_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Avec un jeu de donn√©es de taille 150000 dans le plan (petite dim),&#39;</span>\
      <span class="s1">&#39;le temps du fit estim√© est d</span><span class="se">\&#39;</span><span class="s1">environ&#39;</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">scale</span> <span class="o">*</span> <span class="mi">150000</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="mi">60</span><span class="o">/</span><span class="mi">24</span><span class="p">)),</span> <span class="s1">&#39;jours&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Avec un jeu de donn√©es de taille 150000 dans le plan (petite dim),le temps du fit estim√© est d&#39;environ 431 jours
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="ii-construction-du-phi-risk">
<h2>II. Construction du <span class="math notranslate nohighlight">\(\phi\)</span>-risk<a class="headerlink" href="#ii-construction-du-phi-risk" title="Permalink to this headline">¬∂</a></h2>
<p>√Ä la place de chercher √† optimiser notre erreur 0/1, l‚Äôid√©e va √™tre de trouver une fonction avec des propri√©t√©s math√©matiques int√©ressantes et pouvant nous offrir certaines garanties du point de vue de la classification.</p>
<p>Consid√©rons ici les labels <span class="math notranslate nohighlight">\(\mathcal{Y}=\{-1, 1\}\)</span>. Nous allons √©galement supposer que nos mod√®le sont construits comme la composition d‚Äôune fonction de score <span class="math notranslate nohighlight">\(g:\mathcal{X}\mapsto\mathbb{R}\)</span> et de la fonction <span class="math notranslate nohighlight">\(\texttt{sign}\)</span> qui donne le label <span class="math notranslate nohighlight">\(+1\)</span> si le score est positif et <span class="math notranslate nohighlight">\(-1\)</span> si le score est n√©gatif ! L‚Äôobjectif est donc de trouver une fonction de score dont le signe co√Øncide avec le signe de notre label.</p>
<p>Notre risque secondaire ou <span class="math notranslate nohighlight">\(\phi\)</span>-risk se construit de la mani√®re suivante :</p>
<div class="math notranslate nohighlight">
\[L(g)=\mathbb{E}\big[\phi(Yg(X))\big].\]</div>
<p>On remarque que si notre mod√®le pr√©dit le bon label, alors <span class="math notranslate nohighlight">\(g(X)\)</span> a le m√™me signe que <span class="math notranslate nohighlight">\(Y\)</span> et <span class="math notranslate nohighlight">\(Yg(X)&gt;0\)</span>. √Ä l‚Äôinverse, si le label pr√©dit est faux, alors, on a <span class="math notranslate nohighlight">\(Yg(X)&lt;0\)</span>. La fonction <span class="math notranslate nohighlight">\(\phi\)</span> doit majorer le risque <span class="math notranslate nohighlight">\(0/1\)</span> d‚Äôune mani√®re que nous allons d√©tailler ci-dessous, √™tre convexe et d√©rivable en <span class="math notranslate nohighlight">\(0\)</span> avec une d√©riv√©e n√©gative pour des raisons que nous allons d√©tailler plus bas.</p>
<p>Le risque empirique associ√© devient alors :</p>
<div class="math notranslate nohighlight">
\[L_n(g)=\frac{1}{n}\sum_i \phi(Y_ig(X_i)).\]</div>
<p>Observons d√©j√† que si on choisit la fonction <span class="math notranslate nohighlight">\(\phi\)</span> suivante :
$<span class="math notranslate nohighlight">\(\phi(z)=1\{z&lt;0\},\)</span><span class="math notranslate nohighlight">\(
on retombe sur notre erreur \)</span>0/1<span class="math notranslate nohighlight">\(. En effet, on compte \)</span>1<span class="math notranslate nohighlight">\( si le signe de \)</span>Yg(X)<span class="math notranslate nohighlight">\( est n√©gatif, √† savoir, si notre score ne correspond pas au label. D'autres choix de \)</span>\phi$-risk sont les suivants (il y en a beaucoup d‚Äôautres) :</p>
<ul class="simple">
<li><p><strong>Hinge loss</strong> : <span class="math notranslate nohighlight">\(\phi(z)=\text{max}(0, 1-z)\)</span>, ici  on p√©nalise tous les <span class="math notranslate nohighlight">\(z\)</span> tant qu‚Äôils sont inf√©rieur √† <span class="math notranslate nohighlight">\(1\)</span>. Cette loss est notamment utilis√© pour le SVM qui cherche √† obtenir une marge,</p></li>
<li><p><strong>Smoothed hinge loss</strong> : <span class="math notranslate nohighlight">\(\phi(z)=\beta^{-1}\text{log}\big(1+e^{\beta(1-z)}\big)\)</span>, o√π <span class="math notranslate nohighlight">\(\beta&gt;0\)</span> est un param√®tre. On retrouve la hinge loss comme limite lorsque <span class="math notranslate nohighlight">\(\beta\rightarrow\infty\)</span>,</p></li>
<li><p><strong>Logistic loss</strong> : <span class="math notranslate nohighlight">\(\phi(z)=\text{log}(1+e^{-z})\)</span>, c‚Äôest la loss utilis√©e lorsqu‚Äôon fait une r√©gression logistique ou de la classification binaire en <em>deep learning</em>. En effet, on a :</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{log}(1+e^{-z})=-\text{log}\Big(\frac{1}{1+e^{-z}}\Big)=-\text{log}(\sigma(z)),\]</div>
<p>o√π <span class="math notranslate nohighlight">\(\sigma(z)=(1+e^{-z})^{-1}\)</span> est la fonction sigmo√Ød. L‚Äôid√©e est donc de minimiser moins le logarithme appliqu√© √† la sigmo√Ød elle-m√™me appliqu√©e au logit multipli√© par le label de la bonne classe. Dit autrement, si <span class="math notranslate nohighlight">\(y=1\)</span>, alors, on minimise  <span class="math notranslate nohighlight">\(-\text{log}(\sigma(g(x)))\)</span> et si <span class="math notranslate nohighlight">\(y=-1\)</span> et on a <span class="math notranslate nohighlight">\(-\text{log}(1-\sigma(z))\)</span>. En notant cette fois-ci <span class="math notranslate nohighlight">\(\mathcal{Y}=\{0, 1\}\)</span>, on retrouve :</p>
<div class="math notranslate nohighlight">
\[-y\text{log}\big(\sigma(g(x))\big)-(1-y)\text{log}\big(1-\sigma(g(x))\big).\]</div>
<p>C‚Äôest l‚Äôentropie crois√©e appliqu√©e √† une sigmo√Øde elle-m√™me appliqu√©e au logit ! Si <span class="math notranslate nohighlight">\(g\)</span> est un mod√®le lin√©aire, alors on retombe sur la r√©gression logistique.</p>
<p>Visualisons ces quelques <span class="math notranslate nohighlight">\(\phi\)</span>-risk.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">zero_one_loss</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">hinge_loss</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">v</span> <span class="o">=</span> <span class="mi">1</span><span class="o">-</span><span class="n">x</span>
    <span class="k">return</span> <span class="n">v</span> <span class="o">*</span> <span class="p">(</span><span class="n">v</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">v</span><span class="o">&lt;=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">logistic_loss</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">soft_hinge_loos</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">logistic_loss</span><span class="p">(</span><span class="n">beta</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">/</span><span class="n">beta</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">201</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">zero_one_loss</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;0/1 loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">hinge_loss</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Hinge loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">soft_hinge_loos</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Soft Hinge loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">logistic_loss</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Logistic loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\phi$-risks&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_fonctions_proxy_30_0.png" src="../_images/2_fonctions_proxy_30_0.png" />
</div>
</div>
<p><span style="color:blue"><strong>Question :</strong></span>  <strong>Intuitivement, supposons que l‚Äôon minimise l‚Äôun des <span class="math notranslate nohighlight">\(\phi\)</span>-risk sur notre probl√®me de <em>machine learning</em> et qu‚Äôon obtienne <span class="math notranslate nohighlight">\(0\)</span> erreur √† la fin. Que pouvons nous dire quant au risque <span class="math notranslate nohighlight">\(0/1\)</span> ?</strong></p>
</div>
<div class="section" id="iii-proprietes-d-un-phi-risk">
<h2>III. Propri√©t√©s d‚Äôun <span class="math notranslate nohighlight">\(\phi\)</span>-risk<a class="headerlink" href="#iii-proprietes-d-un-phi-risk" title="Permalink to this headline">¬∂</a></h2>
<p>Comme dit plus haut, la fonction <span class="math notranslate nohighlight">\(\phi\)</span> doit notamment majorer le risque <span class="math notranslate nohighlight">\(0/1\)</span>. En r√©alit√©, cette majoration est √† un rescaling pr√®s (on peut multiplier par un scalaire positif). Ainsi la <em>logistic loss</em> ne majore pas le <span class="math notranslate nohighlight">\(\phi\)</span>-risk, sauf si on la multiplie par <span class="math notranslate nohighlight">\(2\)</span> (ou autre). Cette multiplication par <span class="math notranslate nohighlight">\(2\)</span> n‚Äôapporte rien du point de vue de la minimisation. Cependant, si sur notre probl√®me la <em>logistic loss</em> atteint <span class="math notranslate nohighlight">\(0\)</span> erreur (ou s‚Äôen rapproche asymptotiquement comme nous l‚Äôavons vu dans la s√©quence pr√©c√©dente), alors deux fois la <em>logistic loss</em> atteint <span class="math notranslate nohighlight">\(0\)</span> erreur et puisqu‚Äôelle majore l‚Äôerreur <span class="math notranslate nohighlight">\(0/1\)</span>, on peut en conclure que celui-ci fait √©galement <span class="math notranslate nohighlight">\(0\)</span> erreur !</p>
<p>Le <span class="math notranslate nohighlight">\(\phi\)</span>-risk doit √™tre convexe. La convexit√© garantit un certain nombre de propri√©t√©s dont la plus int√©ressante, peut-√™tre, est que si nous trouvons un minimiseur local de notre loss, alors celui-ci est √©galement un minimiseur global.</p>
<p>La derni√®re propri√©t√© que nous avons √©voqu√©e est que notre <span class="math notranslate nohighlight">\(\phi\)</span>-risk doit √™tre d√©rivable en <span class="math notranslate nohighlight">\(0\)</span> avec une d√©riv√©e n√©gative. Cette propri√©t√© garantit que notre <span class="math notranslate nohighlight">\(\phi\)</span>-risk est ‚Äúclassification calibrated‚Äù. Nous allons d√©tailler cette id√©e, un petit peu plus complexe.</p>
<p>Rappellons la notation <span class="math notranslate nohighlight">\(\eta(x)=\mathbb{P}(Y=1|X=x)\)</span>. Il s‚Äôagit de la VRAIE probabilit√© conditionnelle d‚Äôobtenir le label <span class="math notranslate nohighlight">\(1\)</span> sachant qu‚Äôon a observ√© la donn√©e <span class="math notranslate nohighlight">\(x\)</span>. Celle-ci n‚Äôest bien s√ªr pas connue. Nous avons :</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\Big[\phi(Yg(x))|X=x\Big]=\eta(x)\phi(g(x))+(1-\eta(x))\phi(-g(x))=C_\eta(g(x)).\]</div>
<p>C‚Äôest tout simplement l‚Äôesp√©rance de notre <span class="math notranslate nohighlight">\(\phi\)</span>-risk lorsqu‚Äôon a observ√© la donn√©e <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>Rappelons-nous que la fonction <span class="math notranslate nohighlight">\(g\)</span> retourne un score positif si on pr√©dit le label <span class="math notranslate nohighlight">\(y=1\)</span> et un score n√©gatif si on pr√©dit le label <span class="math notranslate nohighlight">\(y=-1\)</span>. Ainsi, si <span class="math notranslate nohighlight">\(\eta(x)&gt;0.5\)</span>, alors, on veut pr√©dire le label <span class="math notranslate nohighlight">\(1\)</span> et on veut que <span class="math notranslate nohighlight">\(g(x)&gt;0\)</span>. √Ä l‚Äôinverse, si <span class="math notranslate nohighlight">\(\eta(x)&lt;0.5\)</span>, alors on veut pr√©dire le label <span class="math notranslate nohighlight">\(-1\)</span> et on veut <span class="math notranslate nohighlight">\(g(x)&lt;0\)</span>. Dit autrement, on souhaite avoir les deux in√©galit√©s suivantes :</p>
<div class="math notranslate nohighlight">
\[\eta&gt;0.5\Leftrightarrow\text{argmin}_{z}C_\eta(z)\subset\mathbb{R}^{\star+},\]</div>
<div class="math notranslate nohighlight">
\[\eta&lt;0.5\Leftrightarrow\text{argmin}_{z}C_\eta(z)\subset\mathbb{R}^{\star-}.\]</div>
<p>Dit autrement, on veut que les minimiseurs de notre loss (du point de vu du score), ait bien le signe attendu selon la valeur <span class="math notranslate nohighlight">\(\eta\)</span>. C‚Äôest cela qu‚Äôon appelle ‚Äúclassification calibrated‚Äù.</p>
<p>Il se trouve que cette propri√©t√© est totalement √©quivalente au fait que <span class="math notranslate nohighlight">\(\phi\)</span> soit d√©rivable en <span class="math notranslate nohighlight">\(0\)</span> et de d√©riv√©e n√©gative.</p>
<p><strong>Proposition</strong> <span class="math notranslate nohighlight">\(\phi\)</span> est d√©rivable en <span class="math notranslate nohighlight">\(0\)</span> et <span class="math notranslate nohighlight">\(\phi^\prime(0)&lt;0\)</span> si et seulement si les deux in√©galit√©s pr√©c√©dentes sont satisfaites.</p>
<p><strong>Preuve :</strong> Notons <span class="math notranslate nohighlight">\(\phi_+\)</span>, <span class="math notranslate nohighlight">\(\phi_-\)</span>, <span class="math notranslate nohighlight">\((C_\eta)_+\)</span> et <span class="math notranslate nohighlight">\((C_\eta)_-\)</span> les d√©riv√©es √† gauche et √† droite. Nous avons alors, pour que les in√©galit√©s soient satisfaites, <span class="math notranslate nohighlight">\((C_\eta)_+^\prime(0)&lt;0\Leftrightarrow \eta&gt;0.5\)</span> et <span class="math notranslate nohighlight">\((C_\eta)_-^\prime(0)&gt;0\Leftrightarrow \eta&lt;0.5\)</span> puisque <span class="math notranslate nohighlight">\(C_\eta\)</span> est convexe en tant que combinaison convexe de fonctions convexes. Pour ce convaincre des √©quivalences pr√©c√©dentes, supposons <span class="math notranslate nohighlight">\(\eta&gt;0.5\)</span>, alors le minimum de <span class="math notranslate nohighlight">\(C_\eta(z)\)</span> doit √™tre atteint avec <span class="math notranslate nohighlight">\(z&gt;0\)</span>. Si <span class="math notranslate nohighlight">\((C_\eta)_+^\prime(0)&lt;0\)</span>, alors le minimum est ‚Äú√† droite‚Äù de 0 et r√©pond au besoin.</p>
<p><span class="math notranslate nohighlight">\((\Leftarrow)\)</span> Supposons que les deux in√©galit√©s soient satisfaites.</p>
<p>Nous voulons montrer (1) que <span class="math notranslate nohighlight">\(\phi\)</span> est d√©rivable en <span class="math notranslate nohighlight">\(0\)</span> et (2) que <span class="math notranslate nohighlight">\(\phi^\prime(0)&lt;0\)</span>.</p>
<p>On a (attention √† la d√©riv√©e) :</p>
<div class="math notranslate nohighlight">
\[\lim_{\eta\rightarrow 0.5^+}(C_\eta)_+^\prime(0)=\lim_{\eta\rightarrow 0.5^+}\eta\phi_+^\prime(0)-(1-\eta)\phi_-^\prime(0)=0.5\big(\phi_+^\prime(0)-\phi_-^\prime(0)\big)\leq 0.\]</div>
<p>La derni√®re in√©galit√© est v√©rifi√©e par hypoth√®se sur <span class="math notranslate nohighlight">\(C_\eta\)</span>. Cela nous donne donc <span class="math notranslate nohighlight">\(\phi_-^\prime(0)\geq\phi_+^\prime(0)\)</span>. Par hypoth√®se, <span class="math notranslate nohighlight">\(\phi\)</span> est convexe et on a toujours <span class="math notranslate nohighlight">\(\phi_-^\prime(0)\leq\phi_+^\prime(0)\)</span>. On a donc <span class="math notranslate nohighlight">\(\phi_+^\prime(0)=\phi_-^\prime(0)\)</span> et <span class="math notranslate nohighlight">\(\phi\)</span> est d√©rivable en <span class="math notranslate nohighlight">\(0\)</span>. C‚Äôest le premier point.</p>
<p>Nous avons ensuite :</p>
<div class="math notranslate nohighlight">
\[C_\eta^\prime(0)=\eta\phi^\prime(0)-(1-\eta)\phi^\prime(0)=(2\eta-1)\phi^\prime(0).\]</div>
<p>Si <span class="math notranslate nohighlight">\(\eta&gt;0.5\)</span>, alors <span class="math notranslate nohighlight">\(2\eta-1&gt;0\)</span> et <span class="math notranslate nohighlight">\(C_\eta^\prime(0)&lt;0\)</span>. On a alors <span class="math notranslate nohighlight">\(\phi^\prime(0)&lt;0\)</span>. On obtient le m√™me r√©sultat si <span class="math notranslate nohighlight">\(\eta&lt;0.5\)</span> !</p>
<p><span class="math notranslate nohighlight">\((\Rightarrow)\)</span> Supposons que <span class="math notranslate nohighlight">\(\phi\)</span> est d√©rivable en <span class="math notranslate nohighlight">\(0\)</span> et que <span class="math notranslate nohighlight">\(\phi^\prime(0)&lt;0\)</span>. On obtient alors :</p>
<div class="math notranslate nohighlight">
\[C_\eta^\prime(0)=(2\eta-1)\phi^\prime(0)\]</div>
<p>dont le signe est n√©gatif si <span class="math notranslate nohighlight">\(\eta&gt;0.5\)</span> et positif si <span class="math notranslate nohighlight">\(\eta&lt;0.5\)</span>. <strong><span class="math notranslate nohighlight">\(\boxed{}\)</span></strong></p>
</div>
<hr class="docutils" />
<div class="section" id="iv-logistic-loss-et-regression-logistique">
<h2>IV. <em>Logistic loss</em> et r√©gression logistique<a class="headerlink" href="#iv-logistic-loss-et-regression-logistique" title="Permalink to this headline">¬∂</a></h2>
<p>La <em>logistic loss</em> est <span class="math notranslate nohighlight">\(\phi(z)=\text{log}(1+e^{-z})\)</span>. Consid√©rons un jeu de donn√©es <span class="math notranslate nohighlight">\(S_n\)</span>, notre objectif pratique est de minimiser le <span class="math notranslate nohighlight">\(\phi\)</span>-risk empirique :</p>
<div class="math notranslate nohighlight">
\[L_n^\phi(g)=\frac{1}{n}\sum_i\phi(y_ig(x_i))=\frac{1}{n}\sum_i\text{log}(1+e^{-y_ig(x_i)})\]</div>
<p>Reprenons le cas d‚Äôun mod√®le lin√©aire. Cela nous permettra de comparer les co√ªts calculatoires avec le vrai risque empirique.</p>
<p>Nous avons donc <span class="math notranslate nohighlight">\(\mathcal{H}=\{x\mapsto \langle \omega, x\rangle +b:\ \omega\in\mathbb{R}^d,\ b\in\mathbb{R}\}\)</span>.</p>
<p>Notre objectif ici, est de calculer le gradient de notre <span class="math notranslate nohighlight">\(\phi\)</span>-risk empirique. La premi√®re √©tape est de calculer la d√©riv√©e de la fonction <span class="math notranslate nohighlight">\(\phi(yz)\)</span> :</p>
<div class="math notranslate nohighlight">
\[\phi^\prime(yz)=-\frac{ye^{-yz}}{1+e^{-yz}}=-\frac{y}{1+e^{yz}}.\]</div>
<p>Par simplicit√© de notation consid√©rons les vecteur <span class="math notranslate nohighlight">\(x_i=[1, x_1, \ldots, x_d]^T\)</span> afin d‚Äô√©viter la notation avec le biais <span class="math notranslate nohighlight">\(b\)</span>.</p>
<p>Nous avons donc naturellement que :</p>
<div class="math notranslate nohighlight">
\[\nabla g(x)=x\]</div>
<p>et donc :</p>
<div class="math notranslate nohighlight">
\[\nabla \phi(y_ig(x_i))=-x_i\frac{y_i}{1+e^{y_i\langle \omega, x_i\rangle}},\]</div>
<p>On retrouve le gradient que nous avons calcul√© lors de la pr√©c√©dente s√©quence o√π, si on note <span class="math notranslate nohighlight">\(\mathcal{Y}=\{0, 1\}\)</span>, on avait :</p>
<div class="math notranslate nohighlight">
\[\nabla L_n(\omega)=\frac{1}{n}X^T(\sigma(X\boldsymbol{\omega})-\boldsymbol{y})\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CrossEntropy</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">CrossEntropy</span><span class="o">.</span><span class="n">_format_ndarray</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pos</span> <span class="o">=</span> <span class="mi">0</span>
        
    <span class="k">def</span> <span class="nf">_format_ndarray</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
        <span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="k">else</span> <span class="n">arr</span>
        <span class="k">return</span> <span class="n">arr</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">arr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">arr</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">)))</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">y_pred</span>
    
    <span class="k">def</span> <span class="nf">_sigmoid</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)))</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">val</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">CrossEntropy</span><span class="o">.</span><span class="n">_format_ndarray</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">CrossEntropy</span><span class="o">.</span><span class="n">_sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
        <span class="n">log_p</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)),</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">log_p</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">_shuffle</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">batch_size</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span> <span class="k">else</span> <span class="n">batch_size</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_pos</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">_pos</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_pos</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pos</span><span class="o">+</span><span class="n">batch_size</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pos</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_shuffle</span><span class="p">()</span>
            
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">CrossEntropy</span><span class="o">.</span><span class="n">_format_ndarray</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        
        <span class="n">beta</span> <span class="o">=</span> <span class="n">CrossEntropy</span><span class="o">.</span><span class="n">_format_ndarray</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
        
        <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">CrossEntropy</span><span class="o">.</span><span class="n">_sigmoid</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>   <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">grad</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GradientDescent</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="n">init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">CrossEntropy</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">nb_iterations</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">init</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">param_trace</span> <span class="o">=</span> <span class="p">[</span><span class="n">beta</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
        <span class="n">loss_trace</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">beta</span><span class="p">)]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_iterations</span><span class="p">):</span>
            <span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
            <span class="n">param_trace</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">beta</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">loss_trace</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">beta</span><span class="p">))</span>
            
        <span class="k">return</span> <span class="n">param_trace</span><span class="p">,</span> <span class="n">loss_trace</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">construct_dataset</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>
<span class="n">gd</span> <span class="o">=</span> <span class="n">GradientDescent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">params</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">gd</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">nb_iterations</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">_</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_fonctions_proxy_38_0.png" src="../_images/2_fonctions_proxy_38_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
<span class="n">omega</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">x_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">)</span>
<span class="n">y_</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">b</span><span class="o">-</span><span class="n">omega</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">x_</span><span class="p">)</span><span class="o">/</span><span class="n">omega</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span> <span class="n">y_</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Our data and our model&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_fonctions_proxy_39_0.png" src="../_images/2_fonctions_proxy_39_0.png" />
</div>
</div>
<p>√âtudions l‚Äô√©volution du temps de calcul et comparons le au minimiseur du risque empirique.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="n">fit_duration_erm</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">fit_duration_phi_risk</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">dataset_sizes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">291</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">dataset_sizes</span><span class="p">:</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">construct_dataset</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">omega</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">fit_empirical_risk</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">fit_duration_erm</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="p">)</span>
    
    <span class="n">gd</span> <span class="o">=</span> <span class="n">GradientDescent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">params</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">gd</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">nb_iterations</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">fit_duration_phi_risk</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dataset_sizes</span><span class="p">,</span> <span class="n">fit_duration_erm</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ERM fit time&#39;</span><span class="p">)</span>

<span class="n">scale</span> <span class="o">=</span> <span class="n">fit_duration_erm</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="n">dataset_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">theoretical_fit_duration</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dataset_sizes</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">scale</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dataset_sizes</span><span class="p">,</span> <span class="n">theoretical_fit_duration</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Estimated ERM fit time&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dataset_sizes</span><span class="p">,</span> <span class="n">fit_duration_phi_risk</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$\phi$-risk fit time&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Fit duration&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Time (s)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Dataset size&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_fonctions_proxy_42_0.png" src="../_images/2_fonctions_proxy_42_0.png" />
</div>
</div>
<p><span style="color:blue"><strong>Question :</strong></span>  <strong>Qu‚Äôen conclure ? Dans quel cas le minimiseur du risque empirique a-t-il toujours du sens ?</strong></p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./3_logistic_regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="1_logistic_regression.html" title="previous page">La R√©gression Logistique ‚òïÔ∏è</a>
    <a class='right-next' id="next-link" href="3_bayes_classifier.html" title="next page">Le classifieur de Bayes ‚òïÔ∏è</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By The LMIPR team<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>
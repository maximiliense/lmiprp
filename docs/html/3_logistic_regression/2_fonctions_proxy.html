
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Les fonctions proxy ☕️☕️☕️ &#8212; Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Le classifieur de Bayes ☕️" href="3_bayes_classifier.html" />
    <link rel="prev" title="La Régression Logistique ☕️" href="1_logistic_regression.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Machine Learning
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../0_requisite/rappels_python.html">
   Rappels de Python et Numpy
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../1_what_is_ml/0_propos_liminaire.html">
   Le
   <em>
    Machine Learning
   </em>
   , c’est quoi ?
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/1_introduction_ml.html">
     <em>
      Machine learning
     </em>
     et malédiction de la dimension
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/2_regression_and_classification_trees.html">
     Les arbres de régression et de classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../2_linear_regression/0_propos_liminaire.html">
   La
   <em>
    régression
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_linear_regression/1_linear_regression.html">
     La régression linéaire ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_linear_regression/2_optimization.html">
     L’optimisation ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_linear_regression/3_interpolation.html">
     Interpolation ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_linear_regression/4_algo_proximal_lasso.html">
     Sous-différentiel et le cas du Lasso ☕️☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="0_propos_liminaire.html">
   La
   <em>
    classification
   </em>
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="1_logistic_regression.html">
     La Régression Logistique ☕️
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Les fonctions proxy ☕️☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3_bayes_classifier.html">
     Le classifieur de Bayes ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="4_VC_theory.html">
     La théorie de Vapnik et Chervonenkis ☕️☕️☕️☕️ (💆‍♂️)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../4_max_margin/0_propos_liminaire.html">
   Les modèles
   <em>
    max-margin
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../4_max_margin/1_max_margin.html">
     Les modèles max-margin ☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5_ensembles/0_propos_liminaire.html">
   Les méthodes
   <em>
    ensemblistes
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/1_ensembles.html">
     Les méthodes ensemblistes ☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../6_autodiff/0_propos_liminaire.html">
   La différentiation automatique
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_autodiff/1_autodiff.html">
     La différentiation automatique et un début de
     <em>
      deep learning
     </em>
     ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_autodiff/2_filters_representation.html">
     Filtres et espace de représentation des réseaux de neurones ☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../7_regularization/0_propos_liminaire.html">
   La régularisation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/1_regularization_deep.html">
     Régularisation en deep learning ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/2_unsupervised_representation_learning.html">
     Unsupervised representation learning ☕️☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/3_multitask.html">
     L’apprentissage multi-tâches ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/4_adversarial.html">
     Les attaques adversaires ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/5_ridge.html">
     Une analyse de la régularisation Ridge ☕️☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../content.html">
   Content in Jupyter Book
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../markdown.html">
     Markdown Files
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../notebooks.html">
     Content with notebooks
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/3_logistic_regression/2_fonctions_proxy.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/maximiliense/lmiprp"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/maximiliense/lmiprp/issues/new?title=Issue%20on%20page%20%2F3_logistic_regression/2_fonctions_proxy.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/maximiliense/lmiprp/master?urlpath=tree/3_logistic_regression/2_fonctions_proxy.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#i-une-premiere-minimisation-du-risque-empirique">
   I. Une première minimisation du risque empirique
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#construction-du-jeu-de-donnees">
     Construction du jeu de données
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#affichage-du-dataset">
     Affichage du dataset
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#notre-classe-de-fonctions-lineaires-mathcal-h">
     Notre classe de fonctions linéaires
     <span class="math notranslate nohighlight">
      \(\mathcal{H}\)
     </span>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ii-construction-du-phi-risk">
   II. Construction du
   <span class="math notranslate nohighlight">
    \(\phi\)
   </span>
   -risk
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iii-proprietes-d-un-phi-risk">
   III. Propriétés d’un
   <span class="math notranslate nohighlight">
    \(\phi\)
   </span>
   -risk
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iv-logistic-loss-et-regression-logistique">
   IV.
   <em>
    Logistic loss
   </em>
   et régression logistique
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="les-fonctions-proxy">
<h1>Les fonctions proxy ☕️☕️☕️<a class="headerlink" href="#les-fonctions-proxy" title="Permalink to this headline">¶</a></h1>
<p>Situons-nous dans le cadre d’un problème de classification supervisé et notons <span class="math notranslate nohighlight">\(\mathcal{X}\subseteq\mathbb{R}^d\)</span> et <span class="math notranslate nohighlight">\(\mathcal{Y}=\{0, 1\}\)</span> ou <span class="math notranslate nohighlight">\(\mathcal{Y}=\{-1,+1\}\)</span> selon le contexte et afin de simplifier les notations. Notons également <span class="math notranslate nohighlight">\(X,Y\in\mathcal{X}\times\mathcal{Y}\)</span> deux variables aléatoires telles que <span class="math notranslate nohighlight">\(\mu\)</span> est la mesure de <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(\eta(x)=\mathbb{P}(Y=1|X=x)\)</span>. La connaissance de <span class="math notranslate nohighlight">\(\mu\)</span> et de <span class="math notranslate nohighlight">\(\eta\)</span> nous donne toutes les informations propres au processus générateur de notre problème. Nous pouvons d’ailleurs construire la mesure jointe. Soit <span class="math notranslate nohighlight">\(A\subset \mathcal{X}\times\mathcal{Y}\)</span>, nous avons :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}(X, Y \in A)=\int_{A\cap \mathcal{X}\times \{1\}}\eta(x)d\mu+\int_{A\cap \mathcal{X}\times \{0\}}(1-\eta(x))d\mu.\]</div>
<p>Notre objectif est de trouver une application <span class="math notranslate nohighlight">\(h:\mathcal{X}\mapsto\mathcal{Y}\)</span> tel qu’un risque est minimisé. <em>A priori</em> ce que nous souhaitons faire est de minimiser le nombre d’erreurs. Pour cela, nous définissons le risque dit <span class="math notranslate nohighlight">\(0/1\)</span> :</p>
<div class="math notranslate nohighlight">
\[L(h)=\mathbb{P}(h(X)\neq Y)=\mathbb{E}\big[1\{X\neq Y\}\big].\]</div>
<p>Nous ne connaissons malheureusement ni <span class="math notranslate nohighlight">\(\mu\)</span> ni <span class="math notranslate nohighlight">\(\eta\)</span> et pour cela, nous devons estimer le risque <span class="math notranslate nohighlight">\(L\)</span> sur un jeu de données :</p>
<div class="math notranslate nohighlight">
\[S_n=\{(X_i, Y_i)\}_{i\leq n}\sim \mathbb{P}^n,\]</div>
<p>qu’on supposera représentatif du problème dans le sens où les couples <span class="math notranslate nohighlight">\((X, Y)\)</span> sont iid. À partir de là, nous pouvons estimer notre risque empirique :</p>
<div class="math notranslate nohighlight">
\[L_n(h)=\frac{1}{n}\sum_i 1\{h(X_i)\neq Y_i\}.\]</div>
<p>Il s’agit tout simplement de la moyenne des erreurs.</p>
<p>Notre objectif, en tant que <em>machine learner</em> est de minimiser cette erreur. Bien sûr (#nofreelunchtheorem), nous ne pouvons pas considérer toutes les fonctions de <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> dans <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>. Soit <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> la classe de fonctions que nous souhaitons considérer. Le problème à résoudre est :</p>
<div class="math notranslate nohighlight">
\[h_n=\text{argmin}_{h\in\mathcal{H}}L_n(h).\]</div>
<p>C’est ce qu’on appelle le minimiseur du risque empirique. Remarquez que ce n’est pas exactement ce qu’on minimise en pratique. Nous allons voir pourquoi.</p>
<div class="section" id="i-une-premiere-minimisation-du-risque-empirique">
<h2>I. Une première minimisation du risque empirique<a class="headerlink" href="#i-une-premiere-minimisation-du-risque-empirique" title="Permalink to this headline">¶</a></h2>
<div class="section" id="construction-du-jeu-de-donnees">
<h3>Construction du jeu de données<a class="headerlink" href="#construction-du-jeu-de-donnees" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">h_star</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">y</span><span class="p">[</span><span class="n">x</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">&gt;</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">y</span><span class="p">[</span><span class="n">x</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">&lt;=</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="k">return</span> <span class="n">y</span>

<span class="k">def</span> <span class="nf">construct_dataset</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">h_star</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">construct_dataset</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="affichage-du-dataset">
<h3>Affichage du dataset<a class="headerlink" href="#affichage-du-dataset" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_fonctions_proxy_8_0.png" src="../_images/2_fonctions_proxy_8_0.png" />
</div>
</div>
</div>
<div class="section" id="notre-classe-de-fonctions-lineaires-mathcal-h">
<h3>Notre classe de fonctions linéaires <span class="math notranslate nohighlight">\(\mathcal{H}\)</span><a class="headerlink" href="#notre-classe-de-fonctions-lineaires-mathcal-h" title="Permalink to this headline">¶</a></h3>
<p>Considérons le cas très simple des modèles linéaires. Ici, notre ensemble de fonctions <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> est l’ensemble de cardinal infini dont la frontière de décision sont les droites dans le plan :</p>
<div class="math notranslate nohighlight">
\[\mathcal{H}=\{x\mapsto\text{sign}(\langle \omega, x\rangle+b):\ \omega\in\mathbb{R}^2,\ b\in\mathbb{R}\}.\]</div>
<p>Ici, nous supposerons que <span class="math notranslate nohighlight">\(\mathcal{Y}=\{-1, 1\}\)</span>. Notre problème de minimisation devient donc :</p>
<div class="math notranslate nohighlight">
\[(\omega, b)=\text{argmin}_{\omega, b\in \mathbb{R}^2\times\mathbb{R}}\frac{1}{n}\sum_i 1\{\text{sign}(\langle \omega, x_i\rangle+b)=y_i\}.\]</div>
<p>Comment faire cela ? La fonction à optimiser est constante par morceau et les optimiseurs à base d’information du premier ordre (i.e. dérivée, gradient) ne peuvent pas nous aider. Une stratégie est de constater que dans le plan, une droite n’a besoin que de deux points pour se positionner. À partir de deux points <span class="math notranslate nohighlight">\(x_1\)</span> et <span class="math notranslate nohighlight">\(x_2\)</span>, il est possible d’obtenir une valeur de <span class="math notranslate nohighlight">\(\omega\)</span> et de <span class="math notranslate nohighlight">\(b\)</span> en résolvant le système d’équations suivant :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{cases}\langle\omega, x_1\rangle + b=0\\\langle\omega, x_2\rangle +b=0\end{cases}.\end{split}\]</div>
<p>Il y a évidemment une infinité de solutions (deux équations et trois inconnues). On peut résoudre ce problème en contraignant la norme de <span class="math notranslate nohighlight">\(\omega\)</span> à valoir <span class="math notranslate nohighlight">\(1\)</span>. Il reste deux solutions au système précédent. Il faut considérer le cas où la classe <span class="math notranslate nohighlight">\(1\)</span> est d’un côté de la droite et le cas où la classe <span class="math notranslate nohighlight">\(1\)</span> est de l’autre côté.</p>
<p>Il suffit maintenant de tester toutes les valeurs possibles de <span class="math notranslate nohighlight">\(\omega\)</span> et de <span class="math notranslate nohighlight">\(b\)</span> s’appuyant sur des points de notre jeu de données et de prendre celle qui fait le moins d’erreurs !</p>
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Implémentez notre modèle de classification <span class="math notranslate nohighlight">\(h\)</span> (i.e. la fonction qui fait une prédiction quant à la classe).</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">h</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">omega</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="c1">####### Complete this part ######## or die ####################</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">omega</span><span class="p">)</span><span class="o">+</span><span class="n">b</span>
    <span class="n">predictions</span><span class="p">[</span><span class="n">predictions</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">predictions</span><span class="p">[</span><span class="n">predictions</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="c1">###############################################################</span>
    <span class="k">return</span> <span class="n">predictions</span>
</pre></div>
</div>
</div>
</div>
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Implémentez une methode qui prend en paramètre deux points ainsi que le sens (le côté de l’hyperplan) via un boolean et retourne un vecteur <span class="math notranslate nohighlight">\(\omega\)</span> qui décrit l’orientation de l’hyperplan et le biais <span class="math notranslate nohighlight">\(b\)</span>.</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parameterize</span><span class="p">(</span><span class="n">x_1</span><span class="p">,</span> <span class="n">x_2</span><span class="p">,</span> <span class="n">omega_positive</span><span class="p">):</span>
    <span class="c1">####### Complete this part ######## or die ####################</span>
    <span class="n">_a</span> <span class="o">=</span> <span class="n">x_2</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">x_1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">_b</span> <span class="o">=</span> <span class="n">x_1</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">x_2</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">omega</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">_a</span><span class="o">/</span><span class="n">_b</span><span class="p">])</span>
    <span class="n">omega</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">omega</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">omega_positive</span><span class="p">:</span>
        <span class="n">omega</span> <span class="o">*=-</span><span class="mi">1</span>
    <span class="n">b</span> <span class="o">=</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">omega</span><span class="p">,</span> <span class="n">x_1</span><span class="p">)</span>
    <span class="c1">###############################################################</span>
    <span class="k">return</span> <span class="n">omega</span><span class="p">,</span> <span class="n">b</span>
</pre></div>
</div>
</div>
</div>
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Implémentez une methode qui calcule le risque empirique.</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">empirical_risk</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">omega</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="c1">####### Complete this part ######## or die ####################</span>
    <span class="n">risk</span> <span class="o">=</span> <span class="p">(</span><span class="n">h</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">omega</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">!=</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1">###############################################################</span>
    <span class="k">return</span> <span class="n">risk</span>
</pre></div>
</div>
</div>
</div>
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Implémentez une methode qui calcule le minimiseur du risque empirique.</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fit_empirical_risk</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1">####### Complete this part ######## or die ####################</span>
    <span class="n">best_omega</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">best_b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">best_risk</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">j</span><span class="p">:</span>
                <span class="n">omega</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">parameterize</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">j</span><span class="o">&gt;</span><span class="n">i</span><span class="p">)</span>
                <span class="n">risk</span> <span class="o">=</span> <span class="n">empirical_risk</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">omega</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">best_omega</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">risk</span> <span class="o">&lt;</span> <span class="n">best_risk</span><span class="p">:</span>
                    <span class="n">best_risk</span> <span class="o">=</span> <span class="n">risk</span>
                    <span class="n">best_omega</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">omega</span><span class="p">)</span>
                    <span class="n">best_b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="c1">###############################################################</span>
    <span class="k">return</span> <span class="n">best_omega</span><span class="p">,</span> <span class="n">best_b</span><span class="p">,</span> <span class="n">best_risk</span>
</pre></div>
</div>
</div>
</div>
<p>Testons le code précédent.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">omega</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">fit_empirical_risk</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Obtained empirical risk:&#39;</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Obtained empirical risk: 0.0
</pre></div>
</div>
</div>
</div>
<p>Affichons maintenant le séparateur ainsi calculé.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="n">x_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">)</span>
<span class="n">y_</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">b</span><span class="o">-</span><span class="n">omega</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">x_</span><span class="p">)</span><span class="o">/</span><span class="n">omega</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span> <span class="n">y_</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Our data and our model&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_fonctions_proxy_22_0.png" src="../_images/2_fonctions_proxy_22_0.png" />
</div>
</div>
<p>Malheureusement, tester toutes les combinaisons a un coût qui nous empêche de considérer des jeux de données trop grands et en trop grande dimension. Il y a en effet dans <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> <span class="math notranslate nohighlight">\(n!/(d!(n-d)!)\)</span> combinaisons possibles à tester. Dans le plan, cela nous donne :</p>
<div class="math notranslate nohighlight">
\[\frac{n!}{2(n-2)!}=0.5n(n-1)\approx 0.5 n^2\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="n">fit_duration</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">dataset_sizes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">291</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">dataset_sizes</span><span class="p">:</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">construct_dataset</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">omega</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">fit_empirical_risk</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">fit_duration</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dataset_sizes</span><span class="p">,</span> <span class="n">fit_duration</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Fit time&#39;</span><span class="p">)</span>

<span class="n">scale</span> <span class="o">=</span> <span class="n">fit_duration</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="n">dataset_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">theoretical_fit_duration</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dataset_sizes</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">scale</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dataset_sizes</span><span class="p">,</span> <span class="n">theoretical_fit_duration</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Estimated fit time&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Fit duration&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Time (s)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Dataset size&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_fonctions_proxy_25_0.png" src="../_images/2_fonctions_proxy_25_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Avec un jeu de données de taille 150000 dans le plan (petite dim),&#39;</span>\
      <span class="s1">&#39;le temps du fit estimé est d</span><span class="se">\&#39;</span><span class="s1">environ&#39;</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">scale</span> <span class="o">*</span> <span class="mi">150000</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="mi">60</span><span class="o">/</span><span class="mi">24</span><span class="p">)),</span> <span class="s1">&#39;jours&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Avec un jeu de données de taille 150000 dans le plan (petite dim),le temps du fit estimé est d&#39;environ 431 jours
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="ii-construction-du-phi-risk">
<h2>II. Construction du <span class="math notranslate nohighlight">\(\phi\)</span>-risk<a class="headerlink" href="#ii-construction-du-phi-risk" title="Permalink to this headline">¶</a></h2>
<p>À la place de chercher à optimiser notre erreur 0/1, l’idée va être de trouver une fonction avec des propriétés mathématiques intéressantes et pouvant nous offrir certaines garanties du point de vue de la classification.</p>
<p>Considérons ici les labels <span class="math notranslate nohighlight">\(\mathcal{Y}=\{-1, 1\}\)</span>. Nous allons également supposer que nos modèle sont construits comme la composition d’une fonction de score <span class="math notranslate nohighlight">\(g:\mathcal{X}\mapsto\mathbb{R}\)</span> et de la fonction <span class="math notranslate nohighlight">\(\texttt{sign}\)</span> qui donne le label <span class="math notranslate nohighlight">\(+1\)</span> si le score est positif et <span class="math notranslate nohighlight">\(-1\)</span> si le score est négatif ! L’objectif est donc de trouver une fonction de score dont le signe coïncide avec le signe de notre label.</p>
<p>Notre risque secondaire ou <span class="math notranslate nohighlight">\(\phi\)</span>-risk se construit de la manière suivante :</p>
<div class="math notranslate nohighlight">
\[L(g)=\mathbb{E}\big[\phi(Yg(X))\big].\]</div>
<p>On remarque que si notre modèle prédit le bon label, alors <span class="math notranslate nohighlight">\(g(X)\)</span> a le même signe que <span class="math notranslate nohighlight">\(Y\)</span> et <span class="math notranslate nohighlight">\(Yg(X)&gt;0\)</span>. À l’inverse, si le label prédit est faux, alors, on a <span class="math notranslate nohighlight">\(Yg(X)&lt;0\)</span>. La fonction <span class="math notranslate nohighlight">\(\phi\)</span> doit majorer le risque <span class="math notranslate nohighlight">\(0/1\)</span> d’une manière que nous allons détailler ci-dessous, être convexe et dérivable en <span class="math notranslate nohighlight">\(0\)</span> avec une dérivée négative pour des raisons que nous allons détailler plus bas.</p>
<p>Le risque empirique associé devient alors :</p>
<div class="math notranslate nohighlight">
\[L_n(g)=\frac{1}{n}\sum_i \phi(Y_ig(X_i)).\]</div>
<p>Observons déjà que si on choisit la fonction <span class="math notranslate nohighlight">\(\phi\)</span> suivante :
$<span class="math notranslate nohighlight">\(\phi(z)=1\{z&lt;0\},\)</span><span class="math notranslate nohighlight">\(
on retombe sur notre erreur \)</span>0/1<span class="math notranslate nohighlight">\(. En effet, on compte \)</span>1<span class="math notranslate nohighlight">\( si le signe de \)</span>Yg(X)<span class="math notranslate nohighlight">\( est négatif, à savoir, si notre score ne correspond pas au label. D'autres choix de \)</span>\phi$-risk sont les suivants (il y en a beaucoup d’autres) :</p>
<ul class="simple">
<li><p><strong>Hinge loss</strong> : <span class="math notranslate nohighlight">\(\phi(z)=\text{max}(0, 1-z)\)</span>, ici  on pénalise tous les <span class="math notranslate nohighlight">\(z\)</span> tant qu’ils sont inférieur à <span class="math notranslate nohighlight">\(1\)</span>. Cette loss est notamment utilisé pour le SVM qui cherche à obtenir une marge,</p></li>
<li><p><strong>Smoothed hinge loss</strong> : <span class="math notranslate nohighlight">\(\phi(z)=\beta^{-1}\text{log}\big(1+e^{\beta(1-z)}\big)\)</span>, où <span class="math notranslate nohighlight">\(\beta&gt;0\)</span> est un paramètre. On retrouve la hinge loss comme limite lorsque <span class="math notranslate nohighlight">\(\beta\rightarrow\infty\)</span>,</p></li>
<li><p><strong>Logistic loss</strong> : <span class="math notranslate nohighlight">\(\phi(z)=\text{log}(1+e^{-z})\)</span>, c’est la loss utilisée lorsqu’on fait une régression logistique ou de la classification binaire en <em>deep learning</em>. En effet, on a :</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{log}(1+e^{-z})=-\text{log}\Big(\frac{1}{1+e^{-z}}\Big)=-\text{log}(\sigma(z)),\]</div>
<p>où <span class="math notranslate nohighlight">\(\sigma(z)=(1+e^{-z})^{-1}\)</span> est la fonction sigmoïd. L’idée est donc de minimiser moins le logarithme appliqué à la sigmoïd elle-même appliquée au logit multiplié par le label de la bonne classe. Dit autrement, si <span class="math notranslate nohighlight">\(y=1\)</span>, alors, on minimise  <span class="math notranslate nohighlight">\(-\text{log}(\sigma(g(x)))\)</span> et si <span class="math notranslate nohighlight">\(y=-1\)</span> et on a <span class="math notranslate nohighlight">\(-\text{log}(1-\sigma(z))\)</span>. En notant cette fois-ci <span class="math notranslate nohighlight">\(\mathcal{Y}=\{0, 1\}\)</span>, on retrouve :</p>
<div class="math notranslate nohighlight">
\[-y\text{log}\big(\sigma(g(x))\big)-(1-y)\text{log}\big(1-\sigma(g(x))\big).\]</div>
<p>C’est l’entropie croisée appliquée à une sigmoïde elle-même appliquée au logit ! Si <span class="math notranslate nohighlight">\(g\)</span> est un modèle linéaire, alors on retombe sur la régression logistique.</p>
<p>Visualisons ces quelques <span class="math notranslate nohighlight">\(\phi\)</span>-risk.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">zero_one_loss</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">hinge_loss</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">v</span> <span class="o">=</span> <span class="mi">1</span><span class="o">-</span><span class="n">x</span>
    <span class="k">return</span> <span class="n">v</span> <span class="o">*</span> <span class="p">(</span><span class="n">v</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">v</span><span class="o">&lt;=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">logistic_loss</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">soft_hinge_loos</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">logistic_loss</span><span class="p">(</span><span class="n">beta</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">/</span><span class="n">beta</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">201</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">zero_one_loss</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;0/1 loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">hinge_loss</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Hinge loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">soft_hinge_loos</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Soft Hinge loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">logistic_loss</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Logistic loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\phi$-risks&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_fonctions_proxy_30_0.png" src="../_images/2_fonctions_proxy_30_0.png" />
</div>
</div>
<p><span style="color:blue"><strong>Question :</strong></span>  <strong>Intuitivement, supposons que l’on minimise l’un des <span class="math notranslate nohighlight">\(\phi\)</span>-risk sur notre problème de <em>machine learning</em> et qu’on obtienne <span class="math notranslate nohighlight">\(0\)</span> erreur à la fin. Que pouvons nous dire quant au risque <span class="math notranslate nohighlight">\(0/1\)</span> ?</strong></p>
</div>
<div class="section" id="iii-proprietes-d-un-phi-risk">
<h2>III. Propriétés d’un <span class="math notranslate nohighlight">\(\phi\)</span>-risk<a class="headerlink" href="#iii-proprietes-d-un-phi-risk" title="Permalink to this headline">¶</a></h2>
<p>Comme dit plus haut, la fonction <span class="math notranslate nohighlight">\(\phi\)</span> doit notamment majorer le risque <span class="math notranslate nohighlight">\(0/1\)</span>. En réalité, cette majoration est à un rescaling près (on peut multiplier par un scalaire positif). Ainsi la <em>logistic loss</em> ne majore pas le <span class="math notranslate nohighlight">\(\phi\)</span>-risk, sauf si on la multiplie par <span class="math notranslate nohighlight">\(2\)</span> (ou autre). Cette multiplication par <span class="math notranslate nohighlight">\(2\)</span> n’apporte rien du point de vue de la minimisation. Cependant, si sur notre problème la <em>logistic loss</em> atteint <span class="math notranslate nohighlight">\(0\)</span> erreur (ou s’en rapproche asymptotiquement comme nous l’avons vu dans la séquence précédente), alors deux fois la <em>logistic loss</em> atteint <span class="math notranslate nohighlight">\(0\)</span> erreur et puisqu’elle majore l’erreur <span class="math notranslate nohighlight">\(0/1\)</span>, on peut en conclure que celui-ci fait également <span class="math notranslate nohighlight">\(0\)</span> erreur !</p>
<p>Le <span class="math notranslate nohighlight">\(\phi\)</span>-risk doit être convexe. La convexité garantit un certain nombre de propriétés dont la plus intéressante, peut-être, est que si nous trouvons un minimiseur local de notre loss, alors celui-ci est également un minimiseur global.</p>
<p>La dernière propriété que nous avons évoquée est que notre <span class="math notranslate nohighlight">\(\phi\)</span>-risk doit être dérivable en <span class="math notranslate nohighlight">\(0\)</span> avec une dérivée négative. Cette propriété garantit que notre <span class="math notranslate nohighlight">\(\phi\)</span>-risk est “classification calibrated”. Nous allons détailler cette idée, un petit peu plus complexe.</p>
<p>Rappellons la notation <span class="math notranslate nohighlight">\(\eta(x)=\mathbb{P}(Y=1|X=x)\)</span>. Il s’agit de la VRAIE probabilité conditionnelle d’obtenir le label <span class="math notranslate nohighlight">\(1\)</span> sachant qu’on a observé la donnée <span class="math notranslate nohighlight">\(x\)</span>. Celle-ci n’est bien sûr pas connue. Nous avons :</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\Big[\phi(Yg(x))|X=x\Big]=\eta(x)\phi(g(x))+(1-\eta(x))\phi(-g(x))=C_\eta(g(x)).\]</div>
<p>C’est tout simplement l’espérance de notre <span class="math notranslate nohighlight">\(\phi\)</span>-risk lorsqu’on a observé la donnée <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>Rappelons-nous que la fonction <span class="math notranslate nohighlight">\(g\)</span> retourne un score positif si on prédit le label <span class="math notranslate nohighlight">\(y=1\)</span> et un score négatif si on prédit le label <span class="math notranslate nohighlight">\(y=-1\)</span>. Ainsi, si <span class="math notranslate nohighlight">\(\eta(x)&gt;0.5\)</span>, alors, on veut prédire le label <span class="math notranslate nohighlight">\(1\)</span> et on veut que <span class="math notranslate nohighlight">\(g(x)&gt;0\)</span>. À l’inverse, si <span class="math notranslate nohighlight">\(\eta(x)&lt;0.5\)</span>, alors on veut prédire le label <span class="math notranslate nohighlight">\(-1\)</span> et on veut <span class="math notranslate nohighlight">\(g(x)&lt;0\)</span>. Dit autrement, on souhaite avoir les deux inégalités suivantes :</p>
<div class="math notranslate nohighlight">
\[\eta&gt;0.5\Leftrightarrow\text{argmin}_{z}C_\eta(z)\subset\mathbb{R}^{\star+},\]</div>
<div class="math notranslate nohighlight">
\[\eta&lt;0.5\Leftrightarrow\text{argmin}_{z}C_\eta(z)\subset\mathbb{R}^{\star-}.\]</div>
<p>Dit autrement, on veut que les minimiseurs de notre loss (du point de vu du score), ait bien le signe attendu selon la valeur <span class="math notranslate nohighlight">\(\eta\)</span>. C’est cela qu’on appelle “classification calibrated”.</p>
<p>Il se trouve que cette propriété est totalement équivalente au fait que <span class="math notranslate nohighlight">\(\phi\)</span> soit dérivable en <span class="math notranslate nohighlight">\(0\)</span> et de dérivée négative.</p>
<p><strong>Proposition</strong> <span class="math notranslate nohighlight">\(\phi\)</span> est dérivable en <span class="math notranslate nohighlight">\(0\)</span> et <span class="math notranslate nohighlight">\(\phi^\prime(0)&lt;0\)</span> si et seulement si les deux inégalités précédentes sont satisfaites.</p>
<p><strong>Preuve :</strong> Notons <span class="math notranslate nohighlight">\(\phi_+\)</span>, <span class="math notranslate nohighlight">\(\phi_-\)</span>, <span class="math notranslate nohighlight">\((C_\eta)_+\)</span> et <span class="math notranslate nohighlight">\((C_\eta)_-\)</span> les dérivées à gauche et à droite. Nous avons alors, pour que les inégalités soient satisfaites, <span class="math notranslate nohighlight">\((C_\eta)_+^\prime(0)&lt;0\Leftrightarrow \eta&gt;0.5\)</span> et <span class="math notranslate nohighlight">\((C_\eta)_-^\prime(0)&gt;0\Leftrightarrow \eta&lt;0.5\)</span> puisque <span class="math notranslate nohighlight">\(C_\eta\)</span> est convexe en tant que combinaison convexe de fonctions convexes. Pour ce convaincre des équivalences précédentes, supposons <span class="math notranslate nohighlight">\(\eta&gt;0.5\)</span>, alors le minimum de <span class="math notranslate nohighlight">\(C_\eta(z)\)</span> doit être atteint avec <span class="math notranslate nohighlight">\(z&gt;0\)</span>. Si <span class="math notranslate nohighlight">\((C_\eta)_+^\prime(0)&lt;0\)</span>, alors le minimum est “à droite” de 0 et répond au besoin.</p>
<p><span class="math notranslate nohighlight">\((\Leftarrow)\)</span> Supposons que les deux inégalités soient satisfaites.</p>
<p>Nous voulons montrer (1) que <span class="math notranslate nohighlight">\(\phi\)</span> est dérivable en <span class="math notranslate nohighlight">\(0\)</span> et (2) que <span class="math notranslate nohighlight">\(\phi^\prime(0)&lt;0\)</span>.</p>
<p>On a (attention à la dérivée) :</p>
<div class="math notranslate nohighlight">
\[\lim_{\eta\rightarrow 0.5^+}(C_\eta)_+^\prime(0)=\lim_{\eta\rightarrow 0.5^+}\eta\phi_+^\prime(0)-(1-\eta)\phi_-^\prime(0)=0.5\big(\phi_+^\prime(0)-\phi_-^\prime(0)\big)\leq 0.\]</div>
<p>La dernière inégalité est vérifiée par hypothèse sur <span class="math notranslate nohighlight">\(C_\eta\)</span>. Cela nous donne donc <span class="math notranslate nohighlight">\(\phi_-^\prime(0)\geq\phi_+^\prime(0)\)</span>. Par hypothèse, <span class="math notranslate nohighlight">\(\phi\)</span> est convexe et on a toujours <span class="math notranslate nohighlight">\(\phi_-^\prime(0)\leq\phi_+^\prime(0)\)</span>. On a donc <span class="math notranslate nohighlight">\(\phi_+^\prime(0)=\phi_-^\prime(0)\)</span> et <span class="math notranslate nohighlight">\(\phi\)</span> est dérivable en <span class="math notranslate nohighlight">\(0\)</span>. C’est le premier point.</p>
<p>Nous avons ensuite :</p>
<div class="math notranslate nohighlight">
\[C_\eta^\prime(0)=\eta\phi^\prime(0)-(1-\eta)\phi^\prime(0)=(2\eta-1)\phi^\prime(0).\]</div>
<p>Si <span class="math notranslate nohighlight">\(\eta&gt;0.5\)</span>, alors <span class="math notranslate nohighlight">\(2\eta-1&gt;0\)</span> et <span class="math notranslate nohighlight">\(C_\eta^\prime(0)&lt;0\)</span>. On a alors <span class="math notranslate nohighlight">\(\phi^\prime(0)&lt;0\)</span>. On obtient le même résultat si <span class="math notranslate nohighlight">\(\eta&lt;0.5\)</span> !</p>
<p><span class="math notranslate nohighlight">\((\Rightarrow)\)</span> Supposons que <span class="math notranslate nohighlight">\(\phi\)</span> est dérivable en <span class="math notranslate nohighlight">\(0\)</span> et que <span class="math notranslate nohighlight">\(\phi^\prime(0)&lt;0\)</span>. On obtient alors :</p>
<div class="math notranslate nohighlight">
\[C_\eta^\prime(0)=(2\eta-1)\phi^\prime(0)\]</div>
<p>dont le signe est négatif si <span class="math notranslate nohighlight">\(\eta&gt;0.5\)</span> et positif si <span class="math notranslate nohighlight">\(\eta&lt;0.5\)</span>. <strong><span class="math notranslate nohighlight">\(\boxed{}\)</span></strong></p>
</div>
<hr class="docutils" />
<div class="section" id="iv-logistic-loss-et-regression-logistique">
<h2>IV. <em>Logistic loss</em> et régression logistique<a class="headerlink" href="#iv-logistic-loss-et-regression-logistique" title="Permalink to this headline">¶</a></h2>
<p>La <em>logistic loss</em> est <span class="math notranslate nohighlight">\(\phi(z)=\text{log}(1+e^{-z})\)</span>. Considérons un jeu de données <span class="math notranslate nohighlight">\(S_n\)</span>, notre objectif pratique est de minimiser le <span class="math notranslate nohighlight">\(\phi\)</span>-risk empirique :</p>
<div class="math notranslate nohighlight">
\[L_n^\phi(g)=\frac{1}{n}\sum_i\phi(y_ig(x_i))=\frac{1}{n}\sum_i\text{log}(1+e^{-y_ig(x_i)})\]</div>
<p>Reprenons le cas d’un modèle linéaire. Cela nous permettra de comparer les coûts calculatoires avec le vrai risque empirique.</p>
<p>Nous avons donc <span class="math notranslate nohighlight">\(\mathcal{H}=\{x\mapsto \langle \omega, x\rangle +b:\ \omega\in\mathbb{R}^d,\ b\in\mathbb{R}\}\)</span>.</p>
<p>Notre objectif ici, est de calculer le gradient de notre <span class="math notranslate nohighlight">\(\phi\)</span>-risk empirique. La première étape est de calculer la dérivée de la fonction <span class="math notranslate nohighlight">\(\phi(yz)\)</span> :</p>
<div class="math notranslate nohighlight">
\[\phi^\prime(yz)=-\frac{ye^{-yz}}{1+e^{-yz}}=-\frac{y}{1+e^{yz}}.\]</div>
<p>Par simplicité de notation considérons les vecteur <span class="math notranslate nohighlight">\(x_i=[1, x_1, \ldots, x_d]^T\)</span> afin d’éviter la notation avec le biais <span class="math notranslate nohighlight">\(b\)</span>.</p>
<p>Nous avons donc naturellement que :</p>
<div class="math notranslate nohighlight">
\[\nabla g(x)=x\]</div>
<p>et donc :</p>
<div class="math notranslate nohighlight">
\[\nabla \phi(y_ig(x_i))=-x_i\frac{y_i}{1+e^{y_i\langle \omega, x_i\rangle}},\]</div>
<p>On retrouve le gradient que nous avons calculé lors de la précédente séquence où, si on note <span class="math notranslate nohighlight">\(\mathcal{Y}=\{0, 1\}\)</span>, on avait :</p>
<div class="math notranslate nohighlight">
\[\nabla L_n(\omega)=\frac{1}{n}X^T(\sigma(X\boldsymbol{\omega})-\boldsymbol{y})\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CrossEntropy</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">CrossEntropy</span><span class="o">.</span><span class="n">_format_ndarray</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pos</span> <span class="o">=</span> <span class="mi">0</span>
        
    <span class="k">def</span> <span class="nf">_format_ndarray</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
        <span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="k">else</span> <span class="n">arr</span>
        <span class="k">return</span> <span class="n">arr</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">arr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">arr</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">)))</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">y_pred</span>
    
    <span class="k">def</span> <span class="nf">_sigmoid</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)))</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">val</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">CrossEntropy</span><span class="o">.</span><span class="n">_format_ndarray</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">CrossEntropy</span><span class="o">.</span><span class="n">_sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
        <span class="n">log_p</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)),</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">log_p</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">_shuffle</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">batch_size</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span> <span class="k">else</span> <span class="n">batch_size</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_pos</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">_pos</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_pos</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pos</span><span class="o">+</span><span class="n">batch_size</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pos</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_shuffle</span><span class="p">()</span>
            
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">CrossEntropy</span><span class="o">.</span><span class="n">_format_ndarray</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        
        <span class="n">beta</span> <span class="o">=</span> <span class="n">CrossEntropy</span><span class="o">.</span><span class="n">_format_ndarray</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
        
        <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">CrossEntropy</span><span class="o">.</span><span class="n">_sigmoid</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>   <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">grad</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GradientDescent</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="n">init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">CrossEntropy</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">nb_iterations</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">init</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">param_trace</span> <span class="o">=</span> <span class="p">[</span><span class="n">beta</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
        <span class="n">loss_trace</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">beta</span><span class="p">)]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_iterations</span><span class="p">):</span>
            <span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
            <span class="n">param_trace</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">beta</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">loss_trace</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">beta</span><span class="p">))</span>
            
        <span class="k">return</span> <span class="n">param_trace</span><span class="p">,</span> <span class="n">loss_trace</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">construct_dataset</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>
<span class="n">gd</span> <span class="o">=</span> <span class="n">GradientDescent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">params</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">gd</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">nb_iterations</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">_</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_fonctions_proxy_38_0.png" src="../_images/2_fonctions_proxy_38_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
<span class="n">omega</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">x_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">)</span>
<span class="n">y_</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">b</span><span class="o">-</span><span class="n">omega</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">x_</span><span class="p">)</span><span class="o">/</span><span class="n">omega</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span> <span class="n">y_</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Our data and our model&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_fonctions_proxy_39_0.png" src="../_images/2_fonctions_proxy_39_0.png" />
</div>
</div>
<p>Étudions l’évolution du temps de calcul et comparons le au minimiseur du risque empirique.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="n">fit_duration_erm</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">fit_duration_phi_risk</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">dataset_sizes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">291</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">dataset_sizes</span><span class="p">:</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">construct_dataset</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">omega</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">fit_empirical_risk</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">fit_duration_erm</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="p">)</span>
    
    <span class="n">gd</span> <span class="o">=</span> <span class="n">GradientDescent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">params</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">gd</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">nb_iterations</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">fit_duration_phi_risk</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dataset_sizes</span><span class="p">,</span> <span class="n">fit_duration_erm</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ERM fit time&#39;</span><span class="p">)</span>

<span class="n">scale</span> <span class="o">=</span> <span class="n">fit_duration_erm</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="n">dataset_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">theoretical_fit_duration</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dataset_sizes</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">scale</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dataset_sizes</span><span class="p">,</span> <span class="n">theoretical_fit_duration</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Estimated ERM fit time&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dataset_sizes</span><span class="p">,</span> <span class="n">fit_duration_phi_risk</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$\phi$-risk fit time&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Fit duration&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Time (s)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Dataset size&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_fonctions_proxy_42_0.png" src="../_images/2_fonctions_proxy_42_0.png" />
</div>
</div>
<p><span style="color:blue"><strong>Question :</strong></span>  <strong>Qu’en conclure ? Dans quel cas le minimiseur du risque empirique a-t-il toujours du sens ?</strong></p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./3_logistic_regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="1_logistic_regression.html" title="previous page">La Régression Logistique ☕️</a>
    <a class='right-next' id="next-link" href="3_bayes_classifier.html" title="next page">Le classifieur de Bayes ☕️</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By The LMIPR team<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>
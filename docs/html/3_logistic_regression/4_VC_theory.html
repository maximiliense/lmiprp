
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>La théorie de Vapnik et Chervonenkis ☕️☕️☕️☕️ (💆‍♂️) &#8212; Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Les modèles max-margin" href="../4_max_margin/0_propos_liminaire.html" />
    <link rel="prev" title="Le classifieur de Bayes ☕️" href="3_bayes_classifier.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Machine Learning
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../0_requisite/rappels_python.html">
   Rappels de Python et Numpy
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../1_what_is_ml/0_propos_liminaire.html">
   Le
   <em>
    Machine Learning
   </em>
   , c’est quoi ?
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/1_introduction_ml.html">
     <em>
      Machine learning
     </em>
     et malédiction de la dimension
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/2_regression_and_classification_trees.html">
     Les arbres de régression et de classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../2_linear_regression/0_propos_liminaire.html">
   La
   <em>
    régression
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_linear_regression/1_linear_regression.html">
     La régression linéaire ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_linear_regression/2_optimization.html">
     L’optimisation ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_linear_regression/3_interpolation.html">
     Interpolation ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_linear_regression/4_algo_proximal_lasso.html">
     Sous-différentiel et le cas du Lasso ☕️☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="0_propos_liminaire.html">
   La
   <em>
    classification
   </em>
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="1_logistic_regression.html">
     La Régression Logistique ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="2_fonctions_proxy.html">
     Les fonctions proxy ☕️☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3_bayes_classifier.html">
     Le classifieur de Bayes ☕️
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     La théorie de Vapnik et Chervonenkis ☕️☕️☕️☕️ (💆‍♂️)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../4_max_margin/0_propos_liminaire.html">
   Les modèles
   <em>
    max-margin
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../4_max_margin/1_max_margin.html">
     Les modèles max-margin ☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5_ensembles/0_propos_liminaire.html">
   Les méthodes
   <em>
    ensemblistes
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/1_ensembles.html">
     Les méthodes ensemblistes ☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../6_autodiff/0_propos_liminaire.html">
   La différentiation automatique
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_autodiff/1_autodiff.html">
     La différentiation automatique et un début de
     <em>
      deep learning
     </em>
     ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_autodiff/2_filters_representation.html">
     Filtres et espace de représentation des réseaux de neurones ☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../7_regularization/0_propos_liminaire.html">
   La régularisation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/1_regularization_deep.html">
     Régularisation en deep learning ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/2_unsupervised_representation_learning.html">
     Unsupervised representation learning ☕️☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/3_multitask.html">
     L’apprentissage multi-tâches ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/4_adversarial.html">
     Les attaques adversaires ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/5_ridge.html">
     Une analyse de la régularisation Ridge ☕️☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../content.html">
   Content in Jupyter Book
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../markdown.html">
     Markdown Files
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../notebooks.html">
     Content with notebooks
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/3_logistic_regression/4_VC_theory.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/maximiliense/lmiprp"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/maximiliense/lmiprp/issues/new?title=Issue%20on%20page%20%2F3_logistic_regression/4_VC_theory.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/maximiliense/lmiprp/master?urlpath=tree/3_logistic_regression/4_VC_theory.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#i-aparte-sur-les-inegalites-de-concentration">
   I. Aparté sur les inégalités de concentration
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ii-en-supposant-mathcal-h-infty">
   II. En supposant
   <span class="math notranslate nohighlight">
    \(|\mathcal{H}|&lt;\infty\)
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iii-le-cas-general-mathcal-h-infty">
   III. Le cas général :
   <span class="math notranslate nohighlight">
    \(|\mathcal{H}|=\infty\)
   </span>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#preuve">
     PREUVE
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iv-la-dimension-vc">
   IV. La dimension VC
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="la-theorie-de-vapnik-et-chervonenkis">
<h1>La théorie de Vapnik et Chervonenkis ☕️☕️☕️☕️ (💆‍♂️)<a class="headerlink" href="#la-theorie-de-vapnik-et-chervonenkis" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>La théorie de Vapnik et Chervonenkis, ou théorie VC, cherche à expliquer via des approches statistiques pourquoi l’apprentissage automatique fonctionne dans certains cas. Cette section du cours de <em>machine learning</em>, plus théorique, nous permettra de démontrer un résultat fondamental dans le cas d’un problème de classification binaire.</p>
<p>Notons <span class="math notranslate nohighlight">\(\mathcal{X}\subseteq\mathbb{R}^d\)</span> notre espace d’entrée de dimension <span class="math notranslate nohighlight">\(d\)</span> et <span class="math notranslate nohighlight">\(\mathcal{Y}=\{0, 1\}\)</span> l’espace de nos labels que nous restreindrons dans cette section au cas binaire. La théorie VC se généralise bien sûr à bien d’autres choses que la classification binaire. Notons <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> l’ensemble des fonctions de <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> dans <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> parmi lesquelles nous voulons réaliser notre apprentissage. Comme toujours, notre objectif est de trouver une fonction <span class="math notranslate nohighlight">\(h\in\mathcal{H}\)</span> qui fait peu d’erreur relativement au risque suivant :</p>
<div class="math notranslate nohighlight">
\[L(h)=\mathbb{E}\big[\textbf{1}\{h(X)\neq Y\}\big]=\mathbb{P}\big(h(X)\neq Y\big).\]</div>
<p>C’est tout simplement la probabilité que notre fonction <span class="math notranslate nohighlight">\(h\in\mathcal{H}\)</span> prédise le mauvais label pour un <span class="math notranslate nohighlight">\(X\)</span> observé. Bien sûr, nous ne connaissons pas le processus génératifs de nos données ni le modèle probabiliste le décrivant. Pour cela, nous devons estimer ce risque en échantillonnant. Notons <span class="math notranslate nohighlight">\(S_n=\{(X_i, Y_i)\}_{i\leq n}\sim\mathbb{P}^n\)</span> un jeu de données de taille <span class="math notranslate nohighlight">\(n\)</span> où les couples sont iid. Nous pouvons maintenant utiliser ce jeu de données afin de déterminer empiriquement le risque de chaque fonction <span class="math notranslate nohighlight">\(h\in\mathcal{H}\)</span> de la manière suivante :</p>
<div class="math notranslate nohighlight">
\[L_n(h)=\frac{1}{n}\sum_i \textbf{1}\{h(X_i)\neq Y_i\}.\]</div>
<p>C’est juste le nombre moyen d’erreurs commises par la fonction <span class="math notranslate nohighlight">\(h\)</span> sur le jeu de données <span class="math notranslate nohighlight">\(S_n\)</span>. Notre objectif de vient donc :</p>
<div class="math notranslate nohighlight">
\[h_n=\text{argmin}_{h\in\mathcal{H}}L_n(h),\]</div>
<p>où on appellera <span class="math notranslate nohighlight">\(h_n\)</span> le minimiseur du risque empirique. C’est la fonction dans <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> qui fait le moins d’erreur en moyenne sur notre jeu de données. On espère qu’elle ne fera pas non plus beaucoup d’erreurs sur le vrai risque <span class="math notranslate nohighlight">\(L\)</span>. Nous pouvons quantifier ce “mauvais choix” via la formule suivante :</p>
<div class="math notranslate nohighlight">
\[L(h_n)-\inf_{h\in\mathcal{H}}L(h).\]</div>
<p>C’est l’écart entre le vrai risque de notre minimiseur du risque empirique et le vrai risque de la meilleure fonction dans <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>.</p>
<p>L’idée de la théorie VC est que finalement, si nous sommes capable d’estimer précisément le risque de chacune des fonctions en utilisant notre jeu de données, <em>a priori</em>, nous ne devrions pas faire un mauvais choix en choisissant le minimiseur du risque empirique. Le lemme suivant valide cette idée :</p>
<hr class="docutils" />
<p><strong>Lemme :</strong></p>
<div class="math notranslate nohighlight">
\[L(h_n)-\inf_{h\in\mathcal{H}}L(h)\leq 2\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|\]</div>
<p><strong>Preuve :</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
L(h_n)-\inf_{h\in\mathcal{H}}L(h)&amp;=L(h_n)-L_n(h_n)+L_n(h_n)-\inf_{h\in\mathcal{H}}L(h)\\
&amp;\leq L(h_n)-L_n(h_n)+\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|\\
&amp;\leq 2\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|
\end{aligned}\end{split}\]</div>
<hr class="docutils" />
<p>Dit autrement, <span class="math notranslate nohighlight">\(\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|\)</span> est la plus grosse erreur d’estimation du risque sur notre jeu de données <span class="math notranslate nohighlight">\(S_n\)</span> dans notre classe de fonctions <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>. Cette dernière permet de majorer le choix du minimiseur du risque empirique. Naïvement, si <span class="math notranslate nohighlight">\(\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|=0\)</span> (on estime parfaitement nos fonctions), alors <span class="math notranslate nohighlight">\(L(h_n)-\inf_{h\in\mathcal{H}}L(h)=0\)</span> et on choisit la meilleure fonction par rapport au vrai risque.</p>
<p>L’objectif de la théorie VC va être de trouver des stratégies permettant de quantifier l’évolution de <span class="math notranslate nohighlight">\(\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|\)</span>. La calculer exactement étant impossible, nous allons donc la majorer par des quantités plus simples à estimer. Et si nos quantités convergent vers <span class="math notranslate nohighlight">\(0\)</span>, alors <span class="math notranslate nohighlight">\(\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|\)</span>, en étant majoré par quelque chose qui converge vers <span class="math notranslate nohighlight">\(0\)</span> et en restant positif devra nécessairement converger vers <span class="math notranslate nohighlight">\(0\)</span> également.</p>
</div>
<div class="section" id="i-aparte-sur-les-inegalites-de-concentration">
<h2>I. Aparté sur les inégalités de concentration<a class="headerlink" href="#i-aparte-sur-les-inegalites-de-concentration" title="Permalink to this headline">¶</a></h2>
<p>La théorie VC fait souvent appel à ce qu’on appelle une inégalité de concentration. L’idée d’une inégalité de concentration est de constater que nos variables aléatoires ne peuvent finalement pas trop s’écarter de certaines valeurs où elles se retrouvent “concentrées”. La plus simple d’entre elle est <a class="reference external" href="https://fr.wikipedia.org/wiki/In%C3%A9galit%C3%A9_de_Markov">inégalité de Markov</a>:</p>
<hr class="docutils" />
<p><strong>L’inégalité de Markov :</strong> Soit <span class="math notranslate nohighlight">\(X\)</span> une variable aléatoire réelle <strong>positive</strong>, nous avons :</p>
<div class="math notranslate nohighlight">
\[\forall a\in\mathbb{R}^+,\ \mathbb{P}\big(X\geq a\big)\leq \frac{\mathbb{E}\big[X\big]}{a}.\]</div>
<p><strong>Preuve :</strong></p>
<p>Soit <span class="math notranslate nohighlight">\(a\in\mathbb{R}^+\)</span> et <span class="math notranslate nohighlight">\(\textbf{1}\{X\geq a\}\)</span> la fonction qui vaut <span class="math notranslate nohighlight">\(1\)</span> si <span class="math notranslate nohighlight">\(X\)</span> est supérieur ou égal à <span class="math notranslate nohighlight">\(a\)</span> et <span class="math notranslate nohighlight">\(0\)</span> sinon. Nous avons nécessairement :</p>
<div class="math notranslate nohighlight">
\[a \textbf{1}\{X\geq a\}\leq X\textbf{1}\{X\geq a\}\leq X,\]</div>
<p>quelque soit la réalisation de <span class="math notranslate nohighlight">\(X\)</span>. En effet si <span class="math notranslate nohighlight">\(X&lt;a\)</span> alors <span class="math notranslate nohighlight">\(\textbf{1}\{X\geq a\}\)</span> vaut <span class="math notranslate nohighlight">\(0\)</span>. L’espérance étant croissante, nous avons :</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\Big[a\textbf{1}\{X\geq a\}\Big]\leq\mathbb{E}\Big[X\Big].\]</div>
<p>Par définition de l’espérance, nous avons :</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\Big[a\textbf{1}\{X\geq a\}\Big]=a \mathbb{P}\big(X\geq a\big) + 0\mathbb{P}\big(X&lt; a\big)=a \mathbb{P}\big(X\geq a\big).\]</div>
<p>Ainsi, en combinant le tout, nous avons :</p>
<div class="math notranslate nohighlight">
\[a \mathbb{P}\big(X\geq a\big)\leq \mathbb{E}\Big[X\Big]\Leftrightarrow \mathbb{P}\big(X\geq a\big)\leq \frac{\mathbb{E}\big[X\big]}{a}.\]</div>
<hr class="docutils" />
<p>En d’autres termes, pour une variable aléatoire <span class="math notranslate nohighlight">\(X\)</span> de loi fixée sa masse ne se situe pas à l’infinie et plus <span class="math notranslate nohighlight">\(a\)</span> sera grand, plus il sera improbable que <span class="math notranslate nohighlight">\(X\)</span> soit au-delà.</p>
<p>Convainquons-nous du résultat précédent via une simulation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># on simule une variable uniforme entre 0 et 100</span>
<span class="k">def</span> <span class="nf">simulate_random_variable</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">nb_times</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">nb_times</span><span class="p">)</span>
    <span class="n">empirical_expectation</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">empirical_probability</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="o">&gt;</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="n">nb_times</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;P(X&gt;=a)&lt;= E[X]/a :&#39;</span><span class="p">,</span> <span class="n">empirical_probability</span><span class="p">,</span> <span class="s1">&#39;&lt;=&#39;</span><span class="p">,</span> <span class="n">empirical_expectation</span><span class="o">/</span><span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">simulate_random_variable</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">75</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>P(X&gt;=a)&lt;= E[X]/a : 0.232 &lt;= 0.6600132441692687
</pre></div>
</div>
</div>
</div>
<p>Et c’est là tout l’intérêt ! Supposons que nous ne sachions pas calculer <span class="math notranslate nohighlight">\(\mathbb{P}\big(X\geq a)\)</span> mais que nous sachions que <span class="math notranslate nohighlight">\(\mathbb{E}\big[X\big]\)</span> alors on peut garantir un “pire scénario” de la vitesse à la quelle la probabilité converge vers <span class="math notranslate nohighlight">\(0\)</span> lorsque <span class="math notranslate nohighlight">\(a\)</span> augmente. Pour une variable uniforme entre <span class="math notranslate nohighlight">\(0\)</span> et <span class="math notranslate nohighlight">\(100\)</span>, son espérance et <span class="math notranslate nohighlight">\(50\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">999</span><span class="p">)</span>

<span class="n">probability</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">a</span><span class="o">/</span><span class="mi">100</span>
<span class="n">expectation</span> <span class="o">=</span> <span class="mi">50</span><span class="o">/</span><span class="n">a</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">probability</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$\mathbb</span><span class="si">{P}</span><span class="s1">(X\geq a)$ (inconnue)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">expectation</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\mathbb</span><span class="si">{E}</span><span class="s1">[X]/a$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4_VC_theory_6_0.png" src="../_images/4_VC_theory_6_0.png" />
</div>
</div>
<p>La quantité <span class="math notranslate nohighlight">\(\mathbb{E}\big[X\big]/a\)</span> est potentiellement supérieure à <span class="math notranslate nohighlight">\(1\)</span> et nous avons contraint matplotlib à n’afficher que les valeurs entre <span class="math notranslate nohighlight">\(0\)</span> et <span class="math notranslate nohighlight">\(1\)</span>. Pour cette exemple, nous aurions pu très naïvement calculer la probabilité nous-même et nous n’avions aucun intérêt à passer par une inégalité de concentration. Cependant, dans certains problèmes, c’est inévitable !</p>
</div>
<div class="section" id="ii-en-supposant-mathcal-h-infty">
<h2>II. En supposant <span class="math notranslate nohighlight">\(|\mathcal{H}|&lt;\infty\)</span><a class="headerlink" href="#ii-en-supposant-mathcal-h-infty" title="Permalink to this headline">¶</a></h2>
<hr class="docutils" />
<p><strong>Le Union Bound.</strong></p>
<p>Nous avons tous vu au lycée la formule suivante :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\big(A\cup B\big)=\mathbb{P}\big(A\big)+\mathbb{P}\big(B\big)-\mathbb{P}\big(A\cap B\big).\]</div>
<p>Si on élimine la probabilité liée à l’intersection, nous avons :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\big(A\cup B\big)\leq\mathbb{P}\big(A\big)+\mathbb{P}\big(B\big)\]</div>
<p>De manière plus générale, imaginons une famille d’évènements <span class="math notranslate nohighlight">\(A_i\)</span>, <span class="math notranslate nohighlight">\(i\leq N\)</span>, tels que <span class="math notranslate nohighlight">\(\forall i\leq N\)</span>, nous avons <span class="math notranslate nohighlight">\(\mathbb{P}(A_i)\leq K\)</span> (la probabilité est majorée par une quantité <span class="math notranslate nohighlight">\(K\)</span>. Nous avons alors l’inégalité suivante :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\big(\cup_i A_i\big)\leq \sum_i \mathbb{P}\big(A_i\big)\leq \sum_i K\leq KN.\]</div>
<p>C’est ce qu’on appelle un <em>union bound</em> : on majore grâce à une inégalité liée à l’union.</p>
<hr class="docutils" />
<p>Soit une famille de fonction <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> de <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> dans <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> telle que <span class="math notranslate nohighlight">\(|\mathcal{H}|&lt;\infty\)</span>. Et soit <span class="math notranslate nohighlight">\(h\in\mathcal{H}\)</span>. Nous avons défini le risque et le risque empirique notés respectivement <span class="math notranslate nohighlight">\(L_n\)</span> et <span class="math notranslate nohighlight">\(L\)</span>. La probabilité que nous souhaiterions voir la plus petite possible est <span class="math notranslate nohighlight">\(\mathbb{P}\big(|L_n(h)-L(h)|&gt;\epsilon)\)</span>. Nous pouvons la majorer grâce à l’<a class="reference external" href="https://fr.wikipedia.org/wiki/In%C3%A9galit%C3%A9_de_Hoeffding">inégalité d’Hoeffding</a> :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\big(|L_n(h)-L(h)|&gt;\epsilon) \leq 2 e^{-2\epsilon^2n}.\]</div>
<p>Cela nous dit que la probabité que notre estimateur empirique s’écarte de plus de <span class="math notranslate nohighlight">\(\epsilon\)</span> de son espérance décroît exponentiellement vite lorsque la taille du jeu de données augmente !</p>
<p>Cependant, pour pouvoir garantir que l’apprentissage se fera bien (i.e. qu’on choisira un bon minimiseur du risque empirique), nous devons majorer la probabilité que TOUTES les fonctions soient bien estimées. Pour cela, nous utilisons le <em>union bound</em> :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{P}\big(\sup_{h\in\mathcal{H}} |L_n(h)-L(h)|&gt;\epsilon\big)&amp;=\mathbb{P}\big(\cup_{h\in\mathcal{H}} |L_n(h)-L(h)|&gt;\epsilon\big)\\
&amp;\leq \sum_{h\in\mathcal{H}}\mathbb{P}\big(\sup_{h\in\mathcal{H}} |L_n(h)-L(h)|&gt;\epsilon\big)\\
&amp;\leq |\mathcal{H}|2 e^{-2\epsilon^2n}.
\end{aligned}\end{split}\]</div>
<p>C’est un premier résultat fondamental qui nous permet de dire que si <span class="math notranslate nohighlight">\(|\mathcal{H}|&lt;\infty\)</span>, alors on peut réduire uniformément (i.e. pour toutes fonctions de <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>) les déviations entre le risque empirique et son espérance et donc que le choix du minimiseur empirique ne sera pas mauvais (pour peu que le jeu de données soit assez grand).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">scale</span><span class="p">):</span>
    <span class="n">H_size</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.04</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2000</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">H_size</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">epsilon</span><span class="p">):</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="o">*</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">e</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">n</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\epsilon=$</span><span class="si">{}</span><span class="s1">, $|H|=$</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">k</span><span class="p">)))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="kc">None</span> <span class="k">if</span> <span class="n">scale</span> <span class="o">==</span> <span class="s1">&#39;log&#39;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot</span><span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4_VC_theory_11_0.png" src="../_images/4_VC_theory_11_0.png" />
<img alt="../_images/4_VC_theory_11_1.png" src="../_images/4_VC_theory_11_1.png" />
</div>
</div>
</div>
<div class="section" id="iii-le-cas-general-mathcal-h-infty">
<h2>III. Le cas général : <span class="math notranslate nohighlight">\(|\mathcal{H}|=\infty\)</span><a class="headerlink" href="#iii-le-cas-general-mathcal-h-infty" title="Permalink to this headline">¶</a></h2>
<p>Le cas général permet également de conclure pour des cas où le cardinal serait fini.</p>
<p>L’intuition derrière le cas général est qu’au lieu de considérer toutes les fonctions de <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>, nous pouvons discrétiser <span class="math notranslate nohighlight">\(\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|\)</span> de manière à ne considérer que les “valeurs effectives” que notre classe de fonctions peut prendre sur notre jeu de donnée. Ainsi, si on considère <span class="math notranslate nohighlight">\(S_1=\{(X_1, Y_1)\}\)</span>, alors, quelque soit <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>, il ne peut y avoir au plus que deux fonctions différentes sur <span class="math notranslate nohighlight">\(S_1\)</span> : celle qui retourne <span class="math notranslate nohighlight">\(1\)</span> pour <span class="math notranslate nohighlight">\(X_1\)</span> et celle qui retourne <span class="math notranslate nohighlight">\(0\)</span> pour <span class="math notranslate nohighlight">\(X_1\)</span>.</p>
<p><em>A priori</em>, le nombre de valeur que peut prendre <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> sur un jeu de données <span class="math notranslate nohighlight">\(S_n\)</span> dépendra du tirage du jeu de données. Pour cela, nous pouvons majorer cette quantité en ne considérant que le “pire des cas”. C’est l’objet de la définition suivante :</p>
<hr class="docutils" />
<p><strong>La fonction de croissance :</strong></p>
<div class="math notranslate nohighlight">
\[\tau_\mathcal{H}(n)=\max_{\{X_1, ..., X_n\}}|\{h(X_1), ..., h(X_n):\ h\in\mathcal{H}\}|.\]</div>
<p>C’est le plus grand nombre de manières différentes qu’une classe de fonction pourrait labéliser un jeu de données de taille <span class="math notranslate nohighlight">\(n\)</span>. C’est le plus grand nombre configuration de taille <span class="math notranslate nohighlight">\(n\)</span> atteignables par notre classe de fonctions.</p>
<hr class="docutils" />
<p>Notre objectif ici est de démontrer le résultat suivant :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\Big(\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|&gt;\epsilon\Big)\leq 8 \tau_{\mathcal{H}}(n)e^{-n\epsilon^2/32}\]</div>
<hr class="docutils" />
<div class="section" id="preuve">
<h3>PREUVE<a class="headerlink" href="#preuve" title="Permalink to this headline">¶</a></h3>
<p>On remarque que si <span class="math notranslate nohighlight">\(n\epsilon^2&lt;2\)</span>, alors la partie droite de l’inégalité est supérieure à <span class="math notranslate nohighlight">\(1\)</span> et l’inégalité est trivialement vraie. Supposons <span class="math notranslate nohighlight">\(n\epsilon^2\geq 2\)</span>.</p>
<p><strong>Étape 1 : Symétrisation par un échantillon fantôme (échantillon de test).</strong></p>
<p>Construisons un jeu de données de test virtuel <span class="math notranslate nohighlight">\(S_n^\prime=\{(X_i^\prime, Y_i^\prime)\}_{i\leq n}\)</span> tel que <span class="math notranslate nohighlight">\(S_n\sim S_n^\prime\)</span>. Notons <span class="math notranslate nohighlight">\(L_n^\prime\)</span> le risque empirique associé à cet échantillon. Supposons <span class="math notranslate nohighlight">\(n\epsilon^2\geq 2\)</span>, alors nous avons :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\Big(\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|&gt;\epsilon\Big)\leq 2 \mathbb{P}\Big(\sup_{h\in\mathcal{H}}|L_n(h)-L_n^\prime(h)|&gt;\epsilon/2\Big)\]</div>
<p>Pour voir cela, notons <span class="math notranslate nohighlight">\(h^\star\)</span> une fonction telle que <span class="math notranslate nohighlight">\(|L_n(h^\star)-L(h^\star)|&gt;\epsilon\)</span> si une telle fonction existe, sinon une fonction fixée au hasard. Nous avons alors :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{P}\Big(\sup_{h\in\mathcal{H}}|L_n(h)-L_n^\prime(h)|&gt;\epsilon/2\Big)&amp;\geq \mathbb{P}\Big(|L_n(h^\star)-L_n^\prime(h^\star)|&gt;\epsilon/2\Big)\\
&amp;\geq\mathbb{P}\Big(|L_n(h^\star)-L(h^\star)|&gt;\epsilon, |L_n^\prime(h^\star)-L(h^\star)|&lt;\epsilon/2\Big)\\
&amp;=\mathbb{E}\Big[\textbf{1}\{|L_n(h^\star)-L(h^\star)|&gt;\epsilon\}\mathbb{P}\Big(|L_n^\prime(h^\star)-L(h^\star)|&lt;\epsilon/2|Z_1,\ldots, Z_n\Big)\Big]
\end{aligned}\end{split}\]</div>
<p>où <span class="math notranslate nohighlight">\(Z_i=(X_i, Y_i)\)</span>. En utilisant l’<a class="reference external" href="https://fr.wikipedia.org/wiki/In%C3%A9galit%C3%A9_de_Bienaym%C3%A9-Tchebychev">Inégalité de Bienaymé-Tchebychev</a>, nous avons :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{P}\Big(|L_n^\prime(h^\star)-L(h^\star)|&lt;\epsilon/2|Z_1,\ldots, Z_n\Big)&amp;\geq 1-\frac{L(h^\star)(1-L(h^\star))/n}{(\epsilon/2)^2}\\
&amp;\geq 1-\frac{1/4}{n\epsilon^2/4}\geq \frac{1}{2}.
\end{aligned}\end{split}\]</div>
<p>(car <span class="math notranslate nohighlight">\(n\epsilon^2&gt;2\)</span>).
Ainsi :</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\Big[\textbf{1}\{|L_n(h^\star)-L(h^\star)|&gt;\epsilon\}\mathbb{P}\Big(|L_n^\prime(h^\star)-L(h^\star)|&lt;\epsilon/2|Z_1,\ldots, Z_n\Big)\Big] \geq \mathbb{E}\Big[\textbf{1}\{|L_n(h^\star)-L(h^\star)|&gt;\epsilon\}\Big]\frac{1}{2}=\mathbb{P}\Big(|L_n(h^\star)-L(h^\star)|&gt;\epsilon\Big)\frac{1}{2},\]</div>
<p>et nous obtenons le résultat voulu.</p>
<p>Cette première étape nous dit que comparer le score empirique de notre risque par rapport à son espérance est à peu près la même chose que le comparer avec un jeu de test. Testons cela au travers d’une petite expériences avec des tirages binomiaux normalisés.</p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">H</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">]</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">epsilon_list</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>
<span class="c1"># on répète l&#39;expérience pour calculer empiriquement la probabilité</span>
<span class="n">size</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="k">for</span> <span class="n">epsilon</span> <span class="ow">in</span> <span class="n">epsilon_list</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;*&#39;</span> <span class="o">*</span> <span class="mi">20</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Epsilon=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epsilon</span><span class="p">))</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">H</span><span class="p">:</span>
        <span class="n">Ln</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span><span class="o">/</span><span class="n">n</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">Ln</span><span class="o">-</span><span class="n">h</span><span class="p">)</span><span class="o">&gt;</span><span class="n">epsilon</span><span class="p">)</span>
    <span class="n">empirical_probability_1</span> <span class="o">=</span> <span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span> <span class="o">/</span> <span class="n">size</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;* P(sup |L_n-L|&gt;epsilon)=&#39;</span><span class="p">,</span> <span class="n">empirical_probability_1</span><span class="p">)</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">H</span><span class="p">:</span>
        <span class="n">binom</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span><span class="o">/</span><span class="n">n</span>
        <span class="n">binom_ghost</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span><span class="o">/</span><span class="n">n</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">binom</span><span class="o">-</span><span class="n">binom_ghost</span><span class="p">)</span><span class="o">&gt;</span><span class="n">epsilon</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">empirical_probability_2</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">size</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;* P(sup |L_n-L_n</span><span class="se">\&#39;</span><span class="s1">|&gt;epsilon)=&#39;</span><span class="p">,</span> <span class="n">empirical_probability_2</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;* 2xP(sup |L_n-L_n</span><span class="se">\&#39;</span><span class="s1">|&gt;epsilon)=&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">empirical_probability_2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>********************
Epsilon=0.01
* P(sup |L_n-L|&gt;epsilon)= 1.0
* P(sup |L_n-L_n&#39;|&gt;epsilon)= 0.999
* 2xP(sup |L_n-L_n&#39;|&gt;epsilon)= 1.998
********************
Epsilon=0.1
* P(sup |L_n-L|&gt;epsilon)= 0.31
* P(sup |L_n-L_n&#39;|&gt;epsilon)= 0.926
* 2xP(sup |L_n-L_n&#39;|&gt;epsilon)= 1.852
********************
Epsilon=0.2
* P(sup |L_n-L|&gt;epsilon)= 0.002
* P(sup |L_n-L_n&#39;|&gt;epsilon)= 0.613
* 2xP(sup |L_n-L_n&#39;|&gt;epsilon)= 1.226
</pre></div>
</div>
</div>
</div>
<p>La seconde étape va nous permettre d’élimiter ce jeu fantôme auquel nous n’avons pas accès. Aussitôt mis, aussitôt retiré.</p>
<hr class="docutils" />
<p><strong>Étape 2 : Symétrisation avec des signes aléatoires.</strong></p>
<p>Nous avons donc la variable suivante :</p>
<div class="math notranslate nohighlight">
\[\sup_{h\in\mathcal{H}}|L_n(h)-L_n^\prime(h)|=\sup_{h\in\mathcal{H}}\frac{1}{n}|\sum_i\textbf{1}\{h(X_i)\neq Y_i\}-\textbf{1}\{h(X_i^\prime)\neq Y_i^\prime\}|\]</div>
<p>Puisque nos variables <span class="math notranslate nohighlight">\(\textbf{1}\{h(X_i)\neq Y_i\}\)</span> et <span class="math notranslate nohighlight">\(\textbf{1}\{h(X_i^\prime)\neq Y_i^\prime\}\)</span> sont iid, cela revient exactement à</p>
<div class="math notranslate nohighlight">
\[\sup_{h\in\mathcal{H}}\frac{1}{n}|\sum_i\sigma_i(\textbf{1}\{h(X_i)\neq Y_i\}-\textbf{1}\{h(X_i^\prime)\neq Y_i^\prime\})|,\]</div>
<p>où <span class="math notranslate nohighlight">\(\sigma_i\)</span> est une variable de Rademacher (i.e. <span class="math notranslate nohighlight">\(\mathbb{P}\big(\sigma_i=-1\big)=\mathbb{P}\big(\sigma_i=+1\big)=0.5\)</span>).</p>
<p>Nous avons donc :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\Big(\sup_{h\in\mathcal{H}}\frac{1}{n}|\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}-\sigma_i\textbf{1}\{h(X_i^\prime\neq Y_i^\prime\})|&gt;\epsilon/2\Big)\]</div>
<p>Et au moins l’un deux deux termes <span class="math notranslate nohighlight">\(|\frac{1}{n}\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}|\)</span> doit être supérieur à <span class="math notranslate nohighlight">\(\epsilon/4\)</span> pour que la somme soit supérieure à <span class="math notranslate nohighlight">\(\epsilon/2\)</span> (vérifier par contradiction). Ainsi, en appliquant le <em>union bound</em>, nous avons :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{P}\Big(\sup_{h\in\mathcal{H}}\frac{1}{n}|\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}-\sigma_i\textbf{1}\{h(X_i^\prime\neq Y_i^\prime\})&amp;|&gt;\epsilon/2\Big) \\
&amp;\leq\mathbb{P}\Big(\sup_{h\in\mathcal{H}}\frac{1}{n}|\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}|&gt;\epsilon/4\Big)+\mathbb{P}\Big(\sup_{h\in\mathcal{H}}\frac{1}{n}|\sigma_i\textbf{1}\{h(X_i^\prime\neq Y_i^\prime\}|&gt;\epsilon/4\Big)\\
&amp;=2\mathbb{P}\Big(\sup_{h\in\mathcal{H}}\frac{1}{n}|\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}|&gt;\epsilon/4\Big)
\end{aligned}\end{split}\]</div>
<hr class="docutils" />
<hr class="docutils" />
<p><strong>Étape 3 : Conditionnement.</strong>
Jusqu’ici, nous considérions le jeu de données comme une variable aléatoire. Fixons le et étudions un cas particulier. Notons <span class="math notranslate nohighlight">\(z_1, \ldots, z_n\in\mathcal{X}\)</span> cette réalisation.</p>
<p>Nous avons donc :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\Big(\sup_{h\in\mathcal{H}}\frac{1}{n}|\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}|&gt;\epsilon/4|Z_1=z_1, \ldots, Z_n=z_n\Big).\]</div>
<p>Le nombre de configuration à tester est justement la fonction de croissance qui nous indique toutes les valeurs que peut prendre notre classe de fonctions <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> sur un jeu de données fixé de taille <span class="math notranslate nohighlight">\(n\)</span>. En appliquant le <em>union bound</em> à nouveau, nous obtenons donc :</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\mathbb{P}\Big(\sup_{h\in\mathcal{H}}\frac{1}{n}|\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}|&gt;\epsilon/4|Z_1, \ldots, Z_n\Big)\leq\tau_\mathcal{H}(n)\sup_{h\in\mathcal{H}}\mathbb{P}\Big(\frac{1}{n}|\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}|&gt;\epsilon/4|Z_1, \ldots, Z_n\Big).
\end{aligned}\]</div>
<p>De plus, en appliquant <a class="reference external" href="https://fr.wikipedia.org/wiki/In%C3%A9galit%C3%A9_de_Hoeffding">l’inégalité d’Hoeffding</a> , nous pouvons majorer la probabilité de droite quelque soit la fonction :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\Big(\frac{1}{n}|\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}|&gt;\epsilon/4|Z_1, \ldots, Z_n\Big)\leq 2e^{-n\epsilon^2/32}\]</div>
<p>Cette variable ne dépend pas du conditionnement et nous pouvons donc prendre l’espérance :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\Big(\frac{1}{n}|\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}|&gt;\epsilon/4\Big)=\mathbb{E}\Big[\mathbb{P}\Big(\frac{1}{n}|\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}|&gt;\epsilon/4|Z_1, \ldots, Z_n\Big)\Big] \leq 2e^{-n\epsilon^2/32}\]</div>
<p><strong>Conclusion.</strong></p>
<p>En combinait les 3 étapes précédentes, nous obtenons ainsi :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\Big(\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|&gt;\epsilon\Big)\leq 8 \tau_{\mathcal{H}}(n)e^{-n\epsilon^2/32}\]</div>
<hr class="docutils" />
<p>Des résultats beaucoup plus stricts existent.</p>
</div>
</div>
<div class="section" id="iv-la-dimension-vc">
<h2>IV. La dimension VC<a class="headerlink" href="#iv-la-dimension-vc" title="Permalink to this headline">¶</a></h2>
<p>Le théorème précédent est très intéressant, mais sans hypothèse sur notre classe de fonction <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>, nous pourrions très bien avoir :</p>
<div class="math notranslate nohighlight">
\[\tau_\mathcal{H}(n)=2^n,\]</div>
<p>et cela nous donnerait :</p>
<div class="math notranslate nohighlight">
\[\lim_{n\rightarrow\infty}8 \cdot 2^ne^{-n\epsilon^2/32}=\infty,\]</div>
<p>ce qui nous empêcherait de conclure !</p>
<p>Il se trouve que pour certaines classes de fonctions (et nous verront des exemples), quand bien même <span class="math notranslate nohighlight">\(|\mathcal{H}|=\infty\)</span>, nous n’aurions pas <span class="math notranslate nohighlight">\(\tau_\mathcal{H}(n)=2^n\)</span>.</p>
<hr class="docutils" />
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Soit <span class="math notranslate nohighlight">\(\mathcal{X}=\mathbb{R}\)</span>, <span class="math notranslate nohighlight">\(\mathcal{Y}=\{0, 1\}\)</span> et <span class="math notranslate nohighlight">\(\mathcal{H}=\{h_s(x)=\textbf{1}\{x&gt;s\}:\ s\in\mathbb{R}\}\)</span>, l’ensemble des fonctions seuil (on retourne <span class="math notranslate nohighlight">\(1\)</span> si <span class="math notranslate nohighlight">\(x&gt;s\)</span> et <span class="math notranslate nohighlight">\(0\)</span> sinon). Montrer que <span class="math notranslate nohighlight">\(\tau_\mathcal{H}(2)=3\)</span> et non <span class="math notranslate nohighlight">\(2^2=4\)</span>.</strong></p>
<p><span style="color:green"><strong>Réponse :</strong></span> <strong>Soit <span class="math notranslate nohighlight">\(x_1, x_2\in\mathcal{X}\)</span> tels que <span class="math notranslate nohighlight">\(x_1&lt;x_2\)</span> sans perte de généralité. La configuration (1, 0) est atteignable si <span class="math notranslate nohighlight">\(s&lt;x_1\)</span> et <span class="math notranslate nohighlight">\(x_2 &lt; s\)</span> entraînant une contradiction. À l’inverse les configurations (0, 0), (0, 1) et (1, 1) sont facilement atteignables.</strong></p>
<hr class="docutils" />
<p>Nous appelons la dimension VC ou VCdim d’une classe de fonctions <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> le plus grand jeu de données S_n tel que <span class="math notranslate nohighlight">\(\tau_\mathcal{H}(n)=2^n\)</span>. Plus formellement, nous avons :</p>
<div class="math notranslate nohighlight">
\[\text{VCdim}(\mathcal{H})=\max_{\tau_{\mathcal{H}}(n)=2^n}n\]</div>
<p>L’intérêt clé de cette propriété nous vient du lemme suivant :</p>
<hr class="docutils" />
<p><strong>Lemme de Sauer.</strong></p>
<p>Soit <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> notre classe de fonctions telle que <span class="math notranslate nohighlight">\(\text{VCdim}(\mathcal{H})\leq d&lt;\infty\)</span>. Nous avons alors <span class="math notranslate nohighlight">\(\forall n&gt;0\)</span> :</p>
<div class="math notranslate nohighlight">
\[\tau_\mathcal{H}(n)\leq \sum_{i=0}^d{n\choose i}.\]</div>
<p>De plus, si <span class="math notranslate nohighlight">\(n&gt;d+1\)</span>, alors :</p>
<div class="math notranslate nohighlight">
\[\tau_\mathcal{H}(n)\leq\Bigg(\frac{en}{d}\Bigg)^d.\]</div>
<hr class="docutils" />
<p>Ainsi, la fonction de croissance ne croît plus exponentiellement vite mais polynomialement dès qu’on dépasse la dimension VC de notre classe de fonctions. Cela implique que notre majorant de généralisation converge vers <span class="math notranslate nohighlight">\(0\)</span>, qu’on puisse estimer l’erreur de nos fonctions correctement et donc que le choix du minimiseur du risque empirique est un bon choix : ce résultat est ce qu’on appelle le <em>théorème fondamental du machine learning</em>.</p>
<hr class="docutils" />
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Soit <span class="math notranslate nohighlight">\(\mathcal{X}=\mathbb{R}^2\)</span>, <span class="math notranslate nohighlight">\(\mathcal{Y}=\{0, 1\}\)</span> et <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> l’ensemble des classifieurs linéaires de <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>. Démontrer que la dimension VC de <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> est au moins <span class="math notranslate nohighlight">\(3\)</span>.</strong></p>
<div class="toggle docutils container">
<p><span style="color:green"><strong>Réponse :</strong></span> <strong>On vérifie facilement avec un dessin que <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> classifie bien toutes les configurations d’un jeu de données de taille <span class="math notranslate nohighlight">\(3\)</span>.</strong></p>
</div>
<hr class="docutils" />
<hr class="docutils" />
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Soit <span class="math notranslate nohighlight">\(\mathcal{H}_{=k}\)</span> l’ensemble des classifieurs qui ne peuvent associer le label <span class="math notranslate nohighlight">\(1\)</span> qu’à <em>exactement</em> <span class="math notranslate nohighlight">\(k\)</span> éléments de <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>. Trouver la dimension VC. On supposera que <span class="math notranslate nohighlight">\(|\mathcal{X}|\geq k\)</span>.</strong></p>
<div class="toggle docutils container">
<p><span style="color:green"><strong>Réponse :</strong></span> <strong>La dimension VC d’un tel ensemble est <span class="math notranslate nohighlight">\(\text{VCdim}(\mathcal{H})=\text{min}(k, |\mathcal{X}|-k)\)</span>. Montrons (1) que <span class="math notranslate nohighlight">\(\text{VCdim}(\mathcal{H})\leq k\)</span>. Soit un jeu de données <span class="math notranslate nohighlight">\(S_{k+1}\)</span>, alors il n’est pas possible de mettre <span class="math notranslate nohighlight">\(1\)</span> à tous les points. Montrons (2) que <span class="math notranslate nohighlight">\(\text{VCdim}(\mathcal{H})\leq |\mathcal{X}|-k\)</span>. Soit <span class="math notranslate nohighlight">\(S_{|\mathcal{X}|-k+1}\)</span>, alors, il n’est pas possible de mettre <span class="math notranslate nohighlight">\(0\)</span> à tous les points. Montrons (3) maintenant que <span class="math notranslate nohighlight">\(\text{VCdim}(\mathcal{H})\geq \text{min}(k, |\mathcal{X}|-k)\)</span>. Soit <span class="math notranslate nohighlight">\(S_n\)</span> tel que <span class="math notranslate nohighlight">\(n\leq \text{min}(k, |\mathcal{X}|-k)\)</span> alors toutes les configurations sont atteignables. En effet, il y a au plus <span class="math notranslate nohighlight">\(k\)</span> <span class="math notranslate nohighlight">\(1\)</span> à mettre et il restera toujours au moins <span class="math notranslate nohighlight">\(k\)</span> <span class="math notranslate nohighlight">\(1\)</span> à l’extérieur de <span class="math notranslate nohighlight">\(S_k\)</span>.</strong></p>
</div>
<hr class="docutils" />
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./3_logistic_regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="3_bayes_classifier.html" title="previous page">Le classifieur de Bayes ☕️</a>
    <a class='right-next' id="next-link" href="../4_max_margin/0_propos_liminaire.html" title="next page">Les modèles <em>max-margin</em></a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By The LMIPR team<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>
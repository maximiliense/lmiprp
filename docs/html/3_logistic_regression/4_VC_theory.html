
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>La th√©orie de Vapnik et Chervonenkis ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è (üíÜ‚Äç‚ôÇÔ∏è) &#8212; Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Les mod√®les max-margin" href="../4_max_margin/0_propos_liminaire.html" />
    <link rel="prev" title="Le classifieur de Bayes ‚òïÔ∏è" href="3_bayes_classifier.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Machine Learning
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../0_requisite/rappels_python.html">
   Rappels de Python et Numpy
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../1_what_is_ml/0_propos_liminaire.html">
   Le
   <em>
    Machine Learning
   </em>
   , c‚Äôest quoi ?
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/1_introduction_ml.html">
     <em>
      Machine learning
     </em>
     et mal√©diction de la dimension
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/2_regression_and_classification_trees.html">
     Les arbres de r√©gression et de classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../2_linear_regression/0_propos_liminaire.html">
   La
   <em>
    r√©gression
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_linear_regression/1_linear_regression.html">
     La r√©gression lin√©aire ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_linear_regression/2_optimization.html">
     L‚Äôoptimisation ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_linear_regression/3_interpolation.html">
     Interpolation ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_linear_regression/4_algo_proximal_lasso.html">
     Sous-diff√©rentiel et le cas du Lasso ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="0_propos_liminaire.html">
   La
   <em>
    classification
   </em>
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="1_logistic_regression.html">
     La R√©gression Logistique ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="2_fonctions_proxy.html">
     Les fonctions proxy ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3_bayes_classifier.html">
     Le classifieur de Bayes ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     La th√©orie de Vapnik et Chervonenkis ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è (üíÜ‚Äç‚ôÇÔ∏è)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../4_max_margin/0_propos_liminaire.html">
   Les mod√®les
   <em>
    max-margin
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../4_max_margin/1_max_margin.html">
     Les mod√®les max-margin ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5_ensembles/0_propos_liminaire.html">
   Les m√©thodes
   <em>
    ensemblistes
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/1_ensembles.html">
     Les m√©thodes ensemblistes ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../6_autodiff/0_propos_liminaire.html">
   La diff√©rentiation automatique
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_autodiff/1_autodiff.html">
     La diff√©rentiation automatique et un d√©but de
     <em>
      deep learning
     </em>
     ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_autodiff/2_filters_representation.html">
     Filtres et espace de repr√©sentation des r√©seaux de neurones ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../7_regularization/0_propos_liminaire.html">
   La r√©gularisation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/1_regularization_deep.html">
     R√©gularisation en deep learning ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/2_unsupervised_representation_learning.html">
     Unsupervised representation learning ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/3_multitask.html">
     L‚Äôapprentissage multi-t√¢ches ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/4_adversarial.html">
     Les attaques adversaires ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/5_ridge.html">
     Une analyse de la r√©gularisation Ridge ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../content.html">
   Content in Jupyter Book
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../markdown.html">
     Markdown Files
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../notebooks.html">
     Content with notebooks
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/3_logistic_regression/4_VC_theory.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/maximiliense/lmiprp"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/maximiliense/lmiprp/issues/new?title=Issue%20on%20page%20%2F3_logistic_regression/4_VC_theory.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/maximiliense/lmiprp/master?urlpath=tree/3_logistic_regression/4_VC_theory.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#i-aparte-sur-les-inegalites-de-concentration">
   I. Apart√© sur les in√©galit√©s de concentration
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ii-en-supposant-mathcal-h-infty">
   II. En supposant
   <span class="math notranslate nohighlight">
    \(|\mathcal{H}|&lt;\infty\)
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iii-le-cas-general-mathcal-h-infty">
   III. Le cas g√©n√©ral :
   <span class="math notranslate nohighlight">
    \(|\mathcal{H}|=\infty\)
   </span>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#preuve">
     PREUVE
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iv-la-dimension-vc">
   IV. La dimension VC
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="la-theorie-de-vapnik-et-chervonenkis">
<h1>La th√©orie de Vapnik et Chervonenkis ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è (üíÜ‚Äç‚ôÇÔ∏è)<a class="headerlink" href="#la-theorie-de-vapnik-et-chervonenkis" title="Permalink to this headline">¬∂</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¬∂</a></h2>
<p>La th√©orie de Vapnik et Chervonenkis, ou th√©orie VC, cherche √† expliquer via des approches statistiques pourquoi l‚Äôapprentissage automatique fonctionne dans certains cas. Cette section du cours de <em>machine learning</em>, plus th√©orique, nous permettra de d√©montrer un r√©sultat fondamental dans le cas d‚Äôun probl√®me de classification binaire.</p>
<p>Notons <span class="math notranslate nohighlight">\(\mathcal{X}\subseteq\mathbb{R}^d\)</span> notre espace d‚Äôentr√©e de dimension <span class="math notranslate nohighlight">\(d\)</span> et <span class="math notranslate nohighlight">\(\mathcal{Y}=\{0, 1\}\)</span> l‚Äôespace de nos labels que nous restreindrons dans cette section au cas binaire. La th√©orie VC se g√©n√©ralise bien s√ªr √† bien d‚Äôautres choses que la classification binaire. Notons <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> l‚Äôensemble des fonctions de <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> dans <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> parmi lesquelles nous voulons r√©aliser notre apprentissage. Comme toujours, notre objectif est de trouver une fonction <span class="math notranslate nohighlight">\(h\in\mathcal{H}\)</span> qui fait peu d‚Äôerreur relativement au risque suivant :</p>
<div class="math notranslate nohighlight">
\[L(h)=\mathbb{E}\big[\textbf{1}\{h(X)\neq Y\}\big]=\mathbb{P}\big(h(X)\neq Y\big).\]</div>
<p>C‚Äôest tout simplement la probabilit√© que notre fonction <span class="math notranslate nohighlight">\(h\in\mathcal{H}\)</span> pr√©dise le mauvais label pour un <span class="math notranslate nohighlight">\(X\)</span> observ√©. Bien s√ªr, nous ne connaissons pas le processus g√©n√©ratifs de nos donn√©es ni le mod√®le probabiliste le d√©crivant. Pour cela, nous devons estimer ce risque en √©chantillonnant. Notons <span class="math notranslate nohighlight">\(S_n=\{(X_i, Y_i)\}_{i\leq n}\sim\mathbb{P}^n\)</span> un jeu de donn√©es de taille <span class="math notranslate nohighlight">\(n\)</span> o√π les couples sont iid. Nous pouvons maintenant utiliser ce jeu de donn√©es afin de d√©terminer empiriquement le risque de chaque fonction <span class="math notranslate nohighlight">\(h\in\mathcal{H}\)</span> de la mani√®re suivante :</p>
<div class="math notranslate nohighlight">
\[L_n(h)=\frac{1}{n}\sum_i \textbf{1}\{h(X_i)\neq Y_i\}.\]</div>
<p>C‚Äôest juste le nombre moyen d‚Äôerreurs commises par la fonction <span class="math notranslate nohighlight">\(h\)</span> sur le jeu de donn√©es <span class="math notranslate nohighlight">\(S_n\)</span>. Notre objectif de vient donc :</p>
<div class="math notranslate nohighlight">
\[h_n=\text{argmin}_{h\in\mathcal{H}}L_n(h),\]</div>
<p>o√π on appellera <span class="math notranslate nohighlight">\(h_n\)</span> le minimiseur du risque empirique. C‚Äôest la fonction dans <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> qui fait le moins d‚Äôerreur en moyenne sur notre jeu de donn√©es. On esp√®re qu‚Äôelle ne fera pas non plus beaucoup d‚Äôerreurs sur le vrai risque <span class="math notranslate nohighlight">\(L\)</span>. Nous pouvons quantifier ce ‚Äúmauvais choix‚Äù via la formule suivante :</p>
<div class="math notranslate nohighlight">
\[L(h_n)-\inf_{h\in\mathcal{H}}L(h).\]</div>
<p>C‚Äôest l‚Äô√©cart entre le vrai risque de notre minimiseur du risque empirique et le vrai risque de la meilleure fonction dans <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>.</p>
<p>L‚Äôid√©e de la th√©orie VC est que finalement, si nous sommes capable d‚Äôestimer pr√©cis√©ment le risque de chacune des fonctions en utilisant notre jeu de donn√©es, <em>a priori</em>, nous ne devrions pas faire un mauvais choix en choisissant le minimiseur du risque empirique. Le lemme suivant valide cette id√©e :</p>
<hr class="docutils" />
<p><strong>Lemme :</strong></p>
<div class="math notranslate nohighlight">
\[L(h_n)-\inf_{h\in\mathcal{H}}L(h)\leq 2\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|\]</div>
<p><strong>Preuve :</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
L(h_n)-\inf_{h\in\mathcal{H}}L(h)&amp;=L(h_n)-L_n(h_n)+L_n(h_n)-\inf_{h\in\mathcal{H}}L(h)\\
&amp;\leq L(h_n)-L_n(h_n)+\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|\\
&amp;\leq 2\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|
\end{aligned}\end{split}\]</div>
<hr class="docutils" />
<p>Dit autrement, <span class="math notranslate nohighlight">\(\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|\)</span> est la plus grosse erreur d‚Äôestimation du risque sur notre jeu de donn√©es <span class="math notranslate nohighlight">\(S_n\)</span> dans notre classe de fonctions <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>. Cette derni√®re permet de majorer le choix du minimiseur du risque empirique. Na√Øvement, si <span class="math notranslate nohighlight">\(\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|=0\)</span> (on estime parfaitement nos fonctions), alors <span class="math notranslate nohighlight">\(L(h_n)-\inf_{h\in\mathcal{H}}L(h)=0\)</span> et on choisit la meilleure fonction par rapport au vrai risque.</p>
<p>L‚Äôobjectif de la th√©orie VC va √™tre de trouver des strat√©gies permettant de quantifier l‚Äô√©volution de <span class="math notranslate nohighlight">\(\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|\)</span>. La calculer exactement √©tant impossible, nous allons donc la majorer par des quantit√©s plus simples √† estimer. Et si nos quantit√©s convergent vers <span class="math notranslate nohighlight">\(0\)</span>, alors <span class="math notranslate nohighlight">\(\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|\)</span>, en √©tant major√© par quelque chose qui converge vers <span class="math notranslate nohighlight">\(0\)</span> et en restant positif devra n√©cessairement converger vers <span class="math notranslate nohighlight">\(0\)</span> √©galement.</p>
</div>
<div class="section" id="i-aparte-sur-les-inegalites-de-concentration">
<h2>I. Apart√© sur les in√©galit√©s de concentration<a class="headerlink" href="#i-aparte-sur-les-inegalites-de-concentration" title="Permalink to this headline">¬∂</a></h2>
<p>La th√©orie VC fait souvent appel √† ce qu‚Äôon appelle une in√©galit√© de concentration. L‚Äôid√©e d‚Äôune in√©galit√© de concentration est de constater que nos variables al√©atoires ne peuvent finalement pas trop s‚Äô√©carter de certaines valeurs o√π elles se retrouvent ‚Äúconcentr√©es‚Äù. La plus simple d‚Äôentre elle est <a class="reference external" href="https://fr.wikipedia.org/wiki/In%C3%A9galit%C3%A9_de_Markov">in√©galit√© de Markov</a>:</p>
<hr class="docutils" />
<p><strong>L‚Äôin√©galit√© de Markov :</strong> Soit <span class="math notranslate nohighlight">\(X\)</span> une variable al√©atoire r√©elle <strong>positive</strong>, nous avons :</p>
<div class="math notranslate nohighlight">
\[\forall a\in\mathbb{R}^+,\ \mathbb{P}\big(X\geq a\big)\leq \frac{\mathbb{E}\big[X\big]}{a}.\]</div>
<p><strong>Preuve :</strong></p>
<p>Soit <span class="math notranslate nohighlight">\(a\in\mathbb{R}^+\)</span> et <span class="math notranslate nohighlight">\(\textbf{1}\{X\geq a\}\)</span> la fonction qui vaut <span class="math notranslate nohighlight">\(1\)</span> si <span class="math notranslate nohighlight">\(X\)</span> est sup√©rieur ou √©gal √† <span class="math notranslate nohighlight">\(a\)</span> et <span class="math notranslate nohighlight">\(0\)</span> sinon. Nous avons n√©cessairement :</p>
<div class="math notranslate nohighlight">
\[a \textbf{1}\{X\geq a\}\leq X\textbf{1}\{X\geq a\}\leq X,\]</div>
<p>quelque soit la r√©alisation de <span class="math notranslate nohighlight">\(X\)</span>. En effet si <span class="math notranslate nohighlight">\(X&lt;a\)</span> alors <span class="math notranslate nohighlight">\(\textbf{1}\{X\geq a\}\)</span> vaut <span class="math notranslate nohighlight">\(0\)</span>. L‚Äôesp√©rance √©tant croissante, nous avons :</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\Big[a\textbf{1}\{X\geq a\}\Big]\leq\mathbb{E}\Big[X\Big].\]</div>
<p>Par d√©finition de l‚Äôesp√©rance, nous avons :</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\Big[a\textbf{1}\{X\geq a\}\Big]=a \mathbb{P}\big(X\geq a\big) + 0\mathbb{P}\big(X&lt; a\big)=a \mathbb{P}\big(X\geq a\big).\]</div>
<p>Ainsi, en combinant le tout, nous avons :</p>
<div class="math notranslate nohighlight">
\[a \mathbb{P}\big(X\geq a\big)\leq \mathbb{E}\Big[X\Big]\Leftrightarrow \mathbb{P}\big(X\geq a\big)\leq \frac{\mathbb{E}\big[X\big]}{a}.\]</div>
<hr class="docutils" />
<p>En d‚Äôautres termes, pour une variable al√©atoire <span class="math notranslate nohighlight">\(X\)</span> de loi fix√©e sa masse ne se situe pas √† l‚Äôinfinie et plus <span class="math notranslate nohighlight">\(a\)</span> sera grand, plus il sera improbable que <span class="math notranslate nohighlight">\(X\)</span> soit au-del√†.</p>
<p>Convainquons-nous du r√©sultat pr√©c√©dent via une simulation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># on simule une variable uniforme entre 0 et 100</span>
<span class="k">def</span> <span class="nf">simulate_random_variable</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">nb_times</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">nb_times</span><span class="p">)</span>
    <span class="n">empirical_expectation</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">empirical_probability</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="o">&gt;</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="n">nb_times</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;P(X&gt;=a)&lt;= E[X]/a :&#39;</span><span class="p">,</span> <span class="n">empirical_probability</span><span class="p">,</span> <span class="s1">&#39;&lt;=&#39;</span><span class="p">,</span> <span class="n">empirical_expectation</span><span class="o">/</span><span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">simulate_random_variable</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">75</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>P(X&gt;=a)&lt;= E[X]/a : 0.232 &lt;= 0.6600132441692687
</pre></div>
</div>
</div>
</div>
<p>Et c‚Äôest l√† tout l‚Äôint√©r√™t ! Supposons que nous ne sachions pas calculer <span class="math notranslate nohighlight">\(\mathbb{P}\big(X\geq a)\)</span> mais que nous sachions que <span class="math notranslate nohighlight">\(\mathbb{E}\big[X\big]\)</span> alors on peut garantir un ‚Äúpire sc√©nario‚Äù de la vitesse √† la quelle la probabilit√© converge vers <span class="math notranslate nohighlight">\(0\)</span> lorsque <span class="math notranslate nohighlight">\(a\)</span> augmente. Pour une variable uniforme entre <span class="math notranslate nohighlight">\(0\)</span> et <span class="math notranslate nohighlight">\(100\)</span>, son esp√©rance et <span class="math notranslate nohighlight">\(50\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">999</span><span class="p">)</span>

<span class="n">probability</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">a</span><span class="o">/</span><span class="mi">100</span>
<span class="n">expectation</span> <span class="o">=</span> <span class="mi">50</span><span class="o">/</span><span class="n">a</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">probability</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$\mathbb</span><span class="si">{P}</span><span class="s1">(X\geq a)$ (inconnue)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">expectation</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\mathbb</span><span class="si">{E}</span><span class="s1">[X]/a$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4_VC_theory_6_0.png" src="../_images/4_VC_theory_6_0.png" />
</div>
</div>
<p>La quantit√© <span class="math notranslate nohighlight">\(\mathbb{E}\big[X\big]/a\)</span> est potentiellement sup√©rieure √† <span class="math notranslate nohighlight">\(1\)</span> et nous avons contraint matplotlib √† n‚Äôafficher que les valeurs entre <span class="math notranslate nohighlight">\(0\)</span> et <span class="math notranslate nohighlight">\(1\)</span>. Pour cette exemple, nous aurions pu tr√®s na√Øvement calculer la probabilit√© nous-m√™me et nous n‚Äôavions aucun int√©r√™t √† passer par une in√©galit√© de concentration. Cependant, dans certains probl√®mes, c‚Äôest in√©vitable !</p>
</div>
<div class="section" id="ii-en-supposant-mathcal-h-infty">
<h2>II. En supposant <span class="math notranslate nohighlight">\(|\mathcal{H}|&lt;\infty\)</span><a class="headerlink" href="#ii-en-supposant-mathcal-h-infty" title="Permalink to this headline">¬∂</a></h2>
<hr class="docutils" />
<p><strong>Le Union Bound.</strong></p>
<p>Nous avons tous vu au lyc√©e la formule suivante :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\big(A\cup B\big)=\mathbb{P}\big(A\big)+\mathbb{P}\big(B\big)-\mathbb{P}\big(A\cap B\big).\]</div>
<p>Si on √©limine la probabilit√© li√©e √† l‚Äôintersection, nous avons :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\big(A\cup B\big)\leq\mathbb{P}\big(A\big)+\mathbb{P}\big(B\big)\]</div>
<p>De mani√®re plus g√©n√©rale, imaginons une famille d‚Äô√©v√®nements <span class="math notranslate nohighlight">\(A_i\)</span>, <span class="math notranslate nohighlight">\(i\leq N\)</span>, tels que <span class="math notranslate nohighlight">\(\forall i\leq N\)</span>, nous avons <span class="math notranslate nohighlight">\(\mathbb{P}(A_i)\leq K\)</span> (la probabilit√© est major√©e par une quantit√© <span class="math notranslate nohighlight">\(K\)</span>. Nous avons alors l‚Äôin√©galit√© suivante :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\big(\cup_i A_i\big)\leq \sum_i \mathbb{P}\big(A_i\big)\leq \sum_i K\leq KN.\]</div>
<p>C‚Äôest ce qu‚Äôon appelle un <em>union bound</em> : on majore gr√¢ce √† une in√©galit√© li√©e √† l‚Äôunion.</p>
<hr class="docutils" />
<p>Soit une famille de fonction <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> de <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> dans <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> telle que <span class="math notranslate nohighlight">\(|\mathcal{H}|&lt;\infty\)</span>. Et soit <span class="math notranslate nohighlight">\(h\in\mathcal{H}\)</span>. Nous avons d√©fini le risque et le risque empirique not√©s respectivement <span class="math notranslate nohighlight">\(L_n\)</span> et <span class="math notranslate nohighlight">\(L\)</span>. La probabilit√© que nous souhaiterions voir la plus petite possible est <span class="math notranslate nohighlight">\(\mathbb{P}\big(|L_n(h)-L(h)|&gt;\epsilon)\)</span>. Nous pouvons la majorer gr√¢ce √† l‚Äô<a class="reference external" href="https://fr.wikipedia.org/wiki/In%C3%A9galit%C3%A9_de_Hoeffding">in√©galit√© d‚ÄôHoeffding</a> :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\big(|L_n(h)-L(h)|&gt;\epsilon) \leq 2 e^{-2\epsilon^2n}.\]</div>
<p>Cela nous dit que la probabit√© que notre estimateur empirique s‚Äô√©carte de plus de <span class="math notranslate nohighlight">\(\epsilon\)</span> de son esp√©rance d√©cro√Æt exponentiellement vite lorsque la taille du jeu de donn√©es augmente !</p>
<p>Cependant, pour pouvoir garantir que l‚Äôapprentissage se fera bien (i.e. qu‚Äôon choisira un bon minimiseur du risque empirique), nous devons majorer la probabilit√© que TOUTES les fonctions soient bien estim√©es. Pour cela, nous utilisons le <em>union bound</em> :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{P}\big(\sup_{h\in\mathcal{H}} |L_n(h)-L(h)|&gt;\epsilon\big)&amp;=\mathbb{P}\big(\cup_{h\in\mathcal{H}} |L_n(h)-L(h)|&gt;\epsilon\big)\\
&amp;\leq \sum_{h\in\mathcal{H}}\mathbb{P}\big(\sup_{h\in\mathcal{H}} |L_n(h)-L(h)|&gt;\epsilon\big)\\
&amp;\leq |\mathcal{H}|2 e^{-2\epsilon^2n}.
\end{aligned}\end{split}\]</div>
<p>C‚Äôest un premier r√©sultat fondamental qui nous permet de dire que si <span class="math notranslate nohighlight">\(|\mathcal{H}|&lt;\infty\)</span>, alors on peut r√©duire uniform√©ment (i.e. pour toutes fonctions de <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>) les d√©viations entre le risque empirique et son esp√©rance et donc que le choix du minimiseur empirique ne sera pas mauvais (pour peu que le jeu de donn√©es soit assez grand).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">scale</span><span class="p">):</span>
    <span class="n">H_size</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.04</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2000</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">H_size</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">epsilon</span><span class="p">):</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="o">*</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">e</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">n</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\epsilon=$</span><span class="si">{}</span><span class="s1">, $|H|=$</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">k</span><span class="p">)))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="kc">None</span> <span class="k">if</span> <span class="n">scale</span> <span class="o">==</span> <span class="s1">&#39;log&#39;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot</span><span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4_VC_theory_11_0.png" src="../_images/4_VC_theory_11_0.png" />
<img alt="../_images/4_VC_theory_11_1.png" src="../_images/4_VC_theory_11_1.png" />
</div>
</div>
</div>
<div class="section" id="iii-le-cas-general-mathcal-h-infty">
<h2>III. Le cas g√©n√©ral : <span class="math notranslate nohighlight">\(|\mathcal{H}|=\infty\)</span><a class="headerlink" href="#iii-le-cas-general-mathcal-h-infty" title="Permalink to this headline">¬∂</a></h2>
<p>Le cas g√©n√©ral permet √©galement de conclure pour des cas o√π le cardinal serait fini.</p>
<p>L‚Äôintuition derri√®re le cas g√©n√©ral est qu‚Äôau lieu de consid√©rer toutes les fonctions de <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>, nous pouvons discr√©tiser <span class="math notranslate nohighlight">\(\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|\)</span> de mani√®re √† ne consid√©rer que les ‚Äúvaleurs effectives‚Äù que notre classe de fonctions peut prendre sur notre jeu de donn√©e. Ainsi, si on consid√®re <span class="math notranslate nohighlight">\(S_1=\{(X_1, Y_1)\}\)</span>, alors, quelque soit <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>, il ne peut y avoir au plus que deux fonctions diff√©rentes sur <span class="math notranslate nohighlight">\(S_1\)</span> : celle qui retourne <span class="math notranslate nohighlight">\(1\)</span> pour <span class="math notranslate nohighlight">\(X_1\)</span> et celle qui retourne <span class="math notranslate nohighlight">\(0\)</span> pour <span class="math notranslate nohighlight">\(X_1\)</span>.</p>
<p><em>A priori</em>, le nombre de valeur que peut prendre <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> sur un jeu de donn√©es <span class="math notranslate nohighlight">\(S_n\)</span> d√©pendra du tirage du jeu de donn√©es. Pour cela, nous pouvons majorer cette quantit√© en ne consid√©rant que le ‚Äúpire des cas‚Äù. C‚Äôest l‚Äôobjet de la d√©finition suivante :</p>
<hr class="docutils" />
<p><strong>La fonction de croissance :</strong></p>
<div class="math notranslate nohighlight">
\[\tau_\mathcal{H}(n)=\max_{\{X_1, ..., X_n\}}|\{h(X_1), ..., h(X_n):\ h\in\mathcal{H}\}|.\]</div>
<p>C‚Äôest le plus grand nombre de mani√®res diff√©rentes qu‚Äôune classe de fonction pourrait lab√©liser un jeu de donn√©es de taille <span class="math notranslate nohighlight">\(n\)</span>. C‚Äôest le plus grand nombre configuration de taille <span class="math notranslate nohighlight">\(n\)</span> atteignables par notre classe de fonctions.</p>
<hr class="docutils" />
<p>Notre objectif ici est de d√©montrer le r√©sultat suivant :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\Big(\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|&gt;\epsilon\Big)\leq 8 \tau_{\mathcal{H}}(n)e^{-n\epsilon^2/32}\]</div>
<hr class="docutils" />
<div class="section" id="preuve">
<h3>PREUVE<a class="headerlink" href="#preuve" title="Permalink to this headline">¬∂</a></h3>
<p>On remarque que si <span class="math notranslate nohighlight">\(n\epsilon^2&lt;2\)</span>, alors la partie droite de l‚Äôin√©galit√© est sup√©rieure √† <span class="math notranslate nohighlight">\(1\)</span> et l‚Äôin√©galit√© est trivialement vraie. Supposons <span class="math notranslate nohighlight">\(n\epsilon^2\geq 2\)</span>.</p>
<p><strong>√âtape 1 : Sym√©trisation par un √©chantillon fant√¥me (√©chantillon de test).</strong></p>
<p>Construisons un jeu de donn√©es de test virtuel <span class="math notranslate nohighlight">\(S_n^\prime=\{(X_i^\prime, Y_i^\prime)\}_{i\leq n}\)</span> tel que <span class="math notranslate nohighlight">\(S_n\sim S_n^\prime\)</span>. Notons <span class="math notranslate nohighlight">\(L_n^\prime\)</span> le risque empirique associ√© √† cet √©chantillon. Supposons <span class="math notranslate nohighlight">\(n\epsilon^2\geq 2\)</span>, alors nous avons :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\Big(\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|&gt;\epsilon\Big)\leq 2 \mathbb{P}\Big(\sup_{h\in\mathcal{H}}|L_n(h)-L_n^\prime(h)|&gt;\epsilon/2\Big)\]</div>
<p>Pour voir cela, notons <span class="math notranslate nohighlight">\(h^\star\)</span> une fonction telle que <span class="math notranslate nohighlight">\(|L_n(h^\star)-L(h^\star)|&gt;\epsilon\)</span> si une telle fonction existe, sinon une fonction fix√©e au hasard. Nous avons alors :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{P}\Big(\sup_{h\in\mathcal{H}}|L_n(h)-L_n^\prime(h)|&gt;\epsilon/2\Big)&amp;\geq \mathbb{P}\Big(|L_n(h^\star)-L_n^\prime(h^\star)|&gt;\epsilon/2\Big)\\
&amp;\geq\mathbb{P}\Big(|L_n(h^\star)-L(h^\star)|&gt;\epsilon, |L_n^\prime(h^\star)-L(h^\star)|&lt;\epsilon/2\Big)\\
&amp;=\mathbb{E}\Big[\textbf{1}\{|L_n(h^\star)-L(h^\star)|&gt;\epsilon\}\mathbb{P}\Big(|L_n^\prime(h^\star)-L(h^\star)|&lt;\epsilon/2|Z_1,\ldots, Z_n\Big)\Big]
\end{aligned}\end{split}\]</div>
<p>o√π <span class="math notranslate nohighlight">\(Z_i=(X_i, Y_i)\)</span>. En utilisant l‚Äô<a class="reference external" href="https://fr.wikipedia.org/wiki/In%C3%A9galit%C3%A9_de_Bienaym%C3%A9-Tchebychev">In√©galit√© de Bienaym√©-Tchebychev</a>, nous avons :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{P}\Big(|L_n^\prime(h^\star)-L(h^\star)|&lt;\epsilon/2|Z_1,\ldots, Z_n\Big)&amp;\geq 1-\frac{L(h^\star)(1-L(h^\star))/n}{(\epsilon/2)^2}\\
&amp;\geq 1-\frac{1/4}{n\epsilon^2/4}\geq \frac{1}{2}.
\end{aligned}\end{split}\]</div>
<p>(car <span class="math notranslate nohighlight">\(n\epsilon^2&gt;2\)</span>).
Ainsi :</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\Big[\textbf{1}\{|L_n(h^\star)-L(h^\star)|&gt;\epsilon\}\mathbb{P}\Big(|L_n^\prime(h^\star)-L(h^\star)|&lt;\epsilon/2|Z_1,\ldots, Z_n\Big)\Big] \geq \mathbb{E}\Big[\textbf{1}\{|L_n(h^\star)-L(h^\star)|&gt;\epsilon\}\Big]\frac{1}{2}=\mathbb{P}\Big(|L_n(h^\star)-L(h^\star)|&gt;\epsilon\Big)\frac{1}{2},\]</div>
<p>et nous obtenons le r√©sultat voulu.</p>
<p>Cette premi√®re √©tape nous dit que comparer le score empirique de notre risque par rapport √† son esp√©rance est √† peu pr√®s la m√™me chose que le comparer avec un jeu de test. Testons cela au travers d‚Äôune petite exp√©riences avec des tirages binomiaux normalis√©s.</p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">H</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">]</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">epsilon_list</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>
<span class="c1"># on r√©p√®te l&#39;exp√©rience pour calculer empiriquement la probabilit√©</span>
<span class="n">size</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="k">for</span> <span class="n">epsilon</span> <span class="ow">in</span> <span class="n">epsilon_list</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;*&#39;</span> <span class="o">*</span> <span class="mi">20</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Epsilon=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epsilon</span><span class="p">))</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">H</span><span class="p">:</span>
        <span class="n">Ln</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span><span class="o">/</span><span class="n">n</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">Ln</span><span class="o">-</span><span class="n">h</span><span class="p">)</span><span class="o">&gt;</span><span class="n">epsilon</span><span class="p">)</span>
    <span class="n">empirical_probability_1</span> <span class="o">=</span> <span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span> <span class="o">/</span> <span class="n">size</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;* P(sup |L_n-L|&gt;epsilon)=&#39;</span><span class="p">,</span> <span class="n">empirical_probability_1</span><span class="p">)</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">H</span><span class="p">:</span>
        <span class="n">binom</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span><span class="o">/</span><span class="n">n</span>
        <span class="n">binom_ghost</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span><span class="o">/</span><span class="n">n</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">binom</span><span class="o">-</span><span class="n">binom_ghost</span><span class="p">)</span><span class="o">&gt;</span><span class="n">epsilon</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">empirical_probability_2</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">size</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;* P(sup |L_n-L_n</span><span class="se">\&#39;</span><span class="s1">|&gt;epsilon)=&#39;</span><span class="p">,</span> <span class="n">empirical_probability_2</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;* 2xP(sup |L_n-L_n</span><span class="se">\&#39;</span><span class="s1">|&gt;epsilon)=&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">empirical_probability_2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>********************
Epsilon=0.01
* P(sup |L_n-L|&gt;epsilon)= 1.0
* P(sup |L_n-L_n&#39;|&gt;epsilon)= 0.999
* 2xP(sup |L_n-L_n&#39;|&gt;epsilon)= 1.998
********************
Epsilon=0.1
* P(sup |L_n-L|&gt;epsilon)= 0.31
* P(sup |L_n-L_n&#39;|&gt;epsilon)= 0.926
* 2xP(sup |L_n-L_n&#39;|&gt;epsilon)= 1.852
********************
Epsilon=0.2
* P(sup |L_n-L|&gt;epsilon)= 0.002
* P(sup |L_n-L_n&#39;|&gt;epsilon)= 0.613
* 2xP(sup |L_n-L_n&#39;|&gt;epsilon)= 1.226
</pre></div>
</div>
</div>
</div>
<p>La seconde √©tape va nous permettre d‚Äô√©limiter ce jeu fant√¥me auquel nous n‚Äôavons pas acc√®s. Aussit√¥t mis, aussit√¥t retir√©.</p>
<hr class="docutils" />
<p><strong>√âtape 2 : Sym√©trisation avec des signes al√©atoires.</strong></p>
<p>Nous avons donc la variable suivante :</p>
<div class="math notranslate nohighlight">
\[\sup_{h\in\mathcal{H}}|L_n(h)-L_n^\prime(h)|=\sup_{h\in\mathcal{H}}\frac{1}{n}|\sum_i\textbf{1}\{h(X_i)\neq Y_i\}-\textbf{1}\{h(X_i^\prime)\neq Y_i^\prime\}|\]</div>
<p>Puisque nos variables <span class="math notranslate nohighlight">\(\textbf{1}\{h(X_i)\neq Y_i\}\)</span> et <span class="math notranslate nohighlight">\(\textbf{1}\{h(X_i^\prime)\neq Y_i^\prime\}\)</span> sont iid, cela revient exactement √†</p>
<div class="math notranslate nohighlight">
\[\sup_{h\in\mathcal{H}}\frac{1}{n}|\sum_i\sigma_i(\textbf{1}\{h(X_i)\neq Y_i\}-\textbf{1}\{h(X_i^\prime)\neq Y_i^\prime\})|,\]</div>
<p>o√π <span class="math notranslate nohighlight">\(\sigma_i\)</span> est une variable de Rademacher (i.e. <span class="math notranslate nohighlight">\(\mathbb{P}\big(\sigma_i=-1\big)=\mathbb{P}\big(\sigma_i=+1\big)=0.5\)</span>).</p>
<p>Nous avons donc :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\Big(\sup_{h\in\mathcal{H}}\frac{1}{n}|\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}-\sigma_i\textbf{1}\{h(X_i^\prime\neq Y_i^\prime\})|&gt;\epsilon/2\Big)\]</div>
<p>Et au moins l‚Äôun deux deux termes <span class="math notranslate nohighlight">\(|\frac{1}{n}\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}|\)</span> doit √™tre sup√©rieur √† <span class="math notranslate nohighlight">\(\epsilon/4\)</span> pour que la somme soit sup√©rieure √† <span class="math notranslate nohighlight">\(\epsilon/2\)</span> (v√©rifier par contradiction). Ainsi, en appliquant le <em>union bound</em>, nous avons :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{P}\Big(\sup_{h\in\mathcal{H}}\frac{1}{n}|\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}-\sigma_i\textbf{1}\{h(X_i^\prime\neq Y_i^\prime\})&amp;|&gt;\epsilon/2\Big) \\
&amp;\leq\mathbb{P}\Big(\sup_{h\in\mathcal{H}}\frac{1}{n}|\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}|&gt;\epsilon/4\Big)+\mathbb{P}\Big(\sup_{h\in\mathcal{H}}\frac{1}{n}|\sigma_i\textbf{1}\{h(X_i^\prime\neq Y_i^\prime\}|&gt;\epsilon/4\Big)\\
&amp;=2\mathbb{P}\Big(\sup_{h\in\mathcal{H}}\frac{1}{n}|\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}|&gt;\epsilon/4\Big)
\end{aligned}\end{split}\]</div>
<hr class="docutils" />
<hr class="docutils" />
<p><strong>√âtape 3 : Conditionnement.</strong>
Jusqu‚Äôici, nous consid√©rions le jeu de donn√©es comme une variable al√©atoire. Fixons le et √©tudions un cas particulier. Notons <span class="math notranslate nohighlight">\(z_1, \ldots, z_n\in\mathcal{X}\)</span> cette r√©alisation.</p>
<p>Nous avons donc :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\Big(\sup_{h\in\mathcal{H}}\frac{1}{n}|\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}|&gt;\epsilon/4|Z_1=z_1, \ldots, Z_n=z_n\Big).\]</div>
<p>Le nombre de configuration √† tester est justement la fonction de croissance qui nous indique toutes les valeurs que peut prendre notre classe de fonctions <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> sur un jeu de donn√©es fix√© de taille <span class="math notranslate nohighlight">\(n\)</span>. En appliquant le <em>union bound</em> √† nouveau, nous obtenons donc :</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\mathbb{P}\Big(\sup_{h\in\mathcal{H}}\frac{1}{n}|\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}|&gt;\epsilon/4|Z_1, \ldots, Z_n\Big)\leq\tau_\mathcal{H}(n)\sup_{h\in\mathcal{H}}\mathbb{P}\Big(\frac{1}{n}|\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}|&gt;\epsilon/4|Z_1, \ldots, Z_n\Big).
\end{aligned}\]</div>
<p>De plus, en appliquant <a class="reference external" href="https://fr.wikipedia.org/wiki/In%C3%A9galit%C3%A9_de_Hoeffding">l‚Äôin√©galit√© d‚ÄôHoeffding</a> , nous pouvons majorer la probabilit√© de droite quelque soit la fonction :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\Big(\frac{1}{n}|\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}|&gt;\epsilon/4|Z_1, \ldots, Z_n\Big)\leq 2e^{-n\epsilon^2/32}\]</div>
<p>Cette variable ne d√©pend pas du conditionnement et nous pouvons donc prendre l‚Äôesp√©rance :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\Big(\frac{1}{n}|\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}|&gt;\epsilon/4\Big)=\mathbb{E}\Big[\mathbb{P}\Big(\frac{1}{n}|\sum_i\sigma_i\textbf{1}\{h(X_i)\neq Y_i\}|&gt;\epsilon/4|Z_1, \ldots, Z_n\Big)\Big] \leq 2e^{-n\epsilon^2/32}\]</div>
<p><strong>Conclusion.</strong></p>
<p>En combinait les 3 √©tapes pr√©c√©dentes, nous obtenons ainsi :</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\Big(\sup_{h\in\mathcal{H}}|L_n(h)-L(h)|&gt;\epsilon\Big)\leq 8 \tau_{\mathcal{H}}(n)e^{-n\epsilon^2/32}\]</div>
<hr class="docutils" />
<p>Des r√©sultats beaucoup plus stricts existent.</p>
</div>
</div>
<div class="section" id="iv-la-dimension-vc">
<h2>IV. La dimension VC<a class="headerlink" href="#iv-la-dimension-vc" title="Permalink to this headline">¬∂</a></h2>
<p>Le th√©or√®me pr√©c√©dent est tr√®s int√©ressant, mais sans hypoth√®se sur notre classe de fonction <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>, nous pourrions tr√®s bien avoir :</p>
<div class="math notranslate nohighlight">
\[\tau_\mathcal{H}(n)=2^n,\]</div>
<p>et cela nous donnerait :</p>
<div class="math notranslate nohighlight">
\[\lim_{n\rightarrow\infty}8 \cdot 2^ne^{-n\epsilon^2/32}=\infty,\]</div>
<p>ce qui nous emp√™cherait de conclure !</p>
<p>Il se trouve que pour certaines classes de fonctions (et nous verront des exemples), quand bien m√™me <span class="math notranslate nohighlight">\(|\mathcal{H}|=\infty\)</span>, nous n‚Äôaurions pas <span class="math notranslate nohighlight">\(\tau_\mathcal{H}(n)=2^n\)</span>.</p>
<hr class="docutils" />
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Soit <span class="math notranslate nohighlight">\(\mathcal{X}=\mathbb{R}\)</span>, <span class="math notranslate nohighlight">\(\mathcal{Y}=\{0, 1\}\)</span> et <span class="math notranslate nohighlight">\(\mathcal{H}=\{h_s(x)=\textbf{1}\{x&gt;s\}:\ s\in\mathbb{R}\}\)</span>, l‚Äôensemble des fonctions seuil (on retourne <span class="math notranslate nohighlight">\(1\)</span> si <span class="math notranslate nohighlight">\(x&gt;s\)</span> et <span class="math notranslate nohighlight">\(0\)</span> sinon). Montrer que <span class="math notranslate nohighlight">\(\tau_\mathcal{H}(2)=3\)</span> et non <span class="math notranslate nohighlight">\(2^2=4\)</span>.</strong></p>
<p><span style="color:green"><strong>R√©ponse :</strong></span> <strong>Soit <span class="math notranslate nohighlight">\(x_1, x_2\in\mathcal{X}\)</span> tels que <span class="math notranslate nohighlight">\(x_1&lt;x_2\)</span> sans perte de g√©n√©ralit√©. La configuration (1, 0) est atteignable si <span class="math notranslate nohighlight">\(s&lt;x_1\)</span> et <span class="math notranslate nohighlight">\(x_2 &lt; s\)</span> entra√Ænant une contradiction. √Ä l‚Äôinverse les configurations (0, 0), (0, 1) et (1, 1) sont facilement atteignables.</strong></p>
<hr class="docutils" />
<p>Nous appelons la dimension VC ou VCdim d‚Äôune classe de fonctions <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> le plus grand jeu de donn√©es S_n tel que <span class="math notranslate nohighlight">\(\tau_\mathcal{H}(n)=2^n\)</span>. Plus formellement, nous avons :</p>
<div class="math notranslate nohighlight">
\[\text{VCdim}(\mathcal{H})=\max_{\tau_{\mathcal{H}}(n)=2^n}n\]</div>
<p>L‚Äôint√©r√™t cl√© de cette propri√©t√© nous vient du lemme suivant :</p>
<hr class="docutils" />
<p><strong>Lemme de Sauer.</strong></p>
<p>Soit <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> notre classe de fonctions telle que <span class="math notranslate nohighlight">\(\text{VCdim}(\mathcal{H})\leq d&lt;\infty\)</span>. Nous avons alors <span class="math notranslate nohighlight">\(\forall n&gt;0\)</span> :</p>
<div class="math notranslate nohighlight">
\[\tau_\mathcal{H}(n)\leq \sum_{i=0}^d{n\choose i}.\]</div>
<p>De plus, si <span class="math notranslate nohighlight">\(n&gt;d+1\)</span>, alors :</p>
<div class="math notranslate nohighlight">
\[\tau_\mathcal{H}(n)\leq\Bigg(\frac{en}{d}\Bigg)^d.\]</div>
<hr class="docutils" />
<p>Ainsi, la fonction de croissance ne cro√Æt plus exponentiellement vite mais polynomialement d√®s qu‚Äôon d√©passe la dimension VC de notre classe de fonctions. Cela implique que notre majorant de g√©n√©ralisation converge vers <span class="math notranslate nohighlight">\(0\)</span>, qu‚Äôon puisse estimer l‚Äôerreur de nos fonctions correctement et donc que le choix du minimiseur du risque empirique est un bon choix : ce r√©sultat est ce qu‚Äôon appelle le <em>th√©or√®me fondamental du machine learning</em>.</p>
<hr class="docutils" />
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Soit <span class="math notranslate nohighlight">\(\mathcal{X}=\mathbb{R}^2\)</span>, <span class="math notranslate nohighlight">\(\mathcal{Y}=\{0, 1\}\)</span> et <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> l‚Äôensemble des classifieurs lin√©aires de <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>. D√©montrer que la dimension VC de <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> est au moins <span class="math notranslate nohighlight">\(3\)</span>.</strong></p>
<div class="toggle docutils container">
<p><span style="color:green"><strong>R√©ponse :</strong></span> <strong>On v√©rifie facilement avec un dessin que <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> classifie bien toutes les configurations d‚Äôun jeu de donn√©es de taille <span class="math notranslate nohighlight">\(3\)</span>.</strong></p>
</div>
<hr class="docutils" />
<hr class="docutils" />
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Soit <span class="math notranslate nohighlight">\(\mathcal{H}_{=k}\)</span> l‚Äôensemble des classifieurs qui ne peuvent associer le label <span class="math notranslate nohighlight">\(1\)</span> qu‚Äô√† <em>exactement</em> <span class="math notranslate nohighlight">\(k\)</span> √©l√©ments de <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>. Trouver la dimension VC. On supposera que <span class="math notranslate nohighlight">\(|\mathcal{X}|\geq k\)</span>.</strong></p>
<div class="toggle docutils container">
<p><span style="color:green"><strong>R√©ponse :</strong></span> <strong>La dimension VC d‚Äôun tel ensemble est <span class="math notranslate nohighlight">\(\text{VCdim}(\mathcal{H})=\text{min}(k, |\mathcal{X}|-k)\)</span>. Montrons (1) que <span class="math notranslate nohighlight">\(\text{VCdim}(\mathcal{H})\leq k\)</span>. Soit un jeu de donn√©es <span class="math notranslate nohighlight">\(S_{k+1}\)</span>, alors il n‚Äôest pas possible de mettre <span class="math notranslate nohighlight">\(1\)</span> √† tous les points. Montrons (2) que <span class="math notranslate nohighlight">\(\text{VCdim}(\mathcal{H})\leq |\mathcal{X}|-k\)</span>. Soit <span class="math notranslate nohighlight">\(S_{|\mathcal{X}|-k+1}\)</span>, alors, il n‚Äôest pas possible de mettre <span class="math notranslate nohighlight">\(0\)</span> √† tous les points. Montrons (3) maintenant que <span class="math notranslate nohighlight">\(\text{VCdim}(\mathcal{H})\geq \text{min}(k, |\mathcal{X}|-k)\)</span>. Soit <span class="math notranslate nohighlight">\(S_n\)</span> tel que <span class="math notranslate nohighlight">\(n\leq \text{min}(k, |\mathcal{X}|-k)\)</span> alors toutes les configurations sont atteignables. En effet, il y a au plus <span class="math notranslate nohighlight">\(k\)</span> <span class="math notranslate nohighlight">\(1\)</span> √† mettre et il restera toujours au moins <span class="math notranslate nohighlight">\(k\)</span> <span class="math notranslate nohighlight">\(1\)</span> √† l‚Äôext√©rieur de <span class="math notranslate nohighlight">\(S_k\)</span>.</strong></p>
</div>
<hr class="docutils" />
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./3_logistic_regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="3_bayes_classifier.html" title="previous page">Le classifieur de Bayes ‚òïÔ∏è</a>
    <a class='right-next' id="next-link" href="../4_max_margin/0_propos_liminaire.html" title="next page">Les mod√®les <em>max-margin</em></a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By The LMIPR team<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>
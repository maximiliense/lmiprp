
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>La régression linéaire ☕️ &#8212; Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="L’optimisation ☕️☕️" href="2_optimization.html" />
    <link rel="prev" title="La régression" href="0_propos_liminaire.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Machine Learning
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../0_requisite/rappels_python.html">
   Rappels de Python et Numpy
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../1_what_is_ml/0_propos_liminaire.html">
   Le
   <em>
    Machine Learning
   </em>
   , c’est quoi ?
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/1_introduction_ml.html">
     <em>
      Machine learning
     </em>
     et malédiction de la dimension
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/2_regression_and_classification_trees.html">
     Les arbres de régression et de classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="0_propos_liminaire.html">
   La
   <em>
    régression
   </em>
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     La régression linéaire ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="2_optimization.html">
     L’optimisation ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3_interpolation.html">
     Interpolation ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="4_algo_proximal_lasso.html">
     Sous-différentiel et le cas du Lasso ☕️☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../3_logistic_regression/0_propos_liminaire.html">
   La
   <em>
    classification
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_logistic_regression/1_logistic_regression.html">
     La Régression Logistique ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_logistic_regression/2_fonctions_proxy.html">
     Les fonctions proxy ☕️☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_logistic_regression/3_bayes_classifier.html">
     Le classifieur de Bayes ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_logistic_regression/4_VC_theory.html">
     La théorie de Vapnik et Chervonenkis ☕️☕️☕️☕️ (💆‍♂️)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../4_max_margin/0_propos_liminaire.html">
   Les modèles
   <em>
    max-margin
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../4_max_margin/1_max_margin.html">
     Les modèles max-margin ☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5_ensembles/0_propos_liminaire.html">
   Les méthodes
   <em>
    ensemblistes
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/1_ensembles.html">
     Les méthodes ensemblistes ☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../6_autodiff/0_propos_liminaire.html">
   La différentiation automatique
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_autodiff/1_autodiff.html">
     La différentiation automatique et un début de
     <em>
      deep learning
     </em>
     ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_autodiff/2_filters_representation.html">
     Filtres et espace de représentation des réseaux de neurones ☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../7_regularization/0_propos_liminaire.html">
   La régularisation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/1_regularization_deep.html">
     Régularisation en deep learning ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/2_unsupervised_representation_learning.html">
     Unsupervised representation learning ☕️☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/3_multitask.html">
     L’apprentissage multi-tâches ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/4_adversarial.html">
     Les attaques adversaires ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/5_ridge.html">
     Une analyse de la régularisation Ridge ☕️☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../content.html">
   Content in Jupyter Book
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../markdown.html">
     Markdown Files
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../notebooks.html">
     Content with notebooks
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/2_linear_regression/1_linear_regression.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/maximiliense/lmiprp"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/maximiliense/lmiprp/issues/new?title=Issue%20on%20page%20%2F2_linear_regression/1_linear_regression.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/maximiliense/lmiprp/master?urlpath=tree/2_linear_regression/1_linear_regression.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#i-introduction">
   I. Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ii-construction-d-un-jeu-de-donnees">
   II. Construction d’un jeu de données
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iii-du-modele-statistique-a-l-optimisation">
   III. Du modèle statistique à l’optimisation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#la-fonction-objectif">
     La fonction objectif
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimisation-par-descente-de-gradient">
     Optimisation par Descente de gradient
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-vous-de-jouer">
     À vous de jouer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#l-algorithme-de-descente-de-gradient">
     L’algorithme de descente de gradient
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#les-equations-normales-de-la-regression-lineaire-la-solution-par-pseudo-inverse">
     Les équations normales de la régression linéaire : la solution par pseudo-inverse
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     À vous de jouer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#avec-sklearn">
     Avec sklearn
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iv-features-variables-explicatives-transformees">
   IV. Features - Variables explicatives transformées
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#construction-du-jeu-de-donnees-polynomial">
     Construction du jeu de données polynomial
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#solution-par-pseudo-inverse">
     Solution par pseudo-inverse
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#solution-sklearn">
     Solution Sklearn
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#v-validation-croisee">
   V. Validation croisée
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#construction-du-jeu-de-donnees">
     Construction du jeu de données
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimiser-une-fonction-est-il-suffisant-pour-parler-d-apprentissage">
     Optimiser une fonction est-il suffisant pour parler d’apprentissage ?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vi-l-effet-double-descente-bonus">
   VI. L’effet “double descente” (Bonus ?)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vii-regularisation">
   VII. Régularisation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     Construction du jeu de données
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sans-regularisation">
     Sans régularisation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#avec-regularisation-ell-1">
     Avec régularisation
     <span class="math notranslate nohighlight">
      \(\ell_1\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#avec-regularisation-ell-2">
     Avec régularisation
     <span class="math notranslate nohighlight">
      \(\ell_2\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#avec-regularisation-elastic-net">
     Avec régularisation
     <em>
      elastic-net
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#viii-selection-de-modeles">
   VIII. Selection de modèles
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     Construction du jeu de données
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#recherche-exhaustive">
     Recherche exhaustive
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#recherche-aleatoire">
     Recherche aléatoire
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ix-le-mot-de-la-fin">
   IX. Le mot de la fin
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="la-regression-lineaire">
<h1>La régression linéaire ☕️<a class="headerlink" href="#la-regression-lineaire" title="Permalink to this headline">¶</a></h1>
<p>Quelques liens pour aller plus loin :</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/maximiliense/lmpirp/blob/main/Notes/Overfitting.pdf">Overfitting</a></p></li>
<li><p><a class="reference external" href="https://github.com/maximiliense/lmpirp/blob/main/Notes/Regularization.pdf">Regularization</a></p></li>
<li><p><a class="reference external" href="https://github.com/maximiliense/lmpirp/blob/main/Notes/D_composition_QR_et_les_moindres_carr_s.pdf">Least square QR decomposition</a></p></li>
</ul>
<div class="section" id="i-introduction">
<h2>I. Introduction<a class="headerlink" href="#i-introduction" title="Permalink to this headline">¶</a></h2>
<p>La régression linéaire est un modèle cherchant à établir un lien linéaire entre des données d’observation et des données à prédire. Plus concrètement, les données observées sont décrites par un vecteur <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^d\)</span> et la variable à prédire, par une quantité scalaire (un réel) <span class="math notranslate nohighlight">\(y \in \mathbb{R}\)</span> (par un abus de langage important, <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> et <span class="math notranslate nohighlight">\(y\)</span> expriment à la fois une variable aléatoire et sa réalisation) et le lien s’exprime sous le format suivant :</p>
<p>\begin{equation}
y = \beta_0 +  x_1 \beta_1 + x_2 \beta_2 + x_3 \beta_3 + … + x_d \beta_d +\epsilon = \beta_0 + \sum_i^d x_i \beta_i+\epsilon,\ \epsilon\sim\mathcal{N}(0, \sigma)
\end{equation}</p>
<p>que l’on peut aussi écrire en notation vectorielle:</p>
<p>\begin{equation}
y = \beta_0  + \langle \boldsymbol{\beta}, \mathbf{x} \rangle +\epsilon,\ \epsilon\sim\mathcal{N}(0, \sigma)
\end{equation}</p>
<p>où <span class="math notranslate nohighlight">\(\boldsymbol{\beta} \in \mathbb{R}^d\)</span> et <span class="math notranslate nohighlight">\(\beta_0\in\mathbb{R}\)</span> correspondent respectivement au vecteur et au scalaire contenant les paramètres du “vrai” modèle qui défini le lien entre les données et que l’on va vouloir apprendre pour prédire la bonne valeur de <span class="math notranslate nohighlight">\(y\)</span> en fonction du vecteur <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. Le modèle linéaire ne peut prédire la variable <span class="math notranslate nohighlight">\(y\)</span> qu’à un bruit <span class="math notranslate nohighlight">\(\epsilon\)</span> près. Une fois ces paramètres appris par notre algorithme d’apprentissage, on pourra utiliser la fonction de prédiction <span class="math notranslate nohighlight">\(f_{\boldsymbol{\beta}}(\mathbf{x}) : \mathbb{R}^d \rightarrow \mathbb{R}\)</span> apprise pour prédire la valeur <span class="math notranslate nohighlight">\(y_{new}\)</span> associée à un nouveau vecteur <span class="math notranslate nohighlight">\(\mathbf{x_{new}}\)</span> que l’on n’a pas encore observé:</p>
<div class="math notranslate nohighlight">
\[\hat{y}_{new} = f_{\boldsymbol{\beta}}(\boldsymbol{x_{new}}) = \beta_0  + \langle \boldsymbol{\beta}, \boldsymbol{x_{new}} \rangle\]</div>
<p>Pour simplifier les calculs et les notations, on préfère que la fonction de prédiction puisse se calculer à partir d’une notation complètement vectorielle. C’est ce que l’on fait en pratique, en ajoutant une composante supplémentaire <span class="math notranslate nohighlight">\(x_0\)</span> au vecteur <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> égale à <span class="math notranslate nohighlight">\(1\)</span>:</p>
<p>\begin{align}
\mathbf{x} &amp;= \begin{bmatrix}
1 \
x_{1} \
\vdots \
x_{d}
\end{bmatrix},
\end{align}</p>
<p>de sorte à ce que la fonction de prédiction linéaire puisse s’exprimer simplement sous la forme du produit scalaire:</p>
<p>\begin{align}
f_{\boldsymbol{\beta}}(\mathbf{x}) &amp;= \langle \boldsymbol{\beta}, \mathbf{x} \rangle &amp;=
\begin{bmatrix}
\beta_{0} \<br />
\vdots \
\beta_{d}
\end{bmatrix}^T
\begin{bmatrix}
1 \
\vdots \
x_{d}
\end{bmatrix} &amp;= \sum_{i=0}^d x_i \beta_i=\langle \boldsymbol{x}, \boldsymbol{\beta}\rangle_{\mathbb{R}^{d+1}}
\end{align}</p>
<p>où cette fois <span class="math notranslate nohighlight">\(\boldsymbol{\beta} \in \mathbb{R}^{d+1}\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{x} \in \mathbb{R}^{d+1}\)</span> et <span class="math notranslate nohighlight">\(\langle \cdot, \cdot\rangle_{\mathbb{R}^{d+1}}\)</span> est le produit scalaire dans <span class="math notranslate nohighlight">\(\mathbb{R}^{d+1}\)</span>. Le but d’un algorithme d’apprentissage sera de trouver un estimateur <span class="math notranslate nohighlight">\(\boldsymbol{\hat{\beta}}\)</span> de <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> à partir d’un ensemble fini de <span class="math notranslate nohighlight">\(n\)</span> exemples d’apprentissage <span class="math notranslate nohighlight">\((\boldsymbol{x}, \langle \boldsymbol{\beta}, \boldsymbol{x} \rangle + \epsilon) \in \mathbb{R}^2\)</span> préalablement collectés. On notera <span class="math notranslate nohighlight">\(\mathcal{S}=\{(\boldsymbol{x_i}, y_i)\}_{i\leq n}\)</span> le jeu de données.</p>
<p>Nous commencerons par implémenter le cas simple d’une régréssion linéaire à une seule variable d’entrée et une seule variable de sortie qui pourra donc s’écrire sous la forme :</p>
<p>\begin{equation}
\hat{y} = f_{\boldsymbol{\beta}}(\mathbf{x}) = \beta_0  + \beta_1 x
\end{equation}</p>
<p>C’est à dire une “brave” fonction affine dont on pourra afficher la représentation graphique (une droite) sur une figure en 2 dimensions. Par la suite vous aurez donc à implémenter le calcul de la fonction de coût du modèle sur l’ensemble d’apprentissage, le calcul du gradient de cette fonction de coût ainsi que l’algorithme de descente de gradient qui, à partir du gradient, permet d’obtenir le vecteur <span class="math notranslate nohighlight">\(\hat{\beta}\)</span>.</p>
<p>La seconde partie de ce notebook étendra ces notions à des concepts plus compliqués.</p>
</div>
<div class="section" id="ii-construction-d-un-jeu-de-donnees">
<h2>II. Construction d’un jeu de données<a class="headerlink" href="#ii-construction-d-un-jeu-de-donnees" title="Permalink to this headline">¶</a></h2>
<p>Commençons tout d’abord par simuler notre jeu de données avec le modèle génératif suivant :</p>
<p>\begin{equation}
\boldsymbol{x} \sim \mathcal{N}(\mu, \sigma) \in \mathbb{R}
\end{equation}</p>
<p>ou <span class="math notranslate nohighlight">\(\sigma\)</span> correspond à la variance de la variable explicative. Nous choissons une règle arbitraire pour générer aléatoirement les paramètres du “vrai” modèle :</p>
<p>\begin{equation}
\boldsymbol{\beta} \sim \mathbb{U}(-1, 1)^2 \in \mathbb{R}^2
\end{equation}</p>
<p>où <span class="math notranslate nohighlight">\(\mathbb{U}^2\)</span> est la loi uniforme dans <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>. Enfin, le bruit est construit de la manière suivante :</p>
<p>\begin{equation}
\epsilon \sim \mathcal{N}(0, 1).
\end{equation}</p>
<p>Chaque exemple d’apprentissage correspond donc à un couple de réels <span class="math notranslate nohighlight">\((x_j, y_j = \beta_0  + \beta_1 x_j + \epsilon) \in \mathbb{R}^2\)</span>. Le code ci dessous construit et affiche le jeux de données ainsi que la représentation graphique de <span class="math notranslate nohighlight">\(f(x)=\beta_1x+\beta_0\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># on simule le vecteur de parametre</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># on construit un jeu de donnees de 10 points selon la methode </span>
<span class="c1"># decrite ci-dessus.</span>
<span class="k">def</span> <span class="nf">sample_data</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">add_noise</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">add_noise</span><span class="o">*</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">beta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">noise</span> <span class="o">+</span> <span class="n">beta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_data</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">add_noise</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># jouer avec le bruit</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># configuration generale de matplotlib</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">12.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>

<span class="c1"># plot de la fonction</span>
<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">ymin_</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="n">eps</span>
    <span class="n">ymax_</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">eps</span>
    <span class="n">min_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="n">eps</span>
    <span class="n">max_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="n">eps</span>
    <span class="k">if</span> <span class="n">beta</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">min_</span><span class="p">,</span> <span class="n">max_</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">y_</span> <span class="o">=</span> <span class="n">beta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_</span> <span class="o">+</span> <span class="n">beta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span> <span class="n">y_</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">func</span> <span class="o">=</span> <span class="p">[(</span><span class="n">func</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)]</span> <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">func</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">list</span> <span class="k">else</span> <span class="n">func</span>
            <span class="n">disp_legend</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">func</span><span class="p">:</span>
                <span class="n">y_</span> <span class="o">=</span> <span class="n">t</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x_</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">x_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)))</span>
                <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span> <span class="n">y_</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">t</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">disp_legend</span> <span class="o">=</span> <span class="n">disp_legend</span> <span class="ow">or</span> <span class="n">t</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="s1">&#39;&#39;</span>
            <span class="k">if</span> <span class="n">disp_legend</span><span class="p">:</span>
                <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="n">min_</span><span class="p">,</span> <span class="n">max_</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="n">ymin_</span><span class="p">,</span> <span class="n">ymax_</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    
<span class="c1"># on plot le dataset precedent</span>
<span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_7_0.png" src="../_images/1_linear_regression_7_0.png" />
</div>
</div>
<hr class="docutils" />
<p><span style="color:blue"><strong>Question :</strong></span> <strong>Que se passe-t-il si le bruit <span class="math notranslate nohighlight">\(\epsilon\)</span> est nul ? Quelle est alors la méthode la plus rapide pour trouver les paramètres du vrai modèle ?</strong></p>
<hr class="docutils" />
<p><strong><span style="color:green">Réponse:</span></strong> Prenons le cas d’une régression <span class="math notranslate nohighlight">\(f_\beta:\mathbb{R}\mapsto\mathbb{R}\)</span>. Si le bruit <span class="math notranslate nohighlight">\(\epsilon\)</span> est nul, alors l’ensemble des couples <span class="math notranslate nohighlight">\((x, y)\)</span> sont alignés sur la même droite. Ainsi, étant donné un jeu de données de taille <span class="math notranslate nohighlight">\(2\)</span>, <span class="math notranslate nohighlight">\((x_1, y_1)\)</span> et <span class="math notranslate nohighlight">\((x_2, y_2)\)</span> tel que <span class="math notranslate nohighlight">\(x_1\neq x_2\)</span>, nous pouvons calculer le coefficient directeur de la droite, noté <span class="math notranslate nohighlight">\(\beta_1\)</span>:</p>
<p>\begin{equation}
\beta_1=\frac{y_2-y_1}{x_2-x_1}
\end{equation}</p>
<p>Le coefficient directeur obtenu, nous pouvons calculer le biais <span class="math notranslate nohighlight">\(\beta_0\)</span>:</p>
<p>\begin{equation}
\beta_0=y_1-\beta_1x_1.
\end{equation}</p>
</div>
<div class="section" id="iii-du-modele-statistique-a-l-optimisation">
<h2>III. Du modèle statistique à l’optimisation<a class="headerlink" href="#iii-du-modele-statistique-a-l-optimisation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="la-fonction-objectif">
<h3>La fonction objectif<a class="headerlink" href="#la-fonction-objectif" title="Permalink to this headline">¶</a></h3>
<p>Nous souhaitons en pratique trouver un paramêtre <span class="math notranslate nohighlight">\(\boldsymbol{\hat{\beta}}\)</span> qui minimise le risque du modèle, c’est-à-dire la quantité d’erreur en espérance de n’importe quel modèle <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>. On notera <span class="math notranslate nohighlight">\(\boldsymbol{\beta}^\star\)</span> le “vrai” modèle, soit celui qui minimise le risque en espérance. Pour la régression linéaire, on peut définir ce risque comme :</p>
<div class="math notranslate nohighlight">
\[R(\boldsymbol{\beta}) = \mathbb{E}_{X\times Y}\Big[ (f_{\boldsymbol{\beta}}(\mathbf{X}) - Y)^2 \Big].\]</div>
<p>On ne sait pas calculer cette fonction. Cependant, on peut en avoir un estimateur via un jeu de données <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>, où <span class="math notranslate nohighlight">\(\mathcal{S} = \Big\{ \big(\boldsymbol{x_j}, y_j \big) \Big\}_{j\leq n}\)</span> est un jeu de données composé de <span class="math notranslate nohighlight">\(n\)</span> points indépendants et identiquement distribués selon le modèle génératif décrit précédement.</p>
<p>A défaut d’avoir accès au risque (i.e. à l’erreur en espérance), on peut utiliser une autre quantité qui consiste en la somme des carrés des erreurs de prédictions pour chaque exemple d’apprentissage, c’est <strong>le risque emprique</strong> :</p>
<p>\begin{equation}
J(\boldsymbol{\beta}) = R_{emp}(\boldsymbol{\beta}) = \frac{1}{n}\sum_j^n (f_{\boldsymbol{\beta}}(x_j) - y_j)^2
\end{equation}</p>
<p>où <span class="math notranslate nohighlight">\(f_{\boldsymbol{\beta}}(x_j) = \beta_0  + \beta_1 x_j\)</span>. On montre assez facilement que pour un <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> quelconque :</p>
<p>\begin{equation}
R(\boldsymbol{\beta})=\mathbb{E}_{\mathcal{S \sim \mathbb{P}_S}}\big[J(\boldsymbol{\beta})\big],
\end{equation}</p>
<p>Notons que minimiser ce risque empirique revient à chercher le maximum de vraisemblance du modèle statistique. Effectivement, avec l’hypothèse gaussienne, la vraissemblance de n’importe quel modèle de paramètres <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> pour un jeu de données <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> peut s’écrire :</p>
<p>\begin{equation}
L_{\mathcal{S}}(\boldsymbol{\beta}) \propto \prod_{\boldsymbol{x}\times y\in\mathcal{S}} \exp\Bigg(-\frac{\big(f_{\boldsymbol{\beta}}(\mathbf{x}) - y\big)^2}{2}\Bigg)
\end{equation}</p>
<p>Le paramètre maximisant la vraissamblance est aussi celui minimisant la log-vraissamblance négative :</p>
<div class="math notranslate nohighlight">
\[- \text{log} \Big( L_{\mathcal{S}}(\boldsymbol{\beta})\Big) = \sum_{\boldsymbol{x}\times y\in\mathcal{S}}\frac{\big(f_{\boldsymbol{\beta}}(\mathbf{x}) - y\big)^2}{2}\propto\sum_{\boldsymbol{x}\times y\in\mathcal{S}}\big(f_{\boldsymbol{\beta}}(\mathbf{x}) - y\big)^2\]</div>
<p>N’ayant accès au véritable risque, on cherche <span class="math notranslate nohighlight">\(\boldsymbol{\hat{\beta}}\)</span> tel que :</p>
<p>\begin{equation}
\boldsymbol{\hat{\beta}} = argmin_{\boldsymbol{\beta}} \Big[ - \log \Big( L_{\mathcal{S}}(\boldsymbol{\beta})\Big) \Big]
\end{equation}</p>
<p>Minimiser le risque emprique se traduit donc naturellement par un problème d’optimisation de la fonction de coût <span class="math notranslate nohighlight">\(J(\boldsymbol{\beta}) : \mathbb{R}^2 \rightarrow \mathbb{R}\)</span> (<span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span> dans notre exemple courant, <span class="math notranslate nohighlight">\(\mathbb{R}^{d+1}\)</span> dans le cas général) par rapport à <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>.</p>
<p>En pratique et pour des raisons de simplicité, on ne minimise pas <span class="math notranslate nohighlight">\(\sum_{\boldsymbol{x}\times y\in\mathcal{S}}\big(f_{\boldsymbol{\beta}}(\mathbf{x}) - y\big)^2\)</span> mais :</p>
<p>\begin{equation}
J(\boldsymbol{\beta}) = \frac{1}{2n}\sum_{\boldsymbol{x}\times y\in\mathcal{S}} (f_{\boldsymbol{\beta}}(x) - y)^2
\end{equation}</p>
<p>Le résultat est bien évidemment le même. La division par <span class="math notranslate nohighlight">\(2\)</span> est là pour simplifier l’expresion du gradient que l’on calculera et la division par <span class="math notranslate nohighlight">\(n\)</span> permet de rendre la norme du gradient indépendente de la taille de notre jeu de données. C’est une propriété importante pour l’algorithme de descente de gradient dont la taille des déplacements affecte sa stabilité.</p>
<p><strong>Note - Notation vectorielle de la régression linéaire :</strong> On peut aussi exprimer ce calcul avec une simple équation en notation vectorielle. Pour cela, on exprime dans un premier temps le résultat de la fonction de prédiction en notation vectorielle (il s’agit de la prédiction pour tout notre jeu de données) :</p>
<p>\begin{equation}
f_{\boldsymbol{\beta}}(\mathbf{X}) = \mathbf{X}\boldsymbol{\beta}\in\mathbb{R}^n
\end{equation}</p>
<p>où <span class="math notranslate nohighlight">\(\boldsymbol{\beta} \in \mathbb{R}^{d+1}\)</span> est une matrice de dimensions <span class="math notranslate nohighlight">\((d+1)\times 1\)</span> et <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> est une matrice de dimensions <span class="math notranslate nohighlight">\(n\times (d+1)\)</span> dont les <span class="math notranslate nohighlight">\(n\)</span> vecteurs lignes correspondent aux vecteurs d’apprentissage d’entrée. Dans notre cas (celui de la régression linéaire à <span class="math notranslate nohighlight">\(1\)</span> variable) la matrice prend la forme suivante :</p>
<p>\begin{equation}
\mathbf{X} =
\begin{pmatrix}
1 &amp; x_{1} \
. &amp; .\
1 &amp; x_{j} \
. &amp; .\
1 &amp; x_{n}
\end{pmatrix},\ \boldsymbol{\beta}=
\begin{bmatrix}
\beta_{0} \<br />
\beta_{1}
\end{bmatrix}
\end{equation}</p>
<p>La fonction de coût peut ainsi s’exprimer :</p>
<p>\begin{equation}
J(\boldsymbol{\beta}) = \frac{1}{2n} (\mathbf{X}\boldsymbol{\beta} - \mathbf{y})^T(\mathbf{X}\boldsymbol{\beta} - \mathbf{y})
\end{equation}</p>
<p>que l’on peut réécrire :</p>
<p>\begin{equation}
J(\boldsymbol{\beta}) = \frac{1}{2n} (\mathbf{\hat{y}} - \mathbf{y})^T(\mathbf{\hat{y}} - \mathbf{y}) =  \frac{1}{2n} ||\mathbf{\hat{y}} - \mathbf{y}||_2^2
\end{equation}</p>
<p>où <span class="math notranslate nohighlight">\(\mathbf{y} \in  \mathbb{R}^n\)</span> est le vecteur dont chacune des composantes <span class="math notranslate nohighlight">\(y_j\)</span> sont les valeurs à prédire à partir de leur <span class="math notranslate nohighlight">\(x_j\)</span> correspondant, et <span class="math notranslate nohighlight">\(\hat{y} \in  \mathbb{R}^n\)</span> correspond aux valeurs prédites par le modèle. On note ici que la fonction objectif à optimiser peut se calculer aisément en utilisant la norme euclidienne du vecteur d’erreur.</p>
</div>
<div class="section" id="optimisation-par-descente-de-gradient">
<h3>Optimisation par Descente de gradient<a class="headerlink" href="#optimisation-par-descente-de-gradient" title="Permalink to this headline">¶</a></h3>
<p>La descente de gradient est une méthode d’optimisation numérique permettant de trouver les valeurs des paramètres qui minimisent une fonction. Dans notre cas, nous voulons minimiser l’erreur de prédiction moyenne de notre modèle, fonction définie précédemment. Cette méthode d’optimisation consiste à calculer le gradient de notre fonction objectif par rapport aux paramètres courant du modèles et de les déplacer “petite” translation dans la direction opposée au gradient (i.e. le gradient donne la plus forte croissance et son opposé la plus forte décroissance).</p>
<p><strong>Définition générale du gradient d’une fonction à plusieurs variables :</strong> Il s’agit simplement du vecteur contenant les dérivées partielles de la fonction, c-à-d les dérivées de la fonction par rapport à chaque variable indépendamment des autres:</p>
<p>\begin{equation}
\nabla_{\boldsymbol{\beta}} J(\boldsymbol{\beta}) = \frac{\partial J(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} =
\begin{bmatrix}
\frac{\partial J(\beta)}{\partial \beta_0}\
\frac{\partial J(\beta)}{\partial \beta_1}\
\vdots \
\frac{\partial J(\beta)}{\partial \beta_d}
\end{bmatrix}
\end{equation}</p>
<p>En descente de gradient, la mise à jour de chaque paramètre <span class="math notranslate nohighlight">\(\beta_j\)</span> du modèle à l’itération <span class="math notranslate nohighlight">\(t\)</span> se fait donc avec la règle suivante:</p>
<p>\begin{equation}
\beta_j^{(t+1)} = \beta_j^{(t)} - \rho  \frac{\partial J(\beta^{(t)})}{\partial \beta_j}
\end{equation}</p>
<p>ou bien, en notation vectorielle:</p>
<p>\begin{equation}
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \rho  \nabla_{\boldsymbol{\beta}} J(\boldsymbol{\beta})^{(t)}
\end{equation}</p>
<p>où <span class="math notranslate nohighlight">\(\rho\)</span> est le learning rate (pas d’apprentissage). Un pas d’apprentissage <span class="math notranslate nohighlight">\(\rho\)</span> trop petit nous fera nous déplacer trop lentement et trop grand rendra l’optimisation instable.</p>
</div>
<div class="section" id="a-vous-de-jouer">
<h3>À vous de jouer<a class="headerlink" href="#a-vous-de-jouer" title="Permalink to this headline">¶</a></h3>
<hr class="docutils" />
<p><span style="color:blue"><strong>Question 1 :</strong></span> <strong>Complétez la méthode <span class="math notranslate nohighlight">\(\texttt{val}\)</span> de l’objet <span class="math notranslate nohighlight">\(\texttt{LeastSquare}\)</span> ci-dessous.</strong></p>
<p><span style="color:blue"><strong>Question 2 :</strong></span> <strong>Calculez les dérivées partielles <span class="math notranslate nohighlight">\(\partial J(\beta)/\partial \beta_0\)</span> et <span class="math notranslate nohighlight">\(\partial J(\beta)/\partial \beta_1\)</span> de la fonction de coût de notre modèle de régréssion linéaire. Complétez la méthode <span class="math notranslate nohighlight">\(\texttt{grad}\)</span> de l’objet <span class="math notranslate nohighlight">\(\texttt{LeastSquare}\)</span> ci dessous.</strong></p>
<p><strong><span style="color:orange">Indice</span></strong>  Rappellez vous que la dérivée d’une composition de fonction s’écrit <span class="math notranslate nohighlight">\((g \circ f)^\prime (x) = f^\prime(x) g^\prime(f(x))\)</span> et que la fonction de coût de notre modèle s’écrit :</p>
<p>\begin{equation}
J(\boldsymbol{\beta}) = \frac{1}{2n}\sum_j^n g(f_{\boldsymbol{\beta}}(x_j) - y_j)
\end{equation}</p>
<p>avec <span class="math notranslate nohighlight">\(g(z) = z ^ 2\)</span> et <span class="math notranslate nohighlight">\(f_{\boldsymbol{\beta}}(x_j) = \beta_0  + \beta_1 x_j\)</span>.</p>
<p><strong><span style="color:green">Réponse :</span></strong></p>
<p>\begin{equation}
\frac{\partial J(\beta)}{\partial \beta_j} = \frac{1}{n}\sum_i^n (f_{\boldsymbol{\beta}}(\mathbf{x_i}) - y_i)   x_i^j
\end{equation}</p>
<p><span style="color:blue"><strong>Question 2<span class="math notranslate nohighlight">\({}^\star\)</span> :</strong></span> <strong>Calculez le gradient de la fonction <span class="math notranslate nohighlight">\(J(\boldsymbol{\beta})\)</span> en utilisant les dérivées matricielles. Complétez la méthode <span class="math notranslate nohighlight">\(\texttt{grad}\)</span> de l’objet <span class="math notranslate nohighlight">\(\texttt{LeastSquare}\)</span> avec le gradient en notation vectorielle.</strong></p>
<p><strong><span style="color:green">Réponse:</span></strong></p>
<p>Le gradient est obtenu par <span class="math notranslate nohighlight">\(\nabla_\beta J(\boldsymbol{\beta})=X^TX\boldsymbol{\beta}-X^T\boldsymbol{y}\)</span>.</p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Le code ci-dessous permettra d&#39;afficher notre fonction de cout (le risque empirique)</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="o">%</span><span class="k">config</span> InlineBackend.print_figure_kwargs = {&#39;bbox_inches&#39;:None}
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">12.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">plot_loss_contour</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">param_trace</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">three_dim</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rotate</span><span class="o">=</span><span class="mi">12</span><span class="p">):</span>
    
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mgrid</span><span class="p">[</span><span class="nb">slice</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span> <span class="o">+</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">),</span>
                    <span class="nb">slice</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span> <span class="o">+</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)]</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">l</span><span class="o">.</span><span class="n">val</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]])</span>
    <span class="k">if</span> <span class="n">figsize</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">f</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">f</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">12.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">three_dim</span><span class="p">:</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">gca</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    
    <span class="k">if</span> <span class="n">three_dim</span><span class="p">:</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">viridis</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">levels</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
    <span class="c1">#</span>
    <span class="k">if</span> <span class="n">param_trace</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">three_dim</span><span class="p">:</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="mf">0.5</span>
            <span class="c1">#ax.scatter(param_trace[:, 0], param_trace[:, 1], param_trace[:, 2] + eps, </span>
            <span class="c1">#           color=&#39;blue&#39;, alpha=1)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">param_trace</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">param_trace</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">param_trace</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">eps</span><span class="p">,</span> 
                    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">rotate</span><span class="p">)</span>
                
        <span class="k">else</span><span class="p">:</span>
            <span class="n">param_trace</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">param_trace</span><span class="p">)</span> <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">param_trace</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">list</span> <span class="k">else</span> <span class="n">param_trace</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">param_trace</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">param_trace</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">param_trace</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">param_trace</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
            <span class="n">f</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="c1">#plt.show()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LeastSquare</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">LeastSquare</span><span class="o">.</span><span class="n">_format_ndarray</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">LeastSquare</span><span class="o">.</span><span class="n">_format_ndarray</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pos</span> <span class="o">=</span> <span class="mi">0</span>
        
    <span class="k">def</span> <span class="nf">_format_ndarray</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
        <span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="k">else</span> <span class="n">arr</span>
        <span class="k">return</span> <span class="n">arr</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">arr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">arr</span>
    
    <span class="k">def</span> <span class="nf">val</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">LeastSquare</span><span class="o">.</span><span class="n">_format_ndarray</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
        <span class="c1">####### Complete this part ######## or die ####################</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">beta</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">errors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">prediction</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">val</span> <span class="o">=</span> <span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="c1">###############################################################</span>
        <span class="k">return</span> <span class="n">val</span>
    
    <span class="k">def</span> <span class="nf">_shuffle</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">batch_size</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span> <span class="k">else</span> <span class="n">batch_size</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_pos</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">_pos</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_pos</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pos</span><span class="o">+</span><span class="n">batch_size</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pos</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_shuffle</span><span class="p">()</span>
            
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

        <span class="n">beta</span> <span class="o">=</span> <span class="n">LeastSquare</span><span class="o">.</span><span class="n">_format_ndarray</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
        
        <span class="c1">####### Complete this part ######## or die ####################</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">),</span> <span class="n">beta</span><span class="p">)</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span><span class="o">/</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1">###############################################################</span>
        <span class="k">return</span> <span class="n">grad</span>
    

<span class="n">l</span> <span class="o">=</span> <span class="n">LeastSquare</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;La valeur de la loss pour le vrai parametre est&#39;</span><span class="p">,</span> <span class="n">l</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">beta</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;La valeur du gradient pour le vrai parametre est</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">l</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">beta</span><span class="p">))</span>
<span class="n">plot_loss_contour</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">three_dim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plot_loss_contour</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">three_dim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>La valeur de la loss pour le vrai parametre est 1.1310123897991868
La valeur du gradient pour le vrai parametre est
 [[ 0.35210703]
 [-0.71255159]]
</pre></div>
</div>
<img alt="../_images/1_linear_regression_17_1.png" src="../_images/1_linear_regression_17_1.png" />
<img alt="../_images/1_linear_regression_17_2.png" src="../_images/1_linear_regression_17_2.png" />
</div>
</div>
<p>Attention, pour des raisons de temps de calcul, l’estimation du gradient n’est pas tout le temps faite sur tout le jeu de données mais sur une partie de celui-ci. Un estimateur calculé de cette manière là aura en espérance la même valeur qu’un gradient calculé sur toutes les données. On appelle généralement Descente de Gradient Stochastique oiu SGD une approche qui ne fait qu’estimer le gradient à partir d’un batch de données.</p>
<hr class="docutils" />
<p><span style="color:blue"><strong>Question :</strong></span> <strong>Saurez-vous retrouver dans le code ci-dessus ce qui permet de jouer sur la taille du batch lors du calcul du gradient ?</strong></p>
</div>
<hr class="docutils" />
<div class="section" id="l-algorithme-de-descente-de-gradient">
<h3>L’algorithme de descente de gradient<a class="headerlink" href="#l-algorithme-de-descente-de-gradient" title="Permalink to this headline">¶</a></h3>
<hr class="docutils" />
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Complétez le code de descente de gradient.</strong></p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GradientDescent</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="n">init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">LeastSquare</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.005</span><span class="p">,</span> <span class="n">nb_iterations</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">init</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">param_trace</span> <span class="o">=</span> <span class="p">[</span><span class="n">beta</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
        <span class="n">loss_trace</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">beta</span><span class="p">)]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_iterations</span><span class="p">):</span>
            <span class="c1">####### Complete this part ######## or die ####################</span>
            <span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
            <span class="c1">###############################################################</span>
            <span class="n">param_trace</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">beta</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">loss_trace</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">beta</span><span class="p">))</span>
            
        <span class="k">return</span> <span class="n">param_trace</span><span class="p">,</span> <span class="n">loss_trace</span>
        
<span class="n">gd</span> <span class="o">=</span> <span class="n">GradientDescent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">param_trace</span><span class="p">,</span> <span class="n">loss_trace</span> <span class="o">=</span> <span class="n">gd</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">nb_iterations</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">param_trace</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">param_trace</span><span class="p">)</span>
<span class="n">loss_trace</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">loss_trace</span><span class="p">)</span>
<span class="n">loss_trace</span> <span class="o">=</span> <span class="n">loss_trace</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">loss_trace</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">xyz</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">param_trace</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">loss_trace</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plot_loss_contour</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">param_trace</span><span class="o">=</span><span class="n">xyz</span><span class="p">,</span> <span class="n">three_dim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plot_loss_contour</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">param_trace</span><span class="o">=</span><span class="n">param_trace</span><span class="p">,</span> <span class="n">three_dim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_23_0.png" src="../_images/1_linear_regression_23_0.png" />
<img alt="../_images/1_linear_regression_23_1.png" src="../_images/1_linear_regression_23_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">clear_output</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="n">output</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">Output</span><span class="p">()</span>

<span class="nd">@output</span><span class="o">.</span><span class="n">capture</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">interactive_gradient_descent</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="n">clear_output</span><span class="p">()</span>
    <span class="n">param_trace</span> <span class="p">,</span> <span class="n">loss_trace</span> <span class="o">=</span> <span class="n">gd</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">)</span>
    <span class="n">plot_loss_contour</span><span class="p">(</span><span class="n">gd</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span> <span class="n">param_trace</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">14.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">widgets</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">interactive_gradient_descent</span><span class="p">,</span>
                 <span class="n">learning_rate</span><span class="o">=</span><span class="n">widgets</span><span class="o">.</span><span class="n">FloatSlider</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> 
                                                   <span class="n">continuous_update</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">readout_format</span><span class="o">=</span><span class="s1">&#39;.5f&#39;</span><span class="p">),</span>
                 <span class="n">batch_size</span><span class="o">=</span><span class="n">widgets</span><span class="o">.</span><span class="n">IntSlider</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">min</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                                              <span class="n">continuous_update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "c945e9a87742490396124b512d299f36"}
</script><script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "5a2d8bae14794814b9ffc0462b6268fc"}
</script></div>
</div>
<p><strong>Remarques et questions sur GD</strong> : On constate que la descente de gradient (GD) se déplace bien orthogonalement aux lignes de niveaux de la fonction de coût. C’est une propriété du gradient d’une fonction. À mesure qu’on avance vers le minimum de la fonction, on se déplace de plus en plus lentement vers ce dernier. Pouvez vous dire pourquoi intuitivement ou analytiquement en regardant l’expression du gradient que vous avez dérivé plus haut ? Soit <span class="math notranslate nohighlight">\(\boldsymbol{\beta}^{(0)} = [0.0,  0.0]^T\)</span> et <span class="math notranslate nohighlight">\(\rho\)</span>, montrez que GD converge necessairement vers la solution otpimale (cette question est interessante par rapport à la partie suivante sur les équations normales de la régression linéaire et la notion de solution par pseudo inverse).</p>
<p><strong>Remarques et questions sur SGD (avec un gradient estimé sur un sous-ensemble)</strong> : La propriété d’orthogonalité par rapport aux lignes de niveau de la fonction de coût est elle conservée dans ce cas ? Pourquoi ? Ne suit-on pourtant toujours pas le gradient ? Que pouvez vous dire sur la nature et la “vitesse” de convergence vers le minimum de la fonction ? Réfléchissez d’un point de vue calculatoire sur ce qui se passe sur des tailles d’échantillons très grandes ?</p>
</div>
<div class="section" id="les-equations-normales-de-la-regression-lineaire-la-solution-par-pseudo-inverse">
<h3>Les équations normales de la régression linéaire : la solution par pseudo-inverse<a class="headerlink" href="#les-equations-normales-de-la-regression-lineaire-la-solution-par-pseudo-inverse" title="Permalink to this headline">¶</a></h3>
<p>Comme calculé plus haut, l’expression du gradient est donnée par <span class="math notranslate nohighlight">\(X^TX\boldsymbol{\beta}-X^T\boldsymbol{y}\)</span>.  La fonction <span class="math notranslate nohighlight">\(J\)</span> étant coercive et convexe, elle admet au moins un minimum local/global. Les points critiques sont donnés en annulant le gradient :</p>
<p>\begin{equation}
X^TX\boldsymbol{\beta}-X^T\boldsymbol{y} = 0 \Leftrightarrow X^TX\boldsymbol{\beta}=X^t\boldsymbol{y}.
\end{equation}</p>
<p>Il s’agit des équations dites “normales”. Tout vecteur <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> solution de ces équations est donc nécessairement un minimiseur de <span class="math notranslate nohighlight">\(J(\boldsymbol{\beta})\)</span>.</p>
<p><strong>Dans le cas standard</strong> où chaque variable explicative est linéairement indépendante des autres et où le nombre d’échantillons de notre jeu de données est supérieur ou égal à la dimension du problème considéré, la matrice <span class="math notranslate nohighlight">\(X^TX\)</span> est inversible (i.e. <span class="math notranslate nohighlight">\(\text{det}(X^TX)\neq 0\)</span>). Dit autrement, il existe une unique solution aux équations normales donnée par :</p>
<p>\begin{equation}
\hat{\boldsymbol{\beta}}=(X^TX)^{-1}X^T\boldsymbol{y}.
\end{equation}</p>
<p>On appelle <span class="math notranslate nohighlight">\(X^\dagger = (X^TX)^{-1}X^T\)</span>  pseudo-inverse de <span class="math notranslate nohighlight">\(X\)</span> (ou inverse généralisé) et la solution analytique à notre problème est donnée par <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}=X^\dagger \boldsymbol{y}\)</span>.</p>
<p><strong>Dans le cas non standard</strong> où certaines variables peuvent être des combinaisons linéaires d’autres variables (inutile en pratique) ou si le nombre d’échantillons est inférieur à la dimension, <span class="math notranslate nohighlight">\(X^TX\)</span> n’est plus inversible. Dans ce cas de figure, il existe une infinité de solutions aux équations normales (i.e. une infinité de minimiseurs). E. H. Moore (1920), A. Bjerhammar (1951) et R. Penrose (1955) proposent indépendamment une expression générale de <span class="math notranslate nohighlight">\(X^\dagger\)</span> appelée pseudo-inverse de Moore-Penrose et calculable à partir d’une décomposition en valeur singulière, notée <span class="math notranslate nohighlight">\(X^\dagger\)</span>. Celle-ci coïncide bien sûr avec l’expression standard lorsqu’elle existe. On obtient donc une expression analytique générale, solution des équations normales :</p>
<p>\begin{equation}
\hat{\boldsymbol{\beta}}=X^\dagger\boldsymbol{y},
\end{equation}</p>
<p>où <span class="math notranslate nohighlight">\(X^\dagger\)</span> est le pseudo-inverse de Moore-Penrose.</p>
<p><em><strong>Quelques précisions d’algèbre</strong></em> : finalement, quel est le lien entre une pseudo-inverse et l’inverse classique. Soit une application linéaire <span class="math notranslate nohighlight">\(A:\mathbb{R}^n\mapsto\mathbb{R}^n\)</span> représentée par une matrice <span class="math notranslate nohighlight">\(A\in\mathbb{R}^{n\times n}\)</span>. On appelle inverse de <span class="math notranslate nohighlight">\(A\)</span> l’unique matrice, notée <span class="math notranslate nohighlight">\(A^{-1}\)</span>, telle que  <span class="math notranslate nohighlight">\(A^{-1}A=\text{Id}\)</span>. Dans le cas inversible, l’inverse de <span class="math notranslate nohighlight">\(A^{-1}\)</span> est donc de manière évidente <span class="math notranslate nohighlight">\(A\)</span>. Cela revient à transformer un vecteur <span class="math notranslate nohighlight">\(x\in\mathbb{R}^n\)</span> par <span class="math notranslate nohighlight">\(A\)</span> puis à annuler sa transformation par <span class="math notranslate nohighlight">\(A^{-1}\)</span>. L’inverse n’existe cependant pas toujours. Ainsi, par exemple, si <span class="math notranslate nohighlight">\(\text{ker}(A)\neq \{\boldsymbol{0}\}\)</span> (i.e. le noyau ne se résume pas à l’élément null, nous avons <span class="math notranslate nohighlight">\(\forall x\in\mathbb{R}^n,\ u\in\text{ker}(A)\)</span> que <span class="math notranslate nohighlight">\(A(x+u)=Ax\)</span>. Finalement l’inverse de <span class="math notranslate nohighlight">\(Ax\)</span> est-il <span class="math notranslate nohighlight">\(x\)</span> ou <span class="math notranslate nohighlight">\(x+u\)</span> ?</p>
<p>Reprenons le cas inversible. Quelques propriétés qui peuvent sembler évidentes émergent :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
AA^{-1}A&amp;=A\text{ (appliquer }A\text{, son inverse }A^{-1}\text{ puis }A\text{ à nouveau revient à appliquer }A\text{)}\\
A^{-1}AA^{-1}&amp;=A^{-1}\text{ (c'est la même chose du point de vu de l'inverse)}\\
(AA^{-1})^T&amp;=AA^{-1}\text{ (la transposition n'a pas d'effet)}\\
(A^{-1}A)^T&amp;=A^{-1}A\text{ (même chose que précédemment du point de vu de l'inverse)}
\end{aligned}\end{split}\]</div>
<p>La pseudo inverse est l’unique matrice <span class="math notranslate nohighlight">\(A^\dagger\)</span> satisfaisant les propriétés précédentes. Dans le cas où <span class="math notranslate nohighlight">\(A\)</span> est inversible, on a alors <span class="math notranslate nohighlight">\(A^\dagger=A^{-1}\)</span>. Intuitivement, l’idée est de ne considérer “que” les éléments qui ne sont pas dans le noyaux. Ainsi <span class="math notranslate nohighlight">\(\text{Im}(A)=\text{Ker}(A^\dagger)^\perp\)</span> et inversement.</p>
<p><strong>La structure de la solution</strong> peut s’étudier en remplaçant <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> par sa construction, à savoir, une combinaison linéaire et du bruit :</p>
<p>\begin{equation*}
\hat{\beta}=X^\dagger y = X^\dagger(X\boldsymbol{\beta} + \eta) = (X^\dagger X)\boldsymbol{\beta} + X^\dagger\eta
\end{equation*}</p>
<p>où on utilise <span class="math notranslate nohighlight">\(\eta\)</span> plutôt que <span class="math notranslate nohighlight">\(\epsilon\)</span> pour différentier la réalisation effective du bruit de la variable aléatoire.</p>
<p>On observe, par propriété de la pseudo-inverse, que la première contribution est la projection orthogonale du vrai modèle sur l’espace des vecteurs ligne de <span class="math notranslate nohighlight">\(X\)</span>. Il est donc une combinaison linéaire des vecteurs que l’on voit pendant l’apprentissage ! La deuxième contribution est l’effet du bruit sur la solution optimale. Nous discuterons plus loin de ces contributions et d’effets étranges qui peuvent se produire notament quand la matrice <span class="math notranslate nohighlight">\(X\)</span> est mal conditionnée (le ratio entre la plus grande valeur propre de <span class="math notranslate nohighlight">\(X^TX\)</span> et sa plus petite valeur propre est très grand).</p>
</div>
<div class="section" id="id1">
<h3>À vous de jouer<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<hr class="docutils" />
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Calculez la solution du problème de régression linéaire en utilisant la pseudo-inverse de Moore-Penrose proposée par <span class="math notranslate nohighlight">\(\texttt{numpy}\)</span> via <span class="math notranslate nohighlight">\(\texttt{np.linalg.pinv}\)</span>.</strong></p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">matplotlib</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">12.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">)</span>
<span class="c1"># descente de gradient</span>

<span class="c1"># descente de gradient sans stochasticite</span>
<span class="n">param_trace</span> <span class="p">,</span> <span class="n">loss_trace</span> <span class="o">=</span> <span class="n">gd</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.04</span><span class="p">,</span> <span class="n">nb_iterations</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span> 

<span class="c1">####### Complete this part ######## or die ####################</span>
<span class="c1"># solution par pseudo inverse</span>
<span class="n">X_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">beta_pinv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_inv</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="c1">###############################################################</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;La loss pour la solution par pseudo-inverse est&#39;</span><span class="p">,</span> <span class="n">l</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">beta_pinv</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;La loss pour la solution obtenue par descente de gradient est&#39;</span><span class="p">,</span> <span class="n">loss_trace</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">beta_pinv</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>La loss pour la solution par pseudo-inverse est 1.0476902762958011
La loss pour la solution obtenue par descente de gradient est 1.0492742274375275
</pre></div>
</div>
<img alt="../_images/1_linear_regression_34_1.png" src="../_images/1_linear_regression_34_1.png" />
</div>
</div>
<hr class="docutils" />
<p><span style="color:blue"><strong>Remarques et question :</strong></span> <strong>On remarque ici que la valeur de la loss atteinte par GD est plus haute que celle atteinte par la solution de la pseudo-inverse. A votre avis pourquoi ? Augmentez le nombre d’itérations de GD. Que constatez vous par rapport ? Est-ce étonnant par rapport à votre brillante démonstration sur GD dans la section précédente ? Au passage, on pourrait s’amuser à montrer qu’avec l’initialisation <span class="math notranslate nohighlight">\(\beta=\boldsymbol{0}\)</span>, chaque step reste bien dans l’espace engendré par les vecteurs lignes de X. Qui veut passer au tableau ?</strong></p>
</div>
<hr class="docutils" />
<div class="section" id="avec-sklearn">
<h3>Avec sklearn<a class="headerlink" href="#avec-sklearn" title="Permalink to this headline">¶</a></h3>
<hr class="docutils" />
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Proposez une régression linéaire sur le même problème en utilisant <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span>.</strong></p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="c1">####### Complete this part ######## or die ####################</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">y</span><span class="p">)</span>
<span class="c1">###############################################################</span>
<span class="n">coef</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="n">coef</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;La loss pour la solution obtenue par Sklearn est&#39;</span><span class="p">,</span> <span class="n">l</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">coef</span><span class="p">))</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">coef</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>La loss pour la solution obtenue par Sklearn est 1.0476902762958011
</pre></div>
</div>
<img alt="../_images/1_linear_regression_38_1.png" src="../_images/1_linear_regression_38_1.png" />
</div>
</div>
</div>
</div>
<div class="section" id="iv-features-variables-explicatives-transformees">
<h2>IV. Features - Variables explicatives transformées<a class="headerlink" href="#iv-features-variables-explicatives-transformees" title="Permalink to this headline">¶</a></h2>
<p>Dans beaucoup de problèmes réels, la variable à expliquer n’est pas une simple combinaison linéaire des variables explicatives. Cela peut-être une dépendence non linéaire (e.g. quadratique), ou des dépendences croisées entre nos variables explicatives. La stratégie permettant d’aborder cette problématique consiste à transformer notre vecteur <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> en rajoutant par exemple des transformations quadratiques et à optimiser notre modèle linéaire sur le vecteur transformé. Afin de simplifier les notations, nous allons volontairement omettre le biais <span class="math notranslate nohighlight">\(\beta_0\)</span> de nos notations.</p>
<p>Construire nos <em>features</em> consiste à chercher une fonction <span class="math notranslate nohighlight">\(\phi:\mathbb{R}^d\mapsto\mathbb{R}^p\)</span> qui transforme non-linéairement nos variables explicatives initiales.</p>
<p>Le problème se reformule ainsi de la manière suivante :</p>
<p>\begin{equation}
\hat{y}=\langle \phi(\boldsymbol{x}), \boldsymbol{\beta}\rangle
\end{equation}</p>
<p>Le gradient est alors calculé en fonction de <span class="math notranslate nohighlight">\(\boldsymbol{z}=\phi(\boldsymbol{x})\)</span> et non en fonction de <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>. Il suffit donc de transformer nos variables explicatives par <span class="math notranslate nohighlight">\(\phi\)</span> et de considérer le résultat comme nos nouvelles variables explicatives.</p>
<div class="section" id="construction-du-jeu-de-donnees-polynomial">
<h3>Construction du jeu de données polynomial<a class="headerlink" href="#construction-du-jeu-de-donnees-polynomial" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># vrais parametres</span>
<span class="n">beta_cube</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">sample_data_cube</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">noise</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">150</span><span class="p">)</span>

<span class="c1">#affichage du polynome</span>
<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_42_0.png" src="../_images/1_linear_regression_42_0.png" />
</div>
</div>
</div>
<div class="section" id="solution-par-pseudo-inverse">
<h3>Solution par pseudo-inverse<a class="headerlink" href="#solution-par-pseudo-inverse" title="Permalink to this headline">¶</a></h3>
<hr class="docutils" />
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Complétez le code ci-dessous en utilisant une solution par pseudo-inverse via <span class="math notranslate nohighlight">\(\texttt{numpy}\)</span>.</strong></p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Polynomial</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">deg</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">deg</span> <span class="o">=</span> <span class="n">deg</span>

    <span class="k">def</span> <span class="nf">_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1"># here we transform the input into a polynomial</span>
        <span class="c1">####### Complete this part ######## or die ####################</span>
        <span class="n">t</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">X</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">deg</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">t</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">X</span><span class="o">**</span><span class="n">i</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1">###############################################################</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="c1">####### Complete this part ######## or die ####################</span>
        <span class="n">X_transformed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">X_transformed</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
        <span class="c1">###############################################################</span>
        
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;You must fit the model first&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">X_transformed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_transformed</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">errors</span> <span class="o">=</span> <span class="p">(</span><span class="n">prediction</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span><span class="mi">2</span>
        <span class="k">return</span> <span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="n">errors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
<span class="c1"># vraie solution</span>
<span class="n">real_model</span> <span class="o">=</span> <span class="n">Polynomial</span><span class="p">(</span><span class="n">deg</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">real_model</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta_cube</span>
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
<p><span style="color:blue"><strong>Remarques et exercice :</strong></span> <strong>Le plot est affiché avec un jeu dit de test. Il s’agit d’un ensemble de points qui n’ont pas été utilisés lors de notre apprentissage (par pseudo-inverse). Le jeu de données <span class="math notranslate nohighlight">\(\texttt{X, y}\)</span> d’une taille différente est celui qui a été utilisé.</strong></p>
<p><strong>Jouez avec le degré du polynôme que vous manipulez et observez le résultat. Que constatez-vous ?</strong></p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">deg</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Polynomial</span><span class="p">(</span><span class="n">deg</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="p">[(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">,</span> <span class="s1">&#39;Our model&#39;</span><span class="p">),</span> <span class="p">(</span><span class="n">real_model</span><span class="o">.</span><span class="n">predict</span><span class="p">,</span> <span class="s1">&#39;Real model&#39;</span><span class="p">)])</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Empirical risk: &#39;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_47_0.png" src="../_images/1_linear_regression_47_0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Empirical risk:  2.6801549254883876e-28
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
<p><span style="color:blue"><strong>Remarque et question :</strong></span> <strong>Le risque empirique est celui calculé directement sur les données utilisées lors du calcul du pseudo-inverse. Que constatez-vous par rapport à ce dernier lorsque vous jouez avec le degré du polynôme ?</strong></p>
<p><strong>Est-il un bon indicateur du véritable risque de généralisation ? Autrement dit, est-il un bon indicateur de la qualité du polynôme obtenu.</strong></p>
</div>
<hr class="docutils" />
<div class="section" id="solution-sklearn">
<h3>Solution Sklearn<a class="headerlink" href="#solution-sklearn" title="Permalink to this headline">¶</a></h3>
<hr class="docutils" />
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Proposez la même solution polynomiale via <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span>. Choisissez le même degré qu’utilisé au-dessus et comparez les résultats.</strong></p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">deg</span><span class="p">),</span> <span class="n">LinearRegression</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">y</span><span class="p">)</span>
<span class="c1">###############################################################</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="p">[(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">,</span> <span class="s1">&#39;sklearn&#39;</span><span class="p">),</span> <span class="p">(</span><span class="n">real_model</span><span class="o">.</span><span class="n">predict</span><span class="p">,</span> <span class="s1">&#39;Real model&#39;</span><span class="p">)])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_52_0.png" src="../_images/1_linear_regression_52_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="v-validation-croisee">
<h2>V. Validation croisée<a class="headerlink" href="#v-validation-croisee" title="Permalink to this headline">¶</a></h2>
<p>Vous avez pu constater qu’en fonction du degré du polynôme choisi dans l’exercice précédent, le modèle obtenu était plus ou moins loin de la solution idéale. De plus, le risque empirique s’est montré être un piètre estimateur de la qualité de notre solution estimée.</p>
<p>En réalité, le risque empirique est un estimateur sans biais du risque de généralisation pour un vecteur de paramètres quelconque. Ce n’est plus vrai si on choisit la solution estimée via notre optimisation. Dit autrement :</p>
<div class="math notranslate nohighlight">
\[R(\boldsymbol{\beta})=\mathbb{E}_{\mathcal{S}}\Big[J(\boldsymbol{\beta})\Big],\text{ }\boldsymbol{\beta}\text{ quelconque, et }R(\text{argmin}_{\boldsymbol{\beta}}J(\boldsymbol{\beta}))\neq \mathbb{E}_{\mathcal{S}}\Big[\text{argmin}_{\boldsymbol{\beta}}J(\boldsymbol{\beta})\Big]\]</div>
<p>Cela implique de mettre en place une procédure expérimentale permettant d’évaluer la qualité de notre modèle.</p>
<div class="section" id="construction-du-jeu-de-donnees">
<h3>Construction du jeu de données<a class="headerlink" href="#construction-du-jeu-de-donnees" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">beta_cube</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">sample_data_cube</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">noise</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">150</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_56_0.png" src="../_images/1_linear_regression_56_0.png" />
</div>
</div>
</div>
<div class="section" id="optimiser-une-fonction-est-il-suffisant-pour-parler-d-apprentissage">
<h3>Optimiser une fonction est-il suffisant pour parler d’apprentissage ?<a class="headerlink" href="#optimiser-une-fonction-est-il-suffisant-pour-parler-d-apprentissage" title="Permalink to this headline">¶</a></h3>
<p>Il existe deux stratégies d’évaluation sans biais de la qualité de notre modèle :</p>
<ul class="simple">
<li><p>La validation non croisée où une partie de notre jeu de donnée est cachée pendant l’apprentissage puis utilisée afin d’évaluer les performances du modèle. Il s’agit du découpage train/test. Cette stratégie est un estimateur sans biais de la qualité de notre modèle mais possède une variance plus forte que la validation croisée. Elle peut-être particulièrement utile lorsque le coup d’apprentissage d’un modèle est très élevé (e.g. <em>deep learning</em>)</p></li>
<li><p>La validation croisée où notre jeu de données est divisé en <em>k</em> parties (on parle aussi de <em>k-fold</em>). Évidemment, <span class="math notranslate nohighlight">\(k\in\{2, ..., n\}\)</span> où <span class="math notranslate nohighlight">\(n\)</span> est la taille du jeu de données. Chacune des parties jouera successivement le rôle de jeu de test pendant que les <span class="math notranslate nohighlight">\(k-1\)</span> autres parties serviront à calculer notre modèle. Le résultat de cette procédure est un vecteur de <span class="math notranslate nohighlight">\(k\)</span> scores dont on peut calculer la moyenne, la variance, etc.</p></li>
</ul>
<p>On peut illustrer la méthode des <em>k-folds</em> via l’exemple suivant :</p>
<p>\begin{align}
\text{Appartient au train set: } \color{red}{\boxed{}}&amp;\text{ et appartient au test set: }\color{green}{\boxed{}}
\end{align}
\begin{align}
\text{Step 1: }\color{green}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align}
\begin{align}
\text{Step 2: }\color{red}{\boxed{}}\color{green}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align}
\begin{align}
\text{Step 3: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{green}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align}
\begin{align}
\text{Step 4: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{green}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align}
\begin{align}
\text{Step 5: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{green}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align}
\begin{align}
\text{Step 6: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{green}{\boxed{}}\color{red}{\boxed{}}
\end{align}
\begin{align}
\text{Step 7: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{green}{\boxed{}}
\end{align}</p>
<p>La méthode <span class="math notranslate nohighlight">\(\texttt{cross_val_score}\)</span> de <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span> permet de réaliser cette procédure. On pourra renseigner le paramètre <span class="math notranslate nohighlight">\(\texttt{cv}\)</span> qui indique le nombre <span class="math notranslate nohighlight">\(k\)</span> et le paramètre <span class="math notranslate nohighlight">\(\texttt{scoring}\)</span> qui donne la métrique que l’on souhaite calculer.</p>
<hr class="docutils" />
<p><strong><span style="color:blue"> Exercice :</span></strong> <strong>Proposez un <em>5-fold</em> avec la métrique <span class="math notranslate nohighlight">\(R^2\)</span> que vous appliquerez à une régression polynomiale de degré <span class="math notranslate nohighlight">\(5\)</span>.</strong></p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>

<span class="c1">####### Complete this part ######## or die ####################</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">5</span><span class="p">),</span> <span class="n">LinearRegression</span><span class="p">())</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;r2&#39;</span><span class="p">)</span>
<span class="c1">###############################################################</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Le score R2 sur chacun des splits de notre k-fold:&#39;</span><span class="p">,</span> <span class="n">scores</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Le score R2 sur chacun des splits de notre k-fold: [0.97601253 0.97932686 0.92543173 0.97598169 0.88889135]
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
<p><strong><span style="color:blue"> Exercice :</span></strong> <strong>Comparez le score obtenu lors de votre validation croisée à un plot de la fonction estimée sur tout le jeu d’apprentissage.</strong></p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="c1">###############################################################</span>
<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_63_0.png" src="../_images/1_linear_regression_63_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="vi-l-effet-double-descente-bonus">
<h2>VI. L’effet “double descente” (Bonus ?)<a class="headerlink" href="#vi-l-effet-double-descente-bonus" title="Permalink to this headline">¶</a></h2>
<p>Comme vu précédemment, l’effet du bruit sur l’estimateur depend du conditionnement de <span class="math notranslate nohighlight">\(X^TX\)</span>. Le conditionnement d’une matrice <span class="math notranslate nohighlight">\(A\)</span> inversible est donné par :</p>
<p>\begin{equation}
C(A)=\lVert A^{-1}\rVert\lvert A\rVert
\end{equation}</p>
<p>Il est évident que si <span class="math notranslate nohighlight">\(A\in{\mathbb{R}^{1\times 1}}^\star\)</span>, alors <span class="math notranslate nohighlight">\(C(A)=1\)</span>. Ce n’est absolument pas vrai dans le cas général.</p>
<p>L’exemple ci-dessous illustre cela via la norme de Frobenius (norme Euclidienne appliquée à une matrice, <span class="math notranslate nohighlight">\(\text{Tr}(A^TA)^{0.5}\)</span>). On préfèrera en pratique la norme d’opérateur qui quantifie les effets d’amplification d’un vecteur <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> lorsqu’on calcule <span class="math notranslate nohighlight">\(A\boldsymbol{x}\)</span>. Cette norme d’opérateur est directement liée aux valeurs propres.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;A=</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;C(A)=A^{-1}A=&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">A</span><span class="p">))</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>A=
 [[1.e+00 0.e+00]
 [0.e+00 1.e-04]]
C(A)=A^{-1}A=10000.000100000001
</pre></div>
</div>
</div>
</div>
<p>On remarque dans l’exemple que la matrice <span class="math notranslate nohighlight">\(A\)</span> possède une toute petite valeur propre qui est responsable de cet écart. L’exercice ci-dessous montre qu’au-delà des considérations théoriques, cela a des répercussions importantes et totalement inattendues en réalité.</p>
<p>Les simulations suivantes permettent de mettre en lumière cela. Elles sont construites comme décrit ci-dessous :</p>
<p>\begin{equation}
\beta\sim\mathbb{U}(-2, 2)^d,\ d\in\mathbb{N}^\star
\end{equation}</p>
<p>dit autrement, on fixe un vecteur de paramètres selon une loi uniforme qui dépend de la dimension du problème.
Nous avons ensuite :</p>
<p>\begin{equation}
x\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I_d}) + \epsilon,\ \epsilon\sim\mathcal{N}(0, \sigma^2)
\end{equation}</p>
<p>On construit ensuite un jeu de test de taille <span class="math notranslate nohighlight">\(500\)</span> et un jeu d’apprentissage de taille variable. L’objectif ici sera d’étudié l’effet de la taille du jeu d’apprentissage sur la qualité de notre modèle, qualité que l’on aura calculée sur le test. Pour chaque taille de jeu de données, l’expérience est répétée <span class="math notranslate nohighlight">\(50\)</span> fois (<span class="math notranslate nohighlight">\(\texttt{redo}\)</span>) afin de lisser les courbes obtenues.</p>
<hr class="docutils" />
<p><strong><span style="color:blue"> Exercice :</span></strong> <strong>Exécutez une première fois le code puis jouez avec <span class="math notranslate nohighlight">\(\texttt{noise}\)</span> (i.e. <span class="math notranslate nohighlight">\(\sigma\)</span>) afin de voir ce qui se passe selon la quantité de bruit. Essayez de décrire rigoureusement ce que vous observez.</strong></p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1">####### Play with the noise #########</span>
<span class="n">noise</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="c1">#####################################</span>

<span class="n">d</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">redo</span> <span class="o">=</span> <span class="mi">50</span>

<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">mu</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">)]</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">)])</span>

<span class="n">test_size</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">test_size</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">test_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">noise</span>

<span class="n">errors</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">train_errors</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
    <span class="n">error</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">train_error</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">redo</span><span class="p">):</span>
        <span class="c1"># dataset construction</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">m</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">noise</span>
        
        <span class="c1"># param estimation</span>
        <span class="n">beta_pinv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
        
        <span class="c1"># risk estimation</span>
        <span class="n">error</span> <span class="o">+=</span> <span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">beta_pinv</span><span class="p">)</span><span class="o">-</span><span class="n">y_test</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="n">test_size</span><span class="o">*</span><span class="n">redo</span><span class="p">)</span>
        <span class="n">train_error</span> <span class="o">+=</span> <span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta_pinv</span><span class="p">)</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="n">m</span><span class="o">*</span><span class="n">redo</span><span class="p">)</span>
    <span class="n">train_errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_error</span><span class="p">)</span>
    <span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">train_errors</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Train error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">d</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Dimension&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">errors</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Risk estimation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_69_0.png" src="../_images/1_linear_regression_69_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">errors</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Risk estimation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">d</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Dimension&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_errors</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">train_errors</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Train error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">d</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Dimension&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_70_0.png" src="../_images/1_linear_regression_70_0.png" />
<img alt="../_images/1_linear_regression_70_1.png" src="../_images/1_linear_regression_70_1.png" />
</div>
</div>
<p>La ligne en pointillé sépare visuellement deux régimes différents. La transition d’un régime à l’autre se produit par une augmentation catastrophique de l’erreur de généralisation de notre modèle.</p>
<hr class="docutils" />
<p><strong><span style="color:blue"> Question :</span></strong> <strong>Quelle particularité différentie les deux phases ?</strong></p>
<hr class="docutils" />
<p>En réalité, les méthodes de <em>machine learning</em> traditionnelles se situent plutôt dans le régime de “droite”. L’étude de ce phénomène est poussée par les approches comme le <em>deep learning</em> qui sont souvent dans le régime de gauche. Comprendre ces phénomènes nous permet par exemple d’éclairer les raisons du succès du <em>deep learning</em>.</p>
</div>
<div class="section" id="vii-regularisation">
<h2>VII. Régularisation<a class="headerlink" href="#vii-regularisation" title="Permalink to this headline">¶</a></h2>
<p>Comme illustré par les quelques scénarios précédents dont le cas catastrophique de la double descente, une certaine parcimonie est attendue par notre modèle. On a pu notamment observer que les “mauvaises” fonctions du point de vue du risque de généralisation avaient une forte tendance à osciller n’importe comment. Au lieu de laisser jouer le “hasard” (ou plutôt le conditionnement de <span class="math notranslate nohighlight">\(X^TX\)</span>), nous pouvons contraindre notre optimisation à favoriser les solutions parcimonieuses ; c’est-à-dire des solutions qui n’oscillent pas n’importe comment.</p>
<p>Intuitivement, on va choisir une solution qui minimise à la fois le risque empirique <span class="math notranslate nohighlight">\(J(\boldsymbol{\beta})\)</span>, mais aussi une pénalité sur la quantité “d’oscillation”. En réalité, les oscillations sont directement contrôlées par la norme des paramètres : un grand poids rendra notre modèle très sensible à la moindre perturbation de la variable explicative associée.</p>
<p>Nous parlons d’optimisation régularisée lorsque la fonction à optimiser s’écrit de la manière suivante :</p>
<p>\begin{equation}
J(\boldsymbol{\beta})=\frac{1}{m}\sum_{i=1}^nr(f_{\boldsymbol{\beta}}(\boldsymbol{x_i}), y_i)+\lambda P(\boldsymbol{\beta})
\end{equation}</p>
<p>où <span class="math notranslate nohighlight">\(r:\mathcal{Y}\times\mathcal{Y}\mapsto \mathbb{R}^+\)</span> est notre risque élémentaire et <span class="math notranslate nohighlight">\(P:\mathbb{R}^d\mapsto\mathbb{R}^+\)</span> une pénalité sur notre vecteur de paramètres. Plus précisément, dans le cas de la régression linéaire, nous avons :</p>
<p>\begin{equation}
r(\hat{y}, y)=(\hat{y}-y)^2
\end{equation}</p>
<p>et</p>
<p>\begin{equation}
P(\boldsymbol{\beta})=\lVert \boldsymbol{\beta} \rVert,
\end{equation}</p>
<p>où <span class="math notranslate nohighlight">\(\lVert \cdot \rVert\)</span> est une norme quelconque. Les choix classiques sont la norme <span class="math notranslate nohighlight">\(\ell_1\)</span> :</p>
<p>\begin{equation}
\lVert \boldsymbol{\beta} \rVert_1=\sum_j |\boldsymbol{\beta}_j|
\end{equation}</p>
<p>et la norme <span class="math notranslate nohighlight">\(\ell_2\)</span> :</p>
<p>\begin{equation}
\lVert \boldsymbol{\beta} \rVert_2 = \sqrt{\sum_j\boldsymbol{\beta}_j^2}=\sqrt{\boldsymbol{\beta}^T\boldsymbol{\beta}}
\end{equation}</p>
<p>Une stratégie intermédiaire consiste à prendre la combinaison convexe des deux normes :</p>
<p>\begin{equation}
P(\boldsymbol{\beta})=\eta \lVert \boldsymbol{\beta} \rVert_1 + (1-\eta) \lVert \boldsymbol{\beta} \rVert_2.
\end{equation}</p>
<p>avec <span class="math notranslate nohighlight">\(\eta\in\big[0,1\big]\)</span>. On parle alors d’<em>elastic-net</em>.</p>
<p>Ces différentes régularisations ne se comportent pas de la même manière. Ainsi la régularisation <span class="math notranslate nohighlight">\(\ell_1\)</span>, aussi appelée Lasso, va forcer certains paramètres à atteindre la valeur <span class="math notranslate nohighlight">\(0\)</span>. Cela permet par exemple de favoriser l’explicabilité de notre modèle. En pratique, <span class="math notranslate nohighlight">\(\ell_2\)</span>, appelée Ridge, a tendance à donner les meilleurs résultats d’un point de vue prédictif.</p>
<div class="section" id="id2">
<h3>Construction du jeu de données<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">beta_cube</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">sample_data_cube</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">noise</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">150</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_76_0.png" src="../_images/1_linear_regression_76_0.png" />
</div>
</div>
</div>
<div class="section" id="sans-regularisation">
<h3>Sans régularisation<a class="headerlink" href="#sans-regularisation" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">LinearRegression</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_79_0.png" src="../_images/1_linear_regression_79_0.png" />
</div>
</div>
</div>
<div class="section" id="avec-regularisation-ell-1">
<h3>Avec régularisation <span class="math notranslate nohighlight">\(\ell_1\)</span><a class="headerlink" href="#avec-regularisation-ell-1" title="Permalink to this headline">¶</a></h3>
<p>Lorsqu’on parle de régresion linéaire avec régularisation <span class="math notranslate nohighlight">\(\ell_1\)</span>, on parle aussi de Lasso.</p>
<hr class="docutils" />
<p><strong><span style="color:blue"> Exercice :</span></strong> <strong>Testez plusieurs valeurs de <span class="math notranslate nohighlight">\(\alpha\)</span> (<span class="math notranslate nohighlight">\(=\lambda\)</span> dans notre texte). Quelle est la fonction la plus parcimonieuse que vous arrivez à obtenir ?</strong></p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">15.</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_83_0.png" src="../_images/1_linear_regression_83_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Les paramètres du modèle sont sparses :</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">model</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Les paramètres du modèle sont sparses :
 [ 0.        -0.        -0.        -0.        -0.        -0.
 -0.        -0.        -0.         0.        -0.0091659]
</pre></div>
</div>
</div>
</div>
<p>Vous avez du constater que selon la quantité de régularisation, les paramètres étaient plus ou moins sparse. Il se trouve qu’une fois qu’un paramètre est à 0, il le sera pour toutes les valeurs de <span class="math notranslate nohighlight">\(\alpha\)</span> suppérieures. Afin d’observer visuellement, l’effet de la régularisation sur la sparsité, il est possible d’afficher ce qu’on appelle les “chemins Lasso” ou Lasso paths.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">lars_path</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">X_transformed</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">coefs</span> <span class="o">=</span> <span class="n">lars_path</span><span class="p">(</span><span class="n">X_transformed</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;lasso&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">coefs</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">xx</span> <span class="o">/=</span> <span class="n">xx</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">coefs</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">ymin</span><span class="p">,</span> <span class="n">ymax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">()</span>
<span class="c1"># plt.vlines(xx, ymin, ymax, linestyle=&#39;dashed&#39;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;|coef| / max|coef|&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Coefficients&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;LASSO Path&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>.
</pre></div>
</div>
<img alt="../_images/1_linear_regression_86_1.png" src="../_images/1_linear_regression_86_1.png" />
</div>
</div>
<p>A gauche se trouve le paramètre le plus parcimonieux (la plus grande valeur de <span class="math notranslate nohighlight">\(\alpha\)</span>). Tous les paramètres y sont donc nuls. Plus la valeur de <span class="math notranslate nohighlight">\(\alpha\)</span> est réduite, plus le nombre de paramètres différents de <span class="math notranslate nohighlight">\(0\)</span> augmente et leur valeur aussi.</p>
</div>
<div class="section" id="avec-regularisation-ell-2">
<h3>Avec régularisation <span class="math notranslate nohighlight">\(\ell_2\)</span><a class="headerlink" href="#avec-regularisation-ell-2" title="Permalink to this headline">¶</a></h3>
<p>Lorsqu’on parle de régresion linéaire avec régularisation <span class="math notranslate nohighlight">\(\ell_2\)</span>, on parle aussi de Ridge.</p>
<hr class="docutils" />
<p><strong><span style="color:blue"> Exercice :</span></strong> <strong>Testez plusieurs valeurs de <span class="math notranslate nohighlight">\(\alpha\)</span> (<span class="math notranslate nohighlight">\(=\lambda\)</span> dans notre texte). Quelle est la fonction la plus parcimonieuse que vous arrivez à obtenir ?</strong></p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1000</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_91_0.png" src="../_images/1_linear_regression_91_0.png" />
</div>
</div>
</div>
<div class="section" id="avec-regularisation-elastic-net">
<h3>Avec régularisation <em>elastic-net</em><a class="headerlink" href="#avec-regularisation-elastic-net" title="Permalink to this headline">¶</a></h3>
<p>Lorsqu’on parle de régresion linéaire avec régularisation <em>elastic-net</em>.</p>
<hr class="docutils" />
<p><strong><span style="color:blue"> Exercice :</span></strong> <strong>Testez plusieurs valeurs de <span class="math notranslate nohighlight">\(\alpha\)</span> (<span class="math notranslate nohighlight">\(=\lambda\)</span> dans notre texte). Quelle est la fonction la plus parcimonieuse que vous arrivez à obtenir (Essayez de trouver la réponse en raisonnant) ?</strong></p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNet</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">ElasticNet</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.8</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_95_0.png" src="../_images/1_linear_regression_95_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="viii-selection-de-modeles">
<h2>VIII. Selection de modèles<a class="headerlink" href="#viii-selection-de-modeles" title="Permalink to this headline">¶</a></h2>
<p>En pratique, nous ne pouvons pas choisir la valeur des paramètres (e.g. degré, régularisation) à l’oeil comme précédemment. Il nous faut (1) un algorithme qui automatise cette tâche et (2) une stratégie d’évaluation rigoureuse afin d’éviter les biais de confirmation (sur-apprentissage).</p>
<div class="section" id="id3">
<h3>Construction du jeu de données<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">beta_cube</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">sample_data_cube</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">noise</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">150</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_99_0.png" src="../_images/1_linear_regression_99_0.png" />
</div>
</div>
</div>
<div class="section" id="recherche-exhaustive">
<h3>Recherche exhaustive<a class="headerlink" href="#recherche-exhaustive" title="Permalink to this headline">¶</a></h3>
<p>L’algorithme de recherche par grille va exhaustivement testé tous les paramètres donnés. Pour chacun combinaison, une validation <em>k-fold</em> est réalisée. Le modèle retenu sera celui qui aura maximisé son score moyen lors du <em>k-fold</em>.</p>
<hr class="docutils" />
<p><strong><span style="color:blue"> Exercice :</span></strong> <strong>Utilisez l’objet <span class="math notranslate nohighlight">\(\texttt{GridSearchCV}\)</span> afin de trouver la meilleure combinaison de paramètres selon le dictionnaire décrit ci-dessous. Toutes les combinaisons seront-elles réellement testées ?</strong></p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNet</span><span class="p">,</span> <span class="n">Ridge</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">,</span> <span class="n">Lasso</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">[</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">LinearRegression</span><span class="p">()],</span> 
   <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]},</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">Ridge</span><span class="p">()],</span> 
   <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> 
   <span class="s1">&#39;model__alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">]},</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">Lasso</span><span class="p">()],</span> 
   <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
   <span class="s1">&#39;model__alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]},</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">ElasticNet</span><span class="p">()],</span> <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
  <span class="s1">&#39;model__alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">],</span>
  <span class="s1">&#39;model__l1_ratio&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]},</span>
 <span class="p">]</span>

<span class="n">pipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">10</span><span class="p">)),</span> <span class="p">(</span><span class="s1">&#39;model&#39;</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">())])</span>
<span class="c1">####### Complete this part ######## or die ####################</span>
<span class="n">search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipe</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="c1">###############################################################</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GridSearchCV(estimator=Pipeline(steps=[(&#39;poly&#39;, PolynomialFeatures(degree=10)),
                                       (&#39;model&#39;, LinearRegression())]),
             n_jobs=-1,
             param_grid=[{&#39;model&#39;: [LinearRegression()],
                          &#39;poly__degree&#39;: [1, 2, 3, 4, 5]},
                         {&#39;model&#39;: [Ridge()],
                          &#39;model__alpha&#39;: [0.0, 0.1, 0.2, 0.5, 0.8, 10.0, 100.0,
                                           100.0],
                          &#39;poly__degree&#39;: [1, 2, 3, 4, 5]},
                         {&#39;model&#39;: [Lasso()],
                          &#39;model__alpha&#39;: [0.0, 0.1, 0.2, 0.5, 0.8],
                          &#39;poly__degree&#39;: [1, 2, 3, 4, 5]},
                         {&#39;model&#39;: [ElasticNet()],
                          &#39;model__alpha&#39;: [0.0, 0.1, 0.2, 0.5, 0.8, 10.0, 100.0,
                                           100.0],
                          &#39;model__l1_ratio&#39;: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6,
                                              0.9, 1.0],
                          &#39;poly__degree&#39;: [1, 2, 3, 4, 5]}])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Le meilleur modele est&#39;</span><span class="p">,</span> <span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Le meilleur modele est Pipeline(steps=[(&#39;poly&#39;, PolynomialFeatures(degree=3)),
                (&#39;model&#39;, LinearRegression())])
</pre></div>
</div>
<img alt="../_images/1_linear_regression_104_1.png" src="../_images/1_linear_regression_104_1.png" />
</div>
</div>
</div>
<div class="section" id="recherche-aleatoire">
<h3>Recherche aléatoire<a class="headerlink" href="#recherche-aleatoire" title="Permalink to this headline">¶</a></h3>
<p>Une recherche exhaustive peut rapidement être limitante. Imaginons que nous testions déjà <span class="math notranslate nohighlight">\(10000\)</span> combinaisons de paramètres. Rajoutons maintenant un paramètre avec 50 modalités. Le nombre de combinaisons est donc multiplié par <span class="math notranslate nohighlight">\(50\)</span> et on monte à <span class="math notranslate nohighlight">\(500000\)</span> combinaisons. L’algorithme devient <span class="math notranslate nohighlight">\(50\)</span> fois plus lent.</p>
<p>Une stratégie alternative est de s’appuyer sur le hasard. On peut spécifier a priori des distributions sur les paramètres en supposant que certaines combinaisons fourniront probablement plus de bons résultats que d’autres. Par défaut, le tirage est uniforme. Cette méthode n’est pas absurde car plusieurs combinaisons peuvent très bien obtenir des résultats très proches. L’approche aléatoire sera ainsi beaucoup plus efficaces que la recherche exhaustive pour des performances généralement assez proches.</p>
<hr class="docutils" />
<p><strong><span style="color:blue"> Exercice :</span></strong> <strong>Complétez le code ci-dessous afin de réaliser une recherche randomisée. Quel paramètre permet de jouer sur le nombre de tirages ?</strong></p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">[</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">LinearRegression</span><span class="p">()],</span> 
   <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)]},</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">Ridge</span><span class="p">()],</span> 
   <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)],</span> 
   <span class="s1">&#39;model__alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">]},</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">Lasso</span><span class="p">()],</span> 
   <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)],</span>
   <span class="s1">&#39;model__alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]},</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">ElasticNet</span><span class="p">()],</span> <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)],</span>
  <span class="s1">&#39;model__alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">],</span>
  <span class="s1">&#39;model__l1_ratio&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]},</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">10</span><span class="p">)),</span> <span class="p">(</span><span class="s1">&#39;model&#39;</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">())])</span>

<span class="n">search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipe</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GridSearchCV(estimator=Pipeline(steps=[(&#39;poly&#39;, PolynomialFeatures(degree=10)),
                                       (&#39;model&#39;, LinearRegression())]),
             n_jobs=-1,
             param_grid=[{&#39;model&#39;: [LinearRegression()],
                          &#39;poly__degree&#39;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
                                           12, 13, 14, 15, 16, 17, 18, 19]},
                         {&#39;model&#39;: [Ridge()],
                          &#39;model__alpha&#39;: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6,
                                           0.7, 0.8, 0.9, 1.0, 10.0, 100.0,
                                           100.0],
                          &#39;poly__degree&#39;: [1, 2, 3, 4, 5,...
                                           12, 13, 14, 15, 16, 17, 18, 19]},
                         {&#39;model&#39;: [Lasso()],
                          &#39;model__alpha&#39;: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6,
                                           0.7, 0.8, 0.9, 1.0],
                          &#39;poly__degree&#39;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
                                           12, 13, 14, 15, 16, 17, 18, 19]},
                         {&#39;model&#39;: [ElasticNet()],
                          &#39;model__alpha&#39;: [0.1, 0.2, 0.5, 0.8, 10.0, 100.0,
                                           100.0],
                          &#39;model__l1_ratio&#39;: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6,
                                              0.9, 1.0],
                          &#39;poly__degree&#39;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
                                           12, 13, 14, 15, 16, 17, 18, 19]}])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Le meilleur modele est&#39;</span><span class="p">,</span> <span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Le meilleur modele est Pipeline(steps=[(&#39;poly&#39;, PolynomialFeatures(degree=3)),
                (&#39;model&#39;, LinearRegression())])
</pre></div>
</div>
<img alt="../_images/1_linear_regression_110_1.png" src="../_images/1_linear_regression_110_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="n">search</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">pipe</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="c1">###############################################################</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/servajean/Library/Python/3.9/lib/python/site-packages/sklearn/pipeline.py:335: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
  self._final_estimator.fit(Xt, y, **fit_params_last_step)
/Users/servajean/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:529: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.
  model = cd_fast.enet_coordinate_descent(
/Users/servajean/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.0441160130073355, tolerance: 0.019408266008669246
  model = cd_fast.enet_coordinate_descent(
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RandomizedSearchCV(estimator=Pipeline(steps=[(&#39;poly&#39;,
                                              PolynomialFeatures(degree=10)),
                                             (&#39;model&#39;, LinearRegression())]),
                   n_iter=200, n_jobs=-1,
                   param_distributions=[{&#39;model&#39;: [LinearRegression()],
                                         &#39;poly__degree&#39;: [1, 2, 3, 4, 5, 6, 7,
                                                          8, 9, 10, 11, 12, 13,
                                                          14, 15, 16, 17, 18,
                                                          19]},
                                        {&#39;model&#39;: [Ridge()],
                                         &#39;model__alpha&#39;: [0.0, 0.1, 0.2, 0.3,
                                                          0.4, 0.5, 0.6, 0.7,
                                                          0.8, 0.9, 1.0, 10.0,
                                                          100.0, 100.0],...
                                        {&#39;model&#39;: [Lasso(alpha=0.0)],
                                         &#39;model__alpha&#39;: [0.0, 0.1, 0.2, 0.3,
                                                          0.4, 0.5, 0.6, 0.7,
                                                          0.8, 0.9, 1.0],
                                         &#39;poly__degree&#39;: [1, 2, 3, 4, 5, 6, 7,
                                                          8, 9, 10, 11, 12, 13,
                                                          14, 15, 16, 17, 18,
                                                          19]},
                                        {&#39;model&#39;: [ElasticNet()],
                                         &#39;model__alpha&#39;: [0.1, 0.2, 0.5, 0.8,
                                                          10.0, 100.0, 100.0],
                                         &#39;model__l1_ratio&#39;: [0.0, 0.1, 0.2, 0.3,
                                                             0.4, 0.5, 0.6, 0.9,
                                                             1.0],
                                         &#39;poly__degree&#39;: [1, 2, 3, 4, 5, 6, 7,
                                                          8, 9, 10, 11, 12, 13,
                                                          14, 15, 16, 17, 18,
                                                          19]}])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Le meilleur modele est&#39;</span><span class="p">,</span> <span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Le meilleur modele est Pipeline(steps=[(&#39;poly&#39;, PolynomialFeatures(degree=3)),
                (&#39;model&#39;, Lasso(alpha=0.0))])
</pre></div>
</div>
<img alt="../_images/1_linear_regression_112_1.png" src="../_images/1_linear_regression_112_1.png" />
</div>
</div>
</div>
</div>
<div class="section" id="ix-le-mot-de-la-fin">
<h2>IX. Le mot de la fin<a class="headerlink" href="#ix-le-mot-de-la-fin" title="Permalink to this headline">¶</a></h2>
<p>Ce propos introductif nous a permis de toucher du doigt la notion de sur-apprentissage. Quand est-ce que le meilleur modèle sur notre jeu d’apprentissage est suffisament bon en général ? Quand peut-on considérer qu’un modèle est suffisamment bon ? Aurions-nous pu trouver un meilleur modèle avec une procédure d’apprentissage différente ?</p>
</div>
</div>


<script type="application/vnd.jupyter.widget-state+json">
{"state": {"e47eafd5f5164bd683a5efd7a02ebe41": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "5a2d8bae14794814b9ffc0462b6268fc": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_e47eafd5f5164bd683a5efd7a02ebe41", "msg_id": "", "outputs": []}}, "71e52b72201d48c285449e4487ca5f03": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "ac8bb2d72f0c481782adb9359a4942c1": {"model_name": "SliderStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "SliderStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": "", "handle_color": null}}, "f3f7b576d15a4fb2a44c2ec57ad7f489": {"model_name": "FloatSliderModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatSliderModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "FloatSliderView", "continuous_update": false, "description": "learning_rate", "description_tooltip": null, "disabled": false, "layout": "IPY_MODEL_71e52b72201d48c285449e4487ca5f03", "max": 0.05, "min": 1e-05, "orientation": "horizontal", "readout": true, "readout_format": ".5f", "step": 0.0001, "style": "IPY_MODEL_ac8bb2d72f0c481782adb9359a4942c1", "value": 1e-05}}, "80c88f44c9804d3db0cd11b37b59f2e3": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "a5d0e2d36df74ff9b1dcfb211ae0865b": {"model_name": "SliderStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "SliderStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": "", "handle_color": null}}, "c18a64ba7bf34f67a63f01d2a9cd325c": {"model_name": "IntSliderModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "IntSliderModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "IntSliderView", "continuous_update": false, "description": "batch_size", "description_tooltip": null, "disabled": false, "layout": "IPY_MODEL_80c88f44c9804d3db0cd11b37b59f2e3", "max": 10, "min": 1, "orientation": "horizontal", "readout": true, "readout_format": "d", "step": 1, "style": "IPY_MODEL_a5d0e2d36df74ff9b1dcfb211ae0865b", "value": 10}}, "d37166070cea4d888fd04bb7867c9d65": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "c945e9a87742490396124b512d299f36": {"model_name": "VBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": ["widget-interact"], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "VBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "VBoxView", "box_style": "", "children": ["IPY_MODEL_f3f7b576d15a4fb2a44c2ec57ad7f489", "IPY_MODEL_c18a64ba7bf34f67a63f01d2a9cd325c", "IPY_MODEL_30111b7a2adb46ae9f44334bd143f632"], "layout": "IPY_MODEL_d37166070cea4d888fd04bb7867c9d65"}}, "bf3baeaa4f6940dd9acc9ed706905831": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "30111b7a2adb46ae9f44334bd143f632": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_bf3baeaa4f6940dd9acc9ed706905831", "msg_id": "", "outputs": [{"output_type": "display_data", "metadata": {"needs_background": "light"}, "data": {"text/plain": "<Figure size 1008x432 with 2 Axes>", "image/png": "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\n"}}]}}}, "version_major": 2, "version_minor": 0}
</script>


    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./2_linear_regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="0_propos_liminaire.html" title="previous page">La <em>régression</em></a>
    <a class='right-next' id="next-link" href="2_optimization.html" title="next page">L’optimisation ☕️☕️</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By The LMIPR team<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>
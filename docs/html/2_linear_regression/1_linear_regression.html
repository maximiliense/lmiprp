
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>La r√©gression lin√©aire ‚òïÔ∏è &#8212; Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="L‚Äôoptimisation ‚òïÔ∏è‚òïÔ∏è" href="2_optimization.html" />
    <link rel="prev" title="La r√©gression" href="0_propos_liminaire.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Machine Learning
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../0_requisite/rappels_python.html">
   Rappels de Python et Numpy
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../1_what_is_ml/0_propos_liminaire.html">
   Le
   <em>
    Machine Learning
   </em>
   , c‚Äôest quoi ?
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/1_introduction_ml.html">
     <em>
      Machine learning
     </em>
     et mal√©diction de la dimension
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_what_is_ml/2_regression_and_classification_trees.html">
     Les arbres de r√©gression et de classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="0_propos_liminaire.html">
   La
   <em>
    r√©gression
   </em>
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     La r√©gression lin√©aire ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="2_optimization.html">
     L‚Äôoptimisation ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3_interpolation.html">
     Interpolation ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="4_algo_proximal_lasso.html">
     Sous-diff√©rentiel et le cas du Lasso ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../3_logistic_regression/0_propos_liminaire.html">
   La
   <em>
    classification
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_logistic_regression/1_logistic_regression.html">
     La R√©gression Logistique ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_logistic_regression/2_fonctions_proxy.html">
     Les fonctions proxy ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_logistic_regression/3_bayes_classifier.html">
     Le classifieur de Bayes ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_logistic_regression/4_VC_theory.html">
     La th√©orie de Vapnik et Chervonenkis ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è (üíÜ‚Äç‚ôÇÔ∏è)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../4_max_margin/0_propos_liminaire.html">
   Les mod√®les
   <em>
    max-margin
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../4_max_margin/1_max_margin.html">
     Les mod√®les max-margin ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5_ensembles/0_propos_liminaire.html">
   Les m√©thodes
   <em>
    ensemblistes
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/1_ensembles.html">
     Les m√©thodes ensemblistes ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../6_autodiff/0_propos_liminaire.html">
   La diff√©rentiation automatique
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_autodiff/1_autodiff.html">
     La diff√©rentiation automatique et un d√©but de
     <em>
      deep learning
     </em>
     ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_autodiff/2_filters_representation.html">
     Filtres et espace de repr√©sentation des r√©seaux de neurones ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../7_regularization/0_propos_liminaire.html">
   La r√©gularisation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/1_regularization_deep.html">
     R√©gularisation en deep learning ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/2_unsupervised_representation_learning.html">
     Unsupervised representation learning ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/3_multitask.html">
     L‚Äôapprentissage multi-t√¢ches ‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/4_adversarial.html">
     Les attaques adversaires ‚òïÔ∏è
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/5_ridge.html">
     Une analyse de la r√©gularisation Ridge ‚òïÔ∏è‚òïÔ∏è‚òïÔ∏è
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../content.html">
   Content in Jupyter Book
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../markdown.html">
     Markdown Files
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../notebooks.html">
     Content with notebooks
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/2_linear_regression/1_linear_regression.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/maximiliense/lmiprp"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/maximiliense/lmiprp/issues/new?title=Issue%20on%20page%20%2F2_linear_regression/1_linear_regression.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/maximiliense/lmiprp/master?urlpath=tree/2_linear_regression/1_linear_regression.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#i-introduction">
   I. Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ii-construction-d-un-jeu-de-donnees">
   II. Construction d‚Äôun jeu de donn√©es
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iii-du-modele-statistique-a-l-optimisation">
   III. Du mod√®le statistique √† l‚Äôoptimisation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#la-fonction-objectif">
     La fonction objectif
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimisation-par-descente-de-gradient">
     Optimisation par Descente de gradient
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-vous-de-jouer">
     √Ä vous de jouer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#l-algorithme-de-descente-de-gradient">
     L‚Äôalgorithme de descente de gradient
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#les-equations-normales-de-la-regression-lineaire-la-solution-par-pseudo-inverse">
     Les √©quations normales de la r√©gression lin√©aire : la solution par pseudo-inverse
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     √Ä vous de jouer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#avec-sklearn">
     Avec sklearn
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iv-features-variables-explicatives-transformees">
   IV. Features - Variables explicatives transform√©es
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#construction-du-jeu-de-donnees-polynomial">
     Construction du jeu de donn√©es polynomial
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#solution-par-pseudo-inverse">
     Solution par pseudo-inverse
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#solution-sklearn">
     Solution Sklearn
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#v-validation-croisee">
   V. Validation crois√©e
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#construction-du-jeu-de-donnees">
     Construction du jeu de donn√©es
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimiser-une-fonction-est-il-suffisant-pour-parler-d-apprentissage">
     Optimiser une fonction est-il suffisant pour parler d‚Äôapprentissage ?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vi-l-effet-double-descente-bonus">
   VI. L‚Äôeffet ‚Äúdouble descente‚Äù (Bonus ?)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vii-regularisation">
   VII. R√©gularisation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     Construction du jeu de donn√©es
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sans-regularisation">
     Sans r√©gularisation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#avec-regularisation-ell-1">
     Avec r√©gularisation
     <span class="math notranslate nohighlight">
      \(\ell_1\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#avec-regularisation-ell-2">
     Avec r√©gularisation
     <span class="math notranslate nohighlight">
      \(\ell_2\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#avec-regularisation-elastic-net">
     Avec r√©gularisation
     <em>
      elastic-net
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#viii-selection-de-modeles">
   VIII. Selection de mod√®les
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     Construction du jeu de donn√©es
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#recherche-exhaustive">
     Recherche exhaustive
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#recherche-aleatoire">
     Recherche al√©atoire
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ix-le-mot-de-la-fin">
   IX. Le mot de la fin
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="la-regression-lineaire">
<h1>La r√©gression lin√©aire ‚òïÔ∏è<a class="headerlink" href="#la-regression-lineaire" title="Permalink to this headline">¬∂</a></h1>
<p>Quelques liens pour aller plus loin :</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/maximiliense/lmpirp/blob/main/Notes/Overfitting.pdf">Overfitting</a></p></li>
<li><p><a class="reference external" href="https://github.com/maximiliense/lmpirp/blob/main/Notes/Regularization.pdf">Regularization</a></p></li>
<li><p><a class="reference external" href="https://github.com/maximiliense/lmpirp/blob/main/Notes/D_composition_QR_et_les_moindres_carr_s.pdf">Least square QR decomposition</a></p></li>
</ul>
<div class="section" id="i-introduction">
<h2>I. Introduction<a class="headerlink" href="#i-introduction" title="Permalink to this headline">¬∂</a></h2>
<p>La r√©gression lin√©aire est un mod√®le cherchant √† √©tablir un lien lin√©aire entre des donn√©es d‚Äôobservation et des donn√©es √† pr√©dire. Plus concr√®tement, les donn√©es observ√©es sont d√©crites par un vecteur <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^d\)</span> et la variable √† pr√©dire, par une quantit√© scalaire (un r√©el) <span class="math notranslate nohighlight">\(y \in \mathbb{R}\)</span> (par un abus de langage important, <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> et <span class="math notranslate nohighlight">\(y\)</span> expriment √† la fois une variable al√©atoire et sa r√©alisation) et le lien s‚Äôexprime sous le format suivant :</p>
<p>\begin{equation}
y = \beta_0 +  x_1 \beta_1 + x_2 \beta_2 + x_3 \beta_3 + ‚Ä¶ + x_d \beta_d +\epsilon = \beta_0 + \sum_i^d x_i \beta_i+\epsilon,\ \epsilon\sim\mathcal{N}(0, \sigma)
\end{equation}</p>
<p>que l‚Äôon peut aussi √©crire en notation vectorielle:</p>
<p>\begin{equation}
y = \beta_0  + \langle \boldsymbol{\beta}, \mathbf{x} \rangle +\epsilon,\ \epsilon\sim\mathcal{N}(0, \sigma)
\end{equation}</p>
<p>o√π <span class="math notranslate nohighlight">\(\boldsymbol{\beta} \in \mathbb{R}^d\)</span> et <span class="math notranslate nohighlight">\(\beta_0\in\mathbb{R}\)</span> correspondent respectivement au vecteur et au scalaire contenant les param√®tres du ‚Äúvrai‚Äù mod√®le qui d√©fini le lien entre les donn√©es et que l‚Äôon va vouloir apprendre pour pr√©dire la bonne valeur de <span class="math notranslate nohighlight">\(y\)</span> en fonction du vecteur <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. Le mod√®le lin√©aire ne peut pr√©dire la variable <span class="math notranslate nohighlight">\(y\)</span> qu‚Äô√† un bruit <span class="math notranslate nohighlight">\(\epsilon\)</span> pr√®s. Une fois ces param√®tres appris par notre algorithme d‚Äôapprentissage, on pourra utiliser la fonction de pr√©diction <span class="math notranslate nohighlight">\(f_{\boldsymbol{\beta}}(\mathbf{x}) : \mathbb{R}^d \rightarrow \mathbb{R}\)</span> apprise pour pr√©dire la valeur <span class="math notranslate nohighlight">\(y_{new}\)</span> associ√©e √† un nouveau vecteur <span class="math notranslate nohighlight">\(\mathbf{x_{new}}\)</span> que l‚Äôon n‚Äôa pas encore observ√©:</p>
<div class="math notranslate nohighlight">
\[\hat{y}_{new} = f_{\boldsymbol{\beta}}(\boldsymbol{x_{new}}) = \beta_0  + \langle \boldsymbol{\beta}, \boldsymbol{x_{new}} \rangle\]</div>
<p>Pour simplifier les calculs et les notations, on pr√©f√®re que la fonction de pr√©diction puisse se calculer √† partir d‚Äôune notation compl√®tement vectorielle. C‚Äôest ce que l‚Äôon fait en pratique, en ajoutant une composante suppl√©mentaire <span class="math notranslate nohighlight">\(x_0\)</span> au vecteur <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> √©gale √† <span class="math notranslate nohighlight">\(1\)</span>:</p>
<p>\begin{align}
\mathbf{x} &amp;= \begin{bmatrix}
1 \
x_{1} \
\vdots \
x_{d}
\end{bmatrix},
\end{align}</p>
<p>de sorte √† ce que la fonction de pr√©diction lin√©aire puisse s‚Äôexprimer simplement sous la forme du produit scalaire:</p>
<p>\begin{align}
f_{\boldsymbol{\beta}}(\mathbf{x}) &amp;= \langle \boldsymbol{\beta}, \mathbf{x} \rangle &amp;=
\begin{bmatrix}
\beta_{0} \<br />
\vdots \
\beta_{d}
\end{bmatrix}^T
\begin{bmatrix}
1 \
\vdots \
x_{d}
\end{bmatrix} &amp;= \sum_{i=0}^d x_i \beta_i=\langle \boldsymbol{x}, \boldsymbol{\beta}\rangle_{\mathbb{R}^{d+1}}
\end{align}</p>
<p>o√π cette fois <span class="math notranslate nohighlight">\(\boldsymbol{\beta} \in \mathbb{R}^{d+1}\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{x} \in \mathbb{R}^{d+1}\)</span> et <span class="math notranslate nohighlight">\(\langle \cdot, \cdot\rangle_{\mathbb{R}^{d+1}}\)</span> est le produit scalaire dans <span class="math notranslate nohighlight">\(\mathbb{R}^{d+1}\)</span>. Le but d‚Äôun algorithme d‚Äôapprentissage sera de trouver un estimateur <span class="math notranslate nohighlight">\(\boldsymbol{\hat{\beta}}\)</span> de <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> √† partir d‚Äôun ensemble fini de <span class="math notranslate nohighlight">\(n\)</span> exemples d‚Äôapprentissage <span class="math notranslate nohighlight">\((\boldsymbol{x}, \langle \boldsymbol{\beta}, \boldsymbol{x} \rangle + \epsilon) \in \mathbb{R}^2\)</span> pr√©alablement collect√©s. On notera <span class="math notranslate nohighlight">\(\mathcal{S}=\{(\boldsymbol{x_i}, y_i)\}_{i\leq n}\)</span> le jeu de donn√©es.</p>
<p>Nous commencerons par impl√©menter le cas simple d‚Äôune r√©gr√©ssion lin√©aire √† une seule variable d‚Äôentr√©e et une seule variable de sortie qui pourra donc s‚Äô√©crire sous la forme :</p>
<p>\begin{equation}
\hat{y} = f_{\boldsymbol{\beta}}(\mathbf{x}) = \beta_0  + \beta_1 x
\end{equation}</p>
<p>C‚Äôest √† dire une ‚Äúbrave‚Äù fonction affine dont on pourra afficher la repr√©sentation graphique (une droite) sur une figure en 2 dimensions. Par la suite vous aurez donc √† impl√©menter le calcul de la fonction de co√ªt du mod√®le sur l‚Äôensemble d‚Äôapprentissage, le calcul du gradient de cette fonction de co√ªt ainsi que l‚Äôalgorithme de descente de gradient qui, √† partir du gradient, permet d‚Äôobtenir le vecteur <span class="math notranslate nohighlight">\(\hat{\beta}\)</span>.</p>
<p>La seconde partie de ce notebook √©tendra ces notions √† des concepts plus compliqu√©s.</p>
</div>
<div class="section" id="ii-construction-d-un-jeu-de-donnees">
<h2>II. Construction d‚Äôun jeu de donn√©es<a class="headerlink" href="#ii-construction-d-un-jeu-de-donnees" title="Permalink to this headline">¬∂</a></h2>
<p>Commen√ßons tout d‚Äôabord par simuler notre jeu de donn√©es avec le mod√®le g√©n√©ratif suivant :</p>
<p>\begin{equation}
\boldsymbol{x} \sim \mathcal{N}(\mu, \sigma) \in \mathbb{R}
\end{equation}</p>
<p>ou <span class="math notranslate nohighlight">\(\sigma\)</span> correspond √† la variance de la variable explicative. Nous choissons une r√®gle arbitraire pour g√©n√©rer al√©atoirement les param√®tres du ‚Äúvrai‚Äù mod√®le :</p>
<p>\begin{equation}
\boldsymbol{\beta} \sim \mathbb{U}(-1, 1)^2 \in \mathbb{R}^2
\end{equation}</p>
<p>o√π <span class="math notranslate nohighlight">\(\mathbb{U}^2\)</span> est la loi uniforme dans <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>. Enfin, le bruit est construit de la mani√®re suivante :</p>
<p>\begin{equation}
\epsilon \sim \mathcal{N}(0, 1).
\end{equation}</p>
<p>Chaque exemple d‚Äôapprentissage correspond donc √† un couple de r√©els <span class="math notranslate nohighlight">\((x_j, y_j = \beta_0  + \beta_1 x_j + \epsilon) \in \mathbb{R}^2\)</span>. Le code ci dessous construit et affiche le jeux de donn√©es ainsi que la repr√©sentation graphique de <span class="math notranslate nohighlight">\(f(x)=\beta_1x+\beta_0\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># on simule le vecteur de parametre</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># on construit un jeu de donnees de 10 points selon la methode </span>
<span class="c1"># decrite ci-dessus.</span>
<span class="k">def</span> <span class="nf">sample_data</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">add_noise</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">add_noise</span><span class="o">*</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">beta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">noise</span> <span class="o">+</span> <span class="n">beta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_data</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">add_noise</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># jouer avec le bruit</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># configuration generale de matplotlib</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">12.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>

<span class="c1"># plot de la fonction</span>
<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">ymin_</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="n">eps</span>
    <span class="n">ymax_</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">eps</span>
    <span class="n">min_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="n">eps</span>
    <span class="n">max_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="n">eps</span>
    <span class="k">if</span> <span class="n">beta</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">min_</span><span class="p">,</span> <span class="n">max_</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">y_</span> <span class="o">=</span> <span class="n">beta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_</span> <span class="o">+</span> <span class="n">beta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span> <span class="n">y_</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">func</span> <span class="o">=</span> <span class="p">[(</span><span class="n">func</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)]</span> <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">func</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">list</span> <span class="k">else</span> <span class="n">func</span>
            <span class="n">disp_legend</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">func</span><span class="p">:</span>
                <span class="n">y_</span> <span class="o">=</span> <span class="n">t</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x_</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">x_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)))</span>
                <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span> <span class="n">y_</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">t</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">disp_legend</span> <span class="o">=</span> <span class="n">disp_legend</span> <span class="ow">or</span> <span class="n">t</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="s1">&#39;&#39;</span>
            <span class="k">if</span> <span class="n">disp_legend</span><span class="p">:</span>
                <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="n">min_</span><span class="p">,</span> <span class="n">max_</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="n">ymin_</span><span class="p">,</span> <span class="n">ymax_</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    
<span class="c1"># on plot le dataset precedent</span>
<span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_7_0.png" src="../_images/1_linear_regression_7_0.png" />
</div>
</div>
<hr class="docutils" />
<p><span style="color:blue"><strong>Question :</strong></span> <strong>Que se passe-t-il si le bruit <span class="math notranslate nohighlight">\(\epsilon\)</span> est nul ? Quelle est alors la m√©thode la plus rapide pour trouver les param√®tres du vrai mod√®le ?</strong></p>
<hr class="docutils" />
<p><strong><span style="color:green">R√©ponse:</span></strong> Prenons le cas d‚Äôune r√©gression <span class="math notranslate nohighlight">\(f_\beta:\mathbb{R}\mapsto\mathbb{R}\)</span>. Si le bruit <span class="math notranslate nohighlight">\(\epsilon\)</span> est nul, alors l‚Äôensemble des couples <span class="math notranslate nohighlight">\((x, y)\)</span> sont align√©s sur la m√™me droite. Ainsi, √©tant donn√© un jeu de donn√©es de taille <span class="math notranslate nohighlight">\(2\)</span>, <span class="math notranslate nohighlight">\((x_1, y_1)\)</span> et <span class="math notranslate nohighlight">\((x_2, y_2)\)</span> tel que <span class="math notranslate nohighlight">\(x_1\neq x_2\)</span>, nous pouvons calculer le coefficient directeur de la droite, not√© <span class="math notranslate nohighlight">\(\beta_1\)</span>:</p>
<p>\begin{equation}
\beta_1=\frac{y_2-y_1}{x_2-x_1}
\end{equation}</p>
<p>Le coefficient directeur obtenu, nous pouvons calculer le biais <span class="math notranslate nohighlight">\(\beta_0\)</span>:</p>
<p>\begin{equation}
\beta_0=y_1-\beta_1x_1.
\end{equation}</p>
</div>
<div class="section" id="iii-du-modele-statistique-a-l-optimisation">
<h2>III. Du mod√®le statistique √† l‚Äôoptimisation<a class="headerlink" href="#iii-du-modele-statistique-a-l-optimisation" title="Permalink to this headline">¬∂</a></h2>
<div class="section" id="la-fonction-objectif">
<h3>La fonction objectif<a class="headerlink" href="#la-fonction-objectif" title="Permalink to this headline">¬∂</a></h3>
<p>Nous souhaitons en pratique trouver un param√™tre <span class="math notranslate nohighlight">\(\boldsymbol{\hat{\beta}}\)</span> qui minimise le risque du mod√®le, c‚Äôest-√†-dire la quantit√© d‚Äôerreur en esp√©rance de n‚Äôimporte quel mod√®le <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>. On notera <span class="math notranslate nohighlight">\(\boldsymbol{\beta}^\star\)</span> le ‚Äúvrai‚Äù mod√®le, soit celui qui minimise le risque en esp√©rance. Pour la r√©gression lin√©aire, on peut d√©finir ce risque comme :</p>
<div class="math notranslate nohighlight">
\[R(\boldsymbol{\beta}) = \mathbb{E}_{X\times Y}\Big[ (f_{\boldsymbol{\beta}}(\mathbf{X}) - Y)^2 \Big].\]</div>
<p>On ne sait pas calculer cette fonction. Cependant, on peut en avoir un estimateur via un jeu de donn√©es <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>, o√π <span class="math notranslate nohighlight">\(\mathcal{S} = \Big\{ \big(\boldsymbol{x_j}, y_j \big) \Big\}_{j\leq n}\)</span> est un jeu de donn√©es compos√© de <span class="math notranslate nohighlight">\(n\)</span> points ind√©pendants et identiquement distribu√©s selon le mod√®le g√©n√©ratif d√©crit pr√©c√©dement.</p>
<p>A d√©faut d‚Äôavoir acc√®s au risque (i.e. √† l‚Äôerreur en esp√©rance), on peut utiliser une autre quantit√© qui consiste en la somme des carr√©s des erreurs de pr√©dictions pour chaque exemple d‚Äôapprentissage, c‚Äôest <strong>le risque emprique</strong> :</p>
<p>\begin{equation}
J(\boldsymbol{\beta}) = R_{emp}(\boldsymbol{\beta}) = \frac{1}{n}\sum_j^n (f_{\boldsymbol{\beta}}(x_j) - y_j)^2
\end{equation}</p>
<p>o√π <span class="math notranslate nohighlight">\(f_{\boldsymbol{\beta}}(x_j) = \beta_0  + \beta_1 x_j\)</span>. On montre assez facilement que pour un <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> quelconque :</p>
<p>\begin{equation}
R(\boldsymbol{\beta})=\mathbb{E}_{\mathcal{S \sim \mathbb{P}_S}}\big[J(\boldsymbol{\beta})\big],
\end{equation}</p>
<p>Notons que minimiser ce risque empirique revient √† chercher le maximum de vraisemblance du mod√®le statistique. Effectivement, avec l‚Äôhypoth√®se gaussienne, la vraissemblance de n‚Äôimporte quel mod√®le de param√®tres <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> pour un jeu de donn√©es <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> peut s‚Äô√©crire :</p>
<p>\begin{equation}
L_{\mathcal{S}}(\boldsymbol{\beta}) \propto \prod_{\boldsymbol{x}\times y\in\mathcal{S}} \exp\Bigg(-\frac{\big(f_{\boldsymbol{\beta}}(\mathbf{x}) - y\big)^2}{2}\Bigg)
\end{equation}</p>
<p>Le param√®tre maximisant la vraissamblance est aussi celui minimisant la log-vraissamblance n√©gative :</p>
<div class="math notranslate nohighlight">
\[- \text{log} \Big( L_{\mathcal{S}}(\boldsymbol{\beta})\Big) = \sum_{\boldsymbol{x}\times y\in\mathcal{S}}\frac{\big(f_{\boldsymbol{\beta}}(\mathbf{x}) - y\big)^2}{2}\propto\sum_{\boldsymbol{x}\times y\in\mathcal{S}}\big(f_{\boldsymbol{\beta}}(\mathbf{x}) - y\big)^2\]</div>
<p>N‚Äôayant acc√®s au v√©ritable risque, on cherche <span class="math notranslate nohighlight">\(\boldsymbol{\hat{\beta}}\)</span> tel que :</p>
<p>\begin{equation}
\boldsymbol{\hat{\beta}} = argmin_{\boldsymbol{\beta}} \Big[ - \log \Big( L_{\mathcal{S}}(\boldsymbol{\beta})\Big) \Big]
\end{equation}</p>
<p>Minimiser le risque emprique se traduit donc naturellement par un probl√®me d‚Äôoptimisation de la fonction de co√ªt <span class="math notranslate nohighlight">\(J(\boldsymbol{\beta}) : \mathbb{R}^2 \rightarrow \mathbb{R}\)</span> (<span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span> dans notre exemple courant, <span class="math notranslate nohighlight">\(\mathbb{R}^{d+1}\)</span> dans le cas g√©n√©ral) par rapport √† <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>.</p>
<p>En pratique et pour des raisons de simplicit√©, on ne minimise pas <span class="math notranslate nohighlight">\(\sum_{\boldsymbol{x}\times y\in\mathcal{S}}\big(f_{\boldsymbol{\beta}}(\mathbf{x}) - y\big)^2\)</span> mais :</p>
<p>\begin{equation}
J(\boldsymbol{\beta}) = \frac{1}{2n}\sum_{\boldsymbol{x}\times y\in\mathcal{S}} (f_{\boldsymbol{\beta}}(x) - y)^2
\end{equation}</p>
<p>Le r√©sultat est bien √©videmment le m√™me. La division par <span class="math notranslate nohighlight">\(2\)</span> est l√† pour simplifier l‚Äôexpresion du gradient que l‚Äôon calculera et la division par <span class="math notranslate nohighlight">\(n\)</span> permet de rendre la norme du gradient ind√©pendente de la taille de notre jeu de donn√©es. C‚Äôest une propri√©t√© importante pour l‚Äôalgorithme de descente de gradient dont la taille des d√©placements affecte sa stabilit√©.</p>
<p><strong>Note - Notation vectorielle de la r√©gression lin√©aire :</strong> On peut aussi exprimer ce calcul avec une simple √©quation en notation vectorielle. Pour cela, on exprime dans un premier temps le r√©sultat de la fonction de pr√©diction en notation vectorielle (il s‚Äôagit de la pr√©diction pour tout notre jeu de donn√©es) :</p>
<p>\begin{equation}
f_{\boldsymbol{\beta}}(\mathbf{X}) = \mathbf{X}\boldsymbol{\beta}\in\mathbb{R}^n
\end{equation}</p>
<p>o√π <span class="math notranslate nohighlight">\(\boldsymbol{\beta} \in \mathbb{R}^{d+1}\)</span> est une matrice de dimensions <span class="math notranslate nohighlight">\((d+1)\times 1\)</span> et <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> est une matrice de dimensions <span class="math notranslate nohighlight">\(n\times (d+1)\)</span> dont les <span class="math notranslate nohighlight">\(n\)</span> vecteurs lignes correspondent aux vecteurs d‚Äôapprentissage d‚Äôentr√©e. Dans notre cas (celui de la r√©gression lin√©aire √† <span class="math notranslate nohighlight">\(1\)</span> variable) la matrice prend la forme suivante :</p>
<p>\begin{equation}
\mathbf{X} =
\begin{pmatrix}
1 &amp; x_{1} \
. &amp; .\
1 &amp; x_{j} \
. &amp; .\
1 &amp; x_{n}
\end{pmatrix},\ \boldsymbol{\beta}=
\begin{bmatrix}
\beta_{0} \<br />
\beta_{1}
\end{bmatrix}
\end{equation}</p>
<p>La fonction de co√ªt peut ainsi s‚Äôexprimer :</p>
<p>\begin{equation}
J(\boldsymbol{\beta}) = \frac{1}{2n} (\mathbf{X}\boldsymbol{\beta} - \mathbf{y})^T(\mathbf{X}\boldsymbol{\beta} - \mathbf{y})
\end{equation}</p>
<p>que l‚Äôon peut r√©√©crire :</p>
<p>\begin{equation}
J(\boldsymbol{\beta}) = \frac{1}{2n} (\mathbf{\hat{y}} - \mathbf{y})^T(\mathbf{\hat{y}} - \mathbf{y}) =  \frac{1}{2n} ||\mathbf{\hat{y}} - \mathbf{y}||_2^2
\end{equation}</p>
<p>o√π <span class="math notranslate nohighlight">\(\mathbf{y} \in  \mathbb{R}^n\)</span> est le vecteur dont chacune des composantes <span class="math notranslate nohighlight">\(y_j\)</span> sont les valeurs √† pr√©dire √† partir de leur <span class="math notranslate nohighlight">\(x_j\)</span> correspondant, et <span class="math notranslate nohighlight">\(\hat{y} \in  \mathbb{R}^n\)</span> correspond aux valeurs pr√©dites par le mod√®le. On note ici que la fonction objectif √† optimiser peut se calculer ais√©ment en utilisant la norme euclidienne du vecteur d‚Äôerreur.</p>
</div>
<div class="section" id="optimisation-par-descente-de-gradient">
<h3>Optimisation par Descente de gradient<a class="headerlink" href="#optimisation-par-descente-de-gradient" title="Permalink to this headline">¬∂</a></h3>
<p>La descente de gradient est une m√©thode d‚Äôoptimisation num√©rique permettant de trouver les valeurs des param√®tres qui minimisent une fonction. Dans notre cas, nous voulons minimiser l‚Äôerreur de pr√©diction moyenne de notre mod√®le, fonction d√©finie pr√©c√©demment. Cette m√©thode d‚Äôoptimisation consiste √† calculer le gradient de notre fonction objectif par rapport aux param√®tres courant du mod√®les et de les d√©placer ‚Äúpetite‚Äù translation dans la direction oppos√©e au gradient (i.e. le gradient donne la plus forte croissance et son oppos√© la plus forte d√©croissance).</p>
<p><strong>D√©finition g√©n√©rale du gradient d‚Äôune fonction √† plusieurs variables :</strong> Il s‚Äôagit simplement du vecteur contenant les d√©riv√©es partielles de la fonction, c-√†-d les d√©riv√©es de la fonction par rapport √† chaque variable ind√©pendamment des autres:</p>
<p>\begin{equation}
\nabla_{\boldsymbol{\beta}} J(\boldsymbol{\beta}) = \frac{\partial J(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} =
\begin{bmatrix}
\frac{\partial J(\beta)}{\partial \beta_0}\
\frac{\partial J(\beta)}{\partial \beta_1}\
\vdots \
\frac{\partial J(\beta)}{\partial \beta_d}
\end{bmatrix}
\end{equation}</p>
<p>En descente de gradient, la mise √† jour de chaque param√®tre <span class="math notranslate nohighlight">\(\beta_j\)</span> du mod√®le √† l‚Äôit√©ration <span class="math notranslate nohighlight">\(t\)</span> se fait donc avec la r√®gle suivante:</p>
<p>\begin{equation}
\beta_j^{(t+1)} = \beta_j^{(t)} - \rho  \frac{\partial J(\beta^{(t)})}{\partial \beta_j}
\end{equation}</p>
<p>ou bien, en notation vectorielle:</p>
<p>\begin{equation}
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \rho  \nabla_{\boldsymbol{\beta}} J(\boldsymbol{\beta})^{(t)}
\end{equation}</p>
<p>o√π <span class="math notranslate nohighlight">\(\rho\)</span> est le learning rate (pas d‚Äôapprentissage). Un pas d‚Äôapprentissage <span class="math notranslate nohighlight">\(\rho\)</span> trop petit nous fera nous d√©placer trop lentement et trop grand rendra l‚Äôoptimisation instable.</p>
</div>
<div class="section" id="a-vous-de-jouer">
<h3>√Ä vous de jouer<a class="headerlink" href="#a-vous-de-jouer" title="Permalink to this headline">¬∂</a></h3>
<hr class="docutils" />
<p><span style="color:blue"><strong>Question 1 :</strong></span> <strong>Compl√©tez la m√©thode <span class="math notranslate nohighlight">\(\texttt{val}\)</span> de l‚Äôobjet <span class="math notranslate nohighlight">\(\texttt{LeastSquare}\)</span> ci-dessous.</strong></p>
<p><span style="color:blue"><strong>Question 2 :</strong></span> <strong>Calculez les d√©riv√©es partielles <span class="math notranslate nohighlight">\(\partial J(\beta)/\partial \beta_0\)</span> et <span class="math notranslate nohighlight">\(\partial J(\beta)/\partial \beta_1\)</span> de la fonction de co√ªt de notre mod√®le de r√©gr√©ssion lin√©aire. Compl√©tez la m√©thode <span class="math notranslate nohighlight">\(\texttt{grad}\)</span> de l‚Äôobjet <span class="math notranslate nohighlight">\(\texttt{LeastSquare}\)</span> ci dessous.</strong></p>
<p><strong><span style="color:orange">Indice</span></strong>  Rappellez vous que la d√©riv√©e d‚Äôune composition de fonction s‚Äô√©crit <span class="math notranslate nohighlight">\((g \circ f)^\prime (x) = f^\prime(x) g^\prime(f(x))\)</span> et que la fonction de co√ªt de notre mod√®le s‚Äô√©crit :</p>
<p>\begin{equation}
J(\boldsymbol{\beta}) = \frac{1}{2n}\sum_j^n g(f_{\boldsymbol{\beta}}(x_j) - y_j)
\end{equation}</p>
<p>avec <span class="math notranslate nohighlight">\(g(z) = z ^ 2\)</span> et <span class="math notranslate nohighlight">\(f_{\boldsymbol{\beta}}(x_j) = \beta_0  + \beta_1 x_j\)</span>.</p>
<p><strong><span style="color:green">R√©ponse :</span></strong></p>
<p>\begin{equation}
\frac{\partial J(\beta)}{\partial \beta_j} = \frac{1}{n}\sum_i^n (f_{\boldsymbol{\beta}}(\mathbf{x_i}) - y_i)   x_i^j
\end{equation}</p>
<p><span style="color:blue"><strong>Question 2<span class="math notranslate nohighlight">\({}^\star\)</span> :</strong></span> <strong>Calculez le gradient de la fonction <span class="math notranslate nohighlight">\(J(\boldsymbol{\beta})\)</span> en utilisant les d√©riv√©es matricielles. Compl√©tez la m√©thode <span class="math notranslate nohighlight">\(\texttt{grad}\)</span> de l‚Äôobjet <span class="math notranslate nohighlight">\(\texttt{LeastSquare}\)</span> avec le gradient en notation vectorielle.</strong></p>
<p><strong><span style="color:green">R√©ponse:</span></strong></p>
<p>Le gradient est obtenu par <span class="math notranslate nohighlight">\(\nabla_\beta J(\boldsymbol{\beta})=X^TX\boldsymbol{\beta}-X^T\boldsymbol{y}\)</span>.</p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Le code ci-dessous permettra d&#39;afficher notre fonction de cout (le risque empirique)</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="o">%</span><span class="k">config</span> InlineBackend.print_figure_kwargs = {&#39;bbox_inches&#39;:None}
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">12.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">plot_loss_contour</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">param_trace</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">three_dim</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rotate</span><span class="o">=</span><span class="mi">12</span><span class="p">):</span>
    
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mgrid</span><span class="p">[</span><span class="nb">slice</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span> <span class="o">+</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">),</span>
                    <span class="nb">slice</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span> <span class="o">+</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)]</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">l</span><span class="o">.</span><span class="n">val</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]])</span>
    <span class="k">if</span> <span class="n">figsize</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">f</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">f</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">12.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">three_dim</span><span class="p">:</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">gca</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    
    <span class="k">if</span> <span class="n">three_dim</span><span class="p">:</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">viridis</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">levels</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
    <span class="c1">#</span>
    <span class="k">if</span> <span class="n">param_trace</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">three_dim</span><span class="p">:</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="mf">0.5</span>
            <span class="c1">#ax.scatter(param_trace[:, 0], param_trace[:, 1], param_trace[:, 2] + eps, </span>
            <span class="c1">#           color=&#39;blue&#39;, alpha=1)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">param_trace</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">param_trace</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">param_trace</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">eps</span><span class="p">,</span> 
                    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">rotate</span><span class="p">)</span>
                
        <span class="k">else</span><span class="p">:</span>
            <span class="n">param_trace</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">param_trace</span><span class="p">)</span> <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">param_trace</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">list</span> <span class="k">else</span> <span class="n">param_trace</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">param_trace</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">param_trace</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">param_trace</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">param_trace</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
            <span class="n">f</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="c1">#plt.show()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LeastSquare</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">LeastSquare</span><span class="o">.</span><span class="n">_format_ndarray</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">LeastSquare</span><span class="o">.</span><span class="n">_format_ndarray</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pos</span> <span class="o">=</span> <span class="mi">0</span>
        
    <span class="k">def</span> <span class="nf">_format_ndarray</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
        <span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="k">else</span> <span class="n">arr</span>
        <span class="k">return</span> <span class="n">arr</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">arr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">arr</span>
    
    <span class="k">def</span> <span class="nf">val</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">LeastSquare</span><span class="o">.</span><span class="n">_format_ndarray</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
        <span class="c1">####### Complete this part ######## or die ####################</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">beta</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">errors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">prediction</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">val</span> <span class="o">=</span> <span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="c1">###############################################################</span>
        <span class="k">return</span> <span class="n">val</span>
    
    <span class="k">def</span> <span class="nf">_shuffle</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">batch_size</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span> <span class="k">else</span> <span class="n">batch_size</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_pos</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">_pos</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_pos</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pos</span><span class="o">+</span><span class="n">batch_size</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pos</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_shuffle</span><span class="p">()</span>
            
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

        <span class="n">beta</span> <span class="o">=</span> <span class="n">LeastSquare</span><span class="o">.</span><span class="n">_format_ndarray</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
        
        <span class="c1">####### Complete this part ######## or die ####################</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">),</span> <span class="n">beta</span><span class="p">)</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span><span class="o">/</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1">###############################################################</span>
        <span class="k">return</span> <span class="n">grad</span>
    

<span class="n">l</span> <span class="o">=</span> <span class="n">LeastSquare</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;La valeur de la loss pour le vrai parametre est&#39;</span><span class="p">,</span> <span class="n">l</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">beta</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;La valeur du gradient pour le vrai parametre est</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">l</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">beta</span><span class="p">))</span>
<span class="n">plot_loss_contour</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">three_dim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plot_loss_contour</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">three_dim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>La valeur de la loss pour le vrai parametre est 1.1310123897991868
La valeur du gradient pour le vrai parametre est
 [[ 0.35210703]
 [-0.71255159]]
</pre></div>
</div>
<img alt="../_images/1_linear_regression_17_1.png" src="../_images/1_linear_regression_17_1.png" />
<img alt="../_images/1_linear_regression_17_2.png" src="../_images/1_linear_regression_17_2.png" />
</div>
</div>
<p>Attention, pour des raisons de temps de calcul, l‚Äôestimation du gradient n‚Äôest pas tout le temps faite sur tout le jeu de donn√©es mais sur une partie de celui-ci. Un estimateur calcul√© de cette mani√®re l√† aura en esp√©rance la m√™me valeur qu‚Äôun gradient calcul√© sur toutes les donn√©es. On appelle g√©n√©ralement Descente de Gradient Stochastique oiu SGD une approche qui ne fait qu‚Äôestimer le gradient √† partir d‚Äôun batch de donn√©es.</p>
<hr class="docutils" />
<p><span style="color:blue"><strong>Question :</strong></span> <strong>Saurez-vous retrouver dans le code ci-dessus ce qui permet de jouer sur la taille du batch lors du calcul du gradient ?</strong></p>
</div>
<hr class="docutils" />
<div class="section" id="l-algorithme-de-descente-de-gradient">
<h3>L‚Äôalgorithme de descente de gradient<a class="headerlink" href="#l-algorithme-de-descente-de-gradient" title="Permalink to this headline">¬∂</a></h3>
<hr class="docutils" />
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Compl√©tez le code de descente de gradient.</strong></p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GradientDescent</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="n">init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">LeastSquare</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.005</span><span class="p">,</span> <span class="n">nb_iterations</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">init</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">param_trace</span> <span class="o">=</span> <span class="p">[</span><span class="n">beta</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
        <span class="n">loss_trace</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">beta</span><span class="p">)]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_iterations</span><span class="p">):</span>
            <span class="c1">####### Complete this part ######## or die ####################</span>
            <span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
            <span class="c1">###############################################################</span>
            <span class="n">param_trace</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">beta</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">loss_trace</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">beta</span><span class="p">))</span>
            
        <span class="k">return</span> <span class="n">param_trace</span><span class="p">,</span> <span class="n">loss_trace</span>
        
<span class="n">gd</span> <span class="o">=</span> <span class="n">GradientDescent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">param_trace</span><span class="p">,</span> <span class="n">loss_trace</span> <span class="o">=</span> <span class="n">gd</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">nb_iterations</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">param_trace</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">param_trace</span><span class="p">)</span>
<span class="n">loss_trace</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">loss_trace</span><span class="p">)</span>
<span class="n">loss_trace</span> <span class="o">=</span> <span class="n">loss_trace</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">loss_trace</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">xyz</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">param_trace</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">loss_trace</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plot_loss_contour</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">param_trace</span><span class="o">=</span><span class="n">xyz</span><span class="p">,</span> <span class="n">three_dim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plot_loss_contour</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">param_trace</span><span class="o">=</span><span class="n">param_trace</span><span class="p">,</span> <span class="n">three_dim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_23_0.png" src="../_images/1_linear_regression_23_0.png" />
<img alt="../_images/1_linear_regression_23_1.png" src="../_images/1_linear_regression_23_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">clear_output</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="n">output</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">Output</span><span class="p">()</span>

<span class="nd">@output</span><span class="o">.</span><span class="n">capture</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">interactive_gradient_descent</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="n">clear_output</span><span class="p">()</span>
    <span class="n">param_trace</span> <span class="p">,</span> <span class="n">loss_trace</span> <span class="o">=</span> <span class="n">gd</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">)</span>
    <span class="n">plot_loss_contour</span><span class="p">(</span><span class="n">gd</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span> <span class="n">param_trace</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">14.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">widgets</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">interactive_gradient_descent</span><span class="p">,</span>
                 <span class="n">learning_rate</span><span class="o">=</span><span class="n">widgets</span><span class="o">.</span><span class="n">FloatSlider</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> 
                                                   <span class="n">continuous_update</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">readout_format</span><span class="o">=</span><span class="s1">&#39;.5f&#39;</span><span class="p">),</span>
                 <span class="n">batch_size</span><span class="o">=</span><span class="n">widgets</span><span class="o">.</span><span class="n">IntSlider</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">min</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                                              <span class="n">continuous_update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "c945e9a87742490396124b512d299f36"}
</script><script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "5a2d8bae14794814b9ffc0462b6268fc"}
</script></div>
</div>
<p><strong>Remarques et questions sur GD</strong> : On constate que la descente de gradient (GD) se d√©place bien orthogonalement aux lignes de niveaux de la fonction de co√ªt. C‚Äôest une propri√©t√© du gradient d‚Äôune fonction. √Ä mesure qu‚Äôon avance vers le minimum de la fonction, on se d√©place de plus en plus lentement vers ce dernier. Pouvez vous dire pourquoi intuitivement ou analytiquement en regardant l‚Äôexpression du gradient que vous avez d√©riv√© plus haut ? Soit <span class="math notranslate nohighlight">\(\boldsymbol{\beta}^{(0)} = [0.0,  0.0]^T\)</span> et <span class="math notranslate nohighlight">\(\rho\)</span>, montrez que GD converge necessairement vers la solution otpimale (cette question est interessante par rapport √† la partie suivante sur les √©quations normales de la r√©gression lin√©aire et la notion de solution par pseudo inverse).</p>
<p><strong>Remarques et questions sur SGD (avec un gradient estim√© sur un sous-ensemble)</strong> : La propri√©t√© d‚Äôorthogonalit√© par rapport aux lignes de niveau de la fonction de co√ªt est elle conserv√©e dans ce cas ? Pourquoi ? Ne suit-on pourtant toujours pas le gradient ? Que pouvez vous dire sur la nature et la ‚Äúvitesse‚Äù de convergence vers le minimum de la fonction ? R√©fl√©chissez d‚Äôun point de vue calculatoire sur ce qui se passe sur des tailles d‚Äô√©chantillons tr√®s grandes ?</p>
</div>
<div class="section" id="les-equations-normales-de-la-regression-lineaire-la-solution-par-pseudo-inverse">
<h3>Les √©quations normales de la r√©gression lin√©aire : la solution par pseudo-inverse<a class="headerlink" href="#les-equations-normales-de-la-regression-lineaire-la-solution-par-pseudo-inverse" title="Permalink to this headline">¬∂</a></h3>
<p>Comme calcul√© plus haut, l‚Äôexpression du gradient est donn√©e par <span class="math notranslate nohighlight">\(X^TX\boldsymbol{\beta}-X^T\boldsymbol{y}\)</span>.  La fonction <span class="math notranslate nohighlight">\(J\)</span> √©tant coercive et convexe, elle admet au moins un minimum local/global. Les points critiques sont donn√©s en annulant le gradient :</p>
<p>\begin{equation}
X^TX\boldsymbol{\beta}-X^T\boldsymbol{y} = 0 \Leftrightarrow X^TX\boldsymbol{\beta}=X^t\boldsymbol{y}.
\end{equation}</p>
<p>Il s‚Äôagit des √©quations dites ‚Äúnormales‚Äù. Tout vecteur <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> solution de ces √©quations est donc n√©cessairement un minimiseur de <span class="math notranslate nohighlight">\(J(\boldsymbol{\beta})\)</span>.</p>
<p><strong>Dans le cas standard</strong> o√π chaque variable explicative est lin√©airement ind√©pendante des autres et o√π le nombre d‚Äô√©chantillons de notre jeu de donn√©es est sup√©rieur ou √©gal √† la dimension du probl√®me consid√©r√©, la matrice <span class="math notranslate nohighlight">\(X^TX\)</span> est inversible (i.e. <span class="math notranslate nohighlight">\(\text{det}(X^TX)\neq 0\)</span>). Dit autrement, il existe une unique solution aux √©quations normales donn√©e par :</p>
<p>\begin{equation}
\hat{\boldsymbol{\beta}}=(X^TX)^{-1}X^T\boldsymbol{y}.
\end{equation}</p>
<p>On appelle <span class="math notranslate nohighlight">\(X^\dagger = (X^TX)^{-1}X^T\)</span>  pseudo-inverse de <span class="math notranslate nohighlight">\(X\)</span> (ou inverse g√©n√©ralis√©) et la solution analytique √† notre probl√®me est donn√©e par <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}=X^\dagger \boldsymbol{y}\)</span>.</p>
<p><strong>Dans le cas non standard</strong> o√π certaines variables peuvent √™tre des combinaisons lin√©aires d‚Äôautres variables (inutile en pratique) ou si le nombre d‚Äô√©chantillons est inf√©rieur √† la dimension, <span class="math notranslate nohighlight">\(X^TX\)</span> n‚Äôest plus inversible. Dans ce cas de figure, il existe une infinit√© de solutions aux √©quations normales (i.e. une infinit√© de minimiseurs). E. H. Moore (1920), A. Bjerhammar (1951) et R. Penrose (1955) proposent ind√©pendamment une expression g√©n√©rale de <span class="math notranslate nohighlight">\(X^\dagger\)</span> appel√©e pseudo-inverse de Moore-Penrose et calculable √† partir d‚Äôune d√©composition en valeur singuli√®re, not√©e <span class="math notranslate nohighlight">\(X^\dagger\)</span>. Celle-ci co√Øncide bien s√ªr avec l‚Äôexpression standard lorsqu‚Äôelle existe. On obtient donc une expression analytique g√©n√©rale, solution des √©quations normales :</p>
<p>\begin{equation}
\hat{\boldsymbol{\beta}}=X^\dagger\boldsymbol{y},
\end{equation}</p>
<p>o√π <span class="math notranslate nohighlight">\(X^\dagger\)</span> est le pseudo-inverse de Moore-Penrose.</p>
<p><em><strong>Quelques pr√©cisions d‚Äôalg√®bre</strong></em> : finalement, quel est le lien entre une pseudo-inverse et l‚Äôinverse classique. Soit une application lin√©aire <span class="math notranslate nohighlight">\(A:\mathbb{R}^n\mapsto\mathbb{R}^n\)</span> repr√©sent√©e par une matrice <span class="math notranslate nohighlight">\(A\in\mathbb{R}^{n\times n}\)</span>. On appelle inverse de <span class="math notranslate nohighlight">\(A\)</span> l‚Äôunique matrice, not√©e <span class="math notranslate nohighlight">\(A^{-1}\)</span>, telle que  <span class="math notranslate nohighlight">\(A^{-1}A=\text{Id}\)</span>. Dans le cas inversible, l‚Äôinverse de <span class="math notranslate nohighlight">\(A^{-1}\)</span> est donc de mani√®re √©vidente <span class="math notranslate nohighlight">\(A\)</span>. Cela revient √† transformer un vecteur <span class="math notranslate nohighlight">\(x\in\mathbb{R}^n\)</span> par <span class="math notranslate nohighlight">\(A\)</span> puis √† annuler sa transformation par <span class="math notranslate nohighlight">\(A^{-1}\)</span>. L‚Äôinverse n‚Äôexiste cependant pas toujours. Ainsi, par exemple, si <span class="math notranslate nohighlight">\(\text{ker}(A)\neq \{\boldsymbol{0}\}\)</span> (i.e. le noyau ne se r√©sume pas √† l‚Äô√©l√©ment null, nous avons <span class="math notranslate nohighlight">\(\forall x\in\mathbb{R}^n,\ u\in\text{ker}(A)\)</span> que <span class="math notranslate nohighlight">\(A(x+u)=Ax\)</span>. Finalement l‚Äôinverse de <span class="math notranslate nohighlight">\(Ax\)</span> est-il <span class="math notranslate nohighlight">\(x\)</span> ou <span class="math notranslate nohighlight">\(x+u\)</span> ?</p>
<p>Reprenons le cas inversible. Quelques propri√©t√©s qui peuvent sembler √©videntes √©mergent :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
AA^{-1}A&amp;=A\text{ (appliquer }A\text{, son inverse }A^{-1}\text{ puis }A\text{ √† nouveau revient √† appliquer }A\text{)}\\
A^{-1}AA^{-1}&amp;=A^{-1}\text{ (c'est la m√™me chose du point de vu de l'inverse)}\\
(AA^{-1})^T&amp;=AA^{-1}\text{ (la transposition n'a pas d'effet)}\\
(A^{-1}A)^T&amp;=A^{-1}A\text{ (m√™me chose que pr√©c√©demment du point de vu de l'inverse)}
\end{aligned}\end{split}\]</div>
<p>La pseudo inverse est l‚Äôunique matrice <span class="math notranslate nohighlight">\(A^\dagger\)</span> satisfaisant les propri√©t√©s pr√©c√©dentes. Dans le cas o√π <span class="math notranslate nohighlight">\(A\)</span> est inversible, on a alors <span class="math notranslate nohighlight">\(A^\dagger=A^{-1}\)</span>. Intuitivement, l‚Äôid√©e est de ne consid√©rer ‚Äúque‚Äù les √©l√©ments qui ne sont pas dans le noyaux. Ainsi <span class="math notranslate nohighlight">\(\text{Im}(A)=\text{Ker}(A^\dagger)^\perp\)</span> et inversement.</p>
<p><strong>La structure de la solution</strong> peut s‚Äô√©tudier en rempla√ßant <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> par sa construction, √† savoir, une combinaison lin√©aire et du bruit :</p>
<p>\begin{equation*}
\hat{\beta}=X^\dagger y = X^\dagger(X\boldsymbol{\beta} + \eta) = (X^\dagger X)\boldsymbol{\beta} + X^\dagger\eta
\end{equation*}</p>
<p>o√π on utilise <span class="math notranslate nohighlight">\(\eta\)</span> plut√¥t que <span class="math notranslate nohighlight">\(\epsilon\)</span> pour diff√©rentier la r√©alisation effective du bruit de la variable al√©atoire.</p>
<p>On observe, par propri√©t√© de la pseudo-inverse, que la premi√®re contribution est la projection orthogonale du vrai mod√®le sur l‚Äôespace des vecteurs ligne de <span class="math notranslate nohighlight">\(X\)</span>. Il est donc une combinaison lin√©aire des vecteurs que l‚Äôon voit pendant l‚Äôapprentissage ! La deuxi√®me contribution est l‚Äôeffet du bruit sur la solution optimale. Nous discuterons plus loin de ces contributions et d‚Äôeffets √©tranges qui peuvent se produire notament quand la matrice <span class="math notranslate nohighlight">\(X\)</span> est mal conditionn√©e (le ratio entre la plus grande valeur propre de <span class="math notranslate nohighlight">\(X^TX\)</span> et sa plus petite valeur propre est tr√®s grand).</p>
</div>
<div class="section" id="id1">
<h3>√Ä vous de jouer<a class="headerlink" href="#id1" title="Permalink to this headline">¬∂</a></h3>
<hr class="docutils" />
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Calculez la solution du probl√®me de r√©gression lin√©aire en utilisant la pseudo-inverse de Moore-Penrose propos√©e par <span class="math notranslate nohighlight">\(\texttt{numpy}\)</span> via <span class="math notranslate nohighlight">\(\texttt{np.linalg.pinv}\)</span>.</strong></p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">matplotlib</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">12.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">)</span>
<span class="c1"># descente de gradient</span>

<span class="c1"># descente de gradient sans stochasticite</span>
<span class="n">param_trace</span> <span class="p">,</span> <span class="n">loss_trace</span> <span class="o">=</span> <span class="n">gd</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.04</span><span class="p">,</span> <span class="n">nb_iterations</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span> 

<span class="c1">####### Complete this part ######## or die ####################</span>
<span class="c1"># solution par pseudo inverse</span>
<span class="n">X_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">beta_pinv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_inv</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="c1">###############################################################</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;La loss pour la solution par pseudo-inverse est&#39;</span><span class="p">,</span> <span class="n">l</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">beta_pinv</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;La loss pour la solution obtenue par descente de gradient est&#39;</span><span class="p">,</span> <span class="n">loss_trace</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">beta_pinv</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>La loss pour la solution par pseudo-inverse est 1.0476902762958011
La loss pour la solution obtenue par descente de gradient est 1.0492742274375275
</pre></div>
</div>
<img alt="../_images/1_linear_regression_34_1.png" src="../_images/1_linear_regression_34_1.png" />
</div>
</div>
<hr class="docutils" />
<p><span style="color:blue"><strong>Remarques et question :</strong></span> <strong>On remarque ici que la valeur de la loss atteinte par GD est plus haute que celle atteinte par la solution de la pseudo-inverse. A votre avis pourquoi ? Augmentez le nombre d‚Äôit√©rations de GD. Que constatez vous par rapport ? Est-ce √©tonnant par rapport √† votre brillante d√©monstration sur GD dans la section pr√©c√©dente ? Au passage, on pourrait s‚Äôamuser √† montrer qu‚Äôavec l‚Äôinitialisation <span class="math notranslate nohighlight">\(\beta=\boldsymbol{0}\)</span>, chaque step reste bien dans l‚Äôespace engendr√© par les vecteurs lignes de X. Qui veut passer au tableau ?</strong></p>
</div>
<hr class="docutils" />
<div class="section" id="avec-sklearn">
<h3>Avec sklearn<a class="headerlink" href="#avec-sklearn" title="Permalink to this headline">¬∂</a></h3>
<hr class="docutils" />
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Proposez une r√©gression lin√©aire sur le m√™me probl√®me en utilisant <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span>.</strong></p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="c1">####### Complete this part ######## or die ####################</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">y</span><span class="p">)</span>
<span class="c1">###############################################################</span>
<span class="n">coef</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="n">coef</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;La loss pour la solution obtenue par Sklearn est&#39;</span><span class="p">,</span> <span class="n">l</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">coef</span><span class="p">))</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">coef</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>La loss pour la solution obtenue par Sklearn est 1.0476902762958011
</pre></div>
</div>
<img alt="../_images/1_linear_regression_38_1.png" src="../_images/1_linear_regression_38_1.png" />
</div>
</div>
</div>
</div>
<div class="section" id="iv-features-variables-explicatives-transformees">
<h2>IV. Features - Variables explicatives transform√©es<a class="headerlink" href="#iv-features-variables-explicatives-transformees" title="Permalink to this headline">¬∂</a></h2>
<p>Dans beaucoup de probl√®mes r√©els, la variable √† expliquer n‚Äôest pas une simple combinaison lin√©aire des variables explicatives. Cela peut-√™tre une d√©pendence non lin√©aire (e.g. quadratique), ou des d√©pendences crois√©es entre nos variables explicatives. La strat√©gie permettant d‚Äôaborder cette probl√©matique consiste √† transformer notre vecteur <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> en rajoutant par exemple des transformations quadratiques et √† optimiser notre mod√®le lin√©aire sur le vecteur transform√©. Afin de simplifier les notations, nous allons volontairement omettre le biais <span class="math notranslate nohighlight">\(\beta_0\)</span> de nos notations.</p>
<p>Construire nos <em>features</em> consiste √† chercher une fonction <span class="math notranslate nohighlight">\(\phi:\mathbb{R}^d\mapsto\mathbb{R}^p\)</span> qui transforme non-lin√©airement nos variables explicatives initiales.</p>
<p>Le probl√®me se reformule ainsi de la mani√®re suivante :</p>
<p>\begin{equation}
\hat{y}=\langle \phi(\boldsymbol{x}), \boldsymbol{\beta}\rangle
\end{equation}</p>
<p>Le gradient est alors calcul√© en fonction de <span class="math notranslate nohighlight">\(\boldsymbol{z}=\phi(\boldsymbol{x})\)</span> et non en fonction de <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>. Il suffit donc de transformer nos variables explicatives par <span class="math notranslate nohighlight">\(\phi\)</span> et de consid√©rer le r√©sultat comme nos nouvelles variables explicatives.</p>
<div class="section" id="construction-du-jeu-de-donnees-polynomial">
<h3>Construction du jeu de donn√©es polynomial<a class="headerlink" href="#construction-du-jeu-de-donnees-polynomial" title="Permalink to this headline">¬∂</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># vrais parametres</span>
<span class="n">beta_cube</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">sample_data_cube</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">noise</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">150</span><span class="p">)</span>

<span class="c1">#affichage du polynome</span>
<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_42_0.png" src="../_images/1_linear_regression_42_0.png" />
</div>
</div>
</div>
<div class="section" id="solution-par-pseudo-inverse">
<h3>Solution par pseudo-inverse<a class="headerlink" href="#solution-par-pseudo-inverse" title="Permalink to this headline">¬∂</a></h3>
<hr class="docutils" />
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Compl√©tez le code ci-dessous en utilisant une solution par pseudo-inverse via <span class="math notranslate nohighlight">\(\texttt{numpy}\)</span>.</strong></p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Polynomial</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">deg</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">deg</span> <span class="o">=</span> <span class="n">deg</span>

    <span class="k">def</span> <span class="nf">_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1"># here we transform the input into a polynomial</span>
        <span class="c1">####### Complete this part ######## or die ####################</span>
        <span class="n">t</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">X</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">deg</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">t</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">X</span><span class="o">**</span><span class="n">i</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1">###############################################################</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="c1">####### Complete this part ######## or die ####################</span>
        <span class="n">X_transformed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">X_transformed</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
        <span class="c1">###############################################################</span>
        
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;You must fit the model first&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">X_transformed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_transformed</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">errors</span> <span class="o">=</span> <span class="p">(</span><span class="n">prediction</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span><span class="mi">2</span>
        <span class="k">return</span> <span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="n">errors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
<span class="c1"># vraie solution</span>
<span class="n">real_model</span> <span class="o">=</span> <span class="n">Polynomial</span><span class="p">(</span><span class="n">deg</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">real_model</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta_cube</span>
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
<p><span style="color:blue"><strong>Remarques et exercice :</strong></span> <strong>Le plot est affich√© avec un jeu dit de test. Il s‚Äôagit d‚Äôun ensemble de points qui n‚Äôont pas √©t√© utilis√©s lors de notre apprentissage (par pseudo-inverse). Le jeu de donn√©es <span class="math notranslate nohighlight">\(\texttt{X, y}\)</span> d‚Äôune taille diff√©rente est celui qui a √©t√© utilis√©.</strong></p>
<p><strong>Jouez avec le degr√© du polyn√¥me que vous manipulez et observez le r√©sultat. Que constatez-vous ?</strong></p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">deg</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Polynomial</span><span class="p">(</span><span class="n">deg</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="p">[(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">,</span> <span class="s1">&#39;Our model&#39;</span><span class="p">),</span> <span class="p">(</span><span class="n">real_model</span><span class="o">.</span><span class="n">predict</span><span class="p">,</span> <span class="s1">&#39;Real model&#39;</span><span class="p">)])</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Empirical risk: &#39;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_47_0.png" src="../_images/1_linear_regression_47_0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Empirical risk:  2.6801549254883876e-28
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
<p><span style="color:blue"><strong>Remarque et question :</strong></span> <strong>Le risque empirique est celui calcul√© directement sur les donn√©es utilis√©es lors du calcul du pseudo-inverse. Que constatez-vous par rapport √† ce dernier lorsque vous jouez avec le degr√© du polyn√¥me ?</strong></p>
<p><strong>Est-il un bon indicateur du v√©ritable risque de g√©n√©ralisation ? Autrement dit, est-il un bon indicateur de la qualit√© du polyn√¥me obtenu.</strong></p>
</div>
<hr class="docutils" />
<div class="section" id="solution-sklearn">
<h3>Solution Sklearn<a class="headerlink" href="#solution-sklearn" title="Permalink to this headline">¬∂</a></h3>
<hr class="docutils" />
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Proposez la m√™me solution polynomiale via <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span>. Choisissez le m√™me degr√© qu‚Äôutilis√© au-dessus et comparez les r√©sultats.</strong></p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">deg</span><span class="p">),</span> <span class="n">LinearRegression</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">y</span><span class="p">)</span>
<span class="c1">###############################################################</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="p">[(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">,</span> <span class="s1">&#39;sklearn&#39;</span><span class="p">),</span> <span class="p">(</span><span class="n">real_model</span><span class="o">.</span><span class="n">predict</span><span class="p">,</span> <span class="s1">&#39;Real model&#39;</span><span class="p">)])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_52_0.png" src="../_images/1_linear_regression_52_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="v-validation-croisee">
<h2>V. Validation crois√©e<a class="headerlink" href="#v-validation-croisee" title="Permalink to this headline">¬∂</a></h2>
<p>Vous avez pu constater qu‚Äôen fonction du degr√© du polyn√¥me choisi dans l‚Äôexercice pr√©c√©dent, le mod√®le obtenu √©tait plus ou moins loin de la solution id√©ale. De plus, le risque empirique s‚Äôest montr√© √™tre un pi√®tre estimateur de la qualit√© de notre solution estim√©e.</p>
<p>En r√©alit√©, le risque empirique est un estimateur sans biais du risque de g√©n√©ralisation pour un vecteur de param√®tres quelconque. Ce n‚Äôest plus vrai si on choisit la solution estim√©e via notre optimisation. Dit autrement :</p>
<div class="math notranslate nohighlight">
\[R(\boldsymbol{\beta})=\mathbb{E}_{\mathcal{S}}\Big[J(\boldsymbol{\beta})\Big],\text{ }\boldsymbol{\beta}\text{ quelconque, et }R(\text{argmin}_{\boldsymbol{\beta}}J(\boldsymbol{\beta}))\neq \mathbb{E}_{\mathcal{S}}\Big[\text{argmin}_{\boldsymbol{\beta}}J(\boldsymbol{\beta})\Big]\]</div>
<p>Cela implique de mettre en place une proc√©dure exp√©rimentale permettant d‚Äô√©valuer la qualit√© de notre mod√®le.</p>
<div class="section" id="construction-du-jeu-de-donnees">
<h3>Construction du jeu de donn√©es<a class="headerlink" href="#construction-du-jeu-de-donnees" title="Permalink to this headline">¬∂</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">beta_cube</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">sample_data_cube</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">noise</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">150</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_56_0.png" src="../_images/1_linear_regression_56_0.png" />
</div>
</div>
</div>
<div class="section" id="optimiser-une-fonction-est-il-suffisant-pour-parler-d-apprentissage">
<h3>Optimiser une fonction est-il suffisant pour parler d‚Äôapprentissage ?<a class="headerlink" href="#optimiser-une-fonction-est-il-suffisant-pour-parler-d-apprentissage" title="Permalink to this headline">¬∂</a></h3>
<p>Il existe deux strat√©gies d‚Äô√©valuation sans biais de la qualit√© de notre mod√®le :</p>
<ul class="simple">
<li><p>La validation non crois√©e o√π une partie de notre jeu de donn√©e est cach√©e pendant l‚Äôapprentissage puis utilis√©e afin d‚Äô√©valuer les performances du mod√®le. Il s‚Äôagit du d√©coupage train/test. Cette strat√©gie est un estimateur sans biais de la qualit√© de notre mod√®le mais poss√®de une variance plus forte que la validation crois√©e. Elle peut-√™tre particuli√®rement utile lorsque le coup d‚Äôapprentissage d‚Äôun mod√®le est tr√®s √©lev√© (e.g. <em>deep learning</em>)</p></li>
<li><p>La validation crois√©e o√π notre jeu de donn√©es est divis√© en <em>k</em> parties (on parle aussi de <em>k-fold</em>). √âvidemment, <span class="math notranslate nohighlight">\(k\in\{2, ..., n\}\)</span> o√π <span class="math notranslate nohighlight">\(n\)</span> est la taille du jeu de donn√©es. Chacune des parties jouera successivement le r√¥le de jeu de test pendant que les <span class="math notranslate nohighlight">\(k-1\)</span> autres parties serviront √† calculer notre mod√®le. Le r√©sultat de cette proc√©dure est un vecteur de <span class="math notranslate nohighlight">\(k\)</span> scores dont on peut calculer la moyenne, la variance, etc.</p></li>
</ul>
<p>On peut illustrer la m√©thode des <em>k-folds</em> via l‚Äôexemple suivant :</p>
<p>\begin{align}
\text{Appartient au train set: } \color{red}{\boxed{}}&amp;\text{ et appartient au test set: }\color{green}{\boxed{}}
\end{align}
\begin{align}
\text{Step 1: }\color{green}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align}
\begin{align}
\text{Step 2: }\color{red}{\boxed{}}\color{green}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align}
\begin{align}
\text{Step 3: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{green}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align}
\begin{align}
\text{Step 4: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{green}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align}
\begin{align}
\text{Step 5: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{green}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align}
\begin{align}
\text{Step 6: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{green}{\boxed{}}\color{red}{\boxed{}}
\end{align}
\begin{align}
\text{Step 7: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{green}{\boxed{}}
\end{align}</p>
<p>La m√©thode <span class="math notranslate nohighlight">\(\texttt{cross_val_score}\)</span> de <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span> permet de r√©aliser cette proc√©dure. On pourra renseigner le param√®tre <span class="math notranslate nohighlight">\(\texttt{cv}\)</span> qui indique le nombre <span class="math notranslate nohighlight">\(k\)</span> et le param√®tre <span class="math notranslate nohighlight">\(\texttt{scoring}\)</span> qui donne la m√©trique que l‚Äôon souhaite calculer.</p>
<hr class="docutils" />
<p><strong><span style="color:blue"> Exercice :</span></strong> <strong>Proposez un <em>5-fold</em> avec la m√©trique <span class="math notranslate nohighlight">\(R^2\)</span> que vous appliquerez √† une r√©gression polynomiale de degr√© <span class="math notranslate nohighlight">\(5\)</span>.</strong></p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>

<span class="c1">####### Complete this part ######## or die ####################</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">5</span><span class="p">),</span> <span class="n">LinearRegression</span><span class="p">())</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;r2&#39;</span><span class="p">)</span>
<span class="c1">###############################################################</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Le score R2 sur chacun des splits de notre k-fold:&#39;</span><span class="p">,</span> <span class="n">scores</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Le score R2 sur chacun des splits de notre k-fold: [0.97601253 0.97932686 0.92543173 0.97598169 0.88889135]
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
<p><strong><span style="color:blue"> Exercice :</span></strong> <strong>Comparez le score obtenu lors de votre validation crois√©e √† un plot de la fonction estim√©e sur tout le jeu d‚Äôapprentissage.</strong></p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="c1">###############################################################</span>
<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_63_0.png" src="../_images/1_linear_regression_63_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="vi-l-effet-double-descente-bonus">
<h2>VI. L‚Äôeffet ‚Äúdouble descente‚Äù (Bonus ?)<a class="headerlink" href="#vi-l-effet-double-descente-bonus" title="Permalink to this headline">¬∂</a></h2>
<p>Comme vu pr√©c√©demment, l‚Äôeffet du bruit sur l‚Äôestimateur depend du conditionnement de <span class="math notranslate nohighlight">\(X^TX\)</span>. Le conditionnement d‚Äôune matrice <span class="math notranslate nohighlight">\(A\)</span> inversible est donn√© par :</p>
<p>\begin{equation}
C(A)=\lVert A^{-1}\rVert\lvert A\rVert
\end{equation}</p>
<p>Il est √©vident que si <span class="math notranslate nohighlight">\(A\in{\mathbb{R}^{1\times 1}}^\star\)</span>, alors <span class="math notranslate nohighlight">\(C(A)=1\)</span>. Ce n‚Äôest absolument pas vrai dans le cas g√©n√©ral.</p>
<p>L‚Äôexemple ci-dessous illustre cela via la norme de Frobenius (norme Euclidienne appliqu√©e √† une matrice, <span class="math notranslate nohighlight">\(\text{Tr}(A^TA)^{0.5}\)</span>). On pr√©f√®rera en pratique la norme d‚Äôop√©rateur qui quantifie les effets d‚Äôamplification d‚Äôun vecteur <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> lorsqu‚Äôon calcule <span class="math notranslate nohighlight">\(A\boldsymbol{x}\)</span>. Cette norme d‚Äôop√©rateur est directement li√©e aux valeurs propres.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;A=</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;C(A)=A^{-1}A=&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">A</span><span class="p">))</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>A=
 [[1.e+00 0.e+00]
 [0.e+00 1.e-04]]
C(A)=A^{-1}A=10000.000100000001
</pre></div>
</div>
</div>
</div>
<p>On remarque dans l‚Äôexemple que la matrice <span class="math notranslate nohighlight">\(A\)</span> poss√®de une toute petite valeur propre qui est responsable de cet √©cart. L‚Äôexercice ci-dessous montre qu‚Äôau-del√† des consid√©rations th√©oriques, cela a des r√©percussions importantes et totalement inattendues en r√©alit√©.</p>
<p>Les simulations suivantes permettent de mettre en lumi√®re cela. Elles sont construites comme d√©crit ci-dessous :</p>
<p>\begin{equation}
\beta\sim\mathbb{U}(-2, 2)^d,\ d\in\mathbb{N}^\star
\end{equation}</p>
<p>dit autrement, on fixe un vecteur de param√®tres selon une loi uniforme qui d√©pend de la dimension du probl√®me.
Nous avons ensuite :</p>
<p>\begin{equation}
x\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I_d}) + \epsilon,\ \epsilon\sim\mathcal{N}(0, \sigma^2)
\end{equation}</p>
<p>On construit ensuite un jeu de test de taille <span class="math notranslate nohighlight">\(500\)</span> et un jeu d‚Äôapprentissage de taille variable. L‚Äôobjectif ici sera d‚Äô√©tudi√© l‚Äôeffet de la taille du jeu d‚Äôapprentissage sur la qualit√© de notre mod√®le, qualit√© que l‚Äôon aura calcul√©e sur le test. Pour chaque taille de jeu de donn√©es, l‚Äôexp√©rience est r√©p√©t√©e <span class="math notranslate nohighlight">\(50\)</span> fois (<span class="math notranslate nohighlight">\(\texttt{redo}\)</span>) afin de lisser les courbes obtenues.</p>
<hr class="docutils" />
<p><strong><span style="color:blue"> Exercice :</span></strong> <strong>Ex√©cutez une premi√®re fois le code puis jouez avec <span class="math notranslate nohighlight">\(\texttt{noise}\)</span> (i.e. <span class="math notranslate nohighlight">\(\sigma\)</span>) afin de voir ce qui se passe selon la quantit√© de bruit. Essayez de d√©crire rigoureusement ce que vous observez.</strong></p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1">####### Play with the noise #########</span>
<span class="n">noise</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="c1">#####################################</span>

<span class="n">d</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">redo</span> <span class="o">=</span> <span class="mi">50</span>

<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">mu</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">)]</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">)])</span>

<span class="n">test_size</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">test_size</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">test_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">noise</span>

<span class="n">errors</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">train_errors</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
    <span class="n">error</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">train_error</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">redo</span><span class="p">):</span>
        <span class="c1"># dataset construction</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">m</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">noise</span>
        
        <span class="c1"># param estimation</span>
        <span class="n">beta_pinv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
        
        <span class="c1"># risk estimation</span>
        <span class="n">error</span> <span class="o">+=</span> <span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">beta_pinv</span><span class="p">)</span><span class="o">-</span><span class="n">y_test</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="n">test_size</span><span class="o">*</span><span class="n">redo</span><span class="p">)</span>
        <span class="n">train_error</span> <span class="o">+=</span> <span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta_pinv</span><span class="p">)</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="n">m</span><span class="o">*</span><span class="n">redo</span><span class="p">)</span>
    <span class="n">train_errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_error</span><span class="p">)</span>
    <span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">train_errors</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Train error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">d</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Dimension&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">errors</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Risk estimation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_69_0.png" src="../_images/1_linear_regression_69_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">errors</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Risk estimation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">d</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Dimension&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_errors</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">train_errors</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Train error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">d</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Dimension&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_70_0.png" src="../_images/1_linear_regression_70_0.png" />
<img alt="../_images/1_linear_regression_70_1.png" src="../_images/1_linear_regression_70_1.png" />
</div>
</div>
<p>La ligne en pointill√© s√©pare visuellement deux r√©gimes diff√©rents. La transition d‚Äôun r√©gime √† l‚Äôautre se produit par une augmentation catastrophique de l‚Äôerreur de g√©n√©ralisation de notre mod√®le.</p>
<hr class="docutils" />
<p><strong><span style="color:blue"> Question :</span></strong> <strong>Quelle particularit√© diff√©rentie les deux phases ?</strong></p>
<hr class="docutils" />
<p>En r√©alit√©, les m√©thodes de <em>machine learning</em> traditionnelles se situent plut√¥t dans le r√©gime de ‚Äúdroite‚Äù. L‚Äô√©tude de ce ph√©nom√®ne est pouss√©e par les approches comme le <em>deep learning</em> qui sont souvent dans le r√©gime de gauche. Comprendre ces ph√©nom√®nes nous permet par exemple d‚Äô√©clairer les raisons du succ√®s du <em>deep learning</em>.</p>
</div>
<div class="section" id="vii-regularisation">
<h2>VII. R√©gularisation<a class="headerlink" href="#vii-regularisation" title="Permalink to this headline">¬∂</a></h2>
<p>Comme illustr√© par les quelques sc√©narios pr√©c√©dents dont le cas catastrophique de la double descente, une certaine parcimonie est attendue par notre mod√®le. On a pu notamment observer que les ‚Äúmauvaises‚Äù fonctions du point de vue du risque de g√©n√©ralisation avaient une forte tendance √† osciller n‚Äôimporte comment. Au lieu de laisser jouer le ‚Äúhasard‚Äù (ou plut√¥t le conditionnement de <span class="math notranslate nohighlight">\(X^TX\)</span>), nous pouvons contraindre notre optimisation √† favoriser les solutions parcimonieuses ; c‚Äôest-√†-dire des solutions qui n‚Äôoscillent pas n‚Äôimporte comment.</p>
<p>Intuitivement, on va choisir une solution qui minimise √† la fois le risque empirique <span class="math notranslate nohighlight">\(J(\boldsymbol{\beta})\)</span>, mais aussi une p√©nalit√© sur la quantit√© ‚Äúd‚Äôoscillation‚Äù. En r√©alit√©, les oscillations sont directement contr√¥l√©es par la norme des param√®tres : un grand poids rendra notre mod√®le tr√®s sensible √† la moindre perturbation de la variable explicative associ√©e.</p>
<p>Nous parlons d‚Äôoptimisation r√©gularis√©e lorsque la fonction √† optimiser s‚Äô√©crit de la mani√®re suivante :</p>
<p>\begin{equation}
J(\boldsymbol{\beta})=\frac{1}{m}\sum_{i=1}^nr(f_{\boldsymbol{\beta}}(\boldsymbol{x_i}), y_i)+\lambda P(\boldsymbol{\beta})
\end{equation}</p>
<p>o√π <span class="math notranslate nohighlight">\(r:\mathcal{Y}\times\mathcal{Y}\mapsto \mathbb{R}^+\)</span> est notre risque √©l√©mentaire et <span class="math notranslate nohighlight">\(P:\mathbb{R}^d\mapsto\mathbb{R}^+\)</span> une p√©nalit√© sur notre vecteur de param√®tres. Plus pr√©cis√©ment, dans le cas de la r√©gression lin√©aire, nous avons :</p>
<p>\begin{equation}
r(\hat{y}, y)=(\hat{y}-y)^2
\end{equation}</p>
<p>et</p>
<p>\begin{equation}
P(\boldsymbol{\beta})=\lVert \boldsymbol{\beta} \rVert,
\end{equation}</p>
<p>o√π <span class="math notranslate nohighlight">\(\lVert \cdot \rVert\)</span> est une norme quelconque. Les choix classiques sont la norme <span class="math notranslate nohighlight">\(\ell_1\)</span> :</p>
<p>\begin{equation}
\lVert \boldsymbol{\beta} \rVert_1=\sum_j |\boldsymbol{\beta}_j|
\end{equation}</p>
<p>et la norme <span class="math notranslate nohighlight">\(\ell_2\)</span> :</p>
<p>\begin{equation}
\lVert \boldsymbol{\beta} \rVert_2 = \sqrt{\sum_j\boldsymbol{\beta}_j^2}=\sqrt{\boldsymbol{\beta}^T\boldsymbol{\beta}}
\end{equation}</p>
<p>Une strat√©gie interm√©diaire consiste √† prendre la combinaison convexe des deux normes :</p>
<p>\begin{equation}
P(\boldsymbol{\beta})=\eta \lVert \boldsymbol{\beta} \rVert_1 + (1-\eta) \lVert \boldsymbol{\beta} \rVert_2.
\end{equation}</p>
<p>avec <span class="math notranslate nohighlight">\(\eta\in\big[0,1\big]\)</span>. On parle alors d‚Äô<em>elastic-net</em>.</p>
<p>Ces diff√©rentes r√©gularisations ne se comportent pas de la m√™me mani√®re. Ainsi la r√©gularisation <span class="math notranslate nohighlight">\(\ell_1\)</span>, aussi appel√©e Lasso, va forcer certains param√®tres √† atteindre la valeur <span class="math notranslate nohighlight">\(0\)</span>. Cela permet par exemple de favoriser l‚Äôexplicabilit√© de notre mod√®le. En pratique, <span class="math notranslate nohighlight">\(\ell_2\)</span>, appel√©e Ridge, a tendance √† donner les meilleurs r√©sultats d‚Äôun point de vue pr√©dictif.</p>
<div class="section" id="id2">
<h3>Construction du jeu de donn√©es<a class="headerlink" href="#id2" title="Permalink to this headline">¬∂</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">beta_cube</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">sample_data_cube</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">noise</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">150</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_76_0.png" src="../_images/1_linear_regression_76_0.png" />
</div>
</div>
</div>
<div class="section" id="sans-regularisation">
<h3>Sans r√©gularisation<a class="headerlink" href="#sans-regularisation" title="Permalink to this headline">¬∂</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">LinearRegression</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_79_0.png" src="../_images/1_linear_regression_79_0.png" />
</div>
</div>
</div>
<div class="section" id="avec-regularisation-ell-1">
<h3>Avec r√©gularisation <span class="math notranslate nohighlight">\(\ell_1\)</span><a class="headerlink" href="#avec-regularisation-ell-1" title="Permalink to this headline">¬∂</a></h3>
<p>Lorsqu‚Äôon parle de r√©gresion lin√©aire avec r√©gularisation <span class="math notranslate nohighlight">\(\ell_1\)</span>, on parle aussi de Lasso.</p>
<hr class="docutils" />
<p><strong><span style="color:blue"> Exercice :</span></strong> <strong>Testez plusieurs valeurs de <span class="math notranslate nohighlight">\(\alpha\)</span> (<span class="math notranslate nohighlight">\(=\lambda\)</span> dans notre texte). Quelle est la fonction la plus parcimonieuse que vous arrivez √† obtenir ?</strong></p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">15.</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_83_0.png" src="../_images/1_linear_regression_83_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Les param√®tres du mod√®le sont sparses :</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">model</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Les param√®tres du mod√®le sont sparses :
 [ 0.        -0.        -0.        -0.        -0.        -0.
 -0.        -0.        -0.         0.        -0.0091659]
</pre></div>
</div>
</div>
</div>
<p>Vous avez du constater que selon la quantit√© de r√©gularisation, les param√®tres √©taient plus ou moins sparse. Il se trouve qu‚Äôune fois qu‚Äôun param√®tre est √† 0, il le sera pour toutes les valeurs de <span class="math notranslate nohighlight">\(\alpha\)</span> supp√©rieures. Afin d‚Äôobserver visuellement, l‚Äôeffet de la r√©gularisation sur la sparsit√©, il est possible d‚Äôafficher ce qu‚Äôon appelle les ‚Äúchemins Lasso‚Äù ou Lasso paths.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">lars_path</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">X_transformed</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">coefs</span> <span class="o">=</span> <span class="n">lars_path</span><span class="p">(</span><span class="n">X_transformed</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;lasso&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">coefs</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">xx</span> <span class="o">/=</span> <span class="n">xx</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">coefs</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">ymin</span><span class="p">,</span> <span class="n">ymax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">()</span>
<span class="c1"># plt.vlines(xx, ymin, ymax, linestyle=&#39;dashed&#39;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;|coef| / max|coef|&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Coefficients&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;LASSO Path&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>.
</pre></div>
</div>
<img alt="../_images/1_linear_regression_86_1.png" src="../_images/1_linear_regression_86_1.png" />
</div>
</div>
<p>A gauche se trouve le param√®tre le plus parcimonieux (la plus grande valeur de <span class="math notranslate nohighlight">\(\alpha\)</span>). Tous les param√®tres y sont donc nuls. Plus la valeur de <span class="math notranslate nohighlight">\(\alpha\)</span> est r√©duite, plus le nombre de param√®tres diff√©rents de <span class="math notranslate nohighlight">\(0\)</span> augmente et leur valeur aussi.</p>
</div>
<div class="section" id="avec-regularisation-ell-2">
<h3>Avec r√©gularisation <span class="math notranslate nohighlight">\(\ell_2\)</span><a class="headerlink" href="#avec-regularisation-ell-2" title="Permalink to this headline">¬∂</a></h3>
<p>Lorsqu‚Äôon parle de r√©gresion lin√©aire avec r√©gularisation <span class="math notranslate nohighlight">\(\ell_2\)</span>, on parle aussi de Ridge.</p>
<hr class="docutils" />
<p><strong><span style="color:blue"> Exercice :</span></strong> <strong>Testez plusieurs valeurs de <span class="math notranslate nohighlight">\(\alpha\)</span> (<span class="math notranslate nohighlight">\(=\lambda\)</span> dans notre texte). Quelle est la fonction la plus parcimonieuse que vous arrivez √† obtenir ?</strong></p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1000</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_91_0.png" src="../_images/1_linear_regression_91_0.png" />
</div>
</div>
</div>
<div class="section" id="avec-regularisation-elastic-net">
<h3>Avec r√©gularisation <em>elastic-net</em><a class="headerlink" href="#avec-regularisation-elastic-net" title="Permalink to this headline">¬∂</a></h3>
<p>Lorsqu‚Äôon parle de r√©gresion lin√©aire avec r√©gularisation <em>elastic-net</em>.</p>
<hr class="docutils" />
<p><strong><span style="color:blue"> Exercice :</span></strong> <strong>Testez plusieurs valeurs de <span class="math notranslate nohighlight">\(\alpha\)</span> (<span class="math notranslate nohighlight">\(=\lambda\)</span> dans notre texte). Quelle est la fonction la plus parcimonieuse que vous arrivez √† obtenir (Essayez de trouver la r√©ponse en raisonnant) ?</strong></p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNet</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">ElasticNet</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.8</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_95_0.png" src="../_images/1_linear_regression_95_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="viii-selection-de-modeles">
<h2>VIII. Selection de mod√®les<a class="headerlink" href="#viii-selection-de-modeles" title="Permalink to this headline">¬∂</a></h2>
<p>En pratique, nous ne pouvons pas choisir la valeur des param√®tres (e.g. degr√©, r√©gularisation) √† l‚Äôoeil comme pr√©c√©demment. Il nous faut (1) un algorithme qui automatise cette t√¢che et (2) une strat√©gie d‚Äô√©valuation rigoureuse afin d‚Äô√©viter les biais de confirmation (sur-apprentissage).</p>
<div class="section" id="id3">
<h3>Construction du jeu de donn√©es<a class="headerlink" href="#id3" title="Permalink to this headline">¬∂</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">beta_cube</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">sample_data_cube</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">beta_cube</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">noise</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">sample_data_cube</span><span class="p">(</span><span class="mi">150</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_linear_regression_99_0.png" src="../_images/1_linear_regression_99_0.png" />
</div>
</div>
</div>
<div class="section" id="recherche-exhaustive">
<h3>Recherche exhaustive<a class="headerlink" href="#recherche-exhaustive" title="Permalink to this headline">¬∂</a></h3>
<p>L‚Äôalgorithme de recherche par grille va exhaustivement test√© tous les param√®tres donn√©s. Pour chacun combinaison, une validation <em>k-fold</em> est r√©alis√©e. Le mod√®le retenu sera celui qui aura maximis√© son score moyen lors du <em>k-fold</em>.</p>
<hr class="docutils" />
<p><strong><span style="color:blue"> Exercice :</span></strong> <strong>Utilisez l‚Äôobjet <span class="math notranslate nohighlight">\(\texttt{GridSearchCV}\)</span> afin de trouver la meilleure combinaison de param√®tres selon le dictionnaire d√©crit ci-dessous. Toutes les combinaisons seront-elles r√©ellement test√©es ?</strong></p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNet</span><span class="p">,</span> <span class="n">Ridge</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">,</span> <span class="n">Lasso</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">[</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">LinearRegression</span><span class="p">()],</span> 
   <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]},</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">Ridge</span><span class="p">()],</span> 
   <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> 
   <span class="s1">&#39;model__alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">]},</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">Lasso</span><span class="p">()],</span> 
   <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
   <span class="s1">&#39;model__alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]},</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">ElasticNet</span><span class="p">()],</span> <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
  <span class="s1">&#39;model__alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">],</span>
  <span class="s1">&#39;model__l1_ratio&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]},</span>
 <span class="p">]</span>

<span class="n">pipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">10</span><span class="p">)),</span> <span class="p">(</span><span class="s1">&#39;model&#39;</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">())])</span>
<span class="c1">####### Complete this part ######## or die ####################</span>
<span class="n">search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipe</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="c1">###############################################################</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GridSearchCV(estimator=Pipeline(steps=[(&#39;poly&#39;, PolynomialFeatures(degree=10)),
                                       (&#39;model&#39;, LinearRegression())]),
             n_jobs=-1,
             param_grid=[{&#39;model&#39;: [LinearRegression()],
                          &#39;poly__degree&#39;: [1, 2, 3, 4, 5]},
                         {&#39;model&#39;: [Ridge()],
                          &#39;model__alpha&#39;: [0.0, 0.1, 0.2, 0.5, 0.8, 10.0, 100.0,
                                           100.0],
                          &#39;poly__degree&#39;: [1, 2, 3, 4, 5]},
                         {&#39;model&#39;: [Lasso()],
                          &#39;model__alpha&#39;: [0.0, 0.1, 0.2, 0.5, 0.8],
                          &#39;poly__degree&#39;: [1, 2, 3, 4, 5]},
                         {&#39;model&#39;: [ElasticNet()],
                          &#39;model__alpha&#39;: [0.0, 0.1, 0.2, 0.5, 0.8, 10.0, 100.0,
                                           100.0],
                          &#39;model__l1_ratio&#39;: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6,
                                              0.9, 1.0],
                          &#39;poly__degree&#39;: [1, 2, 3, 4, 5]}])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Le meilleur modele est&#39;</span><span class="p">,</span> <span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Le meilleur modele est Pipeline(steps=[(&#39;poly&#39;, PolynomialFeatures(degree=3)),
                (&#39;model&#39;, LinearRegression())])
</pre></div>
</div>
<img alt="../_images/1_linear_regression_104_1.png" src="../_images/1_linear_regression_104_1.png" />
</div>
</div>
</div>
<div class="section" id="recherche-aleatoire">
<h3>Recherche al√©atoire<a class="headerlink" href="#recherche-aleatoire" title="Permalink to this headline">¬∂</a></h3>
<p>Une recherche exhaustive peut rapidement √™tre limitante. Imaginons que nous testions d√©j√† <span class="math notranslate nohighlight">\(10000\)</span> combinaisons de param√®tres. Rajoutons maintenant un param√®tre avec 50 modalit√©s. Le nombre de combinaisons est donc multipli√© par <span class="math notranslate nohighlight">\(50\)</span> et on monte √† <span class="math notranslate nohighlight">\(500000\)</span> combinaisons. L‚Äôalgorithme devient <span class="math notranslate nohighlight">\(50\)</span> fois plus lent.</p>
<p>Une strat√©gie alternative est de s‚Äôappuyer sur le hasard. On peut sp√©cifier a priori des distributions sur les param√®tres en supposant que certaines combinaisons fourniront probablement plus de bons r√©sultats que d‚Äôautres. Par d√©faut, le tirage est uniforme. Cette m√©thode n‚Äôest pas absurde car plusieurs combinaisons peuvent tr√®s bien obtenir des r√©sultats tr√®s proches. L‚Äôapproche al√©atoire sera ainsi beaucoup plus efficaces que la recherche exhaustive pour des performances g√©n√©ralement assez proches.</p>
<hr class="docutils" />
<p><strong><span style="color:blue"> Exercice :</span></strong> <strong>Compl√©tez le code ci-dessous afin de r√©aliser une recherche randomis√©e. Quel param√®tre permet de jouer sur le nombre de tirages ?</strong></p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">[</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">LinearRegression</span><span class="p">()],</span> 
   <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)]},</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">Ridge</span><span class="p">()],</span> 
   <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)],</span> 
   <span class="s1">&#39;model__alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">]},</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">Lasso</span><span class="p">()],</span> 
   <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)],</span>
   <span class="s1">&#39;model__alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]},</span>
  <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">ElasticNet</span><span class="p">()],</span> <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)],</span>
  <span class="s1">&#39;model__alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">],</span>
  <span class="s1">&#39;model__l1_ratio&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]},</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">10</span><span class="p">)),</span> <span class="p">(</span><span class="s1">&#39;model&#39;</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">())])</span>

<span class="n">search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipe</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GridSearchCV(estimator=Pipeline(steps=[(&#39;poly&#39;, PolynomialFeatures(degree=10)),
                                       (&#39;model&#39;, LinearRegression())]),
             n_jobs=-1,
             param_grid=[{&#39;model&#39;: [LinearRegression()],
                          &#39;poly__degree&#39;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
                                           12, 13, 14, 15, 16, 17, 18, 19]},
                         {&#39;model&#39;: [Ridge()],
                          &#39;model__alpha&#39;: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6,
                                           0.7, 0.8, 0.9, 1.0, 10.0, 100.0,
                                           100.0],
                          &#39;poly__degree&#39;: [1, 2, 3, 4, 5,...
                                           12, 13, 14, 15, 16, 17, 18, 19]},
                         {&#39;model&#39;: [Lasso()],
                          &#39;model__alpha&#39;: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6,
                                           0.7, 0.8, 0.9, 1.0],
                          &#39;poly__degree&#39;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
                                           12, 13, 14, 15, 16, 17, 18, 19]},
                         {&#39;model&#39;: [ElasticNet()],
                          &#39;model__alpha&#39;: [0.1, 0.2, 0.5, 0.8, 10.0, 100.0,
                                           100.0],
                          &#39;model__l1_ratio&#39;: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6,
                                              0.9, 1.0],
                          &#39;poly__degree&#39;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
                                           12, 13, 14, 15, 16, 17, 18, 19]}])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Le meilleur modele est&#39;</span><span class="p">,</span> <span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Le meilleur modele est Pipeline(steps=[(&#39;poly&#39;, PolynomialFeatures(degree=3)),
                (&#39;model&#39;, LinearRegression())])
</pre></div>
</div>
<img alt="../_images/1_linear_regression_110_1.png" src="../_images/1_linear_regression_110_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="n">search</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">pipe</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="c1">###############################################################</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/servajean/Library/Python/3.9/lib/python/site-packages/sklearn/pipeline.py:335: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
  self._final_estimator.fit(Xt, y, **fit_params_last_step)
/Users/servajean/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:529: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.
  model = cd_fast.enet_coordinate_descent(
/Users/servajean/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.0441160130073355, tolerance: 0.019408266008669246
  model = cd_fast.enet_coordinate_descent(
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RandomizedSearchCV(estimator=Pipeline(steps=[(&#39;poly&#39;,
                                              PolynomialFeatures(degree=10)),
                                             (&#39;model&#39;, LinearRegression())]),
                   n_iter=200, n_jobs=-1,
                   param_distributions=[{&#39;model&#39;: [LinearRegression()],
                                         &#39;poly__degree&#39;: [1, 2, 3, 4, 5, 6, 7,
                                                          8, 9, 10, 11, 12, 13,
                                                          14, 15, 16, 17, 18,
                                                          19]},
                                        {&#39;model&#39;: [Ridge()],
                                         &#39;model__alpha&#39;: [0.0, 0.1, 0.2, 0.3,
                                                          0.4, 0.5, 0.6, 0.7,
                                                          0.8, 0.9, 1.0, 10.0,
                                                          100.0, 100.0],...
                                        {&#39;model&#39;: [Lasso(alpha=0.0)],
                                         &#39;model__alpha&#39;: [0.0, 0.1, 0.2, 0.3,
                                                          0.4, 0.5, 0.6, 0.7,
                                                          0.8, 0.9, 1.0],
                                         &#39;poly__degree&#39;: [1, 2, 3, 4, 5, 6, 7,
                                                          8, 9, 10, 11, 12, 13,
                                                          14, 15, 16, 17, 18,
                                                          19]},
                                        {&#39;model&#39;: [ElasticNet()],
                                         &#39;model__alpha&#39;: [0.1, 0.2, 0.5, 0.8,
                                                          10.0, 100.0, 100.0],
                                         &#39;model__l1_ratio&#39;: [0.0, 0.1, 0.2, 0.3,
                                                             0.4, 0.5, 0.6, 0.9,
                                                             1.0],
                                         &#39;poly__degree&#39;: [1, 2, 3, 4, 5, 6, 7,
                                                          8, 9, 10, 11, 12, 13,
                                                          14, 15, 16, 17, 18,
                                                          19]}])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Le meilleur modele est&#39;</span><span class="p">,</span> <span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Le meilleur modele est Pipeline(steps=[(&#39;poly&#39;, PolynomialFeatures(degree=3)),
                (&#39;model&#39;, Lasso(alpha=0.0))])
</pre></div>
</div>
<img alt="../_images/1_linear_regression_112_1.png" src="../_images/1_linear_regression_112_1.png" />
</div>
</div>
</div>
</div>
<div class="section" id="ix-le-mot-de-la-fin">
<h2>IX. Le mot de la fin<a class="headerlink" href="#ix-le-mot-de-la-fin" title="Permalink to this headline">¬∂</a></h2>
<p>Ce propos introductif nous a permis de toucher du doigt la notion de sur-apprentissage. Quand est-ce que le meilleur mod√®le sur notre jeu d‚Äôapprentissage est suffisament bon en g√©n√©ral ? Quand peut-on consid√©rer qu‚Äôun mod√®le est suffisamment bon ? Aurions-nous pu trouver un meilleur mod√®le avec une proc√©dure d‚Äôapprentissage diff√©rente ?</p>
</div>
</div>


<script type="application/vnd.jupyter.widget-state+json">
{"state": {"e47eafd5f5164bd683a5efd7a02ebe41": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "5a2d8bae14794814b9ffc0462b6268fc": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_e47eafd5f5164bd683a5efd7a02ebe41", "msg_id": "", "outputs": []}}, "71e52b72201d48c285449e4487ca5f03": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "ac8bb2d72f0c481782adb9359a4942c1": {"model_name": "SliderStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "SliderStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": "", "handle_color": null}}, "f3f7b576d15a4fb2a44c2ec57ad7f489": {"model_name": "FloatSliderModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatSliderModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "FloatSliderView", "continuous_update": false, "description": "learning_rate", "description_tooltip": null, "disabled": false, "layout": "IPY_MODEL_71e52b72201d48c285449e4487ca5f03", "max": 0.05, "min": 1e-05, "orientation": "horizontal", "readout": true, "readout_format": ".5f", "step": 0.0001, "style": "IPY_MODEL_ac8bb2d72f0c481782adb9359a4942c1", "value": 1e-05}}, "80c88f44c9804d3db0cd11b37b59f2e3": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "a5d0e2d36df74ff9b1dcfb211ae0865b": {"model_name": "SliderStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "SliderStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": "", "handle_color": null}}, "c18a64ba7bf34f67a63f01d2a9cd325c": {"model_name": "IntSliderModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "IntSliderModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "IntSliderView", "continuous_update": false, "description": "batch_size", "description_tooltip": null, "disabled": false, "layout": "IPY_MODEL_80c88f44c9804d3db0cd11b37b59f2e3", "max": 10, "min": 1, "orientation": "horizontal", "readout": true, "readout_format": "d", "step": 1, "style": "IPY_MODEL_a5d0e2d36df74ff9b1dcfb211ae0865b", "value": 10}}, "d37166070cea4d888fd04bb7867c9d65": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "c945e9a87742490396124b512d299f36": {"model_name": "VBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": ["widget-interact"], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "VBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "VBoxView", "box_style": "", "children": ["IPY_MODEL_f3f7b576d15a4fb2a44c2ec57ad7f489", "IPY_MODEL_c18a64ba7bf34f67a63f01d2a9cd325c", "IPY_MODEL_30111b7a2adb46ae9f44334bd143f632"], "layout": "IPY_MODEL_d37166070cea4d888fd04bb7867c9d65"}}, "bf3baeaa4f6940dd9acc9ed706905831": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "30111b7a2adb46ae9f44334bd143f632": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_bf3baeaa4f6940dd9acc9ed706905831", "msg_id": "", "outputs": [{"output_type": "display_data", "metadata": {"needs_background": "light"}, "data": {"text/plain": "<Figure size 1008x432 with 2 Axes>", "image/png": "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\n"}}]}}}, "version_major": 2, "version_minor": 0}
</script>


    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./2_linear_regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="0_propos_liminaire.html" title="previous page">La <em>r√©gression</em></a>
    <a class='right-next' id="next-link" href="2_optimization.html" title="next page">L‚Äôoptimisation ‚òïÔ∏è‚òïÔ∏è</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By The LMIPR team<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>
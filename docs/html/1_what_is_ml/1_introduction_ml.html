
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Machine learning et malédiction de la dimension &#8212; Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Les arbres de régression et de classification" href="2_regression_and_classification_trees.html" />
    <link rel="prev" title="Le Machine Learning, c’est quoi ?" href="0_propos_liminaire.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Machine Learning
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../0_requisite/rappels_python.html">
   Rappels de Python et Numpy
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="0_propos_liminaire.html">
   Le
   <em>
    Machine Learning
   </em>
   , c’est quoi ?
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     <em>
      Machine learning
     </em>
     et malédiction de la dimension
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="2_regression_and_classification_trees.html">
     Les arbres de régression et de classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../2_linear_regression/0_propos_liminaire.html">
   La
   <em>
    régression
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_linear_regression/1_linear_regression.html">
     La régression linéaire ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_linear_regression/2_optimization.html">
     L’optimisation ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_linear_regression/3_interpolation.html">
     Interpolation ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_linear_regression/4_algo_proximal_lasso.html">
     Sous-différentiel et le cas du Lasso ☕️☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../3_logistic_regression/0_propos_liminaire.html">
   La
   <em>
    classification
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_logistic_regression/1_logistic_regression.html">
     La Régression Logistique ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_logistic_regression/2_fonctions_proxy.html">
     Les fonctions proxy ☕️☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_logistic_regression/3_bayes_classifier.html">
     Le classifieur de Bayes ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_logistic_regression/4_VC_theory.html">
     La théorie de Vapnik et Chervonenkis ☕️☕️☕️☕️ (💆‍♂️)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../4_max_margin/0_propos_liminaire.html">
   Les modèles
   <em>
    max-margin
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../4_max_margin/1_max_margin.html">
     Les modèles max-margin ☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5_ensembles/0_propos_liminaire.html">
   Les méthodes
   <em>
    ensemblistes
   </em>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5_ensembles/1_ensembles.html">
     Les méthodes ensemblistes ☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../6_autodiff/0_propos_liminaire.html">
   La différentiation automatique
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_autodiff/1_autodiff.html">
     La différentiation automatique et un début de
     <em>
      deep learning
     </em>
     ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_autodiff/2_filters_representation.html">
     Filtres et espace de représentation des réseaux de neurones ☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../7_regularization/0_propos_liminaire.html">
   La régularisation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/1_regularization_deep.html">
     Régularisation en deep learning ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/2_unsupervised_representation_learning.html">
     Unsupervised representation learning ☕️☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/3_multitask.html">
     L’apprentissage multi-tâches ☕️☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/4_adversarial.html">
     Les attaques adversaires ☕️
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_regularization/5_ridge.html">
     Une analyse de la régularisation Ridge ☕️☕️☕️
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../content.html">
   Content in Jupyter Book
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../markdown.html">
     Markdown Files
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../notebooks.html">
     Content with notebooks
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/1_what_is_ml/1_introduction_ml.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/maximiliense/lmiprp"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/maximiliense/lmiprp/issues/new?title=Issue%20on%20page%20%2F1_what_is_ml/1_introduction_ml.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/maximiliense/lmiprp/master?urlpath=tree/1_what_is_ml/1_introduction_ml.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#i-introduction">
   I. Introduction
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-l-apprentissage-supervise">
     A. L’apprentissage supervisé
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-l-apprentissage-non-supervise">
     B. L’apprentissage non-supervisé
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ii-on-casse-une-idee-preconcue-as">
   II. On casse une idée préconçue (AS)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iii-une-autre-premiere-approche-logique-le-knn-as">
   III. Une autre première approche logique : le KNN (AS)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iv-les-arbres-de-decision-ou-classification-and-regression-tree-cart-as">
   IV. Les arbres de décision ou Classification and regression Tree (CART) (AS)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#v-les-forets-aleatoires-ou-random-forest-rf-as">
   V. Les forêts aléatoires ou Random Forest (RF) (AS)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vi-choix-des-hyperparametres-as">
   VI. Choix des hyperparamètres (AS)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vii-l-algorithme-des-k-moyennes-ans">
   VII. L’algorithme des K-Moyennes (ANS)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#viii-la-malediction-de-la-dimension">
   VIII. La malédiction de la dimension
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#en-details">
     En détails
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="machine-learning-et-malediction-de-la-dimension">
<h1><em>Machine learning</em> et malédiction de la dimension<a class="headerlink" href="#machine-learning-et-malediction-de-la-dimension" title="Permalink to this headline">¶</a></h1>
<p>Quelques liens pour plus de détails :</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/maximiliense/lmpirp/blob/main/Notes/Overfitting.pdf">Overfitting</a></p></li>
<li><p><a class="reference external" href="https://github.com/maximiliense/lmpirp/blob/main/Notes/KNN.pdf">KNN</a></p></li>
</ul>
<p><strong>Objectif :</strong> Cette séquence a pour objectif d’introduire les bases du <em>machine learning</em> illustrées via quelques algorithmes particuliers. Il s’agit de sensibiliser à l’idée de classification, régression, supervisé ou non supervisé, à la notion de sur-apprentissage ainsi qu’à la malédiction de la dimension.</p>
<p>Des approfondissements et d’autres modèles seront présentés au travers des différentes séquences qui composent ce cours.</p>
<div class="section" id="i-introduction">
<h2>I. Introduction<a class="headerlink" href="#i-introduction" title="Permalink to this headline">¶</a></h2>
<p>Imaginons que nous souhaitions construire une application qui prendrait en entrée une image de chien ou de chat et doive prédire laquelle des deux espèces est représentée. Imaginons encore une application qui prendrait en entrée un mail qu’elle classifierait comme SPAM ou NONSPAM. Supposons qu’il existe deux catégories de clients qu’on ne connait pas <em>a priori</em> et que l’entreprise souhaite prédire pour chacun des clients sa catégorie. On peut vouloir prédire la température qu’il fera demain à partir de données relevées aujourd’hui.</p>
<p>Une constante est partagée par l’ensemble de ces scénarios. Il y a tout d’abord une donnée d’entrée plus ou moins complexe et structurée. On notera <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> l’espace auquel elle appartient. Ensuite, à partir de cette donnée, l’objectif est de faire une prédiction. Notons <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> l’espace auquel appartient cette prédiction. On appelle ça aussi nos labels ou nos variables à expliquer. Notre objectif, en tant que <em>machine learner</em> est de construire une fonction <span class="math notranslate nohighlight">\(h:\mathcal{X}\mapsto\mathcal{Y}\)</span> qui aura de <em>bonnes performances</em> “en production”, c’est-à-dire sur des données nouvelles que nous n’avons jamais vu (i.e. on ne veut pas prédire la météo d’hier à partir d’avant hier, mais bien de demain à partir d’aujourd’hui).</p>
<p>Deux types d’apprentissage sont généralement opposés : l’apprentissage supervisé (AS) et non-supervisé (ANS).</p>
<div class="section" id="a-l-apprentissage-supervise">
<h3>A. L’apprentissage supervisé<a class="headerlink" href="#a-l-apprentissage-supervise" title="Permalink to this headline">¶</a></h3>
<p>L’apprentissage supervisé part du principe que (1) nos labels (i.e. l’espace <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>) est bien défini et (2) que nous avons accès à des données associant des éléments de <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> à leur label <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>.</p>
<p>On parlera de problème de régression si par exemple <span class="math notranslate nohighlight">\(\mathcal{Y}\subseteq\mathbb{R}\)</span> ou de problème de classification si <span class="math notranslate nohighlight">\(\mathcal{Y}=\{1, \ldots, C\}\)</span> où l’ordre n’est pas important. Par exemple, prédire la température est un problème de régression alors que prédire si la photo représente un chien ou un chat est un problème de classification.</p>
<p>A fortiori, toutes les observations dans <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> ne sont pas nécessairement équiprobables. Certains clients ont peut-être, par exemple, un profil plus commun que d’autres. Afin de pouvoir définir plus rigoureusement ce qu’on entend pas <em>bonnes performances</em>, notons <span class="math notranslate nohighlight">\(X\in\mathcal{X}\)</span> une variable aléatoire qui décrit nos données observées et <span class="math notranslate nohighlight">\(Y\in\mathcal{Y}\)</span> la variable aléatoire associée à nos labels. Assez naïvement, notons <span class="math notranslate nohighlight">\(\mathbb{P}\)</span> la loi de notre couple <span class="math notranslate nohighlight">\(X,Y\)</span> :</p>
<div class="math notranslate nohighlight">
\[X, Y\sim \mathbb{P}.\]</div>
<p>Notons <span class="math notranslate nohighlight">\(r:\mathcal{Y}\times\mathcal{Y}\rightarrow\mathbb{R}^+\)</span> une mesure d’erreur, un risque élémentaire. On a par exemple, dans le cas d’un problème de régression, l’erreur quadratique :</p>
<div class="math notranslate nohighlight">
\[r(\hat{y}, y)=(\hat{y}-y)^2,\]</div>
<p>où <span class="math notranslate nohighlight">\(\hat{y}\)</span> est la prédiction que ferait notre modèle. Ou encore, dans le cas de la classification cette fois-ci l’erreur <span class="math notranslate nohighlight">\(0.1\)</span> :</p>
<div class="math notranslate nohighlight">
\[r(\hat{y}, y)=\textbf{1}\{\hat{y}\neq y\},\]</div>
<p>qui vaut <span class="math notranslate nohighlight">\(1\)</span> si la prédiction est mauvaise ou <span class="math notranslate nohighlight">\(0\)</span> sinon.</p>
<p>Notre objectif est tout naturellement de trouver une application <span class="math notranslate nohighlight">\(h:\mathcal{X}\mapsto\mathcal{Y}\)</span> telle que <span class="math notranslate nohighlight">\(R(h)=\mathbb{E}\big[r(h(X), Y)\big]\)</span> est petit. On veut un bon modèle sur de nouvelles données. L’idée va être de collecter des données représentatives (dans le sens iid) et de construire notre modèle avec ces dernières. Notons :</p>
<div class="math notranslate nohighlight">
\[S_n=\{(X_i, Y_i)\}_{i\neq n}\]</div>
<p>un jeu de données de taille <span class="math notranslate nohighlight">\(n\)</span>.</p>
</div>
<div class="section" id="b-l-apprentissage-non-supervise">
<h3>B. L’apprentissage non-supervisé<a class="headerlink" href="#b-l-apprentissage-non-supervise" title="Permalink to this headline">¶</a></h3>
<p>Ici, c’est l’inverse. Nous avons accès à l’espace des observations <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> duquel on peut collecter des données (toujours selon la loi de la variable aléatoire <span class="math notranslate nohighlight">\(X\)</span>). On sait qu’il existe un espace <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> cible mais (1) il n’est pas nécessairement connu et (2) nous ne connaissons pas d’exemple de liens entre exemples d’apprentissage et cibles associées.</p>
<p>Par exemple, si <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> représente des données clients, on peut savoir (se douter) qu’il existe des groupes de clients qui se ressemblent mais ne pas les connaître et ne pas savoir combien il y en a. Il s’agit ici d’une tâche de <em>clustering</em> où on cherche à regroupe des données entre-elles toujours de manière à ce que le regroupement généralise à de nouvelles données.</p>
<p>On peut chercher à transformer nos données dans <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> dans un espace qu’on notera cette fois-ci <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span> où ces dernières auront de meilleures propriété. On note cet “espace de représentation” <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span> et non <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> car il s’agit souvent d’une étape intermédiaire avant une tâche supervisé où on chercherait à prédire un label dans <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>. C’est ce qu’on appelle l’apprentissage de représentation. Ainsi, si <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> est l’ensemble des photos de chiens et de chats, <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span> est l’ensemble de ces dernières où on a mis “d’un côté” les photos de chiens et de “l’autres” celles de chats. Il devient simple de construire une tâche supervisée permettant de prédire le bon label “chien/chat”.</p>
</div>
</div>
<div class="section" id="ii-on-casse-une-idee-preconcue-as">
<h2>II. On casse une idée préconçue (AS)<a class="headerlink" href="#ii-on-casse-une-idee-preconcue-as" title="Permalink to this headline">¶</a></h2>
<p>Soit <span class="math notranslate nohighlight">\(S=\{(x_i, y_i)\}_{i\leq n}\)</span> un jeu de données représentatif de taille <span class="math notranslate nohighlight">\(n\)</span>. Un modèle très performant sur ces données est-il performant sur des données nouvelles ?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">metrics</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<p>Chargeons et affichons notre jeu de données. Ce dernier consiste en des chiffres écrits à la main. L’objectif va être de faire un modèle qui permet de prédire ces derniers.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">digits</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_digits</span><span class="p">()</span>

<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">,</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray_r</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Training: </span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">label</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_introduction_ml_9_0.png" src="../_images/1_introduction_ml_9_0.png" />
</div>
</div>
<p>Construisons notre premier modèle.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Memorize</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
        
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">memorized</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
                <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">==</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                    <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
                    <span class="n">memorized</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="k">break</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">memorized</span><span class="p">:</span>
                <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># flatten the images</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Split data into 50% train and 50% test subsets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Les données de test nous permettront de tester notre modèle sur des données qu’il n’a pas utilisé pour se construire.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Memorize</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>On commence par tester les performances de notre modèle sur notre jeu d’apprentissage.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Classification report for classifier Memorize on train:</span><span class="se">\n</span><span class="s1">&#39;</span>
      <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Classification report for classifier Memorize on train:
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        90
           1       1.00      1.00      1.00        91
           2       1.00      1.00      1.00        91
           3       1.00      1.00      1.00        92
           4       1.00      1.00      1.00        89
           5       1.00      1.00      1.00        91
           6       1.00      1.00      1.00        90
           7       1.00      1.00      1.00        90
           8       1.00      1.00      1.00        86
           9       1.00      1.00      1.00        88

    accuracy                           1.00       898
   macro avg       1.00      1.00      1.00       898
weighted avg       1.00      1.00      1.00       898
</pre></div>
</div>
</div>
</div>
<p>Notre modèle est parfait ! Aucune erreur. On ne peut pas faire mieux ! Et du côté du test ?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Classification report for classifier Memorize on test:</span><span class="se">\n</span><span class="s1">&#39;</span>
      <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Classification report for classifier Memorize on test:
              precision    recall  f1-score   support

           0       0.10      0.09      0.10        88
           1       0.09      0.11      0.10        91
           2       0.04      0.05      0.04        86
           3       0.11      0.11      0.11        91
           4       0.09      0.08      0.08        92
           5       0.12      0.12      0.12        91
           6       0.04      0.04      0.04        91
           7       0.09      0.09      0.09        89
           8       0.15      0.16      0.15        88
           9       0.18      0.15      0.16        92

    accuracy                           0.10       899
   macro avg       0.10      0.10      0.10       899
weighted avg       0.10      0.10      0.10       899
</pre></div>
</div>
</div>
</div>
<p>C’est ridiculement mauvais : on se trompe une fois sur dix, soit exactement ce qu’on attendrait d’une réponse aléatoire.</p>
<p>Oui mais on a fait exprès de construire le modèle de cette manière ! En réalité, il existe une infinité fonction, paramétriques ou non, qu’on peut rendre aussi bonne qu’on veut sur nos données mais qui seraient particulièrement mauvaises sur de nouvelles données (cela inclut les modèles usuels et c’est pour cela qu’on a besoin d’experts !)… Toute la difficulté du <em>machine learner</em> va être de contrôler cela.</p>
</div>
<div class="section" id="iii-une-autre-premiere-approche-logique-le-knn-as">
<h2>III. Une autre première approche logique : le KNN (AS)<a class="headerlink" href="#iii-une-autre-premiere-approche-logique-le-knn-as" title="Permalink to this headline">¶</a></h2>
<p>Intuitivement, on a envie de dire que nos données ne sont pas complètement déstructurées. Deux clients très similaires achèteront très probablement des produits très similaires. Un trois ressemble plus à un trois qu’à un cinq et un cinq ressemble plus à un cinq qu’à un trois. Finalement, on généralise un petit peu l’exemple précédent. Au lieu de répondre aléatoirement si je ne connais pas la donnée, je cherche l’exemple le plus proche et je prédis le même label ! Plus rigoureusement, notre modèle de prédiction fonctionne comme suit :</p>
<div class="math notranslate nohighlight">
\[\hat{y}_\text{new}=y\text{ avec }(x, y)=\text{argmin}_{(x, y)\in S}\lVert x-x_{\text{new}}\rVert_2.\]</div>
<p>On peut imaginer que si plusieurs points sont équidistants, la réponse se fait aléatoirement entre les labels possibles.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Utilisez l’objet <span class="math notranslate nohighlight">\(\texttt{KNeighborsClassifier}\)</span> avec le paramètre <span class="math notranslate nohighlight">\(\texttt{n}\_\texttt{neighbors=1}\)</span> et entraînez le sur <span class="math notranslate nohighlight">\(\texttt{X}\_\texttt{train}\)</span>.</strong></p>
<hr class="docutils" />
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="c1">###############################################################</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>KNeighborsClassifier(n_neighbors=1)
</pre></div>
</div>
</div>
</div>
<p>On teste maintenant sur le jeu de train pour avoir une idée des performances de notre modèle sur ce dernier.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Classification report for classifier </span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s1"> on train:</span><span class="se">\n</span><span class="s1">&#39;</span>
      <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Classification report for classifier KNeighborsClassifier(n_neighbors=1) on train:
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        90
           1       1.00      1.00      1.00        91
           2       1.00      1.00      1.00        91
           3       1.00      1.00      1.00        92
           4       1.00      1.00      1.00        89
           5       1.00      1.00      1.00        91
           6       1.00      1.00      1.00        90
           7       1.00      1.00      1.00        90
           8       1.00      1.00      1.00        86
           9       1.00      1.00      1.00        88

    accuracy                           1.00       898
   macro avg       1.00      1.00      1.00       898
weighted avg       1.00      1.00      1.00       898
</pre></div>
</div>
</div>
</div>
<p>On est toujours aussi bon sur le jeu d’apprentissage ! Cependant, c’est attendu car l’image qui ressemble le plus à une autre est l’image elle-même. Prédisons maintenant sur le test.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Classification report for classifier </span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s1"> on test:</span><span class="se">\n</span><span class="s1">&#39;</span>
      <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Classification report for classifier KNeighborsClassifier(n_neighbors=1) on test:
              precision    recall  f1-score   support

           0       0.99      0.99      0.99        88
           1       0.96      0.97      0.96        91
           2       0.99      0.97      0.98        86
           3       0.91      0.92      0.92        91
           4       0.99      0.95      0.97        92
           5       0.96      0.98      0.97        91
           6       0.99      1.00      0.99        91
           7       0.99      0.99      0.99        89
           8       0.94      0.92      0.93        88
           9       0.91      0.93      0.92        92

    accuracy                           0.96       899
   macro avg       0.96      0.96      0.96       899
weighted avg       0.96      0.96      0.96       899
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">figure_</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_introduction_ml_35_0.png" src="../_images/1_introduction_ml_35_0.png" />
</div>
</div>
<p>Is Machine learning solved ? Minute papillon ! Ce modèle est très sensible au bruit ! Supposons qu’une de nos données soient bruitées (e.g. un 3 qui ressemble à un 8). Si une nouvelle donnée représentant un <span class="math notranslate nohighlight">\(8\)</span> se retrouve à côté de cette anomalie, elle sera mal prédite. Nous pouvons adresser cette limite de la manière suivante : au lieu de regarder le point le plus proche, on regarde les <span class="math notranslate nohighlight">\(k\)</span> points les plus proches et on fait un vote à la majorité. Plus formellement la prédiction est faite comme suit :</p>
<div class="math notranslate nohighlight">
\[\hat{y}_\text{new}=\text{majority$\_$voting}(\texttt{KNN}.\texttt{labels})\text{ où }\texttt{KNN}=\text{argmin}_{S^\prime\subset S,\ |S^\prime|=K}\sum_i \lVert x_i- x_{\text{new}}\rVert.\]</div>
<p>Dans le cas où on chercherait à faire une régression, on remplace le vote à la majorité par une moyenne.</p>
<p>Récupérons le jeu de données <span class="math notranslate nohighlight">\(\texttt{iris}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="c1"># décommentez la ligne suivante pour obtenir des informations</span>
<span class="c1"># sur le dataset iris.</span>
<span class="c1"># print(iris.DESCR)</span>
</pre></div>
</div>
</div>
</div>
<p>De la même manière que précédemment, on construit notre jeu d’apprentissage pour construire notre modèle et notre jeu de test pour en tester les performances.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Utilisez l’objet <span class="math notranslate nohighlight">\(\texttt{KNeighborsClassifier}\)</span> avec le paramètre <span class="math notranslate nohighlight">\(\texttt{n}\_\texttt{neighbors=1}\)</span> et entraînez le sur <span class="math notranslate nohighlight">\(\texttt{X}\_\texttt{train}\)</span>. Faites une prédiction sur <span class="math notranslate nohighlight">\(\texttt{predicted=X}\_\texttt{test}\)</span>.</strong></p>
<hr class="docutils" />
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="c1">###############################################################</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">figure_</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_introduction_ml_42_0.png" src="../_images/1_introduction_ml_42_0.png" />
</div>
</div>
<p>Les performances sont déjà très bonnes ! Mais il est possible de gagner un tout petit peu de performances en considérant plus de voisins :</p>
<hr class="docutils" />
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Utilisez l’objet <span class="math notranslate nohighlight">\(\texttt{KNeighborsClassifier}\)</span> avec le paramètre <span class="math notranslate nohighlight">\(\texttt{n}\_\texttt{neighbors=10}\)</span> et entraînez le sur <span class="math notranslate nohighlight">\(\texttt{X}\_\texttt{train}\)</span>. Faites une prédiction sur <span class="math notranslate nohighlight">\(\texttt{predicted=X}\_\texttt{test}\)</span>.</strong></p>
<hr class="docutils" />
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="c1">###############################################################</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">figure_</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_introduction_ml_46_0.png" src="../_images/1_introduction_ml_46_0.png" />
</div>
</div>
</div>
<div class="section" id="iv-les-arbres-de-decision-ou-classification-and-regression-tree-cart-as">
<h2>IV. Les arbres de décision ou Classification and regression Tree (CART) (AS)<a class="headerlink" href="#iv-les-arbres-de-decision-ou-classification-and-regression-tree-cart-as" title="Permalink to this headline">¶</a></h2>
<p>De la même manière que pour l’algorithme KNN, on supposera ici que nos données admettent une certaine structure et que des points proches possèdent probablement le même label ou une prediction proche dans le cas de la régression. Ici, à la différence du KNN, la notion de voisinage se construit au travers d’hyperrectangles parallèles aux axes. Afin de bien comprendre le fonctionnement, supposons que notre arbre de décision soit déjà construit. Prenons une nouvelle données <span class="math notranslate nohighlight">\(x_{\text{new}}=[3, 5]^T\)</span> et partons de la racine de notre arbre. Cette racine possède deux branches sortantes. Le choix de la branche se fait à partir d’un seuil (paramètre associé au noeud racine) et d’une coordonnée de notre <span class="math notranslate nohighlight">\(x\)</span>. Dans notre exemple imaginons que le seuil soit <span class="math notranslate nohighlight">\(7\)</span> et que la coordonnée à regarder soit <span class="math notranslate nohighlight">\(x_{\text{new}}^{(1)}=5\)</span>. C’est clairement inférieur à <span class="math notranslate nohighlight">\(7\)</span> et nous partons donc la branche de gauche de notre arbre. Nous somme sur un nouveau noeud associé à un nouveau seuil et une nouvelle coordonnée de notre donnée. On répète cette opération jusqu’à ce qu’on arrive aux feuilles de notre arbre. Chaque feuille regroupe les données du jeu d’apprentissage qui y aboutisse lorsqu’on les classe. Le choix du label se fait par un vote à la majorité sur ces données ou en moyennant. L’image suivante illustre cette idée :</p>
<p><img alt="Decision tree" src="https://miro.medium.com/max/360/1*XMId5sJqPtm8-RIwVVz2tg.png" /></p>
<p>Le choix de la règle de décision à chaque noeud peut être adapter afin d’obtenir des régions à la géométrie variable. La construction d’un arbre se fait en partant de la racine vers les feuilles et en choisissant intérativement les variables explicatives qui ont le plus d’effet sur notre prédiction (via diverses critères).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Utilisez l’objet <span class="math notranslate nohighlight">\(\texttt{DecisionTreeClassifier}\)</span> avec le paramètre <span class="math notranslate nohighlight">\(\texttt{n}\_\texttt{max}\_\texttt{depth=10}\)</span> et entraînez le sur <span class="math notranslate nohighlight">\(\texttt{X}\_\texttt{train}\)</span>. Faites une prédiction sur <span class="math notranslate nohighlight">\(\texttt{predicted=X}\_\texttt{test}\)</span>.</strong></p>
<hr class="docutils" />
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="c1">###############################################################</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">figure_</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_introduction_ml_52_0.png" src="../_images/1_introduction_ml_52_0.png" />
</div>
</div>
</div>
<div class="section" id="v-les-forets-aleatoires-ou-random-forest-rf-as">
<h2>V. Les forêts aléatoires ou Random Forest (RF) (AS)<a class="headerlink" href="#v-les-forets-aleatoires-ou-random-forest-rf-as" title="Permalink to this headline">¶</a></h2>
<p>Les arbres de décision peuvent être sujets au surapprentissage. Une manière de compenser le problème est d’en construire plusieurs où chaque arbre est construit en ne voyant qu’une partie des données. Enfin leurs prédictions sont aggrégées. Ces approches sont généralement beaucoup plus performantes que les arbres simples. Malheureusement, autant avec un arbre simple on pouvait essayer de comprendre la prédiction, autant ici, cela devient difficile.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Utilisez l’objet <span class="math notranslate nohighlight">\(\texttt{RandomForestClassifier}\)</span> et entraînez le sur <span class="math notranslate nohighlight">\(\texttt{X}\_\texttt{train}\)</span>. Faites une prédiction sur <span class="math notranslate nohighlight">\(\texttt{predicted=X}\_\texttt{test}\)</span>.</strong></p>
<hr class="docutils" />
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="c1">###############################################################</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">figure_</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_introduction_ml_58_0.png" src="../_images/1_introduction_ml_58_0.png" />
</div>
</div>
</div>
<div class="section" id="vi-choix-des-hyperparametres-as">
<h2>VI. Choix des hyperparamètres (AS)<a class="headerlink" href="#vi-choix-des-hyperparametres-as" title="Permalink to this headline">¶</a></h2>
<p>Nous avons du, pour les modèles précédents, choisir différents paramètres qui affectaient les performances de notre modèle. Nous les avons choisi en regardant les performances de notre modèle sur le jeu de test. Cependant, les bonnes performances étaient peut-être un coup de chance !</p>
<p>Il existe deux stratégies d’évaluation sans biais de la qualité de notre modèle :</p>
<ul class="simple">
<li><p>La validation non croisée où une partie de notre jeu de donnée est cachée pendant l’apprentissage puis utilisée afin d’évaluer les performances du modèle. Il s’agit du découpage train/test. Cette stratégie est un estimateur sans biais de la qualité de notre modèle mais possède une variance plus forte que la validation croisée. Elle peut-être particulièrement utile lorsque le coup d’apprentissage d’un modèle est très élevé (e.g. <em>deep learning</em>)</p></li>
<li><p>La validation croisée où notre jeu de données est divisé en <em>k</em> parties (on parle aussi de <em>k-fold</em>). Évidemment, <span class="math notranslate nohighlight">\(k\in\{2, ..., n\}\)</span> où <span class="math notranslate nohighlight">\(n\)</span> est la taille du jeu de données. Chacune des parties jouera successivement le rôle de jeu de test pendant que les <span class="math notranslate nohighlight">\(k-1\)</span> autres parties serviront à calculer notre modèle. Le résultat de cette procédure est un vecteur de <span class="math notranslate nohighlight">\(k\)</span> scores dont on peut calculer la moyenne, la variance, etc.</p></li>
</ul>
<p>On peut illustrer la méthode des <em>k-folds</em> via l’exemple suivant :</p>
<p>\begin{align}
\text{Appartient au train set: } \color{red}{\boxed{}}&amp;\text{ et appartient au test set: }\color{green}{\boxed{}}
\end{align}
\begin{align}
\text{Step 1: }\color{green}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align}
\begin{align}
\text{Step 2: }\color{red}{\boxed{}}\color{green}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align}
\begin{align}
\text{Step 3: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{green}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align}
\begin{align}
\text{Step 4: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{green}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align}
\begin{align}
\text{Step 5: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{green}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}
\end{align}
\begin{align}
\text{Step 6: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{green}{\boxed{}}\color{red}{\boxed{}}
\end{align}
\begin{align}
\text{Step 7: }\color{red}{\boxed{}}\color{red}{\boxed{}}&amp;\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{red}{\boxed{}}\color{green}{\boxed{}}
\end{align}</p>
<p>La méthode <span class="math notranslate nohighlight">\(\texttt{cross_val_score}\)</span> de <span class="math notranslate nohighlight">\(\texttt{sklearn}\)</span> permet de réaliser cette procédure. On pourra renseigner le paramètre <span class="math notranslate nohighlight">\(\texttt{cv}\)</span> qui indique le nombre <span class="math notranslate nohighlight">\(k\)</span> et le paramètre <span class="math notranslate nohighlight">\(\texttt{scoring}\)</span> qui donne la métrique que l’on souhaite calculer.</p>
<p>Si on cherche à trouver une valeur d’un paramètre, l’objet <span class="math notranslate nohighlight">\(\texttt{GridSearchCV}\)</span> applique une validation croisée en cherchant différentes valeurs d’un paramètres.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Utilisez l’objet <span class="math notranslate nohighlight">\(\texttt{GridSearchCV}\)</span> pour faire une recherche par grille sur le modèle <span class="math notranslate nohighlight">\(\texttt{RandomForestClassifier}\)</span> et entraînez le sur <span class="math notranslate nohighlight">\(\texttt{X}\_\texttt{train}\)</span>.</strong></p>
<hr class="docutils" />
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">####### Complete this part ######## or die ####################</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">RandomForestClassifier</span><span class="p">(),</span> <span class="n">params</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="c1">###############################################################</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GridSearchCV(estimator=RandomForestClassifier(),
             param_grid={&#39;max_depth&#39;: [2, 3, 4, 5, 10, None]})
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">figure_</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_introduction_ml_65_0.png" src="../_images/1_introduction_ml_65_0.png" />
</div>
</div>
<p>Notre RandomForest de base était déjà très bon !</p>
</div>
<div class="section" id="vii-l-algorithme-des-k-moyennes-ans">
<h2>VII. L’algorithme des K-Moyennes (ANS)<a class="headerlink" href="#vii-l-algorithme-des-k-moyennes-ans" title="Permalink to this headline">¶</a></h2>
<p>Il s’agit ici d’un algorithme non supervisé. Imaginons que nous ayons un collecté un jeu de données <span class="math notranslate nohighlight">\(S_n=\{(X_i)\}_{i\leq n}\)</span>. On sait qu’il existe des groupes dans nos données. Supposons même qu’on sâche qu’il existe <span class="math notranslate nohighlight">\(K\)</span> groupes. L’idée de l’algorithme des K-Moyennes va être de détecter ces <span class="math notranslate nohighlight">\(K\)</span> groupes en trouvant une solution au problème d’optimisation suivant :</p>
<div class="math notranslate nohighlight">
\[\text{KMeans}=\text{argmin}_{m_1, \ldots, m_K\in\mathcal{X}, c_1,\ldots,c_n\in\{1,\ldots,K\}}\sum_{i=1}^K\sum_{j=1}^n\textbf{1}\{c_1=i\}\lVert m_i-x_j\rVert_2=\text{argmin}_{m_1, \ldots, m_K\in\mathcal{X}, c_1,\ldots,c_n\in\{1,\ldots,K\}}\sum_{j=1}^n\lVert x_j-m_{c_j}\rVert_2.\]</div>
<p>Dit autrement, chaque groupe est représentée par une coordonnée <span class="math notranslate nohighlight">\(c_i\)</span> (qui s’avère être la moyenne des éléments du groupe) et chaque élément de notre jeu de données n’est associé qu’à un seul groupe. L’objectif va être que leur distance quadratique au centre de leur groupe doit être minimale !</p>
<p>Considérons le jeu de données suivant.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">sample_data</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">]])</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="mi">15</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">means</span>
    <span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="o">//</span><span class="n">n</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="o">*</span><span class="n">n</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
    
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_data</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="k">def</span> <span class="nf">plot_clusters</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">means</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Nos données et leur label inconnu&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">means</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">means</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="p">[</span><span class="n">m</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="n">s</span><span class="o">=</span><span class="mi">55</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">path</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">p</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plot_clusters</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_introduction_ml_70_0.png" src="../_images/1_introduction_ml_70_0.png" />
</div>
</div>
<p>Le problème de K-Means est NP-Difficile. Pour cela, nous utilisons en pratique un heuristique appelé “algorithme de LLoyd” qui fonctionnde la manière suivante :</p>
<ol class="simple">
<li><p>On initialise les k moyennes</p></li>
<li><p>On assigne tous nos points à leur moyenne la plus proche</p></li>
<li><p>On met à jour les moyennes avec les nouveaux points</p></li>
<li><p>Si le déplacement des moyennes est significatif, on reprend à l’étape 2</p></li>
</ol>
<hr class="docutils" />
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Le problème est NP-Difficile et un algorithme permettant de le résoudre est l’algorithme de LLoyd. Implémentez le.</strong></p>
<hr class="docutils" />
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">KMeans</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.001</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1">####### Complete this part ######## or die ####################</span>
        <span class="c1"># K means initialization</span>
        <span class="n">X_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">X_</span><span class="p">)</span>
        <span class="n">means</span> <span class="o">=</span> <span class="n">X_</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">]</span>
        
        <span class="n">variation</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">)</span>
        
        <span class="n">assignment</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))]</span>
        
        <span class="n">path</span> <span class="o">=</span> <span class="p">[</span><span class="n">means</span><span class="o">.</span><span class="n">copy</span><span class="p">()]</span>
        
        <span class="k">while</span> <span class="n">variation</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span>
            <span class="c1"># Step 1</span>

            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)):</span>
                <span class="n">closest</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">closest_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">means</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

                <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">):</span>
                    <span class="n">dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">means</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
                    <span class="k">if</span> <span class="n">dist</span>  <span class="o">&lt;</span> <span class="n">closest_dist</span><span class="p">:</span>
                        <span class="n">closest_dist</span> <span class="o">=</span> <span class="n">dist</span>
                        <span class="n">closest</span> <span class="o">=</span> <span class="n">k</span>

                <span class="n">assignment</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">closest</span>

            <span class="c1"># Step 2</span>
            <span class="n">kmeans</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">k</span> <span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">)</span>
            <span class="p">}</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)):</span>
                <span class="n">kmeans</span><span class="p">[</span><span class="n">assignment</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

            <span class="n">variation</span> <span class="o">=</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">):</span>

                <span class="n">new_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">kmeans</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>


                <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">new_mean</span><span class="o">-</span><span class="n">means</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="o">&gt;</span> <span class="n">variation</span><span class="p">:</span>
                    <span class="n">variation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">new_mean</span><span class="o">-</span><span class="n">means</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
                <span class="n">means</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_mean</span>
            <span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">means</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
        <span class="n">path</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">path</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="c1">###############################################################</span>
        <span class="k">return</span> <span class="n">assignment</span><span class="p">,</span> <span class="n">means</span><span class="p">,</span> <span class="n">path</span>
            

<span class="n">model</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plot_clusters</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="o">*</span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_introduction_ml_73_0.png" src="../_images/1_introduction_ml_73_0.png" />
</div>
</div>
<p>On se rend compte qu’on arrive à retrouver les 3 groupes automatiquement (Les couleurs sont celles calculées par notre modèle des K-Moyennes) !</p>
</div>
<div class="section" id="viii-la-malediction-de-la-dimension">
<h2>VIII. La malédiction de la dimension<a class="headerlink" href="#viii-la-malediction-de-la-dimension" title="Permalink to this headline">¶</a></h2>
<p>Nous avons pu observer des scénarios où l’erreur sur notre jeu de données d’apprentissage était <span class="math notranslate nohighlight">\(0\)</span> alors que notre modèle n’était pas si bon que cela sur notre jeu de test. Cet écart peut même devenir catastrophique ! De manière plus rigoureuse, le gap de généralisation de notre estimateur <span class="math notranslate nohighlight">\(\hat{h}\)</span> est la quantité suivante :</p>
<p>\begin{equation}
\text{gap}(\hat{h})=|Re(\hat{h})-R(\hat{h})|.
\end{equation}</p>
<p>Où <span class="math notranslate nohighlight">\(Re\)</span> fait référence à notre risque empirique, c’est-à-dire l’erreur sur le jeu d’apprentissage et <span class="math notranslate nohighlight">\(R\)</span> à l’erreur en espérance.</p>
<p>Il est possible d’avoir une idée de <span class="math notranslate nohighlight">\(R(\hat{h})\)</span> en passant par un jeu de test ou par une autre stratégie d’évaluation via un jeu de test par exemple, comme nous avons pu le voir.</p>
<p>Deux facteurs principaux sont admis comme entrant en jeu dans <span class="math notranslate nohighlight">\(\text{gap}(\hat{h})\)</span> :</p>
<ul class="simple">
<li><p>la taille de l’ensemble <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> qui est généralement liée au nombre de paramètres de notre modèle,</p></li>
<li><p>la taille du jeu de données <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>.</p></li>
</ul>
<p>Plus <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> est grand, plus on s’attend à voir l’erreur augmenter. L’effet de double descente montre qu’avec un choix réfléchi de paramétrisation, cette tendance n’est pas nécessairement monotone. De la même manière, augmenter la taille du jeu de données <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> permet de réduire l’erreur de généralisation.</p>
<p>La taille de <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> est intrinsèquement liée au nombre de paramètres qui lui-même dépend très souvent de la dimension <span class="math notranslate nohighlight">\(d\)</span> de nos données.</p>
<p>Nous allons ici nous rendre compte que les modèles qui regardent le voisinage de nos données souffre d’une grosse limite liée à ce qu’on appelle <em>la malédiction de la dimension</em>.</p>
<div class="section" id="en-details">
<h3>En détails<a class="headerlink" href="#en-details" title="Permalink to this headline">¶</a></h3>
<p>La malédiction de la dimension fait référence aux résultats contre-intuitifs qui apparaissent lorsque la dimension augmente. Une première manière de l’observer est possible grâce au KNN. Ce dernier classe un nouvel élément en fonction de ses voisins dans le jeu d’apprentissage. Nous allons en particulier étudier l’évolution du risque de généralisation en fonction de la dimension. Plus précisément, les données synthétiques sont construites de la manière suivante :</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="k">def</span> <span class="nf">sample_data</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">))</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">y</span><span class="o">*</span><span class="n">X</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">X</span> <span class="c1"># positive have mean mu and negative, -mu</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="o">-</span><span class="n">k</span><span class="p">))</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">noise</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<p>Dit autrement, <span class="math notranslate nohighlight">\(k\)</span> dimensions contiennent le signal intéressant pour notre tâche et <span class="math notranslate nohighlight">\(d\)</span> dimensions ne servent à rien. Nous observons ci-dessous ce qui se passe lorsqu’on rajouter des dimensions de bruits (i.e. qui ne servent à rien). C’est typiquement ce pourrait se passer avec des images. Une photo de chien ne contient pas que des pixels descriptifs du concept de chien.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>

<span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">redo</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">max_dim</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">first_dim</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">steps</span> <span class="o">=</span> <span class="mi">100</span>

<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">first_dim</span><span class="p">,</span> <span class="n">max_dim</span><span class="p">,</span> <span class="n">steps</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">redo</span><span class="p">):</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_data</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="n">d</span><span class="p">)</span>
        <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">sample_data</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="n">d</span><span class="p">)</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>
        <span class="n">c</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],)))</span>
        <span class="n">s</span> <span class="o">+=</span> <span class="n">c</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],)))</span><span class="o">/</span><span class="n">redo</span>
    <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># configuration generale de matplotlib</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">12.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">first_dim</span><span class="p">,</span> <span class="n">max_dim</span><span class="p">,</span> <span class="n">steps</span><span class="p">)),</span> <span class="n">scores</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Evolution de l</span><span class="se">\&#39;</span><span class="s1">erreur en fonction de la dimension du probleme&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_introduction_ml_82_0.png" src="../_images/1_introduction_ml_82_0.png" />
</div>
</div>
<hr class="docutils" />
<p><span style="color:blue"><strong>Exercice 1 :</strong></span> <strong>Quelle est le risque de généralisation pour l’erreur 0/1 (1 si la classe est mal prédite, 0 sinon) d’un classifieur aléatoire ?</strong></p>
<p><span style="color:green"><strong>Réponse :</strong></span></p>
<p><span style="color:blue"><strong>Exercice 2 :</strong></span> <strong>Expliquez pourquoi l’erreur de généralisation diminue lorsqu’on rajoute des dimensions sans signal.</strong></p>
<p><span style="color:green"><strong>Réponse :</strong></span></p>
<hr class="docutils" />
<p>De manière similaire, affichons ci-dessous l’évolution des distances entre nos points en fonction de la dimension du problème.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">sample_data</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sample_data</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">redo</span> <span class="o">=</span> <span class="mi">50</span>
<span class="k">def</span> <span class="nf">experiment_</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
    <span class="n">min_</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">max_</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">mean_</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">redo</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">sample_data</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
        <span class="n">vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">X</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">min_</span> <span class="o">+=</span> <span class="n">vec</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">/</span><span class="n">redo</span>
        <span class="n">max_</span> <span class="o">+=</span> <span class="n">vec</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">/</span><span class="n">redo</span>
        <span class="n">mean_</span> <span class="o">+=</span> <span class="n">vec</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">/</span><span class="n">redo</span>
    <span class="k">return</span> <span class="n">min_</span><span class="p">,</span> <span class="n">max_</span><span class="p">,</span> <span class="n">mean_</span>
<span class="n">idx</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">val</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
    <span class="n">idx</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">d</span><span class="p">])</span>
    <span class="n">val</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">experiment_</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>
<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">):</span>
    <span class="n">idx</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">d</span><span class="p">])</span>
    <span class="n">val</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">experiment_</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>
<span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">idx</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">val</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">arr</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">arr</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Min&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">arr</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">arr</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Max&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">arr</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">arr</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Moy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Evolution des distances en fonction de la dimension du probleme&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1_introduction_ml_87_0.png" src="../_images/1_introduction_ml_87_0.png" />
</div>
</div>
<hr class="docutils" />
<p><span style="color:blue"><strong>Exercice :</strong></span> <strong>Quelle phénomène mathématique pouvons nous invoquer afin d’expliquer ce phénomène ?</strong></p>
<p><span style="color:green"><strong>Réponse :</strong></span></p>
<hr class="docutils" />
<p>Soit <span class="math notranslate nohighlight">\(x_\text{new}\)</span> une nouvelle donnée. Une petite perturbation du point de notre jeu d’apprentissage le plus différent de <span class="math notranslate nohighlight">\(x_\text{new}\)</span> peut le transformer en le point le plus proche est inversement… C’est une grosse limite des modèles précédents. Il faut soit réfléchir à réduire la dimension, soit injecter de la connaissance dans nos modèles, etc.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./1_what_is_ml"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="0_propos_liminaire.html" title="previous page">Le <em>Machine Learning</em>, c’est quoi ?</a>
    <a class='right-next' id="next-link" href="2_regression_and_classification_trees.html" title="next page">Les arbres de régression et de classification</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By The LMIPR team<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>